<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>GitHub C++ Daily Trending</title>
    <description>Daily Trending of C++ in GitHub</description>
    <pubDate>Sun, 16 Mar 2025 01:29:37 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>ethereum/solidity</title>
      <link>https://github.com/ethereum/solidity</link>
      <description>&lt;p&gt;Solidity, the Smart Contract Programming Language&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;The Solidity Contract-Oriented Programming Language&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://matrix.to/#/%23ethereum_solidity:gitter.im&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Matrix%20-chat-brightgreen?style=plastic&amp;amp;logo=matrix&quot; alt=&quot;Matrix Chat&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://gitter.im/ethereum/solidity&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Gitter%20-chat-brightgreen?style=plastic&amp;amp;logo=gitter&quot; alt=&quot;Gitter Chat&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://forum.soliditylang.org/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Solidity_Forum%20-discuss-brightgreen?style=plastic&amp;amp;logo=discourse&quot; alt=&quot;Solidity&amp;nbsp;Forum&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://X.com/solidity_lang&quot;&gt;&lt;img src=&quot;https://img.shields.io/twitter/follow/solidity_lang?style=plastic&amp;amp;logo=x&quot; alt=&quot;X Follow&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://fosstodon.org/@solidity&quot;&gt;&lt;img src=&quot;https://img.shields.io/mastodon/follow/000335908?domain=https%3A%2F%2Ffosstodon.org%2F&amp;amp;logo=mastodon&amp;amp;style=plastic&quot; alt=&quot;Mastodon Follow&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;You can talk to us on Gitter and Matrix, tweet at us on X (previously Twitter) or create a new topic in the Solidity forum. Questions, feedback, and suggestions are welcome!&lt;/p&gt; 
&lt;p&gt;Solidity is a statically-typed, contract-oriented, high-level language for implementing smart contracts on the Ethereum platform.&lt;/p&gt; 
&lt;p&gt;For a good overview and starting point, please check out the official &lt;a href=&quot;https://soliditylang.org&quot;&gt;Solidity Language Portal&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ethereum/solidity/develop/#background&quot;&gt;Background&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ethereum/solidity/develop/#build-and-install&quot;&gt;Build and Install&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ethereum/solidity/develop/#example&quot;&gt;Example&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ethereum/solidity/develop/#documentation&quot;&gt;Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ethereum/solidity/develop/#development&quot;&gt;Development&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ethereum/solidity/develop/#maintainers&quot;&gt;Maintainers&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ethereum/solidity/develop/#license&quot;&gt;License&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ethereum/solidity/develop/#security&quot;&gt;Security&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Background&lt;/h2&gt; 
&lt;p&gt;Solidity is a statically-typed curly-braces programming language designed for developing smart contracts that run on the Ethereum Virtual Machine. Smart contracts are programs that are executed inside a peer-to-peer network where nobody has special authority over the execution, and thus they allow anyone to implement tokens of value, ownership, voting, and other kinds of logic.&lt;/p&gt; 
&lt;p&gt;When deploying contracts, you should use the latest released version of Solidity. This is because breaking changes, as well as new features and bug fixes, are introduced regularly. We currently use a 0.x version number &lt;a href=&quot;https://semver.org/#spec-item-4&quot;&gt;to indicate this fast pace of change&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Build and Install&lt;/h2&gt; 
&lt;p&gt;Instructions about how to build and install the Solidity compiler can be found in the &lt;a href=&quot;https://docs.soliditylang.org/en/latest/installing-solidity.html#building-from-source&quot;&gt;Solidity documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Example&lt;/h2&gt; 
&lt;p&gt;A &quot;Hello World&quot; program in Solidity is of even less use than in other languages, but still:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-solidity&quot;&gt;// SPDX-License-Identifier: MIT
pragma solidity &amp;gt;=0.6.0 &amp;lt;0.9.0;

contract HelloWorld {
    function helloWorld() external pure returns (string memory) {
        return &quot;Hello, World!&quot;;
    }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To get started with Solidity, you can use &lt;a href=&quot;https://remix.ethereum.org/&quot;&gt;Remix&lt;/a&gt;, which is a browser-based IDE. Here are some example contracts:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.soliditylang.org/en/latest/solidity-by-example.html#voting&quot;&gt;Voting&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.soliditylang.org/en/latest/solidity-by-example.html#blind-auction&quot;&gt;Blind Auction&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.soliditylang.org/en/latest/solidity-by-example.html#safe-remote-purchase&quot;&gt;Safe remote purchase&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.soliditylang.org/en/latest/solidity-by-example.html#micropayment-channel&quot;&gt;Micropayment Channel&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;The Solidity documentation is hosted using &lt;a href=&quot;https://docs.soliditylang.org&quot;&gt;Read the Docs&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Development&lt;/h2&gt; 
&lt;p&gt;Solidity is still under development. Contributions are always welcome! Please follow the &lt;a href=&quot;https://docs.soliditylang.org/en/latest/contributing.html&quot;&gt;Developer&#39;s Guide&lt;/a&gt; if you want to help.&lt;/p&gt; 
&lt;p&gt;You can find our current feature and bug priorities for forthcoming releases in the &lt;a href=&quot;https://github.com/ethereum/solidity/projects&quot;&gt;projects section&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Maintainers&lt;/h2&gt; 
&lt;p&gt;The Solidity programming language and compiler are open-source community projects governed by a core team. The core team is sponsored by the &lt;a href=&quot;https://ethereum.foundation/&quot;&gt;Ethereum Foundation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Solidity is licensed under &lt;a href=&quot;https://raw.githubusercontent.com/ethereum/solidity/develop/LICENSE.txt&quot;&gt;GNU General Public License v3.0&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Some third-party code has its &lt;a href=&quot;https://raw.githubusercontent.com/ethereum/solidity/develop/cmake/templates/license.h.in&quot;&gt;own licensing terms&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Security&lt;/h2&gt; 
&lt;p&gt;The security policy may be &lt;a href=&quot;https://raw.githubusercontent.com/ethereum/solidity/develop/SECURITY.md&quot;&gt;found here&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>gabime/spdlog</title>
      <link>https://github.com/gabime/spdlog</link>
      <description>&lt;p&gt;Fast C++ logging library.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;spdlog&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/gabime/spdlog/actions/workflows/linux.yml&quot;&gt;&lt;img src=&quot;https://github.com/gabime/spdlog/actions/workflows/linux.yml/badge.svg?sanitize=true&quot; alt=&quot;ci&quot;&gt;&lt;/a&gt;&amp;nbsp; &lt;a href=&quot;https://github.com/gabime/spdlog/actions/workflows/windows.yml&quot;&gt;&lt;img src=&quot;https://github.com/gabime/spdlog/actions/workflows/windows.yml/badge.svg?sanitize=true&quot; alt=&quot;ci&quot;&gt;&lt;/a&gt;&amp;nbsp; &lt;a href=&quot;https://github.com/gabime/spdlog/actions/workflows/macos.yml&quot;&gt;&lt;img src=&quot;https://github.com/gabime/spdlog/actions/workflows/macos.yml/badge.svg?sanitize=true&quot; alt=&quot;ci&quot;&gt;&lt;/a&gt;&amp;nbsp; &lt;a href=&quot;https://ci.appveyor.com/project/gabime/spdlog&quot;&gt;&lt;img src=&quot;https://ci.appveyor.com/api/projects/status/d2jnxclg20vd0o50?svg=true&amp;amp;branch=v1.x&quot; alt=&quot;Build status&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/gabime/spdlog/releases/latest&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/release/gabime/spdlog.svg?sanitize=true&quot; alt=&quot;Release&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Fast C++ logging library&lt;/p&gt; 
&lt;h2&gt;Install&lt;/h2&gt; 
&lt;h4&gt;Header-only version&lt;/h4&gt; 
&lt;p&gt;Copy the include &lt;a href=&quot;https://github.com/gabime/spdlog/tree/v1.x/include/spdlog&quot;&gt;folder&lt;/a&gt; to your build tree and use a C++11 compiler.&lt;/p&gt; 
&lt;h4&gt;Compiled version (recommended - much faster compile times)&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-console&quot;&gt;$ git clone https://github.com/gabime/spdlog.git
$ cd spdlog &amp;amp;&amp;amp; mkdir build &amp;amp;&amp;amp; cd build
$ cmake .. &amp;amp;&amp;amp; cmake --build .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;see example &lt;a href=&quot;https://github.com/gabime/spdlog/raw/v1.x/example/CMakeLists.txt&quot;&gt;CMakeLists.txt&lt;/a&gt; on how to use.&lt;/p&gt; 
&lt;h2&gt;Platforms&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Linux, FreeBSD, OpenBSD, Solaris, AIX&lt;/li&gt; 
 &lt;li&gt;Windows (msvc 2013+, cygwin)&lt;/li&gt; 
 &lt;li&gt;macOS (clang 3.5+)&lt;/li&gt; 
 &lt;li&gt;Android&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Package managers:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Debian: &lt;code&gt;sudo apt install libspdlog-dev&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Homebrew: &lt;code&gt;brew install spdlog&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;MacPorts: &lt;code&gt;sudo port install spdlog&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;FreeBSD: &lt;code&gt;pkg install spdlog&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Fedora: &lt;code&gt;dnf install spdlog&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Gentoo: &lt;code&gt;emerge dev-libs/spdlog&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Arch Linux: &lt;code&gt;pacman -S spdlog&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;openSUSE: &lt;code&gt;sudo zypper in spdlog-devel&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;ALT Linux: &lt;code&gt;apt-get install libspdlog-devel&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;vcpkg: &lt;code&gt;vcpkg install spdlog&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;conan: &lt;code&gt;conan install --requires=spdlog/[*]&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;conda: &lt;code&gt;conda install -c conda-forge spdlog&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;build2: &lt;code&gt;depends: spdlog ^1.8.2&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Very fast (see &lt;a href=&quot;https://raw.githubusercontent.com/gabime/spdlog/v1.x/#benchmarks&quot;&gt;benchmarks&lt;/a&gt; below).&lt;/li&gt; 
 &lt;li&gt;Headers only or compiled&lt;/li&gt; 
 &lt;li&gt;Feature-rich formatting, using the excellent &lt;a href=&quot;https://github.com/fmtlib/fmt&quot;&gt;fmt&lt;/a&gt; library.&lt;/li&gt; 
 &lt;li&gt;Asynchronous mode (optional)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/gabime/spdlog/wiki/3.-Custom-formatting&quot;&gt;Custom&lt;/a&gt; formatting.&lt;/li&gt; 
 &lt;li&gt;Multi/Single threaded loggers.&lt;/li&gt; 
 &lt;li&gt;Various log targets: 
  &lt;ul&gt; 
   &lt;li&gt;Rotating log files.&lt;/li&gt; 
   &lt;li&gt;Daily log files.&lt;/li&gt; 
   &lt;li&gt;Console logging (colors supported).&lt;/li&gt; 
   &lt;li&gt;syslog.&lt;/li&gt; 
   &lt;li&gt;Windows event log.&lt;/li&gt; 
   &lt;li&gt;Windows debugger (&lt;code&gt;OutputDebugString(..)&lt;/code&gt;).&lt;/li&gt; 
   &lt;li&gt;Log to Qt widgets (&lt;a href=&quot;https://raw.githubusercontent.com/gabime/spdlog/v1.x/#log-to-qt-with-nice-colors&quot;&gt;example&lt;/a&gt;).&lt;/li&gt; 
   &lt;li&gt;Easily &lt;a href=&quot;https://github.com/gabime/spdlog/wiki/4.-Sinks#implementing-your-own-sink&quot;&gt;extendable&lt;/a&gt; with custom log targets.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Log filtering - log levels can be modified at runtime as well as compile time.&lt;/li&gt; 
 &lt;li&gt;Support for loading log levels from argv or environment var.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/gabime/spdlog/v1.x/#backtrace-support&quot;&gt;Backtrace&lt;/a&gt; support - store debug messages in a ring buffer and display them later on demand.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Usage samples&lt;/h2&gt; 
&lt;h4&gt;Basic usage&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;#include &quot;spdlog/spdlog.h&quot;

int main() 
{
    spdlog::info(&quot;Welcome to spdlog!&quot;);
    spdlog::error(&quot;Some error message with arg: {}&quot;, 1);
    
    spdlog::warn(&quot;Easy padding in numbers like {:08d}&quot;, 12);
    spdlog::critical(&quot;Support for int: {0:d};  hex: {0:x};  oct: {0:o}; bin: {0:b}&quot;, 42);
    spdlog::info(&quot;Support for floats {:03.2f}&quot;, 1.23456);
    spdlog::info(&quot;Positional args are {1} {0}..&quot;, &quot;too&quot;, &quot;supported&quot;);
    spdlog::info(&quot;{:&amp;lt;30}&quot;, &quot;left aligned&quot;);
    
    spdlog::set_level(spdlog::level::debug); // Set global log level to debug
    spdlog::debug(&quot;This message should be displayed..&quot;);    
    
    // change log pattern
    spdlog::set_pattern(&quot;[%H:%M:%S %z] [%n] [%^---%L---%$] [thread %t] %v&quot;);
    
    // Compile time log levels
    // Note that this does not change the current log level, it will only
    // remove (depending on SPDLOG_ACTIVE_LEVEL) the call on the release code.
    SPDLOG_TRACE(&quot;Some trace message with param {}&quot;, 42);
    SPDLOG_DEBUG(&quot;Some debug message&quot;);
}

&lt;/code&gt;&lt;/pre&gt; 
&lt;hr&gt; 
&lt;h4&gt;Create stdout/stderr logger object&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;#include &quot;spdlog/spdlog.h&quot;
#include &quot;spdlog/sinks/stdout_color_sinks.h&quot;
void stdout_example()
{
    // create a color multi-threaded logger
    auto console = spdlog::stdout_color_mt(&quot;console&quot;);    
    auto err_logger = spdlog::stderr_color_mt(&quot;stderr&quot;);    
    spdlog::get(&quot;console&quot;)-&amp;gt;info(&quot;loggers can be retrieved from a global registry using the spdlog::get(logger_name)&quot;);
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr&gt; 
&lt;h4&gt;Basic file logger&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;#include &quot;spdlog/sinks/basic_file_sink.h&quot;
void basic_logfile_example()
{
    try 
    {
        auto logger = spdlog::basic_logger_mt(&quot;basic_logger&quot;, &quot;logs/basic-log.txt&quot;);
    }
    catch (const spdlog::spdlog_ex &amp;amp;ex)
    {
        std::cout &amp;lt;&amp;lt; &quot;Log init failed: &quot; &amp;lt;&amp;lt; ex.what() &amp;lt;&amp;lt; std::endl;
    }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr&gt; 
&lt;h4&gt;Rotating files&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;#include &quot;spdlog/sinks/rotating_file_sink.h&quot;
void rotating_example()
{
    // Create a file rotating logger with 5 MB size max and 3 rotated files
    auto max_size = 1048576 * 5;
    auto max_files = 3;
    auto logger = spdlog::rotating_logger_mt(&quot;some_logger_name&quot;, &quot;logs/rotating.txt&quot;, max_size, max_files);
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr&gt; 
&lt;h4&gt;Daily files&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;
#include &quot;spdlog/sinks/daily_file_sink.h&quot;
void daily_example()
{
    // Create a daily logger - a new file is created every day at 2:30 am
    auto logger = spdlog::daily_logger_mt(&quot;daily_logger&quot;, &quot;logs/daily.txt&quot;, 2, 30);
}

&lt;/code&gt;&lt;/pre&gt; 
&lt;hr&gt; 
&lt;h4&gt;Backtrace support&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;// Debug messages can be stored in a ring buffer instead of being logged immediately.
// This is useful to display debug logs only when needed (e.g. when an error happens).
// When needed, call dump_backtrace() to dump them to your log.

spdlog::enable_backtrace(32); // Store the latest 32 messages in a buffer. 
// or my_logger-&amp;gt;enable_backtrace(32)..
for(int i = 0; i &amp;lt; 100; i++)
{
  spdlog::debug(&quot;Backtrace message {}&quot;, i); // not logged yet..
}
// e.g. if some error happened:
spdlog::dump_backtrace(); // log them now! show the last 32 messages
// or my_logger-&amp;gt;dump_backtrace(32)..
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr&gt; 
&lt;h4&gt;Periodic flush&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;// periodically flush all *registered* loggers every 3 seconds:
// warning: only use if all your loggers are thread-safe (&quot;_mt&quot; loggers)
spdlog::flush_every(std::chrono::seconds(3));

&lt;/code&gt;&lt;/pre&gt; 
&lt;hr&gt; 
&lt;h4&gt;Stopwatch&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;// Stopwatch support for spdlog
#include &quot;spdlog/stopwatch.h&quot;
void stopwatch_example()
{
    spdlog::stopwatch sw;    
    spdlog::debug(&quot;Elapsed {}&quot;, sw);
    spdlog::debug(&quot;Elapsed {:.3}&quot;, sw);       
}

&lt;/code&gt;&lt;/pre&gt; 
&lt;hr&gt; 
&lt;h4&gt;Log binary data in hex&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;// many types of std::container&amp;lt;char&amp;gt; types can be used.
// ranges are supported too.
// format flags:
// {:X} - print in uppercase.
// {:s} - don&#39;t separate each byte with space.
// {:p} - don&#39;t print the position on each line start.
// {:n} - don&#39;t split the output into lines.
// {:a} - show ASCII if :n is not set.

#include &quot;spdlog/fmt/bin_to_hex.h&quot;

void binary_example()
{
    auto console = spdlog::get(&quot;console&quot;);
    std::array&amp;lt;char, 80&amp;gt; buf;
    console-&amp;gt;info(&quot;Binary example: {}&quot;, spdlog::to_hex(buf));
    console-&amp;gt;info(&quot;Another binary example:{:n}&quot;, spdlog::to_hex(std::begin(buf), std::begin(buf) + 10));
    // more examples:
    // logger-&amp;gt;info(&quot;uppercase: {:X}&quot;, spdlog::to_hex(buf));
    // logger-&amp;gt;info(&quot;uppercase, no delimiters: {:Xs}&quot;, spdlog::to_hex(buf));
    // logger-&amp;gt;info(&quot;uppercase, no delimiters, no position info: {:Xsp}&quot;, spdlog::to_hex(buf));
}

&lt;/code&gt;&lt;/pre&gt; 
&lt;hr&gt; 
&lt;h4&gt;Logger with multi sinks - each with a different format and log level&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;
// create a logger with 2 targets, with different log levels and formats.
// The console will show only warnings or errors, while the file will log all.
void multi_sink_example()
{
    auto console_sink = std::make_shared&amp;lt;spdlog::sinks::stdout_color_sink_mt&amp;gt;();
    console_sink-&amp;gt;set_level(spdlog::level::warn);
    console_sink-&amp;gt;set_pattern(&quot;[multi_sink_example] [%^%l%$] %v&quot;);

    auto file_sink = std::make_shared&amp;lt;spdlog::sinks::basic_file_sink_mt&amp;gt;(&quot;logs/multisink.txt&quot;, true);
    file_sink-&amp;gt;set_level(spdlog::level::trace);

    spdlog::logger logger(&quot;multi_sink&quot;, {console_sink, file_sink});
    logger.set_level(spdlog::level::debug);
    logger.warn(&quot;this should appear in both console and file&quot;);
    logger.info(&quot;this message should not appear in the console, only in the file&quot;);
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr&gt; 
&lt;h4&gt;User-defined callbacks about log events&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;
// create a logger with a lambda function callback, the callback will be called
// each time something is logged to the logger
void callback_example()
{
    auto callback_sink = std::make_shared&amp;lt;spdlog::sinks::callback_sink_mt&amp;gt;([](const spdlog::details::log_msg &amp;amp;msg) {
         // for example you can be notified by sending an email to yourself
    });
    callback_sink-&amp;gt;set_level(spdlog::level::err);

    auto console_sink = std::make_shared&amp;lt;spdlog::sinks::stdout_color_sink_mt&amp;gt;();
    spdlog::logger logger(&quot;custom_callback_logger&quot;, {console_sink, callback_sink});

    logger.info(&quot;some info log&quot;);
    logger.error(&quot;critical issue&quot;); // will notify you
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr&gt; 
&lt;h4&gt;Asynchronous logging&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;#include &quot;spdlog/async.h&quot;
#include &quot;spdlog/sinks/basic_file_sink.h&quot;
void async_example()
{
    // default thread pool settings can be modified *before* creating the async logger:
    // spdlog::init_thread_pool(8192, 1); // queue with 8k items and 1 backing thread.
    auto async_file = spdlog::basic_logger_mt&amp;lt;spdlog::async_factory&amp;gt;(&quot;async_file_logger&quot;, &quot;logs/async_log.txt&quot;);
    // alternatively:
    // auto async_file = spdlog::create_async&amp;lt;spdlog::sinks::basic_file_sink_mt&amp;gt;(&quot;async_file_logger&quot;, &quot;logs/async_log.txt&quot;);   
}

&lt;/code&gt;&lt;/pre&gt; 
&lt;hr&gt; 
&lt;h4&gt;Asynchronous logger with multi sinks&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;#include &quot;spdlog/async.h&quot;
#include &quot;spdlog/sinks/stdout_color_sinks.h&quot;
#include &quot;spdlog/sinks/rotating_file_sink.h&quot;

void multi_sink_example2()
{
    spdlog::init_thread_pool(8192, 1);
    auto stdout_sink = std::make_shared&amp;lt;spdlog::sinks::stdout_color_sink_mt &amp;gt;();
    auto rotating_sink = std::make_shared&amp;lt;spdlog::sinks::rotating_file_sink_mt&amp;gt;(&quot;mylog.txt&quot;, 1024*1024*10, 3);
    std::vector&amp;lt;spdlog::sink_ptr&amp;gt; sinks {stdout_sink, rotating_sink};
    auto logger = std::make_shared&amp;lt;spdlog::async_logger&amp;gt;(&quot;loggername&quot;, sinks.begin(), sinks.end(), spdlog::thread_pool(), spdlog::async_overflow_policy::block);
    spdlog::register_logger(logger);
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr&gt; 
&lt;h4&gt;User-defined types&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;template&amp;lt;&amp;gt;
struct fmt::formatter&amp;lt;my_type&amp;gt; : fmt::formatter&amp;lt;std::string&amp;gt;
{
    auto format(my_type my, format_context &amp;amp;ctx) const -&amp;gt; decltype(ctx.out())
    {
        return fmt::format_to(ctx.out(), &quot;[my_type i={}]&quot;, my.i);
    }
};

void user_defined_example()
{
    spdlog::info(&quot;user defined type: {}&quot;, my_type(14));
}

&lt;/code&gt;&lt;/pre&gt; 
&lt;hr&gt; 
&lt;h4&gt;User-defined flags in the log pattern&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;// Log patterns can contain custom flags.
// the following example will add new flag &#39;%*&#39; - which will be bound to a &amp;lt;my_formatter_flag&amp;gt; instance.
#include &quot;spdlog/pattern_formatter.h&quot;
class my_formatter_flag : public spdlog::custom_flag_formatter
{
public:
    void format(const spdlog::details::log_msg &amp;amp;, const std::tm &amp;amp;, spdlog::memory_buf_t &amp;amp;dest) override
    {
        std::string some_txt = &quot;custom-flag&quot;;
        dest.append(some_txt.data(), some_txt.data() + some_txt.size());
    }

    std::unique_ptr&amp;lt;custom_flag_formatter&amp;gt; clone() const override
    {
        return spdlog::details::make_unique&amp;lt;my_formatter_flag&amp;gt;();
    }
};

void custom_flags_example()
{    
    auto formatter = std::make_unique&amp;lt;spdlog::pattern_formatter&amp;gt;();
    formatter-&amp;gt;add_flag&amp;lt;my_formatter_flag&amp;gt;(&#39;*&#39;).set_pattern(&quot;[%n] [%*] [%^%l%$] %v&quot;);
    spdlog::set_formatter(std::move(formatter));
}

&lt;/code&gt;&lt;/pre&gt; 
&lt;hr&gt; 
&lt;h4&gt;Custom error handler&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;void err_handler_example()
{
    // can be set globally or per logger(logger-&amp;gt;set_error_handler(..))
    spdlog::set_error_handler([](const std::string &amp;amp;msg) { spdlog::get(&quot;console&quot;)-&amp;gt;error(&quot;*** LOGGER ERROR ***: {}&quot;, msg); });
    spdlog::get(&quot;console&quot;)-&amp;gt;info(&quot;some invalid message to trigger an error {}{}{}{}&quot;, 3);
}

&lt;/code&gt;&lt;/pre&gt; 
&lt;hr&gt; 
&lt;h4&gt;syslog&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;#include &quot;spdlog/sinks/syslog_sink.h&quot;
void syslog_example()
{
    std::string ident = &quot;spdlog-example&quot;;
    auto syslog_logger = spdlog::syslog_logger_mt(&quot;syslog&quot;, ident, LOG_PID);
    syslog_logger-&amp;gt;warn(&quot;This is warning that will end up in syslog.&quot;);
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr&gt; 
&lt;h4&gt;Android example&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;#include &quot;spdlog/sinks/android_sink.h&quot;
void android_example()
{
    std::string tag = &quot;spdlog-android&quot;;
    auto android_logger = spdlog::android_logger_mt(&quot;android&quot;, tag);
    android_logger-&amp;gt;critical(&quot;Use \&quot;adb shell logcat\&quot; to view this message.&quot;);
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr&gt; 
&lt;h4&gt;Load log levels from the env variable or argv&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;#include &quot;spdlog/cfg/env.h&quot;
int main (int argc, char *argv[])
{
    spdlog::cfg::load_env_levels();
    // or specify the env variable name:
    // MYAPP_LEVEL=info,mylogger=trace &amp;amp;&amp;amp; ./example
    // spdlog::cfg::load_env_levels(&quot;MYAPP_LEVEL&quot;);
    // or from the command line:
    // ./example SPDLOG_LEVEL=info,mylogger=trace
    // #include &quot;spdlog/cfg/argv.h&quot; // for loading levels from argv
    // spdlog::cfg::load_argv_levels(argc, argv);
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;So then you can:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-console&quot;&gt;$ export SPDLOG_LEVEL=info,mylogger=trace
$ ./example
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr&gt; 
&lt;h4&gt;Log file open/close event handlers&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;// You can get callbacks from spdlog before/after a log file has been opened or closed. 
// This is useful for cleanup procedures or for adding something to the start/end of the log file.
void file_events_example()
{
    // pass the spdlog::file_event_handlers to file sinks for open/close log file notifications
    spdlog::file_event_handlers handlers;
    handlers.before_open = [](spdlog::filename_t filename) { spdlog::info(&quot;Before opening {}&quot;, filename); };
    handlers.after_open = [](spdlog::filename_t filename, std::FILE *fstream) { fputs(&quot;After opening\n&quot;, fstream); };
    handlers.before_close = [](spdlog::filename_t filename, std::FILE *fstream) { fputs(&quot;Before closing\n&quot;, fstream); };
    handlers.after_close = [](spdlog::filename_t filename) { spdlog::info(&quot;After closing {}&quot;, filename); };
    auto my_logger = spdlog::basic_logger_st(&quot;some_logger&quot;, &quot;logs/events-sample.txt&quot;, true, handlers);        
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr&gt; 
&lt;h4&gt;Replace the Default Logger&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;void replace_default_logger_example()
{
    auto new_logger = spdlog::basic_logger_mt(&quot;new_default_logger&quot;, &quot;logs/new-default-log.txt&quot;, true);
    spdlog::set_default_logger(new_logger);
    spdlog::info(&quot;new logger log message&quot;);
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr&gt; 
&lt;h4&gt;Log to Qt with nice colors&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;#include &quot;spdlog/spdlog.h&quot;
#include &quot;spdlog/sinks/qt_sinks.h&quot;
MainWindow::MainWindow(QWidget *parent) : QMainWindow(parent)
{
    setMinimumSize(640, 480);
    auto log_widget = new QTextEdit(this);
    setCentralWidget(log_widget);
    int max_lines = 500; // keep the text widget to max 500 lines. remove old lines if needed.
    auto logger = spdlog::qt_color_logger_mt(&quot;qt_logger&quot;, log_widget, max_lines);
    logger-&amp;gt;info(&quot;Some info message&quot;);
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr&gt; 
&lt;h4&gt;Mapped Diagnostic Context&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;// Mapped Diagnostic Context (MDC) is a map that stores key-value pairs (string values) in thread local storage.
// Each thread maintains its own MDC, which loggers use to append diagnostic information to log outputs.
// Note: it is not supported in asynchronous mode due to its reliance on thread-local storage.
#include &quot;spdlog/mdc.h&quot;
void mdc_example()
{
    spdlog::mdc::put(&quot;key1&quot;, &quot;value1&quot;);
    spdlog::mdc::put(&quot;key2&quot;, &quot;value2&quot;);
    // if not using the default format, use the %&amp;amp; formatter to print mdc data
    // spdlog::set_pattern(&quot;[%H:%M:%S %z] [%^%L%$] [%&amp;amp;] %v&quot;);
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr&gt; 
&lt;h2&gt;Benchmarks&lt;/h2&gt; 
&lt;p&gt;Below are some &lt;a href=&quot;https://github.com/gabime/spdlog/raw/v1.x/bench/bench.cpp&quot;&gt;benchmarks&lt;/a&gt; done in Ubuntu 64 bit, Intel i7-4770 CPU @ 3.40GHz&lt;/p&gt; 
&lt;h4&gt;Synchronous mode&lt;/h4&gt; 
&lt;pre&gt;&lt;code&gt;[info] **************************************************************
[info] Single thread, 1,000,000 iterations
[info] **************************************************************
[info] basic_st         Elapsed: 0.17 secs        5,777,626/sec
[info] rotating_st      Elapsed: 0.18 secs        5,475,894/sec
[info] daily_st         Elapsed: 0.20 secs        5,062,659/sec
[info] empty_logger     Elapsed: 0.07 secs       14,127,300/sec
[info] **************************************************************
[info] C-string (400 bytes). Single thread, 1,000,000 iterations
[info] **************************************************************
[info] basic_st         Elapsed: 0.41 secs        2,412,483/sec
[info] rotating_st      Elapsed: 0.72 secs        1,389,196/sec
[info] daily_st         Elapsed: 0.42 secs        2,393,298/sec
[info] null_st          Elapsed: 0.04 secs       27,446,957/sec
[info] **************************************************************
[info] 10 threads, competing over the same logger object, 1,000,000 iterations
[info] **************************************************************
[info] basic_mt         Elapsed: 0.60 secs        1,659,613/sec
[info] rotating_mt      Elapsed: 0.62 secs        1,612,493/sec
[info] daily_mt         Elapsed: 0.61 secs        1,638,305/sec
[info] null_mt          Elapsed: 0.16 secs        6,272,758/sec
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Asynchronous mode&lt;/h4&gt; 
&lt;pre&gt;&lt;code&gt;[info] -------------------------------------------------
[info] Messages     : 1,000,000
[info] Threads      : 10
[info] Queue        : 8,192 slots
[info] Queue memory : 8,192 x 272 = 2,176 KB 
[info] -------------------------------------------------
[info] 
[info] *********************************
[info] Queue Overflow Policy: block
[info] *********************************
[info] Elapsed: 1.70784 secs     585,535/sec
[info] Elapsed: 1.69805 secs     588,910/sec
[info] Elapsed: 1.7026 secs      587,337/sec
[info] 
[info] *********************************
[info] Queue Overflow Policy: overrun
[info] *********************************
[info] Elapsed: 0.372816 secs    2,682,285/sec
[info] Elapsed: 0.379758 secs    2,633,255/sec
[info] Elapsed: 0.373532 secs    2,677,147/sec

&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;Documentation can be found in the &lt;a href=&quot;https://github.com/gabime/spdlog/wiki/1.-QuickStart&quot;&gt;wiki&lt;/a&gt; pages.&lt;/p&gt; 
&lt;hr&gt; 
&lt;p&gt;Thanks to &lt;a href=&quot;https://www.jetbrains.com/?from=spdlog&quot;&gt;JetBrains&lt;/a&gt; for donating product licenses to help develop &lt;strong&gt;spdlog&lt;/strong&gt; &lt;a href=&quot;https://www.jetbrains.com/?from=spdlog&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/gabime/spdlog/v1.x/logos/jetbrains-variant-4.svg?sanitize=true&quot; width=&quot;94&quot; align=&quot;center&quot;&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>TechEmpower/FrameworkBenchmarks</title>
      <link>https://github.com/TechEmpower/FrameworkBenchmarks</link>
      <description>&lt;p&gt;Source for the TechEmpower Framework Benchmarks project&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Welcome to &lt;a href=&quot;http://www.techempower.com/benchmarks/&quot;&gt;TechEmpower Framework Benchmarks (TFB)&lt;/a&gt;&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/TechEmpower/FrameworkBenchmarks/actions?query=workflow%3Abuild+branch%3Amaster&quot;&gt;&lt;img src=&quot;https://github.com/TechEmpower/FrameworkBenchmarks/workflows/build/badge.svg?branch=master&amp;amp;event=push&quot; alt=&quot;Build Status&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;If you&#39;re new to the project, welcome! Please feel free to ask questions &lt;a href=&quot;https://github.com/TechEmpower/FrameworkBenchmarks/issues/2978&quot;&gt;here&lt;/a&gt;. We encourage new frameworks and contributors to ask questions. We&#39;re here to help!&lt;/p&gt; 
&lt;p&gt;This project provides representative performance measures across a wide field of web application frameworks. With much help from the community, coverage is quite broad and we are happy to broaden it further with contributions. The project presently includes frameworks on many languages including &lt;code&gt;Go&lt;/code&gt;, &lt;code&gt;Python&lt;/code&gt;, &lt;code&gt;Java&lt;/code&gt;, &lt;code&gt;Ruby&lt;/code&gt;, &lt;code&gt;PHP&lt;/code&gt;, &lt;code&gt;C#&lt;/code&gt;, &lt;code&gt;F#&lt;/code&gt;,&lt;code&gt;Clojure&lt;/code&gt;, &lt;code&gt;Groovy&lt;/code&gt;, &lt;code&gt;Dart&lt;/code&gt;, &lt;code&gt;JavaScript&lt;/code&gt;, &lt;code&gt;Erlang&lt;/code&gt;, &lt;code&gt;Haskell&lt;/code&gt;, &lt;code&gt;Scala&lt;/code&gt;, &lt;code&gt;Perl&lt;/code&gt;, &lt;code&gt;Lua&lt;/code&gt;, &lt;code&gt;C&lt;/code&gt;, and others. The current tests exercise plaintext responses, JSON serialization, database reads and writes via the object-relational mapper (ORM), collections, sorting, server-side templates, and XSS counter-measures. Future tests will exercise other components and greater computation.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;http://www.techempower.com/benchmarks/&quot;&gt;Read more and see the results of our tests on cloud and physical hardware&lt;/a&gt;. For descriptions of the test types that we run, see the &lt;a href=&quot;https://github.com/TechEmpower/FrameworkBenchmarks/wiki/Project-Information-Framework-Tests-Overview&quot;&gt;test requirements section&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you find yourself in a directory or file that you&#39;re not sure what the purpose is, checkout our &lt;a href=&quot;https://github.com/TechEmpower/FrameworkBenchmarks/wiki/Codebase-File-Structure&quot;&gt;file structure&lt;/a&gt; in our documentation, which will briefly explain the use of relevant directories and files.&lt;/p&gt; 
&lt;h2&gt;Quick Start Guide&lt;/h2&gt; 
&lt;p&gt;To get started developing you&#39;ll need to install &lt;a href=&quot;https://docs.docker.com/install/&quot;&gt;docker&lt;/a&gt; or see our &lt;a href=&quot;https://raw.githubusercontent.com/TechEmpower/FrameworkBenchmarks/master/#quick-start-guide-vagrant&quot;&gt;Quick Start Guide using vagrant&lt;/a&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Clone TFB.&lt;/p&gt; &lt;pre&gt;&lt;code&gt; $ git clone https://github.com/TechEmpower/FrameworkBenchmarks.git
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Change directories&lt;/p&gt; &lt;pre&gt;&lt;code&gt; $ cd FrameworkBenchmarks
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run a test.&lt;/p&gt; &lt;pre&gt;&lt;code&gt; $ ./tfb --mode verify --test gemini
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Explanation of the &lt;code&gt;./tfb&lt;/code&gt; script&lt;/h3&gt; 
&lt;p&gt;The run script is pretty wordy, but each and every flag is required. If you are using windows, either adapt the docker command at the end of the &lt;code&gt;./tfb&lt;/code&gt; shell script (replacing &lt;code&gt;${SCRIPT_ROOT}&lt;/code&gt; with &lt;code&gt;/c/path/to/FrameworkBenchmarks&lt;/code&gt;), or use vagrant.&lt;/p&gt; 
&lt;p&gt;The command looks like this: &lt;code&gt;docker run -it --rm --network tfb -v /var/run/docker.sock:/var/run/docker.sock -v [FWROOT]:/FrameworkBenchmarks techempower/tfb [ARGS]&lt;/code&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;-it&lt;/code&gt; tells docker to run this in &#39;interactive&#39; mode and simulate a TTY, so that &lt;code&gt;ctrl+c&lt;/code&gt; is propagated.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--rm&lt;/code&gt; tells docker to remove the container as soon as the toolset finishes running, meaning there aren&#39;t hundreds of stopped containers lying around.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--network=tfb&lt;/code&gt; tells the container to join the &#39;tfb&#39; Docker virtual network&lt;/li&gt; 
 &lt;li&gt;The first &lt;code&gt;-v&lt;/code&gt; specifies which Docker socket path to mount as a volume in the running container. This allows docker commands run inside this container to use the host container&#39;s docker to create/run/stop/remove containers.&lt;/li&gt; 
 &lt;li&gt;The second &lt;code&gt;-v&lt;/code&gt; mounts the FrameworkBenchmarks source directory as a volume to share with the container so that rebuilding the toolset image is unnecessary and any changes you make on the host system are available in the running toolset container.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;techempower/tfb&lt;/code&gt; is the name of toolset container to run&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;A note on Windows&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Docker expects Linux-style paths. If you cloned on your &lt;code&gt;C:\&lt;/code&gt; drive, then &lt;code&gt;[ABS PATH TO THIS DIR]&lt;/code&gt; would be &lt;code&gt;/c/FrameworkBenchmarks&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.docker.com/docker-windows&quot;&gt;Docker for Windows&lt;/a&gt; understands &lt;code&gt;/var/run/docker.sock&lt;/code&gt; even though that is not a valid path on Windows, but only when using Linux containers (it doesn&#39;t work with Windows containers and LCOW). &lt;a href=&quot;https://docs.docker.com/toolbox/toolbox_install_windows/&quot;&gt;Docker Toolbox&lt;/a&gt; &lt;strong&gt;may&lt;/strong&gt; not understand &lt;code&gt;/var/run/docker.sock&lt;/code&gt;, even when using Linux containers - use at your own risk.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Quick Start Guide (Vagrant)&lt;/h2&gt; 
&lt;p&gt;Get started developing quickly by utilizing vagrant with TFB. &lt;a href=&quot;https://git-scm.com&quot;&gt;Git&lt;/a&gt;, &lt;a href=&quot;https://www.virtualbox.org/&quot;&gt;Virtualbox&lt;/a&gt; and &lt;a href=&quot;https://www.vagrantup.com/&quot;&gt;vagrant&lt;/a&gt; are required.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Clone TFB.&lt;/p&gt; &lt;pre&gt;&lt;code&gt; $ git clone https://github.com/TechEmpower/FrameworkBenchmarks.git
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Change directories&lt;/p&gt; &lt;pre&gt;&lt;code&gt; $ cd FrameworkBenchmarks/deployment/vagrant
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Build the vagrant virtual machine&lt;/p&gt; &lt;pre&gt;&lt;code&gt; $ vagrant up
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run a test&lt;/p&gt; &lt;pre&gt;&lt;code&gt; $ vagrant ssh
 $ tfb --mode verify --test gemini
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Add a New Test&lt;/h2&gt; 
&lt;p&gt;Either on your computer, or once you open an SSH connection to your vagrant box, start the new test initialization wizard.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    vagrant@TFB-all:~/FrameworkBenchmarks$ ./tfb --new
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will walk you through the entire process of creating a new test to include in the suite.&lt;/p&gt; 
&lt;h2&gt;Resources&lt;/h2&gt; 
&lt;h3&gt;Official Documentation&lt;/h3&gt; 
&lt;p&gt;Our official documentation can be found in the &lt;a href=&quot;https://github.com/TechEmpower/FrameworkBenchmarks/wiki&quot;&gt;wiki&lt;/a&gt;. If you find any errors or areas for improvement within the docs, feel free to open an issue in this repo.&lt;/p&gt; 
&lt;h3&gt;Live Results&lt;/h3&gt; 
&lt;p&gt;Results of continuous benchmarking runs are available in real time &lt;a href=&quot;https://tfb-status.techempower.com/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Data Visualization&lt;/h3&gt; 
&lt;p&gt;If you have a &lt;code&gt;results.json&lt;/code&gt; file that you would like to visualize, you can &lt;a href=&quot;https://tfb-status.techempower.com/share&quot;&gt;do that here&lt;/a&gt;. You can also attach a &lt;code&gt;runid&lt;/code&gt; parameter to that url where &lt;code&gt;runid&lt;/code&gt; is a run listed on &lt;a href=&quot;https://tfb-status.techempower.com&quot;&gt;tfb-status&lt;/a&gt; like so: &lt;a href=&quot;https://www.techempower.com/benchmarks/#section=test&amp;amp;runid=fd07b64e-47ce-411e-8b9b-b13368e988c6&quot;&gt;https://www.techempower.com/benchmarks/#section=test&amp;amp;runid=fd07b64e-47ce-411e-8b9b-b13368e988c6&lt;/a&gt;. If you want to visualize them or compare different results files on bash, here is an unofficial &lt;a href=&quot;https://github.com/joeyleeeeeee97/PlainTextResultsParser&quot;&gt;plaintext results parser&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;The community has consistently helped in making these tests better, and we welcome any and all changes. Reviewing our contribution practices and guidelines will help to keep us all on the same page. The &lt;a href=&quot;https://github.com/TechEmpower/FrameworkBenchmarks/wiki/Development-Contributing-Guide&quot;&gt;contribution guide&lt;/a&gt; can be found in the &lt;a href=&quot;https://github.com/TechEmpower/FrameworkBenchmarks/wiki&quot;&gt;TFB documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Join in the conversation in the &lt;a href=&quot;https://github.com/TechEmpower/FrameworkBenchmarks/discussions&quot;&gt;Discussions tab&lt;/a&gt;, on &lt;a href=&quot;https://twitter.com/tfbenchmarks&quot;&gt;Twitter&lt;/a&gt;, or chat with us on &lt;a href=&quot;https://webchat.freenode.net/&quot;&gt;Freenode&lt;/a&gt; at &lt;code&gt;#techempower-fwbm&lt;/code&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>qgis/QGIS</title>
      <link>https://github.com/qgis/QGIS</link>
      <description>&lt;p&gt;QGIS is a free, open source, cross platform (lin/win/mac) geographical information system (GIS)&lt;/p&gt;&lt;hr&gt;&lt;img src=&quot;https://raw.githubusercontent.com/qgis/QGIS/master/images/README-md/main_logo.png&quot; width=&quot;300&quot;&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/qgis/QGIS/actions/workflows/run-tests.yml?query=branch%3Amaster+event%3Apush&quot;&gt;&lt;img src=&quot;https://github.com/qgis/QGIS/actions/workflows/run-tests.yml/badge.svg?sanitize=true&quot; alt=&quot;🧪 QGIS tests&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://hub.docker.com/r/qgis/qgis/tags&quot;&gt;&lt;img src=&quot;https://img.shields.io/docker/automated/qgis/qgis.svg?sanitize=true&quot; alt=&quot;Docker Status&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://dev.azure.com/qgis/QGIS/_build/latest?definitionId=1&amp;amp;branchName=master&quot;&gt;&lt;img src=&quot;https://dev.azure.com/qgis/QGIS/_apis/build/status/qgis.QGIS?branchName=master&quot; alt=&quot;Build Status&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://securityscorecards.dev/viewer/?uri=github.com/qgis/QGIS&quot;&gt;&lt;img src=&quot;https://api.securityscorecards.dev/projects/github.com/qgis/QGIS/badge&quot; alt=&quot;OpenSSF Scorecard&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://www.bestpractices.dev/projects/1581&quot;&gt;&lt;img src=&quot;https://www.bestpractices.dev/projects/1581/badge&quot; alt=&quot;OpenSSF Best Practices&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/qgis/QGIS/actions/workflows/mingw64.yml?query=branch%3Amaster+event%3Apush&quot;&gt;&lt;img src=&quot;https://github.com/qgis/QGIS/actions/workflows/mingw64.yml/badge.svg?sanitize=true&quot; alt=&quot;🪟 MingW64 Windows 64bit Build&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://doi.org/10.5281/zenodo.5869837&quot;&gt;&lt;img src=&quot;https://zenodo.org/badge/DOI/10.5281/zenodo.5869837.svg?sanitize=true&quot; alt=&quot;DOI&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;QGIS is a full-featured, user-friendly, free-and-open-source (FOSS) geographical information system (GIS) that runs on Unix platforms, Windows, and MacOS.&lt;/p&gt; 
&lt;!-- TOC generated with https://freelance-tech-writer.github.io/table-of-contents-generator/index.html --&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/qgis/QGIS/master/#features&quot;&gt;Features&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/qgis/QGIS/master/#1-flexible-and-powerful-spatial-data-management&quot;&gt;1. Flexible and powerful spatial data management&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/qgis/QGIS/master/#2-beautiful-cartography&quot;&gt;2. Beautiful cartography&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/qgis/QGIS/master/#3-advanced-and-robust-geospatial-analysis&quot;&gt;3. Advanced and robust geospatial analysis&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/qgis/QGIS/master/#4-powerful-customization-and-extensibility&quot;&gt;4. Powerful customization and extensibility&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/qgis/QGIS/master/#5-qgis-server&quot;&gt;5. QGIS Server&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/qgis/QGIS/master/#under-the-hood&quot;&gt;Under the hood&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/qgis/QGIS/master/#versions-and-release-cycle&quot;&gt;Versions and release cycle&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/qgis/QGIS/master/#free-and-open-source&quot;&gt;Free and Open Source&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/qgis/QGIS/master/#installing-and-using-qgis&quot;&gt;Installing and using QGIS&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/qgis/QGIS/master/#documentation&quot;&gt;Documentation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/qgis/QGIS/master/#help-and-support-channels&quot;&gt;Help and support channels&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/qgis/QGIS/master/#get-involved-with-the-community&quot;&gt;Get involved with the community&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;h3&gt;1. Flexible and powerful spatial data management&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Support for raster, vector, mesh, and point cloud data in a range of industry-standard formats 
  &lt;ul&gt; 
   &lt;li&gt;&lt;em&gt;Raster formats include&lt;/em&gt;: GeoPackage, GeoTIFF, GRASS, ArcInfo binary and ASCII grids, ERDAS Imagine SDTS, WMS, WCS, PostgreSQL/PostGIS, and &lt;a href=&quot;https://gdal.org/drivers/raster/index.html&quot;&gt;other GDAL supported formats&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;&lt;em&gt;Vector formats include&lt;/em&gt;: GeoPackage, ESRI shapefiles, GRASS, SpatiaLite, PostgreSQL/PostGIS, MSSQL, Oracle, WFS, Vector Tiles and &lt;a href=&quot;https://www.gdal.org/ogr_formats.html&quot;&gt;other OGR supported formats&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;&lt;em&gt;Mesh formats include&lt;/em&gt;: NetCDF, GRIB, 2DM, and &lt;a href=&quot;https://github.com/lutraconsulting/MDAL#supported-formats&quot;&gt;other MDAL supported formats&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;&lt;em&gt;Point-cloud format&lt;/em&gt;: LAS/LAZ and EPT datasets.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Data abstraction framework, with local files, spatial databases (PostGIS, SpatiaLite, SQL Server, Oracle, SAP HANA), and web services (WMS, WCS, WFS, ArcGIS REST) all accessed through a unified data model and browser interface, and as flexible layers in user-created projects&lt;/li&gt; 
 &lt;li&gt;Spatial data creation via visual and numerical digitizing and editing, as well as georeferencing of raster and vector data&lt;/li&gt; 
 &lt;li&gt;On-the-fly reprojection between coordinate reference systems (CRS)&lt;/li&gt; 
 &lt;li&gt;Nominatim (OpenStreetMap) geocoder access&lt;/li&gt; 
 &lt;li&gt;Temporal support&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;em&gt;Example: Temporal animation&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/qgis/QGIS/master/images/README-md/icebergs.gif&quot; alt=&quot;Example: Temporal animation&quot; title=&quot;Temporal animation&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Example: 3D map view&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://docs.qgis.org/latest/en/_images/3dmapview.png&quot; alt=&quot;Example: 3D map view&quot; title=&quot;3D map view&quot;&gt;&lt;/p&gt; 
&lt;h3&gt;2. Beautiful cartography&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Large variety of rendering options in 2D and 3D&lt;/li&gt; 
 &lt;li&gt;Fine control over symbology, labeling, legends and additional graphical elements for beautifully rendered maps&lt;/li&gt; 
 &lt;li&gt;Respect for embedded styling in many spatial data sources (e.g. KML and TAB files, Mapbox-GL styled vector tiles)&lt;/li&gt; 
 &lt;li&gt;In particular, near-complete replication (and significant extension) of symbology options that are available in proprietary software by ESRI&lt;/li&gt; 
 &lt;li&gt;Advanced styling using data-defined overrides, blending modes, and draw effects&lt;/li&gt; 
 &lt;li&gt;500+ built-in color ramps (cpt-city, ColorBrewer, etc.)&lt;/li&gt; 
 &lt;li&gt;Create and update maps with specified scale, extent, style, and decorations via saved layouts&lt;/li&gt; 
 &lt;li&gt;Generate multiple maps (and reports) automatically using QGIS Atlas and QGIS Reports&lt;/li&gt; 
 &lt;li&gt;Display and export elevation profile plots with flexible symbology&lt;/li&gt; 
 &lt;li&gt;Flexible output direct to printer, or as image (raster), PDF, or SVG for further customization&lt;/li&gt; 
 &lt;li&gt;On-the-fly rendering enhancements using geometry generators (e.g. create and style new geometries from existing features)&lt;/li&gt; 
 &lt;li&gt;Preview modes for inclusive map making (e.g. monochrome, color blindness)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;em&gt;&lt;a href=&quot;https://flic.kr/p/2jFfGJP&quot;&gt;Example: Map of Bogota, Colombia in the style of Starry Starry Night, by Andrés Felipe Lancheros Sánchez&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://live.staticflickr.com/65535/50327326323_3da28f0d86_b.jpg&quot; alt=&quot;Map of Bogota, Colombia in the style of Starry Starry Night&quot; title=&quot;Map of Bogota, Colombia in the style of Starry Starry Night&quot;&gt;&lt;/p&gt; 
&lt;p&gt;For more maps created with QGIS, visit the &lt;a href=&quot;https://www.flickr.com/groups/2244553@N22/pool/with/50355460063/&quot;&gt;QGIS Map Showcase Flickr Group&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/qgis/QGIS/master/images/README-md/qgis_map_showcase.png&quot; alt=&quot;QGIS Map Showcase&quot; title=&quot;QGIS Map Showcase&quot;&gt;&lt;/p&gt; 
&lt;h3&gt;3. Advanced and robust geospatial analysis&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Powerful processing framework with 200+ native processing algorithms&lt;/li&gt; 
 &lt;li&gt;Access to 1000+ processing algorithms via providers such as GDAL, SAGA, GRASS, OrfeoToolbox, as well as custom models and processing scripts&lt;/li&gt; 
 &lt;li&gt;Geospatial database engine (filters, joins, relations, forms, etc.), as close to datasource- and format-independent as possible&lt;/li&gt; 
 &lt;li&gt;Immediate visualization of geospatial query and geoprocessing results&lt;/li&gt; 
 &lt;li&gt;Model designer and batch processing&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;em&gt;Example: Travel isochrones&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/qgis/QGIS/master/images/README-md/network_analysis_2.png&quot; alt=&quot;Example: Travel isochrones&quot; title=&quot;Travel isochrones&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Example: Model designer&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://docs.qgis.org/latest/en/_images/models_model.png&quot; alt=&quot;Example: model designer&quot; title=&quot;Model designer&quot;&gt;&lt;/p&gt; 
&lt;h3&gt;4. Powerful customization and extensibility&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Fully customizable user experience, including user interface and application settings that cater to power-users and beginners alike&lt;/li&gt; 
 &lt;li&gt;Rich &lt;a href=&quot;https://docs.qgis.org/testing/en/docs/user_manual/working_with_vector/expression.html&quot;&gt;expression engine&lt;/a&gt; for maximum flexibility in visualization and processing&lt;/li&gt; 
 &lt;li&gt;Broad and varied &lt;a href=&quot;https://plugins.qgis.org/&quot;&gt;plugin ecosystem&lt;/a&gt; that includes data connectors, digitizing aids, advanced analysis and charting tools, in-the-field data capture, conversion of ESRI style files, etc.&lt;/li&gt; 
 &lt;li&gt;Style manager for creating, storing, and managing styles&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://plugins.qgis.org/styles/&quot;&gt;QGIS style hub&lt;/a&gt; for easy sharing of styles&lt;/li&gt; 
 &lt;li&gt;Python and C++ API for standalone (headless) applications as well as in-application comprehensive scripting (PyQGIS)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;em&gt;Example: Style manager&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://docs.qgis.org/latest/en/_images/stylemanager.png&quot; alt=&quot;Example: Style manager&quot; title=&quot;Style Manager&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Example: Plugins&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/qgis/QGIS/master/images/README-md/plugins_1.png&quot; alt=&quot;Example: Plugins&quot; title=&quot;Plugins&quot;&gt;&lt;/p&gt; 
&lt;!-- Kill this one for now, since the example provided is Python2 not 3
Example: Python console

![Example: Python console](https://docs.qgis.org/latest/en/_images/python_console_editor.png &quot;Python console&quot;)
--&gt; 
&lt;h3&gt;5. QGIS Server&lt;/h3&gt; 
&lt;p&gt;Headless map server -- running on Linux, macOS, Windows, or in a docker container -- that shares the same code base as QGIS.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Industry-standard protocols (WMS, WFS, WFS3/OGC API for Features and WCS) allow plug-n-play with any software stack&lt;/li&gt; 
 &lt;li&gt;Works with any web server (Apache, nginx, etc) or standalone&lt;/li&gt; 
 &lt;li&gt;All beautiful QGIS cartography is supported with best-in-class support for printing&lt;/li&gt; 
 &lt;li&gt;Fully customizable with Python scripting support&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;em&gt;Example: QGIS server WMS response&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://docs.qgis.org/latest/en/_images/server_selection_parameter.png&quot; alt=&quot;Example: QGIS Server response to a WMS request&quot; title=&quot;QGIS Server response to a WMS request&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Example: QGIS server WFS response&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://docs.qgis.org/latest/en/_images/server_wfs3_feature.png&quot; alt=&quot;Example: QGIS Server response to a WFS Feature request&quot; title=&quot;QGIS Server response to a WFS Feature request&quot;&gt;&lt;/p&gt; 
&lt;h2&gt;Under the hood&lt;/h2&gt; 
&lt;p&gt;QGIS is developed using the &lt;a href=&quot;https://qt.io&quot;&gt;Qt toolkit&lt;/a&gt; and C++, since 2002, and has a pleasing, easy to use graphical user interface with multilingual support. It is maintained by an active developer team and supported by vibrant community of GIS professionals and enthusiasts as well as geospatial data publishers and end-users.&lt;/p&gt; 
&lt;h3&gt;Versions and release cycle&lt;/h3&gt; 
&lt;p&gt;QGIS development and releases follow a &lt;a href=&quot;https://www.qgis.org/en/site/getinvolved/development/roadmap.html&quot;&gt;time based schedule/roadmap&lt;/a&gt;. There are three main branches of QGIS that users can install. These are the &lt;strong&gt;Long Term Release (LTR)&lt;/strong&gt; branch, the &lt;strong&gt;Latest Release (LR)&lt;/strong&gt; branch, and the &lt;strong&gt;Development (Nightly)&lt;/strong&gt; branch.&lt;/p&gt; 
&lt;p&gt;Every month, there is a &lt;strong&gt;Point Release&lt;/strong&gt; that provides bug-fixes to the LTR and LR.&lt;/p&gt; 
&lt;h3&gt;Free and Open Source&lt;/h3&gt; 
&lt;p&gt;QGIS is released under the GNU Public License (GPL) Version 2 or any later version. Developing QGIS under this license means that you can (if you want to) inspect and modify the source code and guarantees that you, our happy user will always have access to a GIS program that is free of cost and can be freely modified.&lt;/p&gt; 
&lt;p&gt;QGIS is part of the Open-Source Geospatial Foundation (&lt;a href=&quot;https://www.osgeo.org/&quot;&gt;OSGeo&lt;/a&gt;), offering a range of complementary open-source GIS software projects.&lt;/p&gt; 
&lt;h2&gt;Installing and using QGIS&lt;/h2&gt; 
&lt;p&gt;Precompiled binaries for QGIS are available at &lt;a href=&quot;https://www.qgis.org/en/site/forusers/download.html&quot;&gt;the QGIS.org download page&lt;/a&gt;. Please follow the installation instructions carefully.&lt;/p&gt; 
&lt;p&gt;The &lt;a href=&quot;https://raw.githubusercontent.com/qgis/QGIS/master/INSTALL.md&quot;&gt;building guide&lt;/a&gt; can be used to get started with building QGIS from source.&lt;/p&gt; 
&lt;p&gt;For installation of QGIS Server, see its &lt;a href=&quot;https://docs.qgis.org/testing/en/docs/server_manual/getting_started.html&quot;&gt;getting started documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Documentation&lt;/h3&gt; 
&lt;p&gt;A range of &lt;a href=&quot;https://qgis.org/resources/hub/#documentation&quot;&gt;documentation&lt;/a&gt; is available. This includes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.qgis.org/latest/en/docs/training_manual/index.html&quot;&gt;Training Manual&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.qgis.org/latest/en/docs/user_manual/index.html&quot;&gt;QGIS User Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.qgis.org/latest/en/docs/server_manual/index.html&quot;&gt;QGIS Server Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://qgis.org/project/visual-changelogs/&quot;&gt;Visual Changelog&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.qgis.org/latest/en/docs/documentation_guidelines/index.html&quot;&gt;Documentation Guidelines&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.qgis.org/latest/en/docs/pyqgis_developer_cookbook/index.html&quot;&gt;QGIS Python (PyQGIS) Cookbook&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://qgis.org/pyqgis/&quot;&gt;QGIS Python (PyQGIS) API&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://qgis.org/api/&quot;&gt;QGIS C++ API&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.qgis.org/latest/en/docs/developers_guide/index.html&quot;&gt;Developers Guide&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Help and support channels&lt;/h3&gt; 
&lt;p&gt;There are several channels where you can find help and support for QGIS:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Using the &lt;a href=&quot;https://qgis.org&quot;&gt;QGIS community site&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Joining the &lt;a href=&quot;https://lists.osgeo.org/mailman/listinfo/qgis-user&quot;&gt;qgis-users mailing list&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Chatting with other users real-time. &lt;em&gt;Please wait around for a response to your question as many folks on the channel are doing other things and it may take a while for them to notice your question. The following paths all take you to the same chat room:&lt;/em&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Using an IRC client and joining the &lt;a href=&quot;https://web.libera.chat/?channels=#qgis&quot;&gt;#qgis&lt;/a&gt; channel on irc.libera.chat.&lt;/li&gt; 
   &lt;li&gt;Using a Matrix client and joining the &lt;a href=&quot;https://matrix.to/#/%23qgis:osgeo.org&quot;&gt;#qgis:osgeo.org&lt;/a&gt; room.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;At the &lt;a href=&quot;https://gis.stackexchange.com/&quot;&gt;GIS stackexchange&lt;/a&gt; or &lt;a href=&quot;https://www.reddit.com/r/QGIS/&quot;&gt;r/QGIS reddit&lt;/a&gt;, which are not maintained by the QGIS team, but where the QGIS and broader GIS community provides lots of advice&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://qgis.org/resources/support/&quot;&gt;Other support channels&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Get involved with the community&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/qgis/QGIS/master/CONTRIBUTING.md&quot;&gt;Contribution guidelines for this project&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ossrs/srs</title>
      <link>https://github.com/ossrs/srs</link>
      <description>&lt;p&gt;SRS is a simple, high-efficiency, real-time media server supporting RTMP, WebRTC, HLS, HTTP-FLV, HTTP-TS, SRT, MPEG-DASH, and GB28181.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;SRS(Simple Realtime Server)&lt;/h1&gt; 
&lt;p&gt;&lt;img src=&quot;http://ossrs.net/gif/v1/sls.gif?site=github.com&amp;amp;path=/srs/develop&quot; alt=&quot;&quot;&gt; &lt;a href=&quot;https://github.com/ossrs/srs/actions?query=workflow%3ACodeQL+branch%3Adevelop&quot;&gt;&lt;img src=&quot;https://github.com/ossrs/srs/actions/workflows/codeql-analysis.yml/badge.svg?branch=develop&quot; alt=&quot;&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/ossrs/srs/actions/workflows/release.yml?query=workflow%3ARelease&quot;&gt;&lt;img src=&quot;https://github.com/ossrs/srs/actions/workflows/release.yml/badge.svg?sanitize=true&quot; alt=&quot;&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/ossrs/srs/actions?query=workflow%3ATest+branch%3Adevelop&quot;&gt;&lt;img src=&quot;https://github.com/ossrs/srs/actions/workflows/test.yml/badge.svg?branch=develop&quot; alt=&quot;&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://app.codecov.io/gh/ossrs/srs/tree/develop&quot;&gt;&lt;img src=&quot;https://codecov.io/gh/ossrs/srs/branch/develop/graph/badge.svg?token=Zx2LhdtA39&quot; alt=&quot;&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://ossrs.net/lts/zh-cn/contact#discussion&quot;&gt;&lt;img src=&quot;https://ossrs.net/wiki/images/wechat-badge4.svg?sanitize=true&quot; alt=&quot;&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://twitter.com/srs_server&quot;&gt;&lt;img src=&quot;https://img.shields.io/twitter/follow/srs_server?style=social&quot; alt=&quot;&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/@srs_server&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/SRS-YouTube-red&quot; alt=&quot;&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://discord.gg/yZ4BnPmHAd&quot;&gt;&lt;img src=&quot;https://badgen.net/discord/members/yZ4BnPmHAd&quot; alt=&quot;&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://app.fossa.com/projects/git%2Bgithub.com%2Fossrs%2Fsrs?ref=badge_small&quot;&gt;&lt;img src=&quot;https://app.fossa.com/api/projects/git%2Bgithub.com%2Fossrs%2Fsrs.svg?type=small&quot; alt=&quot;&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://stackoverflow.com/questions/tagged/simple-realtime-server&quot;&gt;&lt;img src=&quot;https://badgen.net/badge/srs/stackoverflow/orange?icon=terminal&quot; alt=&quot;&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://opencollective.com/srs-server&quot;&gt;&lt;img src=&quot;https://opencollective.com/srs-server/tiers/badge.svg?sanitize=true&quot; alt=&quot;&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://hub.docker.com/r/ossrs/srs/tags&quot;&gt;&lt;img src=&quot;https://img.shields.io/docker/pulls/ossrs/srs&quot; alt=&quot;&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;SRS/6.0 (&lt;a href=&quot;https://ossrs.io/lts/en-us/product#release-60&quot;&gt;Hang&lt;/a&gt;) is a simple, high-efficiency, and real-time video server, supporting RTMP/WebRTC/HLS/HTTP-FLV/SRT/MPEG-DASH/GB28181, Linux/Windows/macOS, X86_64/ARMv7/AARCH64/M1/RISCV/LOONGARCH/MIPS, and essential &lt;a href=&quot;https://raw.githubusercontent.com/ossrs/srs/develop/trunk/doc/Features.md#features&quot;&gt;features&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://ossrs.net/wiki/images/SRS-SingleNode-4.0-hd.png&quot;&gt;&lt;img src=&quot;https://ossrs.net/wiki/images/SRS-SingleNode-4.0-sd.png?v=114&quot; alt=&quot;SRS Overview&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Note: For more details on the single-node architecture for SRS, please visit the following &lt;a href=&quot;https://www.figma.com/file/333POxVznQ8Wz1Rxlppn36/SRS-4.0-Server-Arch&quot;&gt;link&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;SRS is licenced under &lt;a href=&quot;https://github.com/ossrs/srs/raw/develop/LICENSE&quot;&gt;MIT&lt;/a&gt;, and some third-party libraries are distributed under their &lt;a href=&quot;https://ossrs.io/lts/en-us/license&quot;&gt;licenses&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a name=&quot;product&quot;&gt;&lt;/a&gt; &lt;a name=&quot;usage-docker&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;p&gt;Please check the Getting Started guide in &lt;a href=&quot;https://ossrs.io/lts/en-us/docs/v5/doc/getting-started&quot;&gt;English&lt;/a&gt; or &lt;a href=&quot;https://ossrs.net/lts/zh-cn/docs/v5/doc/getting-started&quot;&gt;Chinese&lt;/a&gt;. We highly recommend using SRS with docker:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;docker run --rm -it -p 1935:1935 -p 1985:1985 -p 8080:8080 \
    -p 8000:8000/udp -p 10080:10080/udp ossrs/srs:5
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Tips: If you&#39;re in China, use this image &lt;code&gt;registry.cn-hangzhou.aliyuncs.com/ossrs/srs:5&lt;/code&gt; for faster speed.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Open &lt;a href=&quot;http://localhost:8080/&quot;&gt;http://localhost:8080/&lt;/a&gt; to verify, and then stream using the following &lt;a href=&quot;https://ffmpeg.org/download.html&quot;&gt;FFmpeg&lt;/a&gt; command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;ffmpeg -re -i ./doc/source.flv -c copy -f flv -y rtmp://localhost/live/livestream
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, stream by &lt;a href=&quot;https://obsproject.com/download&quot;&gt;OBS&lt;/a&gt; using the following configuration:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Service: &lt;code&gt;Custom&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Server: &lt;code&gt;rtmp://localhost/live&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Stream Key: &lt;code&gt;livestream&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Play the following streams using media players:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;To play an RTMP stream with URL &lt;code&gt;rtmp://localhost/live/livestream&lt;/code&gt; on &lt;a href=&quot;https://www.videolan.org/&quot;&gt;VLC player&lt;/a&gt;, open the player, go to Media &amp;gt; Open Network Stream, enter the URL and click Play.&lt;/li&gt; 
 &lt;li&gt;You can play HTTP-FLV stream URL &lt;a href=&quot;http://localhost:8080/players/srs_player.html?autostart=true&amp;amp;stream=livestream.flv&quot;&gt;http://localhost:8080/live/livestream.flv&lt;/a&gt; on a webpage using the srs-player, an HTML5-based player.&lt;/li&gt; 
 &lt;li&gt;Use srs-player for playing HLS stream with URL &lt;a href=&quot;http://localhost:8080/players/srs_player.html?autostart=true&amp;amp;stream=livestream.m3u8&quot;&gt;http://localhost:8080/live/livestream.m3u8&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If you&#39;d like to use WebRTC, convert RTMP to WebRTC, or convert WebRTC to RTMP, please check out the wiki documentation in either &lt;a href=&quot;https://ossrs.io/lts/en-us/docs/v5/doc/getting-started#webrtc&quot;&gt;English&lt;/a&gt; or &lt;a href=&quot;https://ossrs.net/lts/zh-cn/docs/v5/doc/getting-started#webrtc&quot;&gt;Chinese&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;To learn more about RTMP, HLS, HTTP-FLV, SRT, MPEG-DASH, WebRTC protocols, clustering, HTTP API, DVR, and transcoding, please check the documents in &lt;a href=&quot;https://ossrs.io&quot;&gt;English&lt;/a&gt; or &lt;a href=&quot;https://ossrs.net&quot;&gt;Chinese&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Sponsor&lt;/h2&gt; 
&lt;p&gt;Would you like additional assistance from us? By becoming a sponsor or backer of SRS, we can provide you with the support you need:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Backer: $5 per month, online text chat support through Discord.&lt;/li&gt; 
 &lt;li&gt;Sponsor: $100 per month, online text chat plus online meeting support.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Please visit &lt;a href=&quot;https://opencollective.com/srs-server&quot;&gt;OpenCollective&lt;/a&gt; to become a backer or sponsor, and send us a direct message on &lt;a href=&quot;https://discord.gg/yZ4BnPmHAd&quot;&gt;Discord&lt;/a&gt;. We are currently providing support to the developers listed below:&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://opencollective.com/srs-server&quot;&gt;&lt;img src=&quot;https://opencollective.com/srs-server/backers.svg?width=800&amp;amp;button=false&quot; alt=&quot;&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;At SRS, our goal is to create a free, open-source community that helps developers all over the world build high-quality streaming and RTC platforms for their businesses.&lt;/p&gt; 
&lt;p&gt;&lt;a name=&quot;authors&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;The &lt;a href=&quot;https://raw.githubusercontent.com/ossrs/srs/develop/trunk/AUTHORS.md#authors&quot;&gt;authors&lt;/a&gt;, &lt;a href=&quot;https://raw.githubusercontent.com/ossrs/srs/develop/trunk/AUTHORS.md#toc&quot;&gt;TOC(Technical Oversight Committee)&lt;/a&gt;, and &lt;a href=&quot;https://raw.githubusercontent.com/ossrs/srs/develop/trunk/AUTHORS.md#contributors&quot;&gt;contributors&lt;/a&gt; are listed &lt;a href=&quot;https://raw.githubusercontent.com/ossrs/srs/develop/trunk/AUTHORS.md&quot;&gt;here&lt;/a&gt;. The TOC members who made significant contributions and maintained parts of SRS are listed below:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/winlinvip&quot;&gt;Winlin&lt;/a&gt;: Founder of the project, focusing on ST and Issues/PR. Responsible for architecture and maintenance.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/wenjiegit&quot;&gt;ZhaoWenjie&lt;/a&gt;: One of the earliest contributors, focusing on HDS and Windows. Has expertise in client technology.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/runner365&quot;&gt;ShiWei&lt;/a&gt;: Specializes in SRT and H.265, maintaining SRT and FLV patches for FFmpeg. An expert in codecs and FFmpeg.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/xiaozhihong&quot;&gt;XiaoZhihong&lt;/a&gt;: Concentrates on WebRTC/QUIC and SRT, with expertise in network QoS. Contributed to ARM on ST and was the original contributor for WebRTC.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Bepartofyou&quot;&gt;WuPengqiang&lt;/a&gt;: Focused on H.265, initially contributed to the FFmpeg module in SRS for transcoding AAC with OPUS for WebRTC.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/xialixin&quot;&gt;XiaLixin&lt;/a&gt;: Specializes in GB28181, with expertise in live streaming and WebRTC.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/lipeng19811218&quot;&gt;LiPeng&lt;/a&gt;: Concentrates on WebRTC and contributes to memory management and smart pointers.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/chen-guanghua&quot;&gt;ChenGuanghua&lt;/a&gt;: Focused on WebRTC/QoS and introduced the Asan toolchain to SRS.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/duiniuluantanqin&quot;&gt;ChenHaibo&lt;/a&gt;: Specializes in GB28181 and HTTP API, contributing to patches for FFmpeg with WHIP.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/chundonglinlin&quot;&gt;ZhangJunqin&lt;/a&gt;: Focused on H.265, Prometheus Exporter, and API module.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;A huge &lt;code&gt;THANK YOU&lt;/code&gt; goes out to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;All the &lt;a href=&quot;https://raw.githubusercontent.com/ossrs/srs/develop/trunk/AUTHORS.md#contributors&quot;&gt;contributors&lt;/a&gt; of SRS.&lt;/li&gt; 
 &lt;li&gt;All the friends of SRS who gave &lt;a href=&quot;https://ossrs.net/lts/zh-cn/product&quot;&gt;big support&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;http://sourceforge.net/users/genes&quot;&gt;Genes&lt;/a&gt;, &lt;a href=&quot;http://sourceforge.net/users/mabbott&quot;&gt;Mabbott&lt;/a&gt;, and &lt;a href=&quot;https://github.com/michaeltalyansky&quot;&gt;Michael Talyanksy&lt;/a&gt; for making and sharing &lt;a href=&quot;https://github.com/ossrs/state-threads/tree/srs&quot;&gt;State Threads&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We&#39;re really thankful to everyone in the community for helping us find bugs and improve the project. To stay in touch and keep helping our community, please check out this &lt;a href=&quot;https://github.com/ossrs/srs/contribute&quot;&gt;guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;LICENSE&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://app.fossa.com/projects/git%2Bgithub.com%2Fossrs%2Fsrs?ref=badge_small&quot;&gt;&lt;img src=&quot;https://app.fossa.com/api/projects/git%2Bgithub.com%2Fossrs%2Fsrs.svg?type=small&quot; alt=&quot;FOSSA Status&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;SRS is licenced under &lt;a href=&quot;https://github.com/ossrs/srs/raw/develop/LICENSE&quot;&gt;MIT&lt;/a&gt;, and some third-party libraries are distributed under their &lt;a href=&quot;https://ossrs.io/lts/en-us/license&quot;&gt;licenses&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://app.fossa.com/projects/git%2Bgithub.com%2Fossrs%2Fsrs?ref=badge_large&quot;&gt;&lt;img src=&quot;https://app.fossa.com/api/projects/git%2Bgithub.com%2Fossrs%2Fsrs.svg?type=large&quot; alt=&quot;&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Releases&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;2024-09-01, &lt;a href=&quot;https://github.com/ossrs/srs/releases/tag/v6.0-a1&quot;&gt;Release v6.0-a1&lt;/a&gt;, v6.0-a1, 6.0 alpha1, v6.0.155, 169636 lines.&lt;/li&gt; 
 &lt;li&gt;2024-07-27, &lt;a href=&quot;https://github.com/ossrs/srs/releases/tag/v6.0-a0&quot;&gt;Release v6.0-a0&lt;/a&gt;, v6.0-a0, 6.0 alpha0, v6.0.145, 169259 lines.&lt;/li&gt; 
 &lt;li&gt;2024-07-04, &lt;a href=&quot;https://github.com/ossrs/srs/releases/tag/v6.0-d6&quot;&gt;Release v6.0-d6&lt;/a&gt;, v6.0-d6, 6.0 dev6, v6.0.134, 168904 lines.&lt;/li&gt; 
 &lt;li&gt;2024-06-15, &lt;a href=&quot;https://github.com/ossrs/srs/releases/tag/v6.0-d5&quot;&gt;Release v6.0-d5&lt;/a&gt;, v6.0-d5, 6.0 dev5, v6.0.129, 168454 lines.&lt;/li&gt; 
 &lt;li&gt;2024-02-15, &lt;a href=&quot;https://github.com/ossrs/srs/releases/tag/v6.0-d4&quot;&gt;Release v6.0-d4&lt;/a&gt;, v6.0-d4, 6.0 dev4, v6.0.113, 167695 lines.&lt;/li&gt; 
 &lt;li&gt;2023-11-19, &lt;a href=&quot;https://github.com/ossrs/srs/releases/tag/v6.0-d3&quot;&gt;Release v6.0-d3&lt;/a&gt;, v6.0-d3, 6.0 dev3, v6.0.101, 167560 lines.&lt;/li&gt; 
 &lt;li&gt;2023-09-28, &lt;a href=&quot;https://github.com/ossrs/srs/releases/tag/v6.0-d2&quot;&gt;Release v6.0-d2&lt;/a&gt;, v6.0-d2, 6.0 dev2, v6.0.85, 167509 lines.&lt;/li&gt; 
 &lt;li&gt;2023-08-31, &lt;a href=&quot;https://github.com/ossrs/srs/releases/tag/v6.0-d1&quot;&gt;Release v6.0-d1&lt;/a&gt;, v6.0-d1, 6.0 dev1, v6.0.72, 167135 lines.&lt;/li&gt; 
 &lt;li&gt;2023-07-09, &lt;a href=&quot;https://github.com/ossrs/srs/releases/tag/v6.0-d0&quot;&gt;Release v6.0-d0&lt;/a&gt;, v6.0-d0, 6.0 dev0, v6.0.59, 166739 lines.&lt;/li&gt; 
 &lt;li&gt;2024-06-15, &lt;a href=&quot;https://github.com/ossrs/srs/releases/tag/v5.0-r3&quot;&gt;Release v5.0-r3&lt;/a&gt;, v5.0-r3, 5.0 release3, v5.0.213, 163585 lines.&lt;/li&gt; 
 &lt;li&gt;2024-04-03, &lt;a href=&quot;https://github.com/ossrs/srs/releases/tag/v5.0-r2&quot;&gt;Release v5.0-r2&lt;/a&gt;, v5.0-r2, 5.0 release2, v5.0.210, 163515 lines.&lt;/li&gt; 
 &lt;li&gt;2024-02-15, &lt;a href=&quot;https://github.com/ossrs/srs/releases/tag/v5.0-r1&quot;&gt;Release v5.0-r1&lt;/a&gt;, v5.0-r1, 5.0 release1, v5.0.208, 163441 lines.&lt;/li&gt; 
 &lt;li&gt;2023-12-30, &lt;a href=&quot;https://github.com/ossrs/srs/releases/tag/v5.0-r0&quot;&gt;Release v5.0-r0&lt;/a&gt;, v5.0-r0, 5.0 release0, v5.0.205, 163363 lines.&lt;/li&gt; 
 &lt;li&gt;2023-11-19, &lt;a href=&quot;https://github.com/ossrs/srs/releases/tag/v5.0-b7&quot;&gt;Release v5.0-b7&lt;/a&gt;, v5.0-b7, 5.0 beta7, v5.0.200, 163305 lines.&lt;/li&gt; 
 &lt;li&gt;2023-10-25, &lt;a href=&quot;https://github.com/ossrs/srs/releases/tag/v5.0-b6&quot;&gt;Release v5.0-b6&lt;/a&gt;, v5.0-b6, 5.0 beta6, v5.0.195, 163303 lines.&lt;/li&gt; 
 &lt;li&gt;2023-09-28, &lt;a href=&quot;https://github.com/ossrs/srs/releases/tag/v5.0-b5&quot;&gt;Release v5.0-b5&lt;/a&gt;, v5.0-b5, 5.0 beta5, v5.0.185, 163254 lines.&lt;/li&gt; 
 &lt;li&gt;2023-08-31, &lt;a href=&quot;https://github.com/ossrs/srs/releases/tag/v5.0-b4&quot;&gt;Release v5.0-b4&lt;/a&gt;, v5.0-b4, 5.0 beta4, v5.0.176, 162919 lines.&lt;/li&gt; 
 &lt;li&gt;2023-08-02, &lt;a href=&quot;https://github.com/ossrs/srs/releases/tag/v5.0-b3&quot;&gt;Release v5.0-b3&lt;/a&gt;, v5.0-b3, 5.0 beta3, v5.0.170, 162704 lines.&lt;/li&gt; 
 &lt;li&gt;2023-07-09, &lt;a href=&quot;https://github.com/ossrs/srs/releases/tag/v5.0-b2&quot;&gt;Release v5.0-b2&lt;/a&gt;, v5.0-b2, 5.0 beta2, v5.0.166, 162520 lines.&lt;/li&gt; 
 &lt;li&gt;2023-06-11, &lt;a href=&quot;https://github.com/ossrs/srs/releases/tag/v5.0-b1&quot;&gt;Release v5.0-b1&lt;/a&gt;, v5.0-b1, 5.0 beta1, v5.0.157, 162494 lines.&lt;/li&gt; 
 &lt;li&gt;2023-05-14, &lt;a href=&quot;https://github.com/ossrs/srs/releases/tag/v5.0-b0&quot;&gt;Release v5.0-b0&lt;/a&gt;, v5.0-b0, 5.0 beta0, v5.0.155, 162600 lines.&lt;/li&gt; 
 &lt;li&gt;2023-03-23, &lt;a href=&quot;https://github.com/ossrs/srs/releases/tag/v5.0-a5&quot;&gt;Release v5.0-a5&lt;/a&gt;, v5.0-a5, 5.0 alpha5, v5.0.148, 162066 lines.&lt;/li&gt; 
 &lt;li&gt;2023-02-12, &lt;a href=&quot;https://github.com/ossrs/srs/releases/tag/v5.0-a4&quot;&gt;Release v5.0-a4&lt;/a&gt;, v5.0-a4, 5.0 alpha4, v5.0.141, 161897 lines.&lt;/li&gt; 
 &lt;li&gt;2023-01-02, &lt;a href=&quot;https://github.com/ossrs/srs/releases/tag/v5.0-a3&quot;&gt;Release v5.0-a3&lt;/a&gt;, v5.0-a3, 5.0 alpha3, v5.0.128, 161327 lines.&lt;/li&gt; 
 &lt;li&gt;2022-12-18, &lt;a href=&quot;https://github.com/ossrs/srs/releases/tag/v5.0-a2&quot;&gt;Release v5.0-a2&lt;/a&gt;, v5.0-a2, 5.0 alpha2, v5.0.112, 161233 lines.&lt;/li&gt; 
 &lt;li&gt;2022-12-01, &lt;a href=&quot;https://github.com/ossrs/srs/releases/tag/v5.0-a1&quot;&gt;Release v5.0-a1&lt;/a&gt;, v5.0-a1, 5.0 alpha1, v5.0.100, 160817 lines.&lt;/li&gt; 
 &lt;li&gt;2022-11-25, &lt;a href=&quot;https://github.com/ossrs/srs/releases/tag/v5.0-a0&quot;&gt;Release v5.0-a0&lt;/a&gt;, v5.0-a0, 5.0 alpha0, v5.0.98, 159813 lines.&lt;/li&gt; 
 &lt;li&gt;2022-11-22, Release &lt;a href=&quot;https://github.com/ossrs/srs/releases/tag/v4.0-r4&quot;&gt;v4.0-r4&lt;/a&gt;, v4.0-r4, 4.0 release4, v4.0.268, 145482 lines.&lt;/li&gt; 
 &lt;li&gt;2022-09-16, Release &lt;a href=&quot;https://github.com/ossrs/srs/releases/tag/v4.0-r3&quot;&gt;v4.0-r3&lt;/a&gt;, v4.0-r3, 4.0 release3, v4.0.265, 145328 lines.&lt;/li&gt; 
 &lt;li&gt;2022-08-24, Release &lt;a href=&quot;https://github.com/ossrs/srs/releases/tag/v4.0-r2&quot;&gt;v4.0-r2&lt;/a&gt;, v4.0-r2, 4.0 release2, v4.0.257, 144890 lines.&lt;/li&gt; 
 &lt;li&gt;2022-06-29, Release &lt;a href=&quot;https://github.com/ossrs/srs/releases/tag/v4.0-r1&quot;&gt;v4.0-r1&lt;/a&gt;, v4.0-r1, 4.0 release1, v4.0.253, 144680 lines.&lt;/li&gt; 
 &lt;li&gt;2022-06-11, Release &lt;a href=&quot;https://github.com/ossrs/srs/releases/tag/v4.0-r0&quot;&gt;v4.0-r0&lt;/a&gt;, v4.0-r0, 4.0 release0, v4.0.252, 144680 lines.&lt;/li&gt; 
 &lt;li&gt;2020-06-27, &lt;a href=&quot;https://github.com/ossrs/srs/releases/tag/v3.0-r0&quot;&gt;Release v3.0-r0&lt;/a&gt;, 3.0 release0, 3.0.141, 122674 lines.&lt;/li&gt; 
 &lt;li&gt;2020-02-02, &lt;a href=&quot;https://github.com/ossrs/srs/releases/tag/v3.0-b0&quot;&gt;Release v3.0-b0&lt;/a&gt;, 3.0 beta0, 3.0.112, 121709 lines.&lt;/li&gt; 
 &lt;li&gt;2019-10-04, &lt;a href=&quot;https://github.com/ossrs/srs/releases/tag/v3.0-a0&quot;&gt;Release v3.0-a0&lt;/a&gt;, 3.0 alpha0, 3.0.56, 107946 lines.&lt;/li&gt; 
 &lt;li&gt;2017-03-03, &lt;a href=&quot;https://github.com/ossrs/srs/releases/tag/v2.0-r0&quot;&gt;Release v2.0-r0&lt;/a&gt;, 2.0 release0, 2.0.234, 86373 lines.&lt;/li&gt; 
 &lt;li&gt;2016-08-06, &lt;a href=&quot;https://github.com/ossrs/srs/releases/tag/v2.0-b0&quot;&gt;Release v2.0-b0&lt;/a&gt;, 2.0 beta0, 2.0.210, 89704 lines.&lt;/li&gt; 
 &lt;li&gt;2015-08-23, &lt;a href=&quot;https://github.com/ossrs/srs/releases/tag/v2.0-a0&quot;&gt;Release v2.0-a0&lt;/a&gt;, 2.0 alpha0, 2.0.185, 89022 lines.&lt;/li&gt; 
 &lt;li&gt;2014-12-05, &lt;a href=&quot;https://github.com/ossrs/srs/releases/tag/v1.0-r0&quot;&gt;Release v1.0-r0&lt;/a&gt;, all bug fixed, 1.0.10, 59391 lines.&lt;/li&gt; 
 &lt;li&gt;2014-10-09, &lt;a href=&quot;https://github.com/ossrs/srs/releases/tag/v0.9.8&quot;&gt;Release v0.9.8&lt;/a&gt;, all bug fixed, 1.0.0, 59316 lines.&lt;/li&gt; 
 &lt;li&gt;2014-04-07, &lt;a href=&quot;https://github.com/ossrs/srs/releases/tag/v0.9.1&quot;&gt;Release v0.9.1&lt;/a&gt;, live streaming. 30000 lines.&lt;/li&gt; 
 &lt;li&gt;2013-10-23, &lt;a href=&quot;https://github.com/ossrs/srs/releases/tag/v0.1.0&quot;&gt;Release v0.1.0&lt;/a&gt;, rtmp. 8287 lines.&lt;/li&gt; 
 &lt;li&gt;2013-10-17, Created.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;p&gt;Please read &lt;a href=&quot;https://raw.githubusercontent.com/ossrs/srs/develop/trunk/doc/Features.md#features&quot;&gt;FEATURES&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a name=&quot;history&quot;&gt;&lt;/a&gt; &lt;a name=&quot;change-logs&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Changelog&lt;/h2&gt; 
&lt;p&gt;Please read &lt;a href=&quot;https://raw.githubusercontent.com/ossrs/srs/develop/trunk/doc/CHANGELOG.md#changelog&quot;&gt;CHANGELOG&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Performance&lt;/h2&gt; 
&lt;p&gt;Please read &lt;a href=&quot;https://raw.githubusercontent.com/ossrs/srs/develop/trunk/doc/PERFORMANCE.md#performance&quot;&gt;PERFORMANCE&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Architecture&lt;/h2&gt; 
&lt;p&gt;Please read &lt;a href=&quot;https://raw.githubusercontent.com/ossrs/srs/develop/trunk/doc/Architecture.md#architecture&quot;&gt;ARCHITECTURE&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Ports&lt;/h2&gt; 
&lt;p&gt;Please read &lt;a href=&quot;https://raw.githubusercontent.com/ossrs/srs/develop/trunk/doc/Resources.md#ports&quot;&gt;PORTS&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;APIs&lt;/h2&gt; 
&lt;p&gt;Please read &lt;a href=&quot;https://raw.githubusercontent.com/ossrs/srs/develop/trunk/doc/Resources.md#apis&quot;&gt;APIS&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Mirrors&lt;/h2&gt; 
&lt;p&gt;Please read &lt;a href=&quot;https://raw.githubusercontent.com/ossrs/srs/develop/trunk/doc/Resources.md#mirrors&quot;&gt;MIRRORS&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Dockers&lt;/h2&gt; 
&lt;p&gt;Please read &lt;a href=&quot;https://raw.githubusercontent.com/ossrs/srs/develop/trunk/doc/Dockers.md&quot;&gt;DOCKERS&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Beijing, 2013.10&lt;br&gt; Winlin&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>electron/electron</title>
      <link>https://github.com/electron/electron</link>
      <description>&lt;p&gt;Build cross-platform desktop apps with JavaScript, HTML, and CSS&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&quot;https://electronjs.org&quot;&gt;&lt;img src=&quot;https://electronjs.org/images/electron-logo.svg?sanitize=true&quot; alt=&quot;Electron Logo&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/electron/electron/actions/workflows/build.yml&quot;&gt;&lt;img src=&quot;https://github.com/electron/electron/actions/workflows/build.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build Status&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://discord.gg/electronjs&quot;&gt;&lt;img src=&quot;https://img.shields.io/discord/745037351163527189?color=%237289DA&amp;amp;label=chat&amp;amp;logo=discord&amp;amp;logoColor=white&quot; alt=&quot;Electron Discord Invite&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;📝&lt;/span&gt; Available Translations: 🇨🇳 🇧🇷 🇪🇸 🇯🇵 🇷🇺 🇫🇷 🇺🇸 🇩🇪. View these docs in other languages on our &lt;a href=&quot;https://crowdin.com/project/electron&quot;&gt;Crowdin&lt;/a&gt; project.&lt;/p&gt; 
&lt;p&gt;The Electron framework lets you write cross-platform desktop applications using JavaScript, HTML and CSS. It is based on &lt;a href=&quot;https://nodejs.org/&quot;&gt;Node.js&lt;/a&gt; and &lt;a href=&quot;https://www.chromium.org&quot;&gt;Chromium&lt;/a&gt; and is used by the &lt;a href=&quot;https://github.com/Microsoft/vscode/&quot;&gt;Visual Studio Code&lt;/a&gt; and many other &lt;a href=&quot;https://electronjs.org/apps&quot;&gt;apps&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Follow &lt;a href=&quot;https://twitter.com/electronjs&quot;&gt;@electronjs&lt;/a&gt; on Twitter for important announcements.&lt;/p&gt; 
&lt;p&gt;This project adheres to the Contributor Covenant &lt;a href=&quot;https://github.com/electron/electron/tree/main/CODE_OF_CONDUCT.md&quot;&gt;code of conduct&lt;/a&gt;. By participating, you are expected to uphold this code. Please report unacceptable behavior to &lt;a href=&quot;mailto:coc@electronjs.org&quot;&gt;coc@electronjs.org&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;To install prebuilt Electron binaries, use &lt;a href=&quot;https://docs.npmjs.com/&quot;&gt;&lt;code&gt;npm&lt;/code&gt;&lt;/a&gt;. The preferred method is to install Electron as a development dependency in your app:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;npm install electron --save-dev
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more installation options and troubleshooting tips, see &lt;a href=&quot;https://raw.githubusercontent.com/electron/electron/main/docs/tutorial/installation.md&quot;&gt;installation&lt;/a&gt;. For info on how to manage Electron versions in your apps, see &lt;a href=&quot;https://raw.githubusercontent.com/electron/electron/main/docs/tutorial/electron-versioning.md&quot;&gt;Electron versioning&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Platform support&lt;/h2&gt; 
&lt;p&gt;Each Electron release provides binaries for macOS, Windows, and Linux.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;macOS (Big Sur and up): Electron provides 64-bit Intel and Apple Silicon / ARM binaries for macOS.&lt;/li&gt; 
 &lt;li&gt;Windows (Windows 10 and up): Electron provides &lt;code&gt;ia32&lt;/code&gt; (&lt;code&gt;x86&lt;/code&gt;), &lt;code&gt;x64&lt;/code&gt; (&lt;code&gt;amd64&lt;/code&gt;), and &lt;code&gt;arm64&lt;/code&gt; binaries for Windows. Windows on ARM support was added in Electron 5.0.8. Support for Windows 7, 8 and 8.1 was &lt;a href=&quot;https://www.electronjs.org/blog/windows-7-to-8-1-deprecation-notice&quot;&gt;removed in Electron 23, in line with Chromium&#39;s Windows deprecation policy&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Linux: The prebuilt binaries of Electron are built on Ubuntu 20.04. They have also been verified to work on: 
  &lt;ul&gt; 
   &lt;li&gt;Ubuntu 18.04 and newer&lt;/li&gt; 
   &lt;li&gt;Fedora 32 and newer&lt;/li&gt; 
   &lt;li&gt;Debian 10 and newer&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Quick start &amp;amp; Electron Fiddle&lt;/h2&gt; 
&lt;p&gt;Use &lt;a href=&quot;https://github.com/electron/fiddle&quot;&gt;&lt;code&gt;Electron Fiddle&lt;/code&gt;&lt;/a&gt; to build, run, and package small Electron experiments, to see code examples for all of Electron&#39;s APIs, and to try out different versions of Electron. It&#39;s designed to make the start of your journey with Electron easier.&lt;/p&gt; 
&lt;p&gt;Alternatively, clone and run the &lt;a href=&quot;https://github.com/electron/electron-quick-start&quot;&gt;electron/electron-quick-start&lt;/a&gt; repository to see a minimal Electron app in action:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;git clone https://github.com/electron/electron-quick-start
cd electron-quick-start
npm install
npm start
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Resources for learning Electron&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://electronjs.org/docs&quot;&gt;electronjs.org/docs&lt;/a&gt; - All of Electron&#39;s documentation&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/electron/fiddle&quot;&gt;electron/fiddle&lt;/a&gt; - A tool to build, run, and package small Electron experiments&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/electron/electron-quick-start&quot;&gt;electron/electron-quick-start&lt;/a&gt; - A very basic starter Electron app&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://electronjs.org/community#boilerplates&quot;&gt;electronjs.org/community#boilerplates&lt;/a&gt; - Sample starter apps created by the community&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Programmatic usage&lt;/h2&gt; 
&lt;p&gt;Most people use Electron from the command line, but if you require &lt;code&gt;electron&lt;/code&gt; inside your &lt;strong&gt;Node app&lt;/strong&gt; (not your Electron app) it will return the file path to the binary. Use this to spawn Electron from Node scripts:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;const electron = require(&#39;electron&#39;)
const proc = require(&#39;node:child_process&#39;)

// will print something similar to /Users/maf/.../Electron
console.log(electron)

// spawn Electron
const child = proc.spawn(electron)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Mirrors&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://npmmirror.com/mirrors/electron/&quot;&gt;China&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See the &lt;a href=&quot;https://www.electronjs.org/docs/latest/tutorial/installation#mirror&quot;&gt;Advanced Installation Instructions&lt;/a&gt; to learn how to use a custom mirror.&lt;/p&gt; 
&lt;h2&gt;Documentation translations&lt;/h2&gt; 
&lt;p&gt;We crowdsource translations for our documentation via &lt;a href=&quot;https://crowdin.com/project/electron&quot;&gt;Crowdin&lt;/a&gt;. We currently accept translations for Chinese (Simplified), French, German, Japanese, Portuguese, Russian, and Spanish.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;If you are interested in reporting/fixing issues and contributing directly to the code base, please see &lt;a href=&quot;https://raw.githubusercontent.com/electron/electron/main/CONTRIBUTING.md&quot;&gt;CONTRIBUTING.md&lt;/a&gt; for more information on what we&#39;re looking for and how to get started.&lt;/p&gt; 
&lt;h2&gt;Community&lt;/h2&gt; 
&lt;p&gt;Info on reporting bugs, getting help, finding third-party tools and sample apps, and more can be found on the &lt;a href=&quot;https://www.electronjs.org/community&quot;&gt;Community page&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/electron/electron/raw/main/LICENSE&quot;&gt;MIT&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;When using Electron logos, make sure to follow &lt;a href=&quot;https://trademark-policy.openjsf.org/&quot;&gt;OpenJS Foundation Trademark Policy&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>protocolbuffers/protobuf</title>
      <link>https://github.com/protocolbuffers/protobuf</link>
      <description>&lt;p&gt;Protocol Buffers - Google&#39;s data interchange format&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Protocol Buffers - Google&#39;s data interchange format&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://securityscorecards.dev/viewer/?uri=github.com/protocolbuffers/protobuf&quot;&gt;&lt;img src=&quot;https://api.securityscorecards.dev/projects/github.com/protocolbuffers/protobuf/badge&quot; alt=&quot;OpenSSF Scorecard&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Copyright 2023 Google LLC&lt;/p&gt; 
&lt;h2&gt;Overview&lt;/h2&gt; 
&lt;p&gt;Protocol Buffers (a.k.a., protobuf) are Google&#39;s language-neutral, platform-neutral, extensible mechanism for serializing structured data. You can learn more about it in &lt;a href=&quot;https://protobuf.dev&quot;&gt;protobuf&#39;s documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;This README file contains protobuf installation instructions. To install protobuf, you need to install the protocol compiler (used to compile .proto files) and the protobuf runtime for your chosen programming language.&lt;/p&gt; 
&lt;h2&gt;Working With Protobuf Source Code&lt;/h2&gt; 
&lt;p&gt;Most users will find working from &lt;a href=&quot;https://github.com/protocolbuffers/protobuf/releases&quot;&gt;supported releases&lt;/a&gt; to be the easiest path.&lt;/p&gt; 
&lt;p&gt;If you choose to work from the head revision of the main branch your build will occasionally be broken by source-incompatible changes and insufficiently-tested (and therefore broken) behavior.&lt;/p&gt; 
&lt;p&gt;If you are using C++ or otherwise need to build protobuf from source as a part of your project, you should pin to a release commit on a release branch.&lt;/p&gt; 
&lt;p&gt;This is because even release branches can experience some instability in between release commits.&lt;/p&gt; 
&lt;h3&gt;Bazel with Bzlmod&lt;/h3&gt; 
&lt;p&gt;Protobuf supports &lt;a href=&quot;https://bazel.build/external/module&quot;&gt;Bzlmod&lt;/a&gt; with Bazel 7 +. Users should specify a dependency on protobuf in their MODULE.bazel file as follows.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;bazel_dep(name = &quot;protobuf&quot;, version = &amp;lt;VERSION&amp;gt;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Users can optionally override the repo name, such as for compatibility with WORKSPACE.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;bazel_dep(name = &quot;protobuf&quot;, version = &amp;lt;VERSION&amp;gt;, repo_name = &quot;com_google_protobuf&quot;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Bazel with WORKSPACE&lt;/h3&gt; 
&lt;p&gt;Users can also add the following to their legacy &lt;a href=&quot;https://bazel.build/external/overview#workspace-system&quot;&gt;WORKSPACE&lt;/a&gt; file.&lt;/p&gt; 
&lt;p&gt;Note that with the release of 30.x there are a few more load statements to properly set up rules_java and rules_python.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;http_archive(
    name = &quot;com_google_protobuf&quot;,
    strip_prefix = &quot;protobuf-VERSION&quot;,
    sha256 = ...,
    url = ...,
)

load(&quot;@com_google_protobuf//:protobuf_deps.bzl&quot;, &quot;protobuf_deps&quot;)

protobuf_deps()

load(&quot;@rules_java//java:rules_java_deps.bzl&quot;, &quot;rules_java_dependencies&quot;)

rules_java_dependencies()

load(&quot;@rules_java//java:repositories.bzl&quot;, &quot;rules_java_toolchains&quot;)

rules_java_toolchains()

load(&quot;@rules_python//python:repositories.bzl&quot;, &quot;py_repositories&quot;)

py_repositories()
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Protobuf Compiler Installation&lt;/h2&gt; 
&lt;p&gt;The protobuf compiler is written in C++. If you are using C++, please follow the &lt;a href=&quot;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/src/README.md&quot;&gt;C++ Installation Instructions&lt;/a&gt; to install protoc along with the C++ runtime.&lt;/p&gt; 
&lt;p&gt;For non-C++ users, the simplest way to install the protocol compiler is to download a pre-built binary from our &lt;a href=&quot;https://github.com/protocolbuffers/protobuf/releases&quot;&gt;GitHub release page&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;In the downloads section of each release, you can find pre-built binaries in zip packages: &lt;code&gt;protoc-$VERSION-$PLATFORM.zip&lt;/code&gt;. It contains the protoc binary as well as a set of standard &lt;code&gt;.proto&lt;/code&gt; files distributed along with protobuf.&lt;/p&gt; 
&lt;p&gt;If you are looking for an old version that is not available in the release page, check out the &lt;a href=&quot;https://repo1.maven.org/maven2/com/google/protobuf/protoc/&quot;&gt;Maven repository&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;These pre-built binaries are only provided for released versions. If you want to use the github main version at HEAD, or you need to modify protobuf code, or you are using C++, it&#39;s recommended to build your own protoc binary from source.&lt;/p&gt; 
&lt;p&gt;If you would like to build protoc binary from source, see the &lt;a href=&quot;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/src/README.md&quot;&gt;C++ Installation Instructions&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Protobuf Runtime Installation&lt;/h2&gt; 
&lt;p&gt;Protobuf supports several different programming languages. For each programming language, you can find instructions in the corresponding source directory about how to install protobuf runtime for that specific language:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Language&lt;/th&gt; 
   &lt;th&gt;Source&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;C++ (include C++ runtime and protoc)&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/src&quot;&gt;src&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Java&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/java&quot;&gt;java&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Python&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/python&quot;&gt;python&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Objective-C&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/objectivec&quot;&gt;objectivec&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;C#&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/csharp&quot;&gt;csharp&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ruby&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/ruby&quot;&gt;ruby&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Go&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/protocolbuffers/protobuf-go&quot;&gt;protocolbuffers/protobuf-go&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;PHP&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/php&quot;&gt;php&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Dart&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/dart-lang/protobuf&quot;&gt;dart-lang/protobuf&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;JavaScript&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/protocolbuffers/protobuf-javascript&quot;&gt;protocolbuffers/protobuf-javascript&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;p&gt;The best way to learn how to use protobuf is to follow the &lt;a href=&quot;https://protobuf.dev/getting-started&quot;&gt;tutorials in our developer guide&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you want to learn from code examples, take a look at the examples in the &lt;a href=&quot;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/examples&quot;&gt;examples&lt;/a&gt; directory.&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;The complete documentation is available at the &lt;a href=&quot;https://protobuf.dev&quot;&gt;Protocol Buffers doc site&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Support Policy&lt;/h2&gt; 
&lt;p&gt;Read about our &lt;a href=&quot;https://protobuf.dev/version-support/&quot;&gt;version support policy&lt;/a&gt; to stay current on support timeframes for the language libraries.&lt;/p&gt; 
&lt;h2&gt;Developer Community&lt;/h2&gt; 
&lt;p&gt;To be alerted to upcoming changes in Protocol Buffers and connect with protobuf developers and users, &lt;a href=&quot;https://groups.google.com/g/protobuf&quot;&gt;join the Google Group&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>simdjson/simdjson</title>
      <link>https://github.com/simdjson/simdjson</link>
      <description>&lt;p&gt;Parsing gigabytes of JSON per second : used by Facebook/Meta Velox, the Node.js runtime, ClickHouse, WatermelonDB, Apache Doris, Milvus, StarRocks&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&quot;https://bugs.chromium.org/p/oss-fuzz/issues/list?sort=-opened&amp;amp;can=1&amp;amp;q=proj:simdjson&quot;&gt;&lt;img src=&quot;https://oss-fuzz-build-logs.storage.googleapis.com/badges/simdjson.svg?sanitize=true&quot; alt=&quot;Fuzzing Status&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/simdjson/simdjson/master/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/License-Apache%202-blue.svg?sanitize=true&quot; alt=&quot;&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/simdjson/simdjson/master/LICENSE-MIT&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/License-MIT-blue.svg?sanitize=true&quot; alt=&quot;&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://simdjson.github.io/simdjson/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/docs-doxygen-green.svg?sanitize=true&quot; alt=&quot;Doxygen Documentation&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;simdjson : Parsing gigabytes of JSON per second&lt;/h1&gt; 
&lt;img src=&quot;https://raw.githubusercontent.com/simdjson/simdjson/master/images/logo.png&quot; width=&quot;10%&quot; style=&quot;float: right&quot;&gt; JSON is everywhere on the Internet. Servers spend a *lot* of time parsing it. We need a fresh approach. The simdjson library uses commonly available SIMD instructions and microparallel algorithms to parse JSON 4x faster than RapidJSON and 25x faster than JSON for Modern C++. 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Fast:&lt;/strong&gt; Over 4x faster than commonly used production-grade JSON parsers.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Record Breaking Features:&lt;/strong&gt; Minify JSON at 6 GB/s, validate UTF-8 at 13 GB/s, NDJSON at 3.5 GB/s.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Easy:&lt;/strong&gt; First-class, easy to use and carefully documented APIs.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Strict:&lt;/strong&gt; Full JSON and UTF-8 validation, lossless parsing. Performance with no compromises.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Automatic:&lt;/strong&gt; Selects a CPU-tailored parser at runtime. No configuration needed.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Reliable:&lt;/strong&gt; From memory allocation to error handling, simdjson&#39;s design avoids surprises.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Peer Reviewed:&lt;/strong&gt; Our research appears in venues like VLDB Journal, Software: Practice and Experience.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This library is part of the &lt;a href=&quot;https://awesomecpp.com&quot;&gt;Awesome Modern C++&lt;/a&gt; list.&lt;/p&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/simdjson/simdjson/master/#real-world-usage&quot;&gt;Real-world usage&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/simdjson/simdjson/master/#quick-start&quot;&gt;Quick Start&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/simdjson/simdjson/master/#documentation&quot;&gt;Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/simdjson/simdjson/master/#godbolt&quot;&gt;Godbolt&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/simdjson/simdjson/master/#performance-results&quot;&gt;Performance results&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/simdjson/simdjson/master/#packages&quot;&gt;Packages&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/simdjson/simdjson/master/#bindings-and-ports-of-simdjson&quot;&gt;Bindings and Ports of simdjson&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/simdjson/simdjson/master/#about-simdjson&quot;&gt;About simdjson&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/simdjson/simdjson/master/#funding&quot;&gt;Funding&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/simdjson/simdjson/master/#contributing-to-simdjson&quot;&gt;Contributing to simdjson&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/simdjson/simdjson/master/#license&quot;&gt;License&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Real-world usage&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://nodejs.org/&quot;&gt;Node.js&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ClickHouse/ClickHouse&quot;&gt;ClickHouse&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://velox-lib.io&quot;&gt;Meta Velox&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/google/paxml&quot;&gt;Google Pax&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/milvus-io/milvus&quot;&gt;milvus&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://questdb.io/blog/questdb-release-8-0-3/&quot;&gt;QuestDB&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/aras-p/ClangBuildAnalyzer&quot;&gt;Clang Build Analyzer&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Shopify/heap-profiler&quot;&gt;Shopify HeapProfiler&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/StarRocks/starrocks&quot;&gt;StarRocks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/FishStore&quot;&gt;Microsoft FishStore&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/intel/pcm&quot;&gt;Intel PCM&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Nozbe/WatermelonDB&quot;&gt;WatermelonDB&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/apache/doris&quot;&gt;Apache Doris&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/dgraph-io/dgraph&quot;&gt;Dgraph&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/unum-cloud/ujrpc&quot;&gt;UJRPC&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/spnda/fastgltf&quot;&gt;fastgltf&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/tenzir/vast&quot;&gt;vast&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ada-url/ada&quot;&gt;ada-url&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/adamritter/fastgron&quot;&gt;fastgron&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://wasmedge.org&quot;&gt;WasmEdge&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/logicalclocks/rondb&quot;&gt;RonDB&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If you are planning to use simdjson in a product, please work from one of our releases.&lt;/p&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;p&gt;The simdjson library is easily consumable with a single .h and .cpp file.&lt;/p&gt; 
&lt;ol start=&quot;0&quot;&gt; 
 &lt;li&gt; &lt;p&gt;Prerequisites: &lt;code&gt;g++&lt;/code&gt; (version 7 or better) or &lt;code&gt;clang++&lt;/code&gt; (version 6 or better), and a 64-bit system with a command-line shell (e.g., Linux, macOS, freeBSD). We also support programming environments like Visual Studio and Xcode, but different steps are needed. Users of clang++ may need to specify the C++ version (e.g., &lt;code&gt;c++ -std=c++17&lt;/code&gt;) since clang++ tends to default on C++98.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Pull &lt;a href=&quot;https://raw.githubusercontent.com/simdjson/simdjson/master/singleheader/simdjson.h&quot;&gt;simdjson.h&lt;/a&gt; and &lt;a href=&quot;https://raw.githubusercontent.com/simdjson/simdjson/master/singleheader/simdjson.cpp&quot;&gt;simdjson.cpp&lt;/a&gt; into a directory, along with the sample file &lt;a href=&quot;https://raw.githubusercontent.com/simdjson/simdjson/master/jsonexamples/twitter.json&quot;&gt;twitter.json&lt;/a&gt;. You can download them with the &lt;code&gt;wget&lt;/code&gt; utility:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;wget https://raw.githubusercontent.com/simdjson/simdjson/master/singleheader/simdjson.h https://raw.githubusercontent.com/simdjson/simdjson/master/singleheader/simdjson.cpp https://raw.githubusercontent.com/simdjson/simdjson/master/jsonexamples/twitter.json
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Create &lt;code&gt;quickstart.cpp&lt;/code&gt;:&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;#include &amp;lt;iostream&amp;gt;
#include &quot;simdjson.h&quot;
using namespace simdjson;
int main(void) {
    ondemand::parser parser;
    padded_string json = padded_string::load(&quot;twitter.json&quot;);
    ondemand::document tweets = parser.iterate(json);
    std::cout &amp;lt;&amp;lt; uint64_t(tweets[&quot;search_metadata&quot;][&quot;count&quot;]) &amp;lt;&amp;lt; &quot; results.&quot; &amp;lt;&amp;lt; std::endl;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start=&quot;3&quot;&gt; 
 &lt;li&gt;&lt;code&gt;c++ -o quickstart quickstart.cpp simdjson.cpp&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;./quickstart&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code&gt; 100 results.
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;Usage documentation is available:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/simdjson/simdjson/master/doc/basics.md&quot;&gt;Basics&lt;/a&gt; is an overview of how to use simdjson and its APIs.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/simdjson/simdjson/master/doc/performance.md&quot;&gt;Performance&lt;/a&gt; shows some more advanced scenarios and how to tune for them.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/simdjson/simdjson/master/doc/implementation-selection.md&quot;&gt;Implementation Selection&lt;/a&gt; describes runtime CPU detection and how you can work with it.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://simdjson.github.io/simdjson/&quot;&gt;API&lt;/a&gt; contains the automatically generated API documentation.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Godbolt&lt;/h2&gt; 
&lt;p&gt;Some users may want to browse code along with the compiled assembly. You want to check out the following lists of examples:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://godbolt.org/z/7G5qE4sr9&quot;&gt;simdjson examples with errors handled through exceptions&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://godbolt.org/z/e9dWb9E4v&quot;&gt;simdjson examples with errors without exceptions&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Performance results&lt;/h2&gt; 
&lt;p&gt;The simdjson library uses three-quarters less instructions than state-of-the-art parser &lt;a href=&quot;https://rapidjson.org&quot;&gt;RapidJSON&lt;/a&gt;. To our knowledge, simdjson is the first fully-validating JSON parser to run at &lt;a href=&quot;https://en.wikipedia.org/wiki/Gigabyte&quot;&gt;gigabytes per second&lt;/a&gt; (GB/s) on commodity processors. It can parse millions of JSON documents per second on a single core.&lt;/p&gt; 
&lt;p&gt;The following figure represents parsing speed in GB/s for parsing various files on an Intel Skylake processor (3.4 GHz) using the GNU GCC 10 compiler (with the -O3 flag). We compare against the best and fastest C++ libraries on benchmarks that load and process the data. The simdjson library offers full unicode (&lt;a href=&quot;https://en.wikipedia.org/wiki/UTF-8&quot;&gt;UTF-8&lt;/a&gt;) validation and exact number parsing.&lt;/p&gt; 
&lt;img src=&quot;https://raw.githubusercontent.com/simdjson/simdjson/master/doc/rome.png&quot; width=&quot;60%&quot;&gt; 
&lt;p&gt;The simdjson library offers high speed whether it processes tiny files (e.g., 300 bytes) or larger files (e.g., 3MB). The following plot presents parsing speed for &lt;a href=&quot;https://github.com/simdjson/simdjson_experiments_vldb2019/raw/master/experiments/growing/gen.py&quot;&gt;synthetic files over various sizes generated with a script&lt;/a&gt; on a 3.4 GHz Skylake processor (GNU GCC 9, -O3).&lt;/p&gt; 
&lt;img src=&quot;https://raw.githubusercontent.com/simdjson/simdjson/master/doc/growing.png&quot; width=&quot;60%&quot;&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/simdjson/simdjson_experiments_vldb2019&quot;&gt;All our experiments are reproducible&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For NDJSON files, we can exceed 3 GB/s with &lt;a href=&quot;https://github.com/simdjson/simdjson/raw/master/doc/parse_many.md&quot;&gt;our multithreaded parsing functions&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Packages&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://repology.org/project/simdjson/versions&quot;&gt;&lt;img src=&quot;https://repology.org/badge/vertical-allrepos/simdjson.svg?sanitize=true&quot; alt=&quot;Packaging status&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Bindings and Ports of simdjson&lt;/h2&gt; 
&lt;p&gt;We distinguish between &quot;bindings&quot; (which just wrap the C++ code) and a port to another programming language (which reimplements everything).&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/michaeleisel/zippyjson&quot;&gt;ZippyJSON&lt;/a&gt;: Swift bindings for the simdjson project.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/gerrymanoim/libpy_simdjson/&quot;&gt;libpy_simdjson&lt;/a&gt;: high-speed Python bindings for simdjson using &lt;a href=&quot;https://github.com/quantopian/libpy&quot;&gt;libpy&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/TkTech/pysimdjson&quot;&gt;pysimdjson&lt;/a&gt;: Python bindings for the simdjson project.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/TeskaLabs/cysimdjson&quot;&gt;cysimdjson&lt;/a&gt;: high-speed Python bindings for the simdjson project.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/simd-lite&quot;&gt;simdjson-rs&lt;/a&gt;: Rust port.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/SunDoge/simdjson-rust&quot;&gt;simdjson-rust&lt;/a&gt;: Rust wrapper (bindings).&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/EgorBo/SimdJsonSharp&quot;&gt;SimdJsonSharp&lt;/a&gt;: C# version for .NET Core (bindings and full port).&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/luizperes/simdjson_nodejs&quot;&gt;simdjson_nodejs&lt;/a&gt;: Node.js bindings for the simdjson project.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/crazyxman/simdjson_php&quot;&gt;simdjson_php&lt;/a&gt;: PHP bindings for the simdjson project.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/saka1/simdjson_ruby&quot;&gt;simdjson_ruby&lt;/a&gt;: Ruby bindings for the simdjson project.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/anilmaurya/fast_jsonparser&quot;&gt;fast_jsonparser&lt;/a&gt;: Ruby bindings for the simdjson project.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/minio/simdjson-go&quot;&gt;simdjson-go&lt;/a&gt;: Go port using Golang assembly.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/eddelbuettel/rcppsimdjson&quot;&gt;rcppsimdjson&lt;/a&gt;: R bindings.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ChomperT/simdjson_erlang&quot;&gt;simdjson_erlang&lt;/a&gt;: erlang bindings.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/saleyn/simdjsone&quot;&gt;simdjsone&lt;/a&gt;: erlang bindings.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/FourierTransformer/lua-simdjson&quot;&gt;lua-simdjson&lt;/a&gt;: lua bindings.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://hackage.haskell.org/package/hermes-json&quot;&gt;hermes-json&lt;/a&gt;: haskell bindings.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/travisstaloch/simdjzon&quot;&gt;simdjzon&lt;/a&gt;: zig port.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/rawleyfowler/JSON-simd&quot;&gt;JSON-Simd&lt;/a&gt;: Raku bindings.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://metacpan.org/pod/JSON::SIMD&quot;&gt;JSON::SIMD&lt;/a&gt;: Perl bindings; fully-featured JSON module that uses simdjson for decoding.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/sainttttt/gemmaJSON&quot;&gt;gemmaJSON&lt;/a&gt;: Nim JSON parser based on simdjson bindings.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/simdjson/simdjson-java&quot;&gt;simdjson-java&lt;/a&gt;: Java port.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;About simdjson&lt;/h2&gt; 
&lt;p&gt;The simdjson library takes advantage of modern microarchitectures, parallelizing with SIMD vector instructions, reducing branch misprediction, and reducing data dependency to take advantage of each CPU&#39;s multiple execution cores.&lt;/p&gt; 
&lt;p&gt;Our default front-end is called On-Demand, and we wrote a paper about it:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;John Keiser, Daniel Lemire, &lt;a href=&quot;http://arxiv.org/abs/2312.17149&quot;&gt;On-Demand JSON: A Better Way to Parse Documents?&lt;/a&gt;, Software: Practice and Experience 54 (6), 2024.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Some people &lt;a href=&quot;https://arxiv.org/abs/1902.08318&quot;&gt;enjoy reading the first (2019) simdjson paper&lt;/a&gt;: A description of the design and implementation of simdjson is in our research article:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Geoff Langdale, Daniel Lemire, &lt;a href=&quot;https://arxiv.org/abs/1902.08318&quot;&gt;Parsing Gigabytes of JSON per Second&lt;/a&gt;, VLDB Journal 28 (6), 2019.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We have an in-depth paper focused on the UTF-8 validation:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;John Keiser, Daniel Lemire, &lt;a href=&quot;https://arxiv.org/abs/2010.03090&quot;&gt;Validating UTF-8 In Less Than One Instruction Per Byte&lt;/a&gt;, Software: Practice &amp;amp; Experience 51 (5), 2021.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We also have an informal &lt;a href=&quot;https://branchfree.org/2019/02/25/paper-parsing-gigabytes-of-json-per-second/&quot;&gt;blog post providing some background and context&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For the video inclined, &lt;br&gt; &lt;a href=&quot;http://www.youtube.com/watch?v=wlvKAT7SZIQ&quot;&gt;&lt;img src=&quot;http://img.youtube.com/vi/wlvKAT7SZIQ/0.jpg&quot; alt=&quot;simdjson at QCon San Francisco 2019&quot;&gt;&lt;/a&gt;&lt;br&gt; (It was the best voted talk, we&#39;re kinda proud of it.)&lt;/p&gt; 
&lt;h2&gt;Funding&lt;/h2&gt; 
&lt;p&gt;The work is supported by the Natural Sciences and Engineering Research Council of Canada under grants RGPIN-2017-03910 and RGPIN-2024-03787.&lt;/p&gt; 
&lt;h2&gt;Contributing to simdjson&lt;/h2&gt; 
&lt;p&gt;Head over to &lt;a href=&quot;https://raw.githubusercontent.com/simdjson/simdjson/master/CONTRIBUTING.md&quot;&gt;CONTRIBUTING.md&lt;/a&gt; for information on contributing to simdjson, and &lt;a href=&quot;https://raw.githubusercontent.com/simdjson/simdjson/master/HACKING.md&quot;&gt;HACKING.md&lt;/a&gt; for information on source, building, and architecture/design.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This code is made available under the &lt;a href=&quot;https://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License 2.0&lt;/a&gt; as well as under the MIT License. As a user, you can pick the license you prefer.&lt;/p&gt; 
&lt;p&gt;Under Windows, we build some tools using the windows/dirent_portable.h file (which is outside our library code): it is under the liberal (business-friendly) MIT license.&lt;/p&gt; 
&lt;p&gt;For compilers that do not support &lt;a href=&quot;https://en.wikipedia.org/wiki/C%2B%2B17&quot;&gt;C++17&lt;/a&gt;, we bundle the string-view library which is published under the &lt;a href=&quot;http://www.boost.org/LICENSE_1_0.txt&quot;&gt;Boost license&lt;/a&gt;. Like the Apache license, the Boost license is a permissive license allowing commercial redistribution.&lt;/p&gt; 
&lt;p&gt;For efficient number serialization, we bundle Florian Loitsch&#39;s implementation of the Grisu2 algorithm for binary to decimal floating-point numbers. The implementation was slightly modified by JSON for Modern C++ library. Both Florian Loitsch&#39;s implementation and JSON for Modern C++ are provided under the MIT license.&lt;/p&gt; 
&lt;p&gt;For runtime dispatching, we use some code from the PyTorch project licensed under 3-clause BSD.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>fmtlib/fmt</title>
      <link>https://github.com/fmtlib/fmt</link>
      <description>&lt;p&gt;A modern formatting library&lt;/p&gt;&lt;hr&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/576385/156254208-f5b743a9-88cf-439d-b0c0-923d53e8d551.png&quot; alt=&quot;{fmt}&quot; width=&quot;25%&quot;&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/fmtlib/fmt/actions?query=workflow%3Alinux&quot;&gt;&lt;img src=&quot;https://github.com/fmtlib/fmt/workflows/linux/badge.svg?sanitize=true&quot; alt=&quot;image&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/fmtlib/fmt/actions?query=workflow%3Amacos&quot;&gt;&lt;img src=&quot;https://github.com/fmtlib/fmt/workflows/macos/badge.svg?sanitize=true&quot; alt=&quot;image&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/fmtlib/fmt/actions?query=workflow%3Awindows&quot;&gt;&lt;img src=&quot;https://github.com/fmtlib/fmt/workflows/windows/badge.svg?sanitize=true&quot; alt=&quot;image&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://bugs.chromium.org/p/oss-fuzz/issues/list?%0Acolspec=ID%20Type%20Component%20Status%20Proj%20Reported%20Owner%20%0ASummary&amp;amp;q=proj%3Dfmt&amp;amp;can=1&quot;&gt;&lt;img src=&quot;https://oss-fuzz-build-logs.storage.googleapis.com/badges/fmt.svg?sanitize=true&quot; alt=&quot;fmt is continuously fuzzed at oss-fuzz&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://stackoverflow.com/questions/tagged/fmt&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/stackoverflow-fmt-blue.svg?sanitize=true&quot; alt=&quot;Ask questions at StackOverflow with the tag fmt&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://securityscorecards.dev/viewer/?uri=github.com/fmtlib/fmt&quot;&gt;&lt;img src=&quot;https://api.securityscorecards.dev/projects/github.com/fmtlib/fmt/badge&quot; alt=&quot;image&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;{fmt}&lt;/strong&gt; is an open-source formatting library providing a fast and safe alternative to C stdio and C++ iostreams.&lt;/p&gt; 
&lt;p&gt;If you like this project, please consider donating to one of the funds that help victims of the war in Ukraine: &lt;a href=&quot;https://www.stopputin.net/&quot;&gt;https://www.stopputin.net/&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://fmt.dev&quot;&gt;Documentation&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://hackingcpp.com/cpp/libs/fmt.html&quot;&gt;Cheat Sheets&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Q&amp;amp;A: ask questions on &lt;a href=&quot;https://stackoverflow.com/questions/tagged/fmt&quot;&gt;StackOverflow with the tag fmt&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Try {fmt} in &lt;a href=&quot;https://godbolt.org/z/8Mx1EW73v&quot;&gt;Compiler Explorer&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Features&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;Simple &lt;a href=&quot;https://fmt.dev/latest/api/&quot;&gt;format API&lt;/a&gt; with positional arguments for localization&lt;/li&gt; 
 &lt;li&gt;Implementation of &lt;a href=&quot;https://en.cppreference.com/w/cpp/utility/format&quot;&gt;C++20 std::format&lt;/a&gt; and &lt;a href=&quot;https://en.cppreference.com/w/cpp/io/print&quot;&gt;C++23 std::print&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://fmt.dev/latest/syntax/&quot;&gt;Format string syntax&lt;/a&gt; similar to Python&#39;s &lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str.format&quot;&gt;format&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Fast IEEE 754 floating-point formatter with correct rounding, shortness and round-trip guarantees using the &lt;a href=&quot;https://github.com/jk-jeon/dragonbox&quot;&gt;Dragonbox&lt;/a&gt; algorithm&lt;/li&gt; 
 &lt;li&gt;Portable Unicode support&lt;/li&gt; 
 &lt;li&gt;Safe &lt;a href=&quot;https://fmt.dev/latest/api/#printf-formatting&quot;&gt;printf implementation&lt;/a&gt; including the POSIX extension for positional arguments&lt;/li&gt; 
 &lt;li&gt;Extensibility: &lt;a href=&quot;https://fmt.dev/latest/api/#formatting-user-defined-types&quot;&gt;support for user-defined types&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;High performance: faster than common standard library implementations of &lt;code&gt;(s)printf&lt;/code&gt;, iostreams, &lt;code&gt;to_string&lt;/code&gt; and &lt;code&gt;to_chars&lt;/code&gt;, see &lt;a href=&quot;https://raw.githubusercontent.com/fmtlib/fmt/master/#speed-tests&quot;&gt;Speed tests&lt;/a&gt; and &lt;a href=&quot;http://www.zverovich.net/2020/06/13/fast-int-to-string-revisited.html&quot;&gt;Converting a hundred million integers to strings per second&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Small code size both in terms of source code with the minimum configuration consisting of just three files, &lt;code&gt;core.h&lt;/code&gt;, &lt;code&gt;format.h&lt;/code&gt; and &lt;code&gt;format-inl.h&lt;/code&gt;, and compiled code; see &lt;a href=&quot;https://raw.githubusercontent.com/fmtlib/fmt/master/#compile-time-and-code-bloat&quot;&gt;Compile time and code bloat&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Reliability: the library has an extensive set of &lt;a href=&quot;https://github.com/fmtlib/fmt/tree/master/test&quot;&gt;tests&lt;/a&gt; and is &lt;a href=&quot;https://bugs.chromium.org/p/oss-fuzz/issues/list?colspec=ID%20Type%20Component%20Status%20Proj%20Reported%20Owner%20Summary&amp;amp;q=proj%3Dfmt&amp;amp;can=1&quot;&gt;continuously fuzzed&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Safety: the library is fully type-safe, errors in format strings can be reported at compile time, automatic memory management prevents buffer overflow errors&lt;/li&gt; 
 &lt;li&gt;Ease of use: small self-contained code base, no external dependencies, permissive MIT &lt;a href=&quot;https://github.com/fmtlib/fmt/raw/master/LICENSE&quot;&gt;license&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://fmt.dev/latest/#portability&quot;&gt;Portability&lt;/a&gt; with consistent output across platforms and support for older compilers&lt;/li&gt; 
 &lt;li&gt;Clean warning-free codebase even on high warning levels such as &lt;code&gt;-Wall -Wextra -pedantic&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Locale independence by default&lt;/li&gt; 
 &lt;li&gt;Optional header-only configuration enabled with the &lt;code&gt;FMT_HEADER_ONLY&lt;/code&gt; macro&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See the &lt;a href=&quot;https://fmt.dev&quot;&gt;documentation&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h1&gt;Examples&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;Print to stdout&lt;/strong&gt; (&lt;a href=&quot;https://godbolt.org/z/Tevcjh&quot;&gt;run&lt;/a&gt;)&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;#include &amp;lt;fmt/core.h&amp;gt;

int main() {
  fmt::print(&quot;Hello, world!\n&quot;);
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Format a string&lt;/strong&gt; (&lt;a href=&quot;https://godbolt.org/z/oK8h33&quot;&gt;run&lt;/a&gt;)&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;std::string s = fmt::format(&quot;The answer is {}.&quot;, 42);
// s == &quot;The answer is 42.&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Format a string using positional arguments&lt;/strong&gt; (&lt;a href=&quot;https://godbolt.org/z/Yn7Txe&quot;&gt;run&lt;/a&gt;)&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;std::string s = fmt::format(&quot;I&#39;d rather be {1} than {0}.&quot;, &quot;right&quot;, &quot;happy&quot;);
// s == &quot;I&#39;d rather be happy than right.&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Print dates and times&lt;/strong&gt; (&lt;a href=&quot;https://godbolt.org/z/c31ExdY3W&quot;&gt;run&lt;/a&gt;)&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;#include &amp;lt;fmt/chrono.h&amp;gt;

int main() {
  auto now = std::chrono::system_clock::now();
  fmt::print(&quot;Date and time: {}\n&quot;, now);
  fmt::print(&quot;Time: {:%H:%M}\n&quot;, now);
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Output:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Date and time: 2023-12-26 19:10:31.557195597
Time: 19:10
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Print a container&lt;/strong&gt; (&lt;a href=&quot;https://godbolt.org/z/MxM1YqjE7&quot;&gt;run&lt;/a&gt;)&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;#include &amp;lt;vector&amp;gt;
#include &amp;lt;fmt/ranges.h&amp;gt;

int main() {
  std::vector&amp;lt;int&amp;gt; v = {1, 2, 3};
  fmt::print(&quot;{}\n&quot;, v);
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Output:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;[1, 2, 3]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Check a format string at compile time&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;std::string s = fmt::format(&quot;{:d}&quot;, &quot;I am not a number&quot;);
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This gives a compile-time error in C++20 because &lt;code&gt;d&lt;/code&gt; is an invalid format specifier for a string.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Write a file from a single thread&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;#include &amp;lt;fmt/os.h&amp;gt;

int main() {
  auto out = fmt::output_file(&quot;guide.txt&quot;);
  out.print(&quot;Don&#39;t {}&quot;, &quot;Panic&quot;);
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This can be &lt;a href=&quot;http://www.zverovich.net/2020/08/04/optimal-file-buffer-size.html&quot;&gt;5 to 9 times faster than fprintf&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Print with colors and text styles&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;#include &amp;lt;fmt/color.h&amp;gt;

int main() {
  fmt::print(fg(fmt::color::crimson) | fmt::emphasis::bold,
             &quot;Hello, {}!\n&quot;, &quot;world&quot;);
  fmt::print(fg(fmt::color::floral_white) | bg(fmt::color::slate_gray) |
             fmt::emphasis::underline, &quot;Olá, {}!\n&quot;, &quot;Mundo&quot;);
  fmt::print(fg(fmt::color::steel_blue) | fmt::emphasis::italic,
             &quot;你好{}！\n&quot;, &quot;世界&quot;);
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Output on a modern terminal with Unicode support:&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://github.com/fmtlib/fmt/assets/%0A576385/2a93c904-d6fa-4aa6-b453-2618e1c327d7&quot; alt=&quot;image&quot;&gt;&lt;/p&gt; 
&lt;h1&gt;Benchmarks&lt;/h1&gt; 
&lt;h2&gt;Speed tests&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Library&lt;/th&gt; 
   &lt;th&gt;Method&lt;/th&gt; 
   &lt;th&gt;Run Time, s&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;libc&lt;/td&gt; 
   &lt;td&gt;printf&lt;/td&gt; 
   &lt;td&gt;0.91&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;libc++&lt;/td&gt; 
   &lt;td&gt;std::ostream&lt;/td&gt; 
   &lt;td&gt;2.49&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;{fmt} 9.1&lt;/td&gt; 
   &lt;td&gt;fmt::print&lt;/td&gt; 
   &lt;td&gt;0.74&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Boost Format 1.80&lt;/td&gt; 
   &lt;td&gt;boost::format&lt;/td&gt; 
   &lt;td&gt;6.26&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Folly Format&lt;/td&gt; 
   &lt;td&gt;folly::format&lt;/td&gt; 
   &lt;td&gt;1.87&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;{fmt} is the fastest of the benchmarked methods, ~20% faster than &lt;code&gt;printf&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;The above results were generated by building &lt;code&gt;tinyformat_test.cpp&lt;/code&gt; on macOS 12.6.1 with &lt;code&gt;clang++ -O3 -DNDEBUG -DSPEED_TEST -DHAVE_FORMAT&lt;/code&gt;, and taking the best of three runs. In the test, the format string &lt;code&gt;&quot;%0.10f:%04d:%+g:%s:%p:%c:%%\n&quot;&lt;/code&gt; or equivalent is filled 2,000,000 times with output sent to &lt;code&gt;/dev/null&lt;/code&gt;; for further details refer to the &lt;a href=&quot;https://github.com/fmtlib/format-benchmark/raw/master/src/tinyformat-test.cc&quot;&gt;source&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;{fmt} is up to 20-30x faster than &lt;code&gt;std::ostringstream&lt;/code&gt; and &lt;code&gt;sprintf&lt;/code&gt; on IEEE754 &lt;code&gt;float&lt;/code&gt; and &lt;code&gt;double&lt;/code&gt; formatting (&lt;a href=&quot;https://github.com/fmtlib/dtoa-benchmark&quot;&gt;dtoa-benchmark&lt;/a&gt;) and faster than &lt;a href=&quot;https://github.com/google/double-conversion&quot;&gt;double-conversion&lt;/a&gt; and &lt;a href=&quot;https://github.com/ulfjack/ryu&quot;&gt;ryu&lt;/a&gt;:&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://fmt.dev/unknown_mac64_clang12.0.html&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/576385/95684665-11719600-0ba8-11eb-8e5b-972ff4e49428.png&quot; alt=&quot;image&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Compile time and code bloat&lt;/h2&gt; 
&lt;p&gt;The script &lt;a href=&quot;https://github.com/fmtlib/format-benchmark/raw/master/bloat-test.py&quot;&gt;bloat-test.py&lt;/a&gt; from &lt;a href=&quot;https://github.com/fmtlib/format-benchmark&quot;&gt;format-benchmark&lt;/a&gt; tests compile time and code bloat for nontrivial projects. It generates 100 translation units and uses &lt;code&gt;printf()&lt;/code&gt; or its alternative five times in each to simulate a medium-sized project. The resulting executable size and compile time (Apple clang version 15.0.0 (clang-1500.1.0.2.5), macOS Sonoma, best of three) is shown in the following tables.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Optimized build (-O3)&lt;/strong&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Method&lt;/th&gt; 
   &lt;th&gt;Compile Time, s&lt;/th&gt; 
   &lt;th&gt;Executable size, KiB&lt;/th&gt; 
   &lt;th&gt;Stripped size, KiB&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;printf&lt;/td&gt; 
   &lt;td&gt;1.6&lt;/td&gt; 
   &lt;td&gt;54&lt;/td&gt; 
   &lt;td&gt;50&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;IOStreams&lt;/td&gt; 
   &lt;td&gt;25.9&lt;/td&gt; 
   &lt;td&gt;98&lt;/td&gt; 
   &lt;td&gt;84&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;fmt 83652df&lt;/td&gt; 
   &lt;td&gt;4.8&lt;/td&gt; 
   &lt;td&gt;54&lt;/td&gt; 
   &lt;td&gt;50&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;tinyformat&lt;/td&gt; 
   &lt;td&gt;29.1&lt;/td&gt; 
   &lt;td&gt;161&lt;/td&gt; 
   &lt;td&gt;136&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Boost Format&lt;/td&gt; 
   &lt;td&gt;55.0&lt;/td&gt; 
   &lt;td&gt;530&lt;/td&gt; 
   &lt;td&gt;317&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;{fmt} is fast to compile and is comparable to &lt;code&gt;printf&lt;/code&gt; in terms of per-call binary size (within a rounding error on this system).&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Non-optimized build&lt;/strong&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Method&lt;/th&gt; 
   &lt;th&gt;Compile Time, s&lt;/th&gt; 
   &lt;th&gt;Executable size, KiB&lt;/th&gt; 
   &lt;th&gt;Stripped size, KiB&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;printf&lt;/td&gt; 
   &lt;td&gt;1.4&lt;/td&gt; 
   &lt;td&gt;54&lt;/td&gt; 
   &lt;td&gt;50&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;IOStreams&lt;/td&gt; 
   &lt;td&gt;23.4&lt;/td&gt; 
   &lt;td&gt;92&lt;/td&gt; 
   &lt;td&gt;68&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;{fmt} 83652df&lt;/td&gt; 
   &lt;td&gt;4.4&lt;/td&gt; 
   &lt;td&gt;89&lt;/td&gt; 
   &lt;td&gt;85&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;tinyformat&lt;/td&gt; 
   &lt;td&gt;24.5&lt;/td&gt; 
   &lt;td&gt;204&lt;/td&gt; 
   &lt;td&gt;161&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Boost Format&lt;/td&gt; 
   &lt;td&gt;36.4&lt;/td&gt; 
   &lt;td&gt;831&lt;/td&gt; 
   &lt;td&gt;462&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;code&gt;libc&lt;/code&gt;, &lt;code&gt;lib(std)c++&lt;/code&gt;, and &lt;code&gt;libfmt&lt;/code&gt; are all linked as shared libraries to compare formatting function overhead only. Boost Format is a header-only library so it doesn&#39;t provide any linkage options.&lt;/p&gt; 
&lt;h2&gt;Running the tests&lt;/h2&gt; 
&lt;p&gt;Please refer to &lt;a href=&quot;https://fmt.dev/latest/get-started/#building-from-source&quot;&gt;Building the library&lt;/a&gt; for instructions on how to build the library and run the unit tests.&lt;/p&gt; 
&lt;p&gt;Benchmarks reside in a separate repository, &lt;a href=&quot;https://github.com/fmtlib/format-benchmark&quot;&gt;format-benchmarks&lt;/a&gt;, so to run the benchmarks you first need to clone this repository and generate Makefiles with CMake:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ git clone --recursive https://github.com/fmtlib/format-benchmark.git
$ cd format-benchmark
$ cmake .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then you can run the speed test:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ make speed-test
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or the bloat test:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ make bloat-test
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;Migrating code&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://clang.llvm.org/extra/clang-tidy/&quot;&gt;clang-tidy&lt;/a&gt; v18 provides the &lt;a href=&quot;https://clang.llvm.org/extra/clang-tidy/checks/modernize/use-std-print.html&quot;&gt;modernize-use-std-print&lt;/a&gt; check that is capable of converting occurrences of &lt;code&gt;printf&lt;/code&gt; and &lt;code&gt;fprintf&lt;/code&gt; to &lt;code&gt;fmt::print&lt;/code&gt; if configured to do so. (By default it converts to &lt;code&gt;std::print&lt;/code&gt;.)&lt;/p&gt; 
&lt;h1&gt;Notable projects using this library&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://play0ad.com/&quot;&gt;0 A.D.&lt;/a&gt;: a free, open-source, cross-platform real-time strategy game&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ampl/mp&quot;&gt;AMPL/MP&lt;/a&gt;: an open-source library for mathematical programming&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/apple/foundationdb&quot;&gt;Apple&#39;s FoundationDB&lt;/a&gt;: an open-source, distributed, transactional key-value store&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/aseprite/aseprite&quot;&gt;Aseprite&lt;/a&gt;: animated sprite editor &amp;amp; pixel art tool&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.aviobook.aero/en&quot;&gt;AvioBook&lt;/a&gt;: a comprehensive aircraft operations suite&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://battle.net/&quot;&gt;Blizzard Battle.net&lt;/a&gt;: an online gaming platform&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://celestia.space/&quot;&gt;Celestia&lt;/a&gt;: real-time 3D visualization of space&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://ceph.com/&quot;&gt;Ceph&lt;/a&gt;: a scalable distributed storage system&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://ccache.dev/&quot;&gt;ccache&lt;/a&gt;: a compiler cache&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ClickHouse/ClickHouse&quot;&gt;ClickHouse&lt;/a&gt;: an analytical database management system&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.contextvision.com/&quot;&gt;ContextVision&lt;/a&gt;: medical imaging software&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/contour-terminal/contour/&quot;&gt;Contour&lt;/a&gt;: a modern terminal emulator&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://cuauv.org/&quot;&gt;CUAUV&lt;/a&gt;: Cornell University&#39;s autonomous underwater vehicle&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://drake.mit.edu/&quot;&gt;Drake&lt;/a&gt;: a planning, control, and analysis toolbox for nonlinear dynamical systems (MIT)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/envoyproxy/envoy&quot;&gt;Envoy&lt;/a&gt;: C++ L7 proxy and communication bus (Lyft)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://fivem.net/&quot;&gt;FiveM&lt;/a&gt;: a modification framework for GTA V&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/MengRao/fmtlog&quot;&gt;fmtlog&lt;/a&gt;: a performant fmtlib-style logging library with latency in nanoseconds&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/facebook/folly&quot;&gt;Folly&lt;/a&gt;: Facebook open-source library&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://gemrb.org/&quot;&gt;GemRB&lt;/a&gt;: a portable open-source implementation of Bioware&#39;s Infinity Engine&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://store.steampowered.com/app/1247360/Grand_Mountain_Adventure/&quot;&gt;Grand Mountain Adventure&lt;/a&gt;: a beautiful open-world ski &amp;amp; snowboarding game&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/pvpgn/pvpgn-server&quot;&gt;HarpyWar/pvpgn&lt;/a&gt;: Player vs Player Gaming Network with tweaks&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/kbengine/kbengine&quot;&gt;KBEngine&lt;/a&gt;: an open-source MMOG server engine&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://keypirinha.com/&quot;&gt;Keypirinha&lt;/a&gt;: a semantic launcher for Windows&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://kodi.tv/&quot;&gt;Kodi&lt;/a&gt; (formerly xbmc): home theater software&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://kth.cash/&quot;&gt;Knuth&lt;/a&gt;: high-performance Bitcoin full-node&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/contour-terminal/libunicode/&quot;&gt;libunicode&lt;/a&gt;: a modern C++17 Unicode library&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://mariadb.org/&quot;&gt;MariaDB&lt;/a&gt;: relational database management system&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/verona&quot;&gt;Microsoft Verona&lt;/a&gt;: research programming language for concurrent ownership&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://mongodb.com/&quot;&gt;MongoDB&lt;/a&gt;: distributed document database&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/duckie/mongo_smasher&quot;&gt;MongoDB Smasher&lt;/a&gt;: a small tool to generate randomized datasets&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://openspaceproject.com/&quot;&gt;OpenSpace&lt;/a&gt;: an open-source astrovisualization framework&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.polserver.com/&quot;&gt;PenUltima Online (POL)&lt;/a&gt;: an MMO server, compatible with most Ultima Online clients&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch/pytorch&quot;&gt;PyTorch&lt;/a&gt;: an open-source machine learning library&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.quasardb.net/&quot;&gt;quasardb&lt;/a&gt;: a distributed, high-performance, associative database&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/odygrd/quill&quot;&gt;Quill&lt;/a&gt;: asynchronous low-latency logging library&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ravijanjam/qkw&quot;&gt;QKW&lt;/a&gt;: generalizing aliasing to simplify navigation, and execute complex multi-line terminal command sequences&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/HunanTV/redis-cerberus&quot;&gt;redis-cerberus&lt;/a&gt;: a Redis cluster proxy&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://vectorized.io/redpanda&quot;&gt;redpanda&lt;/a&gt;: a 10x faster Kafka® replacement for mission-critical systems written in C++&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;http://rpclib.net/&quot;&gt;rpclib&lt;/a&gt;: a modern C++ msgpack-RPC server and client library&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.salesforce.com/analytics-cloud/overview/&quot;&gt;Salesforce Analytics Cloud&lt;/a&gt;: business intelligence software&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.scylladb.com/&quot;&gt;Scylla&lt;/a&gt;: a Cassandra-compatible NoSQL data store that can handle 1 million transactions per second on a single server&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;http://www.seastar-project.org/&quot;&gt;Seastar&lt;/a&gt;: an advanced, open-source C++ framework for high-performance server applications on modern hardware&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/gabime/spdlog&quot;&gt;spdlog&lt;/a&gt;: super fast C++ logging library&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.stellar.org/&quot;&gt;Stellar&lt;/a&gt;: financial platform&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.touchsurgery.com/&quot;&gt;Touch Surgery&lt;/a&gt;: surgery simulator&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/TrinityCore/TrinityCore&quot;&gt;TrinityCore&lt;/a&gt;: open-source MMORPG framework&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://userver.tech/&quot;&gt;🐙 userver framework&lt;/a&gt;: open-source asynchronous framework with a rich set of abstractions and database drivers&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/terminal&quot;&gt;Windows Terminal&lt;/a&gt;: the new Windows terminal&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/search?q=fmtlib&amp;amp;type=Code&quot;&gt;More...&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;If you are aware of other projects using this library, please let me know by &lt;a href=&quot;mailto:victor.zverovich@gmail.com&quot;&gt;email&lt;/a&gt; or by submitting an &lt;a href=&quot;https://github.com/fmtlib/fmt/issues&quot;&gt;issue&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Motivation&lt;/h1&gt; 
&lt;p&gt;So why yet another formatting library?&lt;/p&gt; 
&lt;p&gt;There are plenty of methods for doing this task, from standard ones like the printf family of function and iostreams to Boost Format and FastFormat libraries. The reason for creating a new library is that every existing solution that I found either had serious issues or didn&#39;t provide all the features I needed.&lt;/p&gt; 
&lt;h2&gt;printf&lt;/h2&gt; 
&lt;p&gt;The good thing about &lt;code&gt;printf&lt;/code&gt; is that it is pretty fast and readily available being a part of the C standard library. The main drawback is that it doesn&#39;t support user-defined types. &lt;code&gt;printf&lt;/code&gt; also has safety issues although they are somewhat mitigated with &lt;a href=&quot;https://gcc.gnu.org/onlinedocs/gcc/Function-Attributes.html&quot;&gt;__attribute__ ((format (printf, ...))&lt;/a&gt; in GCC. There is a POSIX extension that adds positional arguments required for &lt;a href=&quot;https://en.wikipedia.org/wiki/Internationalization_and_localization&quot;&gt;i18n&lt;/a&gt; to &lt;code&gt;printf&lt;/code&gt; but it is not a part of C99 and may not be available on some platforms.&lt;/p&gt; 
&lt;h2&gt;iostreams&lt;/h2&gt; 
&lt;p&gt;The main issue with iostreams is best illustrated with an example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;std::cout &amp;lt;&amp;lt; std::setprecision(2) &amp;lt;&amp;lt; std::fixed &amp;lt;&amp;lt; 1.23456 &amp;lt;&amp;lt; &quot;\n&quot;;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;which is a lot of typing compared to printf:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;printf(&quot;%.2f\n&quot;, 1.23456);
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Matthew Wilson, the author of FastFormat, called this &quot;chevron hell&quot;. iostreams don&#39;t support positional arguments by design.&lt;/p&gt; 
&lt;p&gt;The good part is that iostreams support user-defined types and are safe although error handling is awkward.&lt;/p&gt; 
&lt;h2&gt;Boost Format&lt;/h2&gt; 
&lt;p&gt;This is a very powerful library that supports both &lt;code&gt;printf&lt;/code&gt;-like format strings and positional arguments. Its main drawback is performance. According to various benchmarks, it is much slower than other methods considered here. Boost Format also has excessive build times and severe code bloat issues (see &lt;a href=&quot;https://raw.githubusercontent.com/fmtlib/fmt/master/#benchmarks&quot;&gt;Benchmarks&lt;/a&gt;).&lt;/p&gt; 
&lt;h2&gt;FastFormat&lt;/h2&gt; 
&lt;p&gt;This is an interesting library that is fast, safe and has positional arguments. However, it has significant limitations, citing its author:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Three features that have no hope of being accommodated within the current design are:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Leading zeros (or any other non-space padding)&lt;/li&gt; 
  &lt;li&gt;Octal/hexadecimal encoding&lt;/li&gt; 
  &lt;li&gt;Runtime width/alignment specification&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;It is also quite big and has a heavy dependency, on STLSoft, which might be too restrictive for use in some projects.&lt;/p&gt; 
&lt;h2&gt;Boost Spirit.Karma&lt;/h2&gt; 
&lt;p&gt;This is not a formatting library but I decided to include it here for completeness. As iostreams, it suffers from the problem of mixing verbatim text with arguments. The library is pretty fast, but slower on integer formatting than &lt;code&gt;fmt::format_to&lt;/code&gt; with format string compilation on Karma&#39;s own benchmark, see &lt;a href=&quot;http://www.zverovich.net/2020/06/13/fast-int-to-string-revisited.html&quot;&gt;Converting a hundred million integers to strings per second&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;License&lt;/h1&gt; 
&lt;p&gt;{fmt} is distributed under the MIT &lt;a href=&quot;https://github.com/fmtlib/fmt/raw/master/LICENSE&quot;&gt;license&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Documentation License&lt;/h1&gt; 
&lt;p&gt;The &lt;a href=&quot;https://fmt.dev/latest/syntax/&quot;&gt;Format String Syntax&lt;/a&gt; section in the documentation is based on the one from Python &lt;a href=&quot;https://docs.python.org/3/library/string.html#module-string&quot;&gt;string module documentation&lt;/a&gt;. For this reason, the documentation is distributed under the Python Software Foundation license available in &lt;a href=&quot;https://raw.github.com/fmtlib/fmt/master/doc/python-license.txt&quot;&gt;doc/python-license.txt&lt;/a&gt;. It only applies if you distribute the documentation of {fmt}.&lt;/p&gt; 
&lt;h1&gt;Maintainers&lt;/h1&gt; 
&lt;p&gt;The {fmt} library is maintained by Victor Zverovich (&lt;a href=&quot;https://github.com/vitaut&quot;&gt;vitaut&lt;/a&gt;) with contributions from many other people. See &lt;a href=&quot;https://github.com/fmtlib/fmt/graphs/contributors&quot;&gt;Contributors&lt;/a&gt; and &lt;a href=&quot;https://github.com/fmtlib/fmt/releases&quot;&gt;Releases&lt;/a&gt; for some of the names. Let us know if your contribution is not listed or mentioned incorrectly and we&#39;ll make it right.&lt;/p&gt; 
&lt;h1&gt;Security Policy&lt;/h1&gt; 
&lt;p&gt;To report a security issue, please disclose it at &lt;a href=&quot;https://github.com/fmtlib/fmt/security/advisories/new&quot;&gt;security advisory&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;This project is maintained by a team of volunteers on a reasonable-effort basis. As such, please give us at least &lt;em&gt;90&lt;/em&gt; days to work on a fix before public exposure.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>cameron314/concurrentqueue</title>
      <link>https://github.com/cameron314/concurrentqueue</link>
      <description>&lt;p&gt;A fast multi-producer, multi-consumer lock-free concurrent queue for C++11&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;moodycamel::ConcurrentQueue
 &lt;t&gt;&lt;/t&gt;&lt;/h1&gt; 
&lt;p&gt;An industrial-strength lock-free queue for C++.&lt;/p&gt; 
&lt;p&gt;Note: If all you need is a single-producer, single-consumer queue, I have &lt;a href=&quot;https://github.com/cameron314/readerwriterqueue&quot;&gt;one of those too&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Knock-your-socks-off &lt;a href=&quot;http://moodycamel.com/blog/2014/a-fast-general-purpose-lock-free-queue-for-c++#benchmarks&quot;&gt;blazing fast performance&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Single-header implementation. Just drop it in your project.&lt;/li&gt; 
 &lt;li&gt;Fully thread-safe lock-free queue. Use concurrently from any number of threads.&lt;/li&gt; 
 &lt;li&gt;C++11 implementation -- elements are moved (instead of copied) where possible.&lt;/li&gt; 
 &lt;li&gt;Templated, obviating the need to deal exclusively with pointers -- memory is managed for you.&lt;/li&gt; 
 &lt;li&gt;No artificial limitations on element types or maximum count.&lt;/li&gt; 
 &lt;li&gt;Memory can be allocated once up-front, or dynamically as needed.&lt;/li&gt; 
 &lt;li&gt;Fully portable (no assembly; all is done through standard C++11 primitives).&lt;/li&gt; 
 &lt;li&gt;Supports super-fast bulk operations.&lt;/li&gt; 
 &lt;li&gt;Includes a low-overhead blocking version (BlockingConcurrentQueue).&lt;/li&gt; 
 &lt;li&gt;Exception safe.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Reasons to use&lt;/h2&gt; 
&lt;p&gt;There are not that many full-fledged lock-free queues for C++. Boost has one, but it&#39;s limited to objects with trivial assignment operators and trivial destructors, for example. Intel&#39;s TBB queue isn&#39;t lock-free, and requires trivial constructors too. There&#39;re many academic papers that implement lock-free queues in C++, but usable source code is hard to find, and tests even more so.&lt;/p&gt; 
&lt;p&gt;This queue not only has less limitations than others (for the most part), but &lt;a href=&quot;http://moodycamel.com/blog/2014/a-fast-general-purpose-lock-free-queue-for-c++#benchmarks&quot;&gt;it&#39;s also faster&lt;/a&gt;. It&#39;s been fairly well-tested, and offers advanced features like &lt;strong&gt;bulk enqueueing/dequeueing&lt;/strong&gt; (which, with my new design, is much faster than one element at a time, approaching and even surpassing the speed of a non-concurrent queue even under heavy contention).&lt;/p&gt; 
&lt;p&gt;In short, there was a lock-free queue shaped hole in the C++ open-source universe, and I set out to fill it with the fastest, most complete, and well-tested design and implementation I could. The result is &lt;code&gt;moodycamel::ConcurrentQueue&lt;/code&gt; :-)&lt;/p&gt; 
&lt;h2&gt;Reasons &lt;em&gt;not&lt;/em&gt; to use&lt;/h2&gt; 
&lt;p&gt;The fastest synchronization of all is the kind that never takes place. Fundamentally, concurrent data structures require some synchronization, and that takes time. Every effort was made, of course, to minimize the overhead, but if you can avoid sharing data between threads, do so!&lt;/p&gt; 
&lt;p&gt;Why use concurrent data structures at all, then? Because they&#39;re gosh darn convenient! (And, indeed, sometimes sharing data concurrently is unavoidable.)&lt;/p&gt; 
&lt;p&gt;My queue is &lt;strong&gt;not linearizable&lt;/strong&gt; (see the next section on high-level design). The foundations of its design assume that producers are independent; if this is not the case, and your producers co-ordinate amongst themselves in some fashion, be aware that the elements won&#39;t necessarily come out of the queue in the same order they were put in &lt;em&gt;relative to the ordering formed by that co-ordination&lt;/em&gt; (but they will still come out in the order they were put in by any &lt;em&gt;individual&lt;/em&gt; producer). If this affects your use case, you may be better off with another implementation; either way, it&#39;s an important limitation to be aware of.&lt;/p&gt; 
&lt;p&gt;My queue is also &lt;strong&gt;not NUMA aware&lt;/strong&gt;, and does a lot of memory re-use internally, meaning it probably doesn&#39;t scale particularly well on NUMA architectures; however, I don&#39;t know of any other lock-free queue that &lt;em&gt;is&lt;/em&gt; NUMA aware (except for &lt;a href=&quot;http://webee.technion.ac.il/~idish/ftp/spaa049-gidron.pdf&quot;&gt;SALSA&lt;/a&gt;, which is very cool, but has no publicly available implementation that I know of).&lt;/p&gt; 
&lt;p&gt;Finally, the queue is &lt;strong&gt;not sequentially consistent&lt;/strong&gt;; there &lt;em&gt;is&lt;/em&gt; a happens-before relationship between when an element is put in the queue and when it comes out, but other things (such as pumping the queue until it&#39;s empty) require more thought to get right in all eventualities, because explicit memory ordering may have to be done to get the desired effect. In other words, it can sometimes be difficult to use the queue correctly. This is why it&#39;s a good idea to follow the &lt;a href=&quot;https://github.com/cameron314/concurrentqueue/raw/master/samples.md&quot;&gt;samples&lt;/a&gt; where possible. On the other hand, the upside of this lack of sequential consistency is better performance.&lt;/p&gt; 
&lt;h2&gt;High-level design&lt;/h2&gt; 
&lt;p&gt;Elements are stored internally using contiguous blocks instead of linked lists for better performance. The queue is made up of a collection of sub-queues, one for each producer. When a consumer wants to dequeue an element, it checks all the sub-queues until it finds one that&#39;s not empty. All of this is largely transparent to the user of the queue, however -- it mostly just works&lt;sup&gt;TM&lt;/sup&gt;.&lt;/p&gt; 
&lt;p&gt;One particular consequence of this design, however, (which seems to be non-intuitive) is that if two producers enqueue at the same time, there is no defined ordering between the elements when they&#39;re later dequeued. Normally this is fine, because even with a fully linearizable queue there&#39;d be a race between the producer threads and so you couldn&#39;t rely on the ordering anyway. However, if for some reason you do extra explicit synchronization between the two producer threads yourself, thus defining a total order between enqueue operations, you might expect that the elements would come out in the same total order, which is a guarantee my queue does not offer. At that point, though, there semantically aren&#39;t really two separate producers, but rather one that happens to be spread across multiple threads. In this case, you can still establish a total ordering with my queue by creating a single producer token, and using that from both threads to enqueue (taking care to synchronize access to the token, of course, but there was already extra synchronization involved anyway).&lt;/p&gt; 
&lt;p&gt;I&#39;ve written a more detailed &lt;a href=&quot;http://moodycamel.com/blog/2014/a-fast-general-purpose-lock-free-queue-for-c++&quot;&gt;overview of the internal design&lt;/a&gt;, as well as &lt;a href=&quot;http://moodycamel.com/blog/2014/detailed-design-of-a-lock-free-queue&quot;&gt;the full nitty-gritty details of the design&lt;/a&gt;, on my blog. Finally, the &lt;a href=&quot;https://github.com/cameron314/concurrentqueue&quot;&gt;source&lt;/a&gt; itself is available for perusal for those interested in its implementation.&lt;/p&gt; 
&lt;h2&gt;Basic use&lt;/h2&gt; 
&lt;p&gt;The entire queue&#39;s implementation is contained in &lt;strong&gt;one header&lt;/strong&gt;, &lt;a href=&quot;https://github.com/cameron314/concurrentqueue/raw/master/concurrentqueue.h&quot;&gt;&lt;code&gt;concurrentqueue.h&lt;/code&gt;&lt;/a&gt;. Simply download and include that to use the queue. The blocking version is in a separate header, &lt;a href=&quot;https://github.com/cameron314/concurrentqueue/raw/master/blockingconcurrentqueue.h&quot;&gt;&lt;code&gt;blockingconcurrentqueue.h&lt;/code&gt;&lt;/a&gt;, that depends on &lt;a href=&quot;https://github.com/cameron314/concurrentqueue/raw/master/concurrentqueue.h&quot;&gt;&lt;code&gt;concurrentqueue.h&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;https://github.com/cameron314/concurrentqueue/raw/master/lightweightsemaphore.h&quot;&gt;&lt;code&gt;lightweightsemaphore.h&lt;/code&gt;&lt;/a&gt;. The implementation makes use of certain key C++11 features, so it requires a relatively recent compiler (e.g. VS2012+ or g++ 4.8; note that g++ 4.6 has a known bug with &lt;code&gt;std::atomic&lt;/code&gt; and is thus not supported). The algorithm implementations themselves are platform independent.&lt;/p&gt; 
&lt;p&gt;Use it like you would any other templated queue, with the exception that you can use it from many threads at once :-)&lt;/p&gt; 
&lt;p&gt;Simple example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-C++&quot;&gt;#include &quot;concurrentqueue.h&quot;

moodycamel::ConcurrentQueue&amp;lt;int&amp;gt; q;
q.enqueue(25);

int item;
bool found = q.try_dequeue(item);
assert(found &amp;amp;&amp;amp; item == 25);
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Description of basic methods:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;ConcurrentQueue(size_t initialSizeEstimate)&lt;/code&gt; Constructor which optionally accepts an estimate of the number of elements the queue will hold&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;enqueue(T&amp;amp;&amp;amp; item)&lt;/code&gt; Enqueues one item, allocating extra space if necessary&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;try_enqueue(T&amp;amp;&amp;amp; item)&lt;/code&gt; Enqueues one item, but only if enough memory is already allocated&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;try_dequeue(T&amp;amp; item)&lt;/code&gt; Dequeues one item, returning true if an item was found or false if the queue appeared empty&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Note that it is up to the user to ensure that the queue object is completely constructed before being used by any other threads (this includes making the memory effects of construction visible, possibly via a memory barrier). Similarly, it&#39;s important that all threads have finished using the queue (and the memory effects have fully propagated) before it is destructed.&lt;/p&gt; 
&lt;p&gt;There&#39;s usually two versions of each method, one &quot;explicit&quot; version that takes a user-allocated per-producer or per-consumer token, and one &quot;implicit&quot; version that works without tokens. Using the explicit methods is almost always faster (though not necessarily by a huge factor). Apart from performance, the primary distinction between them is their sub-queue allocation behaviour for enqueue operations: Using the implicit enqueue methods causes an automatically-allocated thread-local producer sub-queue to be allocated. Explicit producers, on the other hand, are tied directly to their tokens&#39; lifetimes (but are recycled internally).&lt;/p&gt; 
&lt;p&gt;In order to avoid the number of sub-queues growing without bound, implicit producers are marked for reuse once their thread exits. However, this is not supported on all platforms. If using the queue from short-lived threads, it is recommended to use explicit producer tokens instead.&lt;/p&gt; 
&lt;p&gt;Full API (pseudocode):&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;# Allocates more memory if necessary
enqueue(item) : bool
enqueue(prod_token, item) : bool
enqueue_bulk(item_first, count) : bool
enqueue_bulk(prod_token, item_first, count) : bool

# Fails if not enough memory to enqueue
try_enqueue(item) : bool
try_enqueue(prod_token, item) : bool
try_enqueue_bulk(item_first, count) : bool
try_enqueue_bulk(prod_token, item_first, count) : bool

# Attempts to dequeue from the queue (never allocates)
try_dequeue(item&amp;amp;) : bool
try_dequeue(cons_token, item&amp;amp;) : bool
try_dequeue_bulk(item_first, max) : size_t
try_dequeue_bulk(cons_token, item_first, max) : size_t

# If you happen to know which producer you want to dequeue from
try_dequeue_from_producer(prod_token, item&amp;amp;) : bool
try_dequeue_bulk_from_producer(prod_token, item_first, max) : size_t

# A not-necessarily-accurate count of the total number of elements
size_approx() : size_t
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Blocking version&lt;/h2&gt; 
&lt;p&gt;As mentioned above, a full blocking wrapper of the queue is provided that adds &lt;code&gt;wait_dequeue&lt;/code&gt; and &lt;code&gt;wait_dequeue_bulk&lt;/code&gt; methods in addition to the regular interface. This wrapper is extremely low-overhead, but slightly less fast than the non-blocking queue (due to the necessary bookkeeping involving a lightweight semaphore).&lt;/p&gt; 
&lt;p&gt;There are also timed versions that allow a timeout to be specified (either in microseconds or with a &lt;code&gt;std::chrono&lt;/code&gt; object).&lt;/p&gt; 
&lt;p&gt;The only major caveat with the blocking version is that you must be careful not to destroy the queue while somebody is waiting on it. This generally means you need to know for certain that another element is going to come along before you call one of the blocking methods. (To be fair, the non-blocking version cannot be destroyed while in use either, but it can be easier to coordinate the cleanup.)&lt;/p&gt; 
&lt;p&gt;Blocking example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-C++&quot;&gt;#include &quot;blockingconcurrentqueue.h&quot;

moodycamel::BlockingConcurrentQueue&amp;lt;int&amp;gt; q;
std::thread producer([&amp;amp;]() {
    for (int i = 0; i != 100; ++i) {
        std::this_thread::sleep_for(std::chrono::milliseconds(i % 10));
        q.enqueue(i);
    }
});
std::thread consumer([&amp;amp;]() {
    for (int i = 0; i != 100; ++i) {
        int item;
        q.wait_dequeue(item);
        assert(item == i);
        
        if (q.wait_dequeue_timed(item, std::chrono::milliseconds(5))) {
            ++i;
            assert(item == i);
        }
    }
});
producer.join();
consumer.join();

assert(q.size_approx() == 0);
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Advanced features&lt;/h2&gt; 
&lt;h4&gt;Tokens&lt;/h4&gt; 
&lt;p&gt;The queue can take advantage of extra per-producer and per-consumer storage if it&#39;s available to speed up its operations. This takes the form of &quot;tokens&quot;: You can create a consumer token and/or a producer token for each thread or task (tokens themselves are not thread-safe), and use the methods that accept a token as their first parameter:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-C++&quot;&gt;moodycamel::ConcurrentQueue&amp;lt;int&amp;gt; q;

moodycamel::ProducerToken ptok(q);
q.enqueue(ptok, 17);

moodycamel::ConsumerToken ctok(q);
int item;
q.try_dequeue(ctok, item);
assert(item == 17);
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you happen to know which producer you want to consume from (e.g. in a single-producer, multi-consumer scenario), you can use the &lt;code&gt;try_dequeue_from_producer&lt;/code&gt; methods, which accept a producer token instead of a consumer token, and cut some overhead.&lt;/p&gt; 
&lt;p&gt;Note that tokens work with the blocking version of the queue too.&lt;/p&gt; 
&lt;p&gt;When producing or consuming many elements, the most efficient way is to:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Use the bulk methods of the queue with tokens&lt;/li&gt; 
 &lt;li&gt;Failing that, use the bulk methods without tokens&lt;/li&gt; 
 &lt;li&gt;Failing that, use the single-item methods with tokens&lt;/li&gt; 
 &lt;li&gt;Failing that, use the single-item methods without tokens&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Having said that, don&#39;t create tokens willy-nilly -- ideally there would be one token (of each kind) per thread. The queue will work with what it is given, but it performs best when used with tokens.&lt;/p&gt; 
&lt;p&gt;Note that tokens aren&#39;t actually tied to any given thread; it&#39;s not technically required that they be local to the thread, only that they be used by a single producer/consumer at a time.&lt;/p&gt; 
&lt;h4&gt;Bulk operations&lt;/h4&gt; 
&lt;p&gt;Thanks to the &lt;a href=&quot;http://moodycamel.com/blog/2014/a-fast-general-purpose-lock-free-queue-for-c++&quot;&gt;novel design&lt;/a&gt; of the queue, it&#39;s just as easy to enqueue/dequeue multiple items as it is to do one at a time. This means that overhead can be cut drastically for bulk operations. Example syntax:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-C++&quot;&gt;moodycamel::ConcurrentQueue&amp;lt;int&amp;gt; q;

int items[] = { 1, 2, 3, 4, 5 };
q.enqueue_bulk(items, 5);

int results[5];     // Could also be any iterator
size_t count = q.try_dequeue_bulk(results, 5);
for (size_t i = 0; i != count; ++i) {
    assert(results[i] == items[i]);
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Preallocation (correctly using &lt;code&gt;try_enqueue&lt;/code&gt;)&lt;/h4&gt; 
&lt;p&gt;&lt;code&gt;try_enqueue&lt;/code&gt;, unlike just plain &lt;code&gt;enqueue&lt;/code&gt;, will never allocate memory. If there&#39;s not enough room in the queue, it simply returns false. The key to using this method properly, then, is to ensure enough space is pre-allocated for your desired maximum element count.&lt;/p&gt; 
&lt;p&gt;The constructor accepts a count of the number of elements that it should reserve space for. Because the queue works with blocks of elements, however, and not individual elements themselves, the value to pass in order to obtain an effective number of pre-allocated element slots is non-obvious.&lt;/p&gt; 
&lt;p&gt;First, be aware that the count passed is rounded up to the next multiple of the block size. Note that the default block size is 32 (this can be changed via the traits). Second, once a slot in a block has been enqueued to, that slot cannot be re-used until the rest of the block has been completely filled up and then completely emptied. This affects the number of blocks you need in order to account for the overhead of partially-filled blocks. Third, each producer (whether implicit or explicit) claims and recycles blocks in a different manner, which again affects the number of blocks you need to account for a desired number of usable slots.&lt;/p&gt; 
&lt;p&gt;Suppose you want the queue to be able to hold at least &lt;code&gt;N&lt;/code&gt; elements at any given time. Without delving too deep into the rather arcane implementation details, here are some simple formulas for the number of elements to request for pre-allocation in such a case. Note the division is intended to be arithmetic division and not integer division (in order for &lt;code&gt;ceil()&lt;/code&gt; to work).&lt;/p&gt; 
&lt;p&gt;For explicit producers (using tokens to enqueue):&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-C++&quot;&gt;(ceil(N / BLOCK_SIZE) + 1) * MAX_NUM_PRODUCERS * BLOCK_SIZE
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For implicit producers (no tokens):&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-C++&quot;&gt;(ceil(N / BLOCK_SIZE) - 1 + 2 * MAX_NUM_PRODUCERS) * BLOCK_SIZE
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;When using mixed producer types:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-C++&quot;&gt;((ceil(N / BLOCK_SIZE) - 1) * (MAX_EXPLICIT_PRODUCERS + 1) + 2 * (MAX_IMPLICIT_PRODUCERS + MAX_EXPLICIT_PRODUCERS)) * BLOCK_SIZE
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If these formulas seem rather inconvenient, you can use the constructor overload that accepts the minimum number of elements (&lt;code&gt;N&lt;/code&gt;) and the maximum number of explicit and implicit producers directly, and let it do the computation for you.&lt;/p&gt; 
&lt;p&gt;In addition to blocks, there are other internal data structures that require allocating memory if they need to resize (grow). If using &lt;code&gt;try_enqueue&lt;/code&gt; exclusively, the initial sizes may be exceeded, causing subsequent &lt;code&gt;try_enqueue&lt;/code&gt; operations to fail. Specifically, the &lt;code&gt;INITIAL_IMPLICIT_PRODUCER_HASH_SIZE&lt;/code&gt; trait limits the number of implicit producers that can be active at once before the internal hash needs resizing. Along the same lines, the &lt;code&gt;IMPLICIT_INITIAL_INDEX_SIZE&lt;/code&gt; trait limits the number of unconsumed elements that an implicit producer can insert before its internal hash needs resizing. Similarly, the &lt;code&gt;EXPLICIT_INITIAL_INDEX_SIZE&lt;/code&gt; trait limits the number of unconsumed elements that an explicit producer can insert before its internal hash needs resizing. In order to avoid hitting these limits when using &lt;code&gt;try_enqueue&lt;/code&gt;, it is crucial to adjust the initial sizes in the traits appropriately, in addition to sizing the number of blocks properly as outlined above.&lt;/p&gt; 
&lt;p&gt;Finally, it&#39;s important to note that because the queue is only eventually consistent and takes advantage of weak memory ordering for speed, there&#39;s always a possibility that under contention &lt;code&gt;try_enqueue&lt;/code&gt; will fail even if the queue is correctly pre-sized for the desired number of elements. (e.g. A given thread may think that the queue&#39;s full even when that&#39;s no longer the case.) So no matter what, you still need to handle the failure case (perhaps looping until it succeeds), unless you don&#39;t mind dropping elements.&lt;/p&gt; 
&lt;h4&gt;Exception safety&lt;/h4&gt; 
&lt;p&gt;The queue is exception safe, and will never become corrupted if used with a type that may throw exceptions. The queue itself never throws any exceptions (operations fail gracefully (return false) if memory allocation fails instead of throwing &lt;code&gt;std::bad_alloc&lt;/code&gt;).&lt;/p&gt; 
&lt;p&gt;It is important to note that the guarantees of exception safety only hold if the element type never throws from its destructor, and that any iterators passed into the queue (for bulk operations) never throw either. Note that in particular this means &lt;code&gt;std::back_inserter&lt;/code&gt; iterators must be used with care, since the vector being inserted into may need to allocate and throw a &lt;code&gt;std::bad_alloc&lt;/code&gt; exception from inside the iterator; so be sure to reserve enough capacity in the target container first if you do this.&lt;/p&gt; 
&lt;p&gt;The guarantees are presently as follows:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Enqueue operations are rolled back completely if an exception is thrown from an element&#39;s constructor. For bulk enqueue operations, this means that elements are copied instead of moved (in order to avoid having only some objects moved in the event of an exception). Non-bulk enqueues always use the move constructor if one is available.&lt;/li&gt; 
 &lt;li&gt;If the assignment operator throws during a dequeue operation (both single and bulk), the element(s) are considered dequeued regardless. In such a case, the dequeued elements are all properly destructed before the exception is propagated, but there&#39;s no way to get the elements themselves back.&lt;/li&gt; 
 &lt;li&gt;Any exception that is thrown is propagated up the call stack, at which point the queue is in a consistent state.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Note: If any of your type&#39;s copy constructors/move constructors/assignment operators don&#39;t throw, be sure to annotate them with &lt;code&gt;noexcept&lt;/code&gt;; this will avoid the exception-checking overhead in the queue where possible (even with zero-cost exceptions, there&#39;s still a code size impact that has to be taken into account).&lt;/p&gt; 
&lt;h4&gt;Traits&lt;/h4&gt; 
&lt;p&gt;The queue also supports a traits template argument which defines various types, constants, and the memory allocation and deallocation functions that are to be used by the queue. The typical pattern to providing your own traits is to create a class that inherits from the default traits and override only the values you wish to change. Example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-C++&quot;&gt;struct MyTraits : public moodycamel::ConcurrentQueueDefaultTraits
{
	static const size_t BLOCK_SIZE = 256;		// Use bigger blocks
};

moodycamel::ConcurrentQueue&amp;lt;int, MyTraits&amp;gt; q;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;How to dequeue types without calling the constructor&lt;/h4&gt; 
&lt;p&gt;The normal way to dequeue an item is to pass in an existing object by reference, which is then assigned to internally by the queue (using the move-assignment operator if possible). This can pose a problem for types that are expensive to construct or don&#39;t have a default constructor; fortunately, there is a simple workaround: Create a wrapper class that copies the memory contents of the object when it is assigned by the queue (a poor man&#39;s move, essentially). Note that this only works if the object contains no internal pointers. Example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-C++&quot;&gt;struct MyObjectMover {
    inline void operator=(MyObject&amp;amp;&amp;amp; obj) {
        std::memcpy(data, &amp;amp;obj, sizeof(MyObject));
        
        // TODO: Cleanup obj so that when it&#39;s destructed by the queue
        // it doesn&#39;t corrupt the data of the object we just moved it into
    }

    inline MyObject&amp;amp; obj() { return *reinterpret_cast&amp;lt;MyObject*&amp;gt;(data); }

private:
    align(alignof(MyObject)) char data[sizeof(MyObject)];
};
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;A less dodgy alternative, if moves are cheap but default construction is not, is to use a wrapper that defers construction until the object is assigned, enabling use of the move constructor:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-C++&quot;&gt;struct MyObjectMover {
    inline void operator=(MyObject&amp;amp;&amp;amp; x) {
        new (data) MyObject(std::move(x));
        created = true;
    }

    inline MyObject&amp;amp; obj() {
        assert(created);
        return *reinterpret_cast&amp;lt;MyObject*&amp;gt;(data);
    }

    ~MyObjectMover() {
        if (created)
            obj().~MyObject();
    }

private:
    align(alignof(MyObject)) char data[sizeof(MyObject)];
    bool created = false;
};
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Samples&lt;/h2&gt; 
&lt;p&gt;There are some more detailed samples &lt;a href=&quot;https://github.com/cameron314/concurrentqueue/raw/master/samples.md&quot;&gt;here&lt;/a&gt;. The source of the &lt;a href=&quot;https://github.com/cameron314/concurrentqueue/tree/master/tests/unittests&quot;&gt;unit tests&lt;/a&gt; and &lt;a href=&quot;https://github.com/cameron314/concurrentqueue/tree/master/benchmarks&quot;&gt;benchmarks&lt;/a&gt; are available for reference as well.&lt;/p&gt; 
&lt;h2&gt;Benchmarks&lt;/h2&gt; 
&lt;p&gt;See my blog post for some &lt;a href=&quot;http://moodycamel.com/blog/2014/a-fast-general-purpose-lock-free-queue-for-c++#benchmarks&quot;&gt;benchmark results&lt;/a&gt; (including versus &lt;code&gt;boost::lockfree::queue&lt;/code&gt; and &lt;code&gt;tbb::concurrent_queue&lt;/code&gt;), or run the benchmarks yourself (requires MinGW and certain GnuWin32 utilities to build on Windows, or a recent g++ on Linux):&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-Shell&quot;&gt;cd build
make benchmarks
bin/benchmarks
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The short version of the benchmarks is that it&#39;s so fast (especially the bulk methods), that if you&#39;re actually using the queue to &lt;em&gt;do&lt;/em&gt; anything, the queue won&#39;t be your bottleneck.&lt;/p&gt; 
&lt;h2&gt;Tests (and bugs)&lt;/h2&gt; 
&lt;p&gt;I&#39;ve written quite a few unit tests as well as a randomized long-running fuzz tester. I also ran the core queue algorithm through the &lt;a href=&quot;http://demsky.eecs.uci.edu/c11modelchecker.html&quot;&gt;CDSChecker&lt;/a&gt; C++11 memory model model checker. Some of the inner algorithms were tested separately using the &lt;a href=&quot;http://www.1024cores.net/home/relacy-race-detector&quot;&gt;Relacy&lt;/a&gt; model checker, and full integration tests were also performed with Relacy. I&#39;ve tested on Linux (Fedora 19) and Windows (7), but only on x86 processors so far (Intel and AMD). The code was written to be platform-independent, however, and should work across all processors and OSes.&lt;/p&gt; 
&lt;p&gt;Due to the complexity of the implementation and the difficult-to-test nature of lock-free code in general, there may still be bugs. If anyone is seeing buggy behaviour, I&#39;d like to hear about it! (Especially if a unit test for it can be cooked up.) Just open an issue on GitHub.&lt;/p&gt; 
&lt;h2&gt;Using vcpkg&lt;/h2&gt; 
&lt;p&gt;You can download and install &lt;code&gt;moodycamel::ConcurrentQueue&lt;/code&gt; using the &lt;a href=&quot;https://github.com/Microsoft/vcpkg&quot;&gt;vcpkg&lt;/a&gt; dependency manager:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-Shell&quot;&gt;git clone https://github.com/Microsoft/vcpkg.git
cd vcpkg
./bootstrap-vcpkg.sh
./vcpkg integrate install
vcpkg install concurrentqueue
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The &lt;code&gt;moodycamel::ConcurrentQueue&lt;/code&gt; port in vcpkg is kept up to date by Microsoft team members and community contributors. If the version is out of date, please &lt;a href=&quot;https://github.com/Microsoft/vcpkg&quot;&gt;create an issue or pull request&lt;/a&gt; on the vcpkg repository.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;I&#39;m releasing the source of this repository (with the exception of third-party code, i.e. the Boost queue (used in the benchmarks for comparison), Intel&#39;s TBB library (ditto), CDSChecker, Relacy, and Jeff Preshing&#39;s cross-platform semaphore, which all have their own licenses) under a simplified BSD license. I&#39;m also dual-licensing under the Boost Software License. See the &lt;a href=&quot;https://github.com/cameron314/concurrentqueue/raw/master/LICENSE.md&quot;&gt;LICENSE.md&lt;/a&gt; file for more details.&lt;/p&gt; 
&lt;p&gt;Note that lock-free programming is a patent minefield, and this code may very well violate a pending patent (I haven&#39;t looked), though it does not to my present knowledge. I did design and implement this queue from scratch.&lt;/p&gt; 
&lt;h2&gt;Diving into the code&lt;/h2&gt; 
&lt;p&gt;If you&#39;re interested in the source code itself, it helps to have a rough idea of how it&#39;s laid out. This section attempts to describe that.&lt;/p&gt; 
&lt;p&gt;The queue is formed of several basic parts (listed here in roughly the order they appear in the source). There&#39;s the helper functions (e.g. for rounding to a power of 2). There&#39;s the default traits of the queue, which contain the constants and malloc/free functions used by the queue. There&#39;s the producer and consumer tokens. Then there&#39;s the queue&#39;s public API itself, starting with the constructor, destructor, and swap/assignment methods. There&#39;s the public enqueue methods, which are all wrappers around a small set of private enqueue methods found later on. There&#39;s the dequeue methods, which are defined inline and are relatively straightforward.&lt;/p&gt; 
&lt;p&gt;Then there&#39;s all the main internal data structures. First, there&#39;s a lock-free free list, used for recycling spent blocks (elements are enqueued to blocks internally). Then there&#39;s the block structure itself, which has two different ways of tracking whether it&#39;s fully emptied or not (remember, given two parallel consumers, there&#39;s no way to know which one will finish first) depending on where it&#39;s used. Then there&#39;s a small base class for the two types of internal SPMC producer queues (one for explicit producers that holds onto memory but attempts to be faster, and one for implicit ones which attempt to recycle more memory back into the parent but is a little slower). The explicit producer is defined first, then the implicit one. They both contain the same general four methods: One to enqueue, one to dequeue, one to enqueue in bulk, and one to dequeue in bulk. (Obviously they have constructors and destructors too, and helper methods.) The main difference between them is how the block handling is done (they both use the same blocks, but in different ways, and map indices to them in different ways).&lt;/p&gt; 
&lt;p&gt;Finally, there&#39;s the miscellaneous internal methods: There&#39;s the ones that handle the initial block pool (populated when the queue is constructed), and an abstract block pool that comprises the initial pool and any blocks on the free list. There&#39;s ones that handle the producer list (a lock-free add-only linked list of all the producers in the system). There&#39;s ones that handle the implicit producer lookup table (which is really a sort of specialized TLS lookup). And then there&#39;s some helper methods for allocating and freeing objects, and the data members of the queue itself, followed lastly by the free-standing swap functions.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>google/gemma.cpp</title>
      <link>https://github.com/google/gemma.cpp</link>
      <description>&lt;p&gt;lightweight, standalone C++ inference engine for Google&#39;s Gemma models.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;gemma.cpp&lt;/h1&gt; 
&lt;p&gt;gemma.cpp is a lightweight, standalone C++ inference engine for the Gemma foundation models from Google.&lt;/p&gt; 
&lt;p&gt;For additional information about Gemma, see &lt;a href=&quot;https://ai.google.dev/gemma&quot;&gt;ai.google.dev/gemma&lt;/a&gt;. Model weights, including gemma.cpp specific artifacts, are &lt;a href=&quot;https://www.kaggle.com/models/google/gemma&quot;&gt;available on kaggle&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Who is this project for?&lt;/h2&gt; 
&lt;p&gt;Modern LLM inference engines are sophisticated systems, often with bespoke capabilities extending beyond traditional neural network runtimes. With this comes opportunities for research and innovation through co-design of high level algorithms and low-level computation. However, there is a gap between deployment-oriented C++ inference runtimes, which are not designed for experimentation, and Python-centric ML research frameworks, which abstract away low-level computation through compilation.&lt;/p&gt; 
&lt;p&gt;gemma.cpp provides a minimalist implementation of Gemma-1, Gemma-2, Gemma-3, and PaliGemma models, focusing on simplicity and directness rather than full generality. This is inspired by vertically-integrated model implementations such as &lt;a href=&quot;https://github.com/ggerganov/ggml&quot;&gt;ggml&lt;/a&gt;, &lt;a href=&quot;https://github.com/karpathy/llama2.c&quot;&gt;llama.c&lt;/a&gt;, and &lt;a href=&quot;https://github.com/srush/llama2.rs&quot;&gt;llama.rs&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;gemma.cpp targets experimentation and research use cases. It is intended to be straightforward to embed in other projects with minimal dependencies and also easily modifiable with a small ~2K LoC core implementation (along with ~4K LoC of supporting utilities). We use the &lt;a href=&quot;https://github.com/google/highway&quot;&gt;Google Highway&lt;/a&gt; Library to take advantage of portable SIMD for CPU inference.&lt;/p&gt; 
&lt;p&gt;For production-oriented edge deployments we recommend standard deployment pathways using Python frameworks like JAX, Keras, PyTorch, and Transformers (&lt;a href=&quot;https://www.kaggle.com/models/google/gemma&quot;&gt;all model variations here&lt;/a&gt;).&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Community contributions large and small are welcome. See &lt;a href=&quot;https://github.com/google/gemma.cpp/raw/main/DEVELOPERS.md&quot;&gt;DEVELOPERS.md&lt;/a&gt; for additional notes contributing developers and &lt;a href=&quot;https://discord.gg/H5jCBAWxAe&quot;&gt;join the discord by following this invite link&lt;/a&gt;. This project follows &lt;a href=&quot;https://opensource.google.com/conduct/&quot;&gt;Google&#39;s Open Source Community Guidelines&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Active development is currently done on the &lt;code&gt;dev&lt;/code&gt; branch. Please open pull requests targeting &lt;code&gt;dev&lt;/code&gt; branch instead of &lt;code&gt;main&lt;/code&gt;, which is intended to be more stable.&lt;/em&gt;&lt;/p&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;h3&gt;System requirements&lt;/h3&gt; 
&lt;p&gt;Before starting, you should have installed:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://cmake.org/&quot;&gt;CMake&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://clang.llvm.org/get_started.html&quot;&gt;Clang C++ compiler&lt;/a&gt;, supporting at least C++17.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;tar&lt;/code&gt; for extracting archives from Kaggle.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Building natively on Windows requires the Visual Studio 2012 Build Tools with the optional Clang/LLVM C++ frontend (&lt;code&gt;clang-cl&lt;/code&gt;). This can be installed from the command line with &lt;a href=&quot;https://learn.microsoft.com/en-us/windows/package-manager/winget/&quot;&gt;&lt;code&gt;winget&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;winget install --id Kitware.CMake
winget install --id Microsoft.VisualStudio.2022.BuildTools --force --override &quot;--passive --wait --add Microsoft.VisualStudio.Workload.VCTools;installRecommended --add Microsoft.VisualStudio.Component.VC.Llvm.Clang --add Microsoft.VisualStudio.Component.VC.Llvm.ClangToolset&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Step 1: Obtain model weights and tokenizer from Kaggle or Hugging Face Hub&lt;/h3&gt; 
&lt;p&gt;Visit the &lt;a href=&quot;https://www.kaggle.com/models/google/gemma-2/gemmaCpp&quot;&gt;Kaggle page for Gemma-2&lt;/a&gt; &lt;a href=&quot;https://www.kaggle.com/models/google/gemma/frameworks/gemmaCpp&quot;&gt;or Gemma-1&lt;/a&gt;, and select &lt;code&gt;Model Variations |&amp;gt; Gemma C++&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;On this tab, the &lt;code&gt;Variation&lt;/code&gt; dropdown includes the options below. Note bfloat16 weights are higher fidelity, while 8-bit switched floating point weights enable faster inference. In general, we recommend starting with the &lt;code&gt;-sfp&lt;/code&gt; checkpoints.&lt;/p&gt; 
&lt;p&gt;If you are unsure which model to start with, we recommend starting with the smallest Gemma-2 model, i.e. &lt;code&gt;2.0-2b-it-sfp&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Alternatively, visit the &lt;a href=&quot;https://huggingface.co/models?other=gemma.cpp&quot;&gt;gemma.cpp&lt;/a&gt; models on the Hugging Face Hub. First go the model repository of the model of interest (see recommendations below). Then, click the &lt;code&gt;Files and versions&lt;/code&gt; tab and download the model and tokenizer files. For programmatic downloading, if you have &lt;code&gt;huggingface_hub&lt;/code&gt; installed, you can also download by running:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;huggingface-cli login # Just the first time
huggingface-cli download google/gemma-2b-sfp-cpp --local-dir build/
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Gemma-1 2B instruction-tuned (&lt;code&gt;it&lt;/code&gt;) and pre-trained (&lt;code&gt;pt&lt;/code&gt;) models:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model name&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;2b-it&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;2 billion parameter instruction-tuned model, bfloat16&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;2b-it-sfp&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;2 billion parameter instruction-tuned model, 8-bit switched floating point&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;2b-pt&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;2 billion parameter pre-trained model, bfloat16&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;2b-pt-sfp&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;2 billion parameter pre-trained model, 8-bit switched floating point&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Gemma-1 7B instruction-tuned (&lt;code&gt;it&lt;/code&gt;) and pre-trained (&lt;code&gt;pt&lt;/code&gt;) models:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model name&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;7b-it&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;7 billion parameter instruction-tuned model, bfloat16&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;7b-it-sfp&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;7 billion parameter instruction-tuned model, 8-bit switched floating point&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;7b-pt&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;7 billion parameter pre-trained model, bfloat16&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;7b-pt-sfp&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;7 billion parameter pre-trained model, 8-bit switched floating point&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] &lt;strong&gt;Important&lt;/strong&gt;: We strongly recommend starting off with the &lt;code&gt;2b-it-sfp&lt;/code&gt; model to get up and running.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Gemma 2 models are named &lt;code&gt;gemma2-2b-it&lt;/code&gt; for 2B and &lt;code&gt;9b-it&lt;/code&gt; or &lt;code&gt;27b-it&lt;/code&gt;. See the &lt;code&gt;kModelFlags&lt;/code&gt; definition in &lt;code&gt;common.cc&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Step 2: Extract Files&lt;/h3&gt; 
&lt;p&gt;If you downloaded the models from Hugging Face, skip to step 3.&lt;/p&gt; 
&lt;p&gt;After filling out the consent form, the download should proceed to retrieve a tar archive file &lt;code&gt;archive.tar.gz&lt;/code&gt;. Extract files from &lt;code&gt;archive.tar.gz&lt;/code&gt; (this can take a few minutes):&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;tar -xf archive.tar.gz
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This should produce a file containing model weights such as &lt;code&gt;2b-it-sfp.sbs&lt;/code&gt; and a tokenizer file (&lt;code&gt;tokenizer.spm&lt;/code&gt;). You may want to move these files to a convenient directory location (e.g. the &lt;code&gt;build/&lt;/code&gt; directory in this repo).&lt;/p&gt; 
&lt;h3&gt;Step 3: Build&lt;/h3&gt; 
&lt;p&gt;The build system uses &lt;a href=&quot;https://cmake.org/&quot;&gt;CMake&lt;/a&gt;. To build the gemma inference runtime, create a build directory and generate the build files using &lt;code&gt;cmake&lt;/code&gt; from the top-level project directory. Note if you previous ran &lt;code&gt;cmake&lt;/code&gt; and are re-running with a different setting, be sure to delete all files in the &lt;code&gt;build/&lt;/code&gt; directory with &lt;code&gt;rm -rf build/*&lt;/code&gt;.&lt;/p&gt; 
&lt;h4&gt;Unix-like Platforms&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;cmake -B build
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After running &lt;code&gt;cmake&lt;/code&gt;, you can enter the &lt;code&gt;build/&lt;/code&gt; directory and run &lt;code&gt;make&lt;/code&gt; to build the &lt;code&gt;./gemma&lt;/code&gt; executable:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;# Configure `build` directory
cmake --preset make

# Build project using make
cmake --build --preset make -j [number of parallel threads to use]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Replace &lt;code&gt;[number of parallel threads to use]&lt;/code&gt; with a number - the number of cores available on your system is a reasonable heuristic. For example, &lt;code&gt;make -j4 gemma&lt;/code&gt; will build using 4 threads. If the &lt;code&gt;nproc&lt;/code&gt; command is available, you can use &lt;code&gt;make -j$(nproc) gemma&lt;/code&gt; as a reasonable default for the number of threads.&lt;/p&gt; 
&lt;p&gt;If you aren&#39;t sure of the right value for the &lt;code&gt;-j&lt;/code&gt; flag, you can simply run &lt;code&gt;make gemma&lt;/code&gt; instead and it should still build the &lt;code&gt;./gemma&lt;/code&gt; executable.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] On Windows Subsystem for Linux (WSL) users should set the number of parallel threads to 1. Using a larger number may result in errors.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;If the build is successful, you should now have a &lt;code&gt;gemma&lt;/code&gt; executable in the &lt;code&gt;build/&lt;/code&gt; directory.&lt;/p&gt; 
&lt;h4&gt;Windows&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;# Configure `build` directory
cmake --preset windows

# Build project using Visual Studio Build Tools
cmake --build --preset windows -j [number of parallel threads to use]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If the build is successful, you should now have a &lt;code&gt;gemma.exe&lt;/code&gt; executable in the &lt;code&gt;build/&lt;/code&gt; directory.&lt;/p&gt; 
&lt;h4&gt;Bazel&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;bazel build -c opt --cxxopt=-std=c++20 :gemma
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If the build is successful, you should now have a &lt;code&gt;gemma&lt;/code&gt; executable in the &lt;code&gt;bazel-bin/&lt;/code&gt; directory.&lt;/p&gt; 
&lt;h4&gt;Make&lt;/h4&gt; 
&lt;p&gt;If you prefer Makefiles, @jart has made one available here:&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/jart/gemma3/raw/main/Makefile&quot;&gt;https://github.com/jart/gemma3/blob/main/Makefile&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Step 4: Run&lt;/h3&gt; 
&lt;p&gt;You can now run &lt;code&gt;gemma&lt;/code&gt; from inside the &lt;code&gt;build/&lt;/code&gt; directory.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;gemma&lt;/code&gt; has the following required arguments:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Argument&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Example value&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--model&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;The model type.&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;2b-it&lt;/code&gt; ... (see below)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--weights&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;The compressed weights file.&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;2b-it-sfp.sbs&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--weight_type&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;The compressed weight type.&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;sfp&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--tokenizer&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;The tokenizer file.&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;tokenizer.spm&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;code&gt;gemma&lt;/code&gt; is invoked as:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;./gemma \
--tokenizer [tokenizer file] \
--weights [compressed weights file] \
--weight_type [f32 or bf16 or sfp (default:sfp)] \
--model [2b-it or 2b-pt or 7b-it or 7b-pt or ...]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Example invocation for the following configuration:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Compressed weights file &lt;code&gt;2b-it-sfp.sbs&lt;/code&gt; (2B instruction-tuned model, 8-bit switched floating point).&lt;/li&gt; 
 &lt;li&gt;Tokenizer file &lt;code&gt;tokenizer.spm&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;./gemma \
--tokenizer tokenizer.spm \
--weights 2b-it-sfp.sbs --model 2b-it
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;RecurrentGemma&lt;/h3&gt; 
&lt;p&gt;This repository includes a version of Gemma based on Griffin (&lt;a href=&quot;https://arxiv.org/abs/2402.19427&quot;&gt;paper&lt;/a&gt;, &lt;a href=&quot;https://github.com/google-deepmind/recurrentgemma&quot;&gt;code&lt;/a&gt;). Its architecture includes both recurrent layers and local attention, thus it is more efficient for longer sequences and has a smaller memory footprint than standard Gemma. We here provide a C++ implementation of this model based on the paper.&lt;/p&gt; 
&lt;p&gt;To use the recurrent version of Gemma included in this repository, build the gemma binary as noted above in Step 3. Download the compressed weights and tokenizer from the RecurrentGemma &lt;a href=&quot;https://www.kaggle.com/models/google/recurrentgemma/gemmaCpp&quot;&gt;Kaggle&lt;/a&gt; as in Step 1, and run the binary as follows:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;./gemma --tokenizer tokenizer.spm --model gr2b-it --weights 2b-it-sfp.sbs&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;PaliGemma Vision-Language Model&lt;/h3&gt; 
&lt;p&gt;This repository includes a version of the PaliGemma VLM (&lt;a href=&quot;https://arxiv.org/abs/2407.07726&quot;&gt;paper&lt;/a&gt;, &lt;a href=&quot;https://github.com/google-research/big_vision/tree/main/big_vision/configs/proj/paligemma&quot;&gt;code&lt;/a&gt;) and its successor PaliGemma 2 (&lt;a href=&quot;https://arxiv.org/abs/2412.03555&quot;&gt;paper&lt;/a&gt;). We provide a C++ implementation of the PaliGemma model family here.&lt;/p&gt; 
&lt;p&gt;To use the version of PaliGemma included in this repository, build the gemma binary as noted above in Step 3. Download the compressed weights and tokenizer from &lt;a href=&quot;https://www.kaggle.com/models/google/paligemma/gemmaCpp/paligemma-3b-mix-224&quot;&gt;Kaggle&lt;/a&gt; and run the binary as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;./gemma \
--tokenizer paligemma_tokenizer.model \
--model paligemma-224 \
--weights paligemma-3b-mix-224-sfp.sbs \
--image_file paligemma/testdata/image.ppm
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note that the image reading code is very basic to avoid depending on an image processing library for now. We currently only support reading binary PPMs (P6). So use a tool like &lt;code&gt;convert&lt;/code&gt; to first convert your images into that format, e.g.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;convert image.jpeg -resize 224x224^ image.ppm&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;(As the image will be resized for processing anyway, we can already resize at this stage for slightly faster loading.)&lt;/p&gt; 
&lt;p&gt;The interaction with the image (using the mix-224 checkpoint) may then look something like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&amp;gt; Describe the image briefly
A large building with two towers in the middle of a city.
&amp;gt; What type of building is it?
church
&amp;gt; What color is the church?
gray
&amp;gt; caption image
A large building with two towers stands tall on the water&#39;s edge. The building
has a brown roof and a window on the side. A tree stands in front of the
building, and a flag waves proudly from its top. The water is calm and blue,
reflecting the sky above. A bridge crosses the water, and a red and white boat
rests on its surface. The building has a window on the side, and a flag on top.
A tall tree stands in front of the building, and a window on the building is
visible from the water. The water is green, and the sky is blue.
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Migrating to single-file format&lt;/h3&gt; 
&lt;p&gt;There is now a new format for the weights file, which is a single file that allows to contain the tokenizer (and the model type) directly. A tool to migrate from the multi-file format to the single-file format is available.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;compression/migrate_weights \
  --tokenizer .../tokenizer.spm --weights .../gemma2-2b-it-sfp.sbs \
  --model gemma2-2b-it --output_weights .../gemma2-2b-it-sfp-single.sbs
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After migration, you can use the new weights file with gemma.cpp like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;./gemma --weights .../gemma2-2b-it-sfp-single.sbs
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Troubleshooting and FAQs&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Running &lt;code&gt;./gemma&lt;/code&gt; fails with &quot;Failed to read cache gating_ein_0 (error 294) ...&quot;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;The most common problem is that the &lt;code&gt;--weight_type&lt;/code&gt; argument does not match that of the model file. Revisit step #3 and check which weights you downloaded.&lt;/p&gt; 
&lt;p&gt;Note that we have already moved weight type from a compile-time decision to a runtime argument. In a subsequent step, we plan to bake this information into the weights.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Problems building in Windows / Visual Studio&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Currently if you&#39;re using Windows, we recommend building in WSL (Windows Subsystem for Linux). We are exploring options to enable other build configurations, see issues for active discussion.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Model does not respond to instructions and produces strange output&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;A common issue is that you are using a pre-trained model, which is not instruction-tuned and thus does not respond to instructions. Make sure you are using an instruction-tuned model (&lt;code&gt;2b-it-sfp&lt;/code&gt;, &lt;code&gt;2b-it&lt;/code&gt;, &lt;code&gt;7b-it-sfp&lt;/code&gt;, &lt;code&gt;7b-it&lt;/code&gt;) and not a pre-trained model (any model with a &lt;code&gt;-pt&lt;/code&gt; suffix).&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;How do I convert my fine-tune to a &lt;code&gt;.sbs&lt;/code&gt; compressed model file?&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;For PaliGemma (1 and 2) checkpoints, you can use python/convert_from_safetensors.py to convert from safetensors format (tested with building via bazel). For an adapter model, you will likely need to call merge_and_unload() to convert the adapter model to a single-file format before converting it.&lt;/p&gt; 
&lt;p&gt;Here is how to use it using a bazel build of the compression library assuming locally installed (venv) torch, numpy, safetensors, absl-py, etc.:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;bazel build //compression/python:compression
BAZEL_OUTPUT_DIR=&quot;${PWD}/bazel-bin/compression&quot;
python3 -c &quot;import site; print(site.getsitepackages())&quot;
# Use your sites-packages file here:
ln -s $BAZEL_OUTPUT_DIR [...]/site-packages/compression
python3 python/convert_from_safetensors.py --load_path [...].safetensors.index.json
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See also compression/convert_weights.py for a slightly older option to convert a pytorch checkpoint. (The code may need updates to work with Gemma-2 models.)&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;What are some easy ways to make the model run faster?&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Make sure you are using the 8-bit switched floating point &lt;code&gt;-sfp&lt;/code&gt; models.&lt;/li&gt; 
 &lt;li&gt;If you&#39;re on a laptop, make sure power mode is set to maximize performance and saving mode is &lt;strong&gt;off&lt;/strong&gt;. For most laptops, the power saving modes get activated automatically if the computer is not plugged in.&lt;/li&gt; 
 &lt;li&gt;Close other unused cpu-intensive applications.&lt;/li&gt; 
 &lt;li&gt;On macs, anecdotally we observe a &quot;warm-up&quot; ramp-up in speed as performance cores get engaged.&lt;/li&gt; 
 &lt;li&gt;Experiment with the &lt;code&gt;--num_threads&lt;/code&gt; argument value. Depending on the device, larger numbers don&#39;t always mean better performance.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;We&#39;re also working on algorithmic and optimization approaches for faster inference, stay tuned.&lt;/p&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;p&gt;&lt;code&gt;gemma&lt;/code&gt; has different usage modes, controlled by the verbosity flag.&lt;/p&gt; 
&lt;p&gt;All usage modes are currently interactive, triggering text generation upon newline input.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Verbosity&lt;/th&gt; 
   &lt;th&gt;Usage mode&lt;/th&gt; 
   &lt;th&gt;Details&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--verbosity 0&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Minimal&lt;/td&gt; 
   &lt;td&gt;Only prints generation output. Suitable as a CLI tool.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--verbosity 1&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Default&lt;/td&gt; 
   &lt;td&gt;Standard user-facing terminal UI.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--verbosity 2&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Detailed&lt;/td&gt; 
   &lt;td&gt;Shows additional developer and debug info.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Interactive Terminal App&lt;/h3&gt; 
&lt;p&gt;By default, verbosity is set to 1, bringing up a terminal-based interactive interface when &lt;code&gt;gemma&lt;/code&gt; is invoked:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-console&quot;&gt;$ ./gemma [...]
  __ _  ___ _ __ ___  _ __ ___   __ _   ___ _ __  _ __
 / _` |/ _ \ &#39;_ ` _ \| &#39;_ ` _ \ / _` | / __| &#39;_ \| &#39;_ \
| (_| |  __/ | | | | | | | | | | (_| || (__| |_) | |_) |
 \__, |\___|_| |_| |_|_| |_| |_|\__,_(_)___| .__/| .__/
  __/ |                                    | |   | |
 |___/                                     |_|   |_|

tokenizer                     : tokenizer.spm
compressed_weights            : 2b-it-sfp.sbs
model                         : 2b-it
weights                       : [no path specified]
max_generated_tokens          : 2048

*Usage*
  Enter an instruction and press enter (%C reset conversation, %Q quits).

*Examples*
  - Write an email to grandma thanking her for the cookies.
  - What are some historical attractions to visit around Massachusetts?
  - Compute the nth fibonacci number in javascript.
  - Write a standup comedy bit about WebGPU programming.

&amp;gt; What are some outdoorsy places to visit around Boston?

[ Reading prompt ] .....................


**Boston Harbor and Islands:**

* **Boston Harbor Islands National and State Park:** Explore pristine beaches, wildlife, and maritime history.
* **Charles River Esplanade:** Enjoy scenic views of the harbor and city skyline.
* **Boston Harbor Cruise Company:** Take a relaxing harbor cruise and admire the city from a different perspective.
* **Seaport Village:** Visit a charming waterfront area with shops, restaurants, and a seaport museum.

**Forest and Nature:**

* **Forest Park:** Hike through a scenic forest with diverse wildlife.
* **Quabbin Reservoir:** Enjoy boating, fishing, and hiking in a scenic setting.
* **Mount Forest:** Explore a mountain with breathtaking views of the city and surrounding landscape.

...
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Usage as a Command Line Tool&lt;/h3&gt; 
&lt;p&gt;For using the &lt;code&gt;gemma&lt;/code&gt; executable as a command line tool, it may be useful to create an alias for gemma.cpp with arguments fully specified:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;alias gemma2b=&quot;~/gemma.cpp/build/gemma -- --tokenizer ~/gemma.cpp/build/tokenizer.spm --weights ~/gemma.cpp/build/gemma2-2b-it-sfp.sbs --model gemma2-2b-it --verbosity 0&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Replace the above paths with your own paths to the model and tokenizer paths from the download.&lt;/p&gt; 
&lt;p&gt;Here is an example of prompting &lt;code&gt;gemma&lt;/code&gt; with a truncated input file (using a &lt;code&gt;gemma2b&lt;/code&gt; alias like defined above):&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;cat configs.h | tail -n 35 | tr &#39;\n&#39; &#39; &#39; | xargs -0 echo &quot;What does this C++ code do: &quot; | gemma2b
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] CLI usage of gemma.cpp is experimental and should take context length limitations into account.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;The output of the above command should look like:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-console&quot;&gt;[ Reading prompt ] [...]
This C++ code snippet defines a set of **constants** used in a large language model (LLM) implementation, likely related to the **attention mechanism**.

Let&#39;s break down the code:
[...]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Incorporating gemma.cpp as a Library in your Project&lt;/h3&gt; 
&lt;p&gt;The easiest way to incorporate gemma.cpp in your own project is to pull in gemma.cpp and dependencies using &lt;code&gt;FetchContent&lt;/code&gt;. You can add the following to your CMakeLists.txt:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;include(FetchContent)

FetchContent_Declare(sentencepiece GIT_REPOSITORY https://github.com/google/sentencepiece GIT_TAG 53de76561cfc149d3c01037f0595669ad32a5e7c)
FetchContent_MakeAvailable(sentencepiece)

FetchContent_Declare(gemma GIT_REPOSITORY https://github.com/google/gemma.cpp GIT_TAG origin/main)
FetchContent_MakeAvailable(gemma)

FetchContent_Declare(highway GIT_REPOSITORY https://github.com/google/highway.git GIT_TAG da250571a45826b21eebbddc1e50d0c1137dee5f)
FetchContent_MakeAvailable(highway)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note for the gemma.cpp &lt;code&gt;GIT_TAG&lt;/code&gt;, you may replace &lt;code&gt;origin/main&lt;/code&gt; for a specific commit hash if you would like to pin the library version.&lt;/p&gt; 
&lt;p&gt;After your executable is defined (substitute your executable name for &lt;code&gt;[Executable Name]&lt;/code&gt; below):&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;target_link_libraries([Executable Name] libgemma hwy hwy_contrib sentencepiece)
FetchContent_GetProperties(gemma)
FetchContent_GetProperties(sentencepiece)
target_include_directories([Executable Name] PRIVATE ${gemma_SOURCE_DIR})
target_include_directories([Executable Name] PRIVATE ${sentencepiece_SOURCE_DIR})
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Building gemma.cpp as a Library&lt;/h3&gt; 
&lt;p&gt;gemma.cpp can also be used as a library dependency in your own project. The shared library artifact can be built by modifying the make invocation to build the &lt;code&gt;libgemma&lt;/code&gt; target instead of &lt;code&gt;gemma&lt;/code&gt;.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] If you are using gemma.cpp in your own project with the &lt;code&gt;FetchContent&lt;/code&gt; steps in the previous section, building the library is done automatically by &lt;code&gt;cmake&lt;/code&gt; and this section can be skipped.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;First, run &lt;code&gt;cmake&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;cmake -B build
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then, run &lt;code&gt;make&lt;/code&gt; with the &lt;code&gt;libgemma&lt;/code&gt; target:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;cd build
make -j [number of parallel threads to use] libgemma
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If this is successful, you should now have a &lt;code&gt;libgemma&lt;/code&gt; library file in the &lt;code&gt;build/&lt;/code&gt; directory. On Unix platforms, the filename is &lt;code&gt;libgemma.a&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Independent Projects Using gemma.cpp&lt;/h2&gt; 
&lt;p&gt;Some independent projects using gemma.cpp:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/namtranase/gemma-cpp-python&quot;&gt;gemma-cpp-python - Python bindings&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ufownl/lua-cgemma&quot;&gt;lua-cgemma - Lua bindings&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Rliop913/Gemma-godot-demo-project&quot;&gt;Godot engine demo project&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If you would like to have your project included, feel free to get in touch or submit a PR with a &lt;code&gt;README.md&lt;/code&gt; edit.&lt;/p&gt; 
&lt;h2&gt;Acknowledgements and Contacts&lt;/h2&gt; 
&lt;p&gt;gemma.cpp was started in fall 2023 by &lt;a href=&quot;mailto:austinvhuang@google.com&quot;&gt;Austin Huang&lt;/a&gt; and &lt;a href=&quot;mailto:janwas@google.com&quot;&gt;Jan Wassenberg&lt;/a&gt;, and subsequently released February 2024 thanks to contributions from Phil Culliton, Paul Chang, and Dan Zheng.&lt;/p&gt; 
&lt;p&gt;Griffin support was implemented in April 2024 thanks to contributions by Andrey Mikhaylov, Eugene Kliuchnikov, Jan Wassenberg, Jyrki Alakuijala, Lode Vandevenne, Luca Versari, Martin Bruse, Phil Culliton, Sami Boukortt, Thomas Fischbacher and Zoltan Szabadka.&lt;/p&gt; 
&lt;p&gt;Gemma-2 support was implemented in June/July 2024 with the help of several people.&lt;/p&gt; 
&lt;p&gt;PaliGemma support was implemented in September 2024 with contributions from Daniel Keysers.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;mailto:janwas@google.com&quot;&gt;Jan Wassenberg&lt;/a&gt; has continued to contribute many improvements, including major gains in efficiency, since the initial release.&lt;/p&gt; 
&lt;p&gt;This is not an officially supported Google product.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>doitsujin/dxvk</title>
      <link>https://github.com/doitsujin/dxvk</link>
      <description>&lt;p&gt;Vulkan-based implementation of D3D8, 9, 10 and 11 for Linux / Wine&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;DXVK&lt;/h1&gt; 
&lt;p&gt;A Vulkan-based translation layer for Direct3D 8/9/10/11 which allows running 3D applications on Linux using Wine.&lt;/p&gt; 
&lt;p&gt;For the current status of the project, please refer to the &lt;a href=&quot;https://github.com/doitsujin/dxvk/wiki&quot;&gt;project wiki&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The most recent development builds can be found &lt;a href=&quot;https://github.com/doitsujin/dxvk/actions/workflows/artifacts.yml?query=branch%3Amaster&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Release builds can be found &lt;a href=&quot;https://github.com/doitsujin/dxvk/releases&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;How to use&lt;/h2&gt; 
&lt;p&gt;In order to install a DXVK package obtained from the &lt;a href=&quot;https://github.com/doitsujin/dxvk/releases&quot;&gt;release&lt;/a&gt; page into a given wine prefix, copy or symlink the DLLs into the following directories as follows, then open &lt;code&gt;winecfg&lt;/code&gt; and manually add &lt;code&gt;native&lt;/code&gt; DLL overrides for &lt;code&gt;d3d8&lt;/code&gt;, &lt;code&gt;d3d9&lt;/code&gt;, &lt;code&gt;d3d10core&lt;/code&gt;, &lt;code&gt;d3d11&lt;/code&gt; and &lt;code&gt;dxgi&lt;/code&gt; under the Libraries tab.&lt;/p&gt; 
&lt;p&gt;In a default Wine prefix that would be as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;export WINEPREFIX=/path/to/wineprefix
cp x64/*.dll $WINEPREFIX/drive_c/windows/system32
cp x32/*.dll $WINEPREFIX/drive_c/windows/syswow64
winecfg
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For a pure 32-bit Wine prefix (non default) the 32-bit DLLs instead go to the &lt;code&gt;system32&lt;/code&gt; directory:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;export WINEPREFIX=/path/to/wineprefix
cp x32/*.dll $WINEPREFIX/drive_c/windows/system32
winecfg
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Verify that your application uses DXVK instead of wined3d by enabling the HUD (see notes below).&lt;/p&gt; 
&lt;p&gt;In order to remove DXVK from a prefix, remove the DLLs and DLL overrides, and run &lt;code&gt;wineboot -u&lt;/code&gt; to restore the original DLL files.&lt;/p&gt; 
&lt;p&gt;Tools such as Steam Play, Lutris, Bottles, Heroic Launcher, etc will automatically handle setup of dxvk on their own when enabled.&lt;/p&gt; 
&lt;h4&gt;DLL dependencies&lt;/h4&gt; 
&lt;p&gt;Listed below are the DLL requirements for using DXVK with any single API.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;d3d8: &lt;code&gt;d3d8.dll&lt;/code&gt; and &lt;code&gt;d3d9.dll&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;d3d9: &lt;code&gt;d3d9.dll&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;d3d10: &lt;code&gt;d3d10core.dll&lt;/code&gt;, &lt;code&gt;d3d11.dll&lt;/code&gt; and &lt;code&gt;dxgi.dll&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;d3d11: &lt;code&gt;d3d11.dll&lt;/code&gt; and &lt;code&gt;dxgi.dll&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Notes on Vulkan drivers&lt;/h3&gt; 
&lt;p&gt;Before reporting an issue, please check the &lt;a href=&quot;https://github.com/doitsujin/dxvk/wiki/Driver-support&quot;&gt;Wiki&lt;/a&gt; page on the current driver status and make sure you run a recent enough driver version for your hardware.&lt;/p&gt; 
&lt;h3&gt;Online multi-player games&lt;/h3&gt; 
&lt;p&gt;Manipulation of Direct3D libraries in multi-player games may be considered cheating and can get your account &lt;strong&gt;banned&lt;/strong&gt;. This may also apply to single-player games with an embedded or dedicated multiplayer portion. &lt;strong&gt;Use at your own risk.&lt;/strong&gt;&lt;/p&gt; 
&lt;h3&gt;HUD&lt;/h3&gt; 
&lt;p&gt;The &lt;code&gt;DXVK_HUD&lt;/code&gt; environment variable controls a HUD which can display the framerate and some stat counters. It accepts a comma-separated list of the following options:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;devinfo&lt;/code&gt;: Displays the name of the GPU and the driver version.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;fps&lt;/code&gt;: Shows the current frame rate.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;frametimes&lt;/code&gt;: Shows a frame time graph.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;submissions&lt;/code&gt;: Shows the number of command buffers submitted per frame.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;drawcalls&lt;/code&gt;: Shows the number of draw calls and render passes per frame.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;pipelines&lt;/code&gt;: Shows the total number of graphics and compute pipelines.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;descriptors&lt;/code&gt;: Shows the number of descriptor pools and descriptor sets.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;memory&lt;/code&gt;: Shows the amount of device memory allocated and used.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;allocations&lt;/code&gt;: Shows detailed memory chunk suballocation info.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;gpuload&lt;/code&gt;: Shows estimated GPU load. May be inaccurate.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;version&lt;/code&gt;: Shows DXVK version.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;api&lt;/code&gt;: Shows the D3D feature level used by the application.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;cs&lt;/code&gt;: Shows worker thread statistics.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;compiler&lt;/code&gt;: Shows shader compiler activity&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;samplers&lt;/code&gt;: Shows the current number of sampler pairs used &lt;em&gt;[D3D9 Only]&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ffshaders&lt;/code&gt;: Shows the current number of shaders generated from fixed function state &lt;em&gt;[D3D9 Only]&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;swvp&lt;/code&gt;: Shows whether or not the device is running in software vertex processing mode &lt;em&gt;[D3D9 Only]&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;scale=x&lt;/code&gt;: Scales the HUD by a factor of &lt;code&gt;x&lt;/code&gt; (e.g. &lt;code&gt;1.5&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;opacity=y&lt;/code&gt;: Adjusts the HUD opacity by a factor of &lt;code&gt;y&lt;/code&gt; (e.g. &lt;code&gt;0.5&lt;/code&gt;, &lt;code&gt;1.0&lt;/code&gt; being fully opaque).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Additionally, &lt;code&gt;DXVK_HUD=1&lt;/code&gt; has the same effect as &lt;code&gt;DXVK_HUD=devinfo,fps&lt;/code&gt;, and &lt;code&gt;DXVK_HUD=full&lt;/code&gt; enables all available HUD elements.&lt;/p&gt; 
&lt;h3&gt;Logs&lt;/h3&gt; 
&lt;p&gt;When used with Wine, DXVK will print log messages to &lt;code&gt;stderr&lt;/code&gt;. Additionally, standalone log files can optionally be generated by setting the &lt;code&gt;DXVK_LOG_PATH&lt;/code&gt; variable, where log files in the given directory will be called &lt;code&gt;app_d3d11.log&lt;/code&gt;, &lt;code&gt;app_dxgi.log&lt;/code&gt; etc., where &lt;code&gt;app&lt;/code&gt; is the name of the game executable.&lt;/p&gt; 
&lt;p&gt;On Windows, log files will be created in the game&#39;s working directory by default, which is usually next to the game executable.&lt;/p&gt; 
&lt;h3&gt;Frame rate limit&lt;/h3&gt; 
&lt;p&gt;The &lt;code&gt;DXVK_FRAME_RATE&lt;/code&gt; environment variable can be used to limit the frame rate. A value of &lt;code&gt;0&lt;/code&gt; uncaps the frame rate, while any positive value will limit rendering to the given number of frames per second. Alternatively, the configuration file can be used.&lt;/p&gt; 
&lt;h3&gt;Device filter&lt;/h3&gt; 
&lt;p&gt;Some applications do not provide a method to select a different GPU. In that case, DXVK can be forced to use a given device:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;DXVK_FILTER_DEVICE_NAME=&quot;Device Name&quot;&lt;/code&gt; Selects devices with a matching Vulkan device name, which can be retrieved with tools such as &lt;code&gt;vulkaninfo&lt;/code&gt;. Matches on substrings, so &quot;VEGA&quot; or &quot;AMD RADV VEGA10&quot; is supported if the full device name is &quot;AMD RADV VEGA10 (LLVM 9.0.0)&quot;, for example. If the substring matches more than one device, the first device matched will be used.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; If the device filter is configured incorrectly, it may filter out all devices and applications will be unable to create a D3D device.&lt;/p&gt; 
&lt;h3&gt;Debugging&lt;/h3&gt; 
&lt;p&gt;The following environment variables can be used for &lt;strong&gt;debugging&lt;/strong&gt; purposes.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;VK_INSTANCE_LAYERS=VK_LAYER_KHRONOS_validation&lt;/code&gt; Enables Vulkan debug layers. Highly recommended for troubleshooting rendering issues and driver crashes. Requires the Vulkan SDK to be installed on the host system.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;DXVK_LOG_LEVEL=none|error|warn|info|debug&lt;/code&gt; Controls message logging.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;DXVK_LOG_PATH=/some/directory&lt;/code&gt; Changes path where log files are stored. Set to &lt;code&gt;none&lt;/code&gt; to disable log file creation entirely, without disabling logging.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;DXVK_DEBUG=markers|validation&lt;/code&gt; Enables use of the &lt;code&gt;VK_EXT_debug_utils&lt;/code&gt; extension for translating performance event markers, or to enable Vulkan validation, respecticely.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;DXVK_CONFIG_FILE=/xxx/dxvk.conf&lt;/code&gt; Sets path to the configuration file.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;DXVK_CONFIG=&quot;dxgi.hideAmdGpu = True; dxgi.syncInterval = 0&quot;&lt;/code&gt; Can be used to set config variables through the environment instead of a configuration file using the same syntax. &lt;code&gt;;&lt;/code&gt; is used as a seperator.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Graphics Pipeline Library&lt;/h3&gt; 
&lt;p&gt;On drivers which support &lt;code&gt;VK_EXT_graphics_pipeline_library&lt;/code&gt; Vulkan shaders will be compiled at the time the game loads its D3D shaders, rather than at draw time. This reduces or eliminates shader compile stutter in many games when compared to the previous system.&lt;/p&gt; 
&lt;p&gt;In games that load their shaders during loading screens or in the menu, this can lead to prolonged periods of very high CPU utilization, especially on weaker CPUs. For affected games it is recommended to wait for shader compilation to finish before starting the game to avoid stutter and low performance. Shader compiler activity can be monitored with &lt;code&gt;DXVK_HUD=compiler&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;This feature largely replaces the state cache.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Games which only load their D3D shaders at draw time (e.g. most Unreal Engine games) will still exhibit some stutter, although it should still be less severe than without this feature.&lt;/p&gt; 
&lt;h3&gt;State cache&lt;/h3&gt; 
&lt;p&gt;DXVK caches pipeline state by default, so that shaders can be recompiled ahead of time on subsequent runs of an application, even if the driver&#39;s own shader cache got invalidated in the meantime. This cache is enabled by default, and generally reduces stuttering.&lt;/p&gt; 
&lt;p&gt;The following environment variables can be used to control the cache:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;DXVK_STATE_CACHE&lt;/code&gt;: Controls the state cache. The following values are supported: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;disable&lt;/code&gt;: Disables the cache entirely.&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;reset&lt;/code&gt;: Clears the cache file.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;DXVK_STATE_CACHE_PATH=/some/directory&lt;/code&gt; Specifies a directory where to put the cache files. Defaults to the current working directory of the application.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This feature is mostly only relevant on systems without support for &lt;code&gt;VK_EXT_graphics_pipeline_library&lt;/code&gt;&lt;/p&gt; 
&lt;h2&gt;Build instructions&lt;/h2&gt; 
&lt;p&gt;In order to pull in all submodules that are needed for building, clone the repository using the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;git clone --recursive https://github.com/doitsujin/dxvk.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Requirements:&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.winehq.org/&quot;&gt;wine 7.1&lt;/a&gt; or newer&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://mesonbuild.com/&quot;&gt;Meson&lt;/a&gt; build system (at least version 0.58)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.mingw-w64.org&quot;&gt;Mingw-w64&lt;/a&gt; compiler and headers (at least version 10.0)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/KhronosGroup/glslang&quot;&gt;glslang&lt;/a&gt; compiler&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Building DLLs&lt;/h3&gt; 
&lt;h4&gt;The simple way&lt;/h4&gt; 
&lt;p&gt;Inside the DXVK directory, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;./package-release.sh master /your/target/directory --no-package
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will create a folder &lt;code&gt;dxvk-master&lt;/code&gt; in &lt;code&gt;/your/target/directory&lt;/code&gt;, which contains both 32-bit and 64-bit versions of DXVK, which can be set up in the same way as the release versions as noted above.&lt;/p&gt; 
&lt;p&gt;In order to preserve the build directories for development, pass &lt;code&gt;--dev-build&lt;/code&gt; to the script. This option implies &lt;code&gt;--no-package&lt;/code&gt;. After making changes to the source code, you can then do the following to rebuild DXVK:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;# change to build.32 for 32-bit
cd /your/target/directory/build.64
ninja install
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Compiling manually&lt;/h4&gt; 
&lt;pre&gt;&lt;code&gt;# 64-bit build. For 32-bit builds, replace
# build-win64.txt with build-win32.txt
meson setup --cross-file build-win64.txt --buildtype release --prefix /your/dxvk/directory build.w64
cd build.w64
ninja install
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The D3D8, D3D9, D3D10, D3D11 and DXGI DLLs will be located in &lt;code&gt;/your/dxvk/directory/bin&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Build troubleshooting&lt;/h3&gt; 
&lt;p&gt;DXVK requires threading support from your mingw-w64 build environment. If you are missing this, you may see &quot;error: ‘std::cv_status’ has not been declared&quot; or similar threading related errors.&lt;/p&gt; 
&lt;p&gt;On Debian and Ubuntu, this can be resolved by using the posix alternate, which supports threading. For example, choose the posix alternate from these commands:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;update-alternatives --config x86_64-w64-mingw32-gcc
update-alternatives --config x86_64-w64-mingw32-g++
update-alternatives --config i686-w64-mingw32-gcc
update-alternatives --config i686-w64-mingw32-g++
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For non debian based distros, make sure that your mingw-w64-gcc cross compiler does have &lt;code&gt;--enable-threads=posix&lt;/code&gt; enabled during configure. If your distro does ship its mingw-w64-gcc binary with &lt;code&gt;--enable-threads=win32&lt;/code&gt; you might have to recompile locally or open a bug at your distro&#39;s bugtracker to ask for it.&lt;/p&gt; 
&lt;h1&gt;DXVK Native&lt;/h1&gt; 
&lt;p&gt;DXVK Native is a version of DXVK which allows it to be used natively without Wine.&lt;/p&gt; 
&lt;p&gt;This is primarily useful for game and application ports to either avoid having to write another rendering backend, or to help with port bringup during development.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/doitsujin/dxvk/releases&quot;&gt;Release builds&lt;/a&gt; are built using the Steam Runtime.&lt;/p&gt; 
&lt;h3&gt;How does it work?&lt;/h3&gt; 
&lt;p&gt;DXVK Native replaces certain Windows-isms with a platform and framework-agnostic replacement, for example, &lt;code&gt;HWND&lt;/code&gt;s can become &lt;code&gt;SDL_Window*&lt;/code&gt;s, etc. All it takes to do that is to add another WSI backend.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; DXVK Native requires a backend to be explicitly set via the &lt;code&gt;DXVK_WSI_DRIVER&lt;/code&gt; environment variable. The current built-in options are &lt;code&gt;SDL3&lt;/code&gt;, &lt;code&gt;SDL2&lt;/code&gt;, and &lt;code&gt;GLFW&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;DXVK Native comes with a slim set of Windows header definitions required for D3D9/11 and the MinGW headers for D3D9/11. In most cases, it will end up being plug and play with your renderer, but there may be certain teething issues such as:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;__uuidof(type)&lt;/code&gt; is supported, but &lt;code&gt;__uuidof(variable)&lt;/code&gt; is not supported. Use &lt;code&gt;__uuidof_var(variable)&lt;/code&gt; instead.&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>zeux/meshoptimizer</title>
      <link>https://github.com/zeux/meshoptimizer</link>
      <description>&lt;p&gt;Mesh optimization library that makes meshes smaller and faster to render&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;🐇 meshoptimizer &lt;a href=&quot;https://github.com/zeux/meshoptimizer/actions&quot;&gt;&lt;img src=&quot;https://github.com/zeux/meshoptimizer/workflows/build/badge.svg?sanitize=true&quot; alt=&quot;Actions Status&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://codecov.io/github/zeux/meshoptimizer?branch=master&quot;&gt;&lt;img src=&quot;https://codecov.io/github/zeux/meshoptimizer/coverage.svg?branch=master&quot; alt=&quot;codecov.io&quot;&gt;&lt;/a&gt; &lt;img src=&quot;https://img.shields.io/badge/license-MIT-blue.svg?sanitize=true&quot; alt=&quot;MIT&quot;&gt; &lt;a href=&quot;https://github.com/zeux/meshoptimizer&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/repo-github-green.svg?sanitize=true&quot; alt=&quot;GitHub&quot;&gt;&lt;/a&gt;&lt;/h1&gt; 
&lt;h2&gt;Purpose&lt;/h2&gt; 
&lt;p&gt;When a GPU renders triangle meshes, various stages of the GPU pipeline have to process vertex and index data. The efficiency of these stages depends on the data you feed to them; this library provides algorithms to help optimize meshes for these stages, as well as algorithms to reduce the mesh complexity and storage overhead.&lt;/p&gt; 
&lt;p&gt;The library provides a C and C++ interface for all algorithms; you can use it from C/C++ or from other languages via FFI (such as P/Invoke). If you want to use this library from Rust, you should use &lt;a href=&quot;https://crates.io/crates/meshopt&quot;&gt;meshopt crate&lt;/a&gt;. JavaScript interface for some algorithms is available through &lt;a href=&quot;https://www.npmjs.com/package/meshoptimizer&quot;&gt;meshoptimizer.js&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/zeux/meshoptimizer/master/gltf/README.md&quot;&gt;gltfpack&lt;/a&gt;, which is a tool that can automatically optimize glTF files, is developed and distributed alongside the library.&lt;/p&gt; 
&lt;h2&gt;Installing&lt;/h2&gt; 
&lt;p&gt;meshoptimizer is hosted on GitHub; you can download the latest release using git:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;git clone -b v0.23 https://github.com/zeux/meshoptimizer.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively you can &lt;a href=&quot;https://github.com/zeux/meshoptimizer/archive/v0.23.zip&quot;&gt;download the .zip archive from GitHub&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The library is also available as a Linux package in several distributions (&lt;a href=&quot;https://aur.archlinux.org/packages/meshoptimizer/&quot;&gt;ArchLinux&lt;/a&gt;, &lt;a href=&quot;https://packages.debian.org/libmeshoptimizer&quot;&gt;Debian&lt;/a&gt;, &lt;a href=&quot;https://www.freshports.org/misc/meshoptimizer/&quot;&gt;FreeBSD&lt;/a&gt;, &lt;a href=&quot;https://mynixos.com/nixpkgs/package/meshoptimizer&quot;&gt;Nix&lt;/a&gt;, &lt;a href=&quot;https://packages.ubuntu.com/libmeshoptimizer&quot;&gt;Ubuntu&lt;/a&gt;), as well as a &lt;a href=&quot;https://github.com/microsoft/vcpkg/tree/master/ports/meshoptimizer&quot;&gt;Vcpkg port&lt;/a&gt; (see &lt;a href=&quot;https://learn.microsoft.com/en-us/vcpkg/get_started/get-started&quot;&gt;installation instructions&lt;/a&gt;) and a &lt;a href=&quot;https://conan.io/center/recipes/meshoptimizer&quot;&gt;Conan package&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/zeux/meshoptimizer/master/gltf/README.md&quot;&gt;gltfpack&lt;/a&gt; is available as a pre-built binary on &lt;a href=&quot;https://github.com/zeux/meshoptimizer/releases&quot;&gt;Releases page&lt;/a&gt; or via &lt;a href=&quot;https://www.npmjs.com/package/gltfpack&quot;&gt;npm package&lt;/a&gt;. Native binaries are recommended since they are more efficient and support texture compression.&lt;/p&gt; 
&lt;h2&gt;Building&lt;/h2&gt; 
&lt;p&gt;meshoptimizer is distributed as a set of C++ source files. To include it into your project, you can use one of the two options:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Use CMake to build the library (either as a standalone project or as part of your project)&lt;/li&gt; 
 &lt;li&gt;Add source files to your project&#39;s build system&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The source files are organized in such a way that you don&#39;t need to change your build-system settings, and you only need to add the source files for the algorithms you use. They should build without warnings or special compilation options on all major compilers.&lt;/p&gt; 
&lt;h2&gt;Pipeline&lt;/h2&gt; 
&lt;p&gt;When optimizing a mesh, you should typically feed it through a set of optimizations (the order is important!):&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Indexing&lt;/li&gt; 
 &lt;li&gt;(optional, discussed last) Simplification&lt;/li&gt; 
 &lt;li&gt;Vertex cache optimization&lt;/li&gt; 
 &lt;li&gt;Overdraw optimization&lt;/li&gt; 
 &lt;li&gt;Vertex fetch optimization&lt;/li&gt; 
 &lt;li&gt;Vertex quantization&lt;/li&gt; 
 &lt;li&gt;Shadow indexing&lt;/li&gt; 
 &lt;li&gt;(optional) Vertex/index buffer compression&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Indexing&lt;/h2&gt; 
&lt;p&gt;Most algorithms in this library assume that a mesh has a vertex buffer and an index buffer. For algorithms to work well and also for GPU to render your mesh efficiently, the vertex buffer has to have no redundant vertices; you can generate an index buffer from an unindexed vertex buffer or reindex an existing (potentially redundant) index buffer as follows:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Note: meshoptimizer generally works with 32-bit (&lt;code&gt;unsigned int&lt;/code&gt;) indices, however when using C++ APIs you can use any integer type for index data by using the provided template overloads. By convention, remap tables always use &lt;code&gt;unsigned int&lt;/code&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;First, generate a remap table from your existing vertex (and, optionally, index) data:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;size_t index_count = face_count * 3;
size_t unindexed_vertex_count = face_count * 3;
std::vector&amp;lt;unsigned int&amp;gt; remap(unindexed_vertex_count); // temporary remap table
size_t vertex_count = meshopt_generateVertexRemap(&amp;amp;remap[0], NULL, index_count, &amp;amp;unindexed_vertices[0], unindexed_vertex_count, sizeof(Vertex));
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note that in this case we only have an unindexed vertex buffer; when input mesh has an index buffer, it will need to be passed to &lt;code&gt;meshopt_generateVertexRemap&lt;/code&gt; instead of &lt;code&gt;NULL&lt;/code&gt;, along with the correct source vertex count. In either case, the remap table is generated based on binary equivalence of the input vertices, so the resulting mesh will render the same way. Binary equivalence considers all input bytes, including padding which should be zero-initialized if the vertex structure has gaps.&lt;/p&gt; 
&lt;p&gt;After generating the remap table, you can allocate space for the target vertex buffer (&lt;code&gt;vertex_count&lt;/code&gt; elements) and index buffer (&lt;code&gt;index_count&lt;/code&gt; elements) and generate them:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;meshopt_remapIndexBuffer(indices, NULL, index_count, &amp;amp;remap[0]);
meshopt_remapVertexBuffer(vertices, &amp;amp;unindexed_vertices[0], unindexed_vertex_count, sizeof(Vertex), &amp;amp;remap[0]);
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can then further optimize the resulting buffers by calling the other functions on them in-place.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;meshopt_generateVertexRemap&lt;/code&gt; uses binary equivalence of vertex data, which is generally a reasonable default; however, in some cases some attributes may have floating point drift causing extra vertices to be generated. For such cases, it may be necessary to quantize some attributes (most importantly, normals and tangents) before generating the remap, or use a custom weld algorithm that supports per-attribute tolerance instead.&lt;/p&gt; 
&lt;h2&gt;Vertex cache optimization&lt;/h2&gt; 
&lt;p&gt;When the GPU renders the mesh, it has to run the vertex shader for each vertex; usually GPUs have a built-in fixed size cache that stores the transformed vertices (the result of running the vertex shader), and uses this cache to reduce the number of vertex shader invocations. This cache is usually small, 16-32 vertices, and can have different replacement policies; to use this cache efficiently, you have to reorder your triangles to maximize the locality of reused vertex references like so:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;meshopt_optimizeVertexCache(indices, indices, index_count, vertex_count);
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Overdraw optimization&lt;/h2&gt; 
&lt;p&gt;After transforming the vertices, GPU sends the triangles for rasterization which results in generating pixels that are usually first ran through the depth test, and pixels that pass it get the pixel shader executed to generate the final color. As pixel shaders get more expensive, it becomes more and more important to reduce overdraw. While in general improving overdraw requires view-dependent operations, this library provides an algorithm to reorder triangles to minimize the overdraw from all directions, which you should run after vertex cache optimization like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;meshopt_optimizeOverdraw(indices, indices, index_count, &amp;amp;vertices[0].x, vertex_count, sizeof(Vertex), 1.05f);
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The overdraw optimizer needs to read vertex positions as a float3 from the vertex; the code snippet above assumes that the vertex stores position as &lt;code&gt;float x, y, z&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;When performing the overdraw optimization you have to specify a floating-point threshold parameter. The algorithm tries to maintain a balance between vertex cache efficiency and overdraw; the threshold determines how much the algorithm can compromise the vertex cache hit ratio, with 1.05 meaning that the resulting ratio should be at most 5% worse than before the optimization.&lt;/p&gt; 
&lt;h2&gt;Vertex fetch optimization&lt;/h2&gt; 
&lt;p&gt;After the final triangle order has been established, we still can optimize the vertex buffer for memory efficiency. Before running the vertex shader GPU has to fetch the vertex attributes from the vertex buffer; the fetch is usually backed by a memory cache, and as such optimizing the data for the locality of memory access is important. You can do this by running this code:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;meshopt_optimizeVertexFetch(vertices, indices, index_count, vertices, vertex_count, sizeof(Vertex));
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will reorder the vertices in the vertex buffer to try to improve the locality of reference, and rewrite the indices in place to match; if the vertex data is stored using multiple streams, you should use &lt;code&gt;meshopt_optimizeVertexFetchRemap&lt;/code&gt; instead. This optimization has to be performed on the final index buffer since the optimal vertex order depends on the triangle order.&lt;/p&gt; 
&lt;p&gt;Note that the algorithm does not try to model cache replacement precisely and instead just orders vertices in the order of use, which generally produces results that are close to optimal.&lt;/p&gt; 
&lt;h2&gt;Vertex quantization&lt;/h2&gt; 
&lt;p&gt;To optimize memory bandwidth when fetching the vertex data even further, and to reduce the amount of memory required to store the mesh, it is often beneficial to quantize the vertex attributes to smaller types. While this optimization can technically run at any part of the pipeline (and sometimes doing quantization as the first step can improve indexing by merging almost identical vertices), it generally is easier to run this after all other optimizations since some of them require access to float3 positions.&lt;/p&gt; 
&lt;p&gt;Quantization is usually domain specific; it&#39;s common to quantize normals using 3 8-bit integers but you can use higher-precision quantization (for example using 10 bits per component in a 10_10_10_2 format), or a different encoding to use just 2 components. For positions and texture coordinate data the two most common storage formats are half precision floats, and 16-bit normalized integers that encode the position relative to the AABB of the mesh or the UV bounding rectangle.&lt;/p&gt; 
&lt;p&gt;The number of possible combinations here is very large but this library does provide the building blocks, specifically functions to quantize floating point values to normalized integers, as well as half-precision floats. For example, here&#39;s how you can quantize a normal:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;unsigned int normal =
    (meshopt_quantizeUnorm(v.nx, 10) &amp;lt;&amp;lt; 20) |
    (meshopt_quantizeUnorm(v.ny, 10) &amp;lt;&amp;lt; 10) |
     meshopt_quantizeUnorm(v.nz, 10);
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;and here&#39;s how you can quantize a position:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;unsigned short px = meshopt_quantizeHalf(v.x);
unsigned short py = meshopt_quantizeHalf(v.y);
unsigned short pz = meshopt_quantizeHalf(v.z);
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Since quantized vertex attributes often need to remain in their compact representations for efficient transfer and storage, they are usually dequantized during vertex processing by configuring the GPU vertex input correctly to expect normalized integers or half precision floats, which often needs no or minimal changes to the shader code. When CPU dequantization is required instead, &lt;code&gt;meshopt_dequantizeHalf&lt;/code&gt; can be used to convert half precision values back to single precision; for normalized integer formats, the dequantization just requires dividing by 2^N-1 for unorm and 2^(N-1)-1 for snorm variants, for example manually reversing &lt;code&gt;meshopt_quantizeUnorm(v, 10)&lt;/code&gt; can be done by dividing by 1023.&lt;/p&gt; 
&lt;h2&gt;Shadow indexing&lt;/h2&gt; 
&lt;p&gt;Many rendering pipelines require meshes to be rendered to depth-only targets, such as shadow maps or during a depth pre-pass, in addition to color/G-buffer targets. While using the same geometry data for both cases is possible, reducing the number of unique vertices for depth-only rendering can be beneficial, especially when the source geometry has many attribute seams due to faceted shading or lightmap texture seams.&lt;/p&gt; 
&lt;p&gt;To achieve this, this library provides the &lt;code&gt;meshopt_generateShadowIndexBuffer&lt;/code&gt; algorithm, which generates a second (shadow) index buffer that can be used with the original vertex data:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;std::vector&amp;lt;unsigned int&amp;gt; shadow_indices(index_count);
// note: this assumes Vertex starts with float3 positions and should be adjusted accordingly for quantized positions
meshopt_generateShadowIndexBuffer(&amp;amp;shadow_indices[0], indices, index_count, &amp;amp;vertices[0].x, vertex_count, sizeof(float) * 3, sizeof(Vertex));
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Because the vertex data is shared, shadow indexing should be done after other optimizations of the vertex/index data. However, it&#39;s possible (and recommended) to optimize the resulting shadow index buffer for vertex cache:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;meshopt_optimizeVertexCache(&amp;amp;shadow_indices[0], &amp;amp;shadow_indices[0], index_count, vertex_count);
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;In some cases, it may be beneficial to split the vertex positions into a separate buffer to maximize efficiency for depth-only rendering. Note that the example above assumes only positions are relevant for shadow rendering, but more complex materials may require adding texture coordinates (for alpha testing) or skinning data to the vertex portion used as a key. &lt;code&gt;meshopt_generateShadowIndexBufferMulti&lt;/code&gt; can be useful for these cases if the relevant data is not contiguous.&lt;/p&gt; 
&lt;h2&gt;Vertex/index buffer compression&lt;/h2&gt; 
&lt;p&gt;In case storage size or transmission bandwidth is of importance, you might want to additionally compress vertex and index data. While several mesh compression libraries, like Google Draco, are available, they typically are designed to maximize the compression ratio at the cost of disturbing the vertex/index order (which makes the meshes inefficient to render on GPU) or decompression performance. They also frequently don&#39;t support custom game-ready quantized vertex formats and thus require to re-quantize the data after loading it, introducing extra quantization errors and making decoding slower.&lt;/p&gt; 
&lt;p&gt;Alternatively you can use general purpose compression libraries like zstd or Oodle to compress vertex/index data - however these compressors aren&#39;t designed to exploit redundancies in vertex/index data and as such compression rates can be unsatisfactory.&lt;/p&gt; 
&lt;p&gt;To that end, this library provides algorithms to &quot;encode&quot; vertex and index data. The result of the encoding is generally significantly smaller than initial data, and remains compressible with general purpose compressors - so you can either store encoded data directly (for modest compression ratios and maximum decoding performance), or further compress it with zstd/Oodle to maximize compression ratio.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Note: this compression scheme is available as a glTF extension &lt;a href=&quot;https://github.com/KhronosGroup/glTF/raw/main/extensions/2.0/Vendor/EXT_meshopt_compression/README.md&quot;&gt;EXT_meshopt_compression&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;To encode, you need to allocate target buffers (preferably using the worst case bound) and call encoding functions:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;std::vector&amp;lt;unsigned char&amp;gt; vbuf(meshopt_encodeVertexBufferBound(vertex_count, sizeof(Vertex)));
vbuf.resize(meshopt_encodeVertexBuffer(&amp;amp;vbuf[0], vbuf.size(), vertices, vertex_count, sizeof(Vertex)));

std::vector&amp;lt;unsigned char&amp;gt; ibuf(meshopt_encodeIndexBufferBound(index_count, vertex_count));
ibuf.resize(meshopt_encodeIndexBuffer(&amp;amp;ibuf[0], ibuf.size(), indices, index_count));
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can then either serialize &lt;code&gt;vbuf&lt;/code&gt;/&lt;code&gt;ibuf&lt;/code&gt; as is, or compress them further. To decode the data at runtime, call decoding functions:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;int resvb = meshopt_decodeVertexBuffer(vertices, vertex_count, sizeof(Vertex), &amp;amp;vbuf[0], vbuf.size());
int resib = meshopt_decodeIndexBuffer(indices, index_count, &amp;amp;ibuf[0], ibuf.size());
assert(resvb == 0 &amp;amp;&amp;amp; resib == 0);
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note that vertex encoding assumes that vertex buffer was optimized for vertex fetch, and that vertices are quantized; index encoding assumes that the vertex/index buffers were optimized for vertex cache and vertex fetch. Feeding unoptimized data into the encoders will produce poor compression ratios. Both codecs are lossless - the only lossy step is quantization that happens before encoding.&lt;/p&gt; 
&lt;p&gt;Decoding functions are heavily optimized and can directly target write-combined memory; you can expect both decoders to run at 3-5 GB/s on modern desktop CPUs. Compression ratios depend on the data; vertex data compression ratio is typically around 2-4x (compared to already quantized data), index data compression ratio is around 5-6x (compared to raw 16-bit index data). General purpose lossless compressors can further improve on these results.&lt;/p&gt; 
&lt;p&gt;For additional improvements in compression ratio and decoding performance, it is recommended to switch to vertex codec v1 (via &lt;code&gt;meshopt_encodeVertexVersion(1)&lt;/code&gt;). This will result in smaller outputs that decode faster, and provide additional control over compression level - &lt;code&gt;meshopt_encodeVertexBuffer&lt;/code&gt; will use compression level 2 by default, but using &lt;code&gt;meshopt_encodeVertexBufferLevel&lt;/code&gt; allows to improve compression in certain cases by using level 3, or to reduce compression ratio and improve encoding speed by using level 1. Note that v1 format requires meshoptimizer v0.23 or later for decoding.&lt;/p&gt; 
&lt;p&gt;Index buffer codec only supports triangle list topology; when encoding triangle strips or line lists, use &lt;code&gt;meshopt_encodeIndexSequence&lt;/code&gt;/&lt;code&gt;meshopt_decodeIndexSequence&lt;/code&gt; instead. This codec typically encodes indices into ~1 byte per index, but compressing the results further with a general purpose compressor can improve the results to 1-3 bits per index.&lt;/p&gt; 
&lt;p&gt;The following guarantees on data compatibility are provided for point releases (&lt;em&gt;no&lt;/em&gt; guarantees are given for development branch):&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Data encoded with older versions of the library can always be decoded with newer versions;&lt;/li&gt; 
 &lt;li&gt;Data encoded with newer versions of the library can be decoded with older versions, provided that encoding versions are set correctly; if binary stability of encoded data is important, use &lt;code&gt;meshopt_encodeVertexVersion&lt;/code&gt; and &lt;code&gt;meshopt_encodeIndexVersion&lt;/code&gt; to &#39;pin&#39; the data versions.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Due to a very high decoding performance and compatibility with general purpose lossless compressors, the compression is a good fit for the use on the web. To that end, meshoptimizer provides both vertex and index decoders compiled into WebAssembly and wrapped into a module with JavaScript-friendly interface, &lt;code&gt;js/meshopt_decoder.js&lt;/code&gt;, that you can use to decode meshes that were encoded offline:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-js&quot;&gt;// ready is a Promise that is resolved when (asynchronous) WebAssembly compilation finishes
await MeshoptDecoder.ready;

// decode from *Data (Uint8Array) into *Buffer (Uint8Array)
MeshoptDecoder.decodeVertexBuffer(vertexBuffer, vertexCount, vertexSize, vertexData);
MeshoptDecoder.decodeIndexBuffer(indexBuffer, indexCount, indexSize, indexData);
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href=&quot;https://meshoptimizer.org/demo/&quot;&gt;Usage example&lt;/a&gt; is available, with source in &lt;code&gt;demo/index.html&lt;/code&gt;; this example uses .GLB files encoded using &lt;code&gt;gltfpack&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Point cloud compression&lt;/h2&gt; 
&lt;p&gt;The vertex encoding algorithms can be used to compress arbitrary streams of attribute data; one other use case besides triangle meshes is point cloud data. Typically point clouds come with position, color and possibly other attributes but don&#39;t have an implied point order.&lt;/p&gt; 
&lt;p&gt;To compress point clouds efficiently, it&#39;s recommended to first preprocess the points by sorting them using the spatial sort algorithm:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;std::vector&amp;lt;unsigned int&amp;gt; remap(point_count);
meshopt_spatialSortRemap(&amp;amp;remap[0], positions, point_count, sizeof(vec3));

// for each attribute stream
meshopt_remapVertexBuffer(positions, positions, point_count, sizeof(vec3), &amp;amp;remap[0]);
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After this the resulting arrays should be quantized (e.g. using 16-bit fixed point numbers for positions and 8-bit color components), and the result can be compressed using &lt;code&gt;meshopt_encodeVertexBuffer&lt;/code&gt; as described in the previous section. To decompress, &lt;code&gt;meshopt_decodeVertexBuffer&lt;/code&gt; will recover the quantized data that can be used directly or converted back to original floating-point data. The compression ratio depends on the nature of source data, for colored points it&#39;s typical to get 35-40 bits per point as a result.&lt;/p&gt; 
&lt;h2&gt;Advanced compression&lt;/h2&gt; 
&lt;p&gt;Both vertex and index codecs are designed to be used in a three-stage pipeline:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Preparation (quantization, filtering, ordering)&lt;/li&gt; 
 &lt;li&gt;Encoding (&lt;code&gt;meshopt_encodeVertexBuffer&lt;/code&gt;/&lt;code&gt;meshopt_encodeIndexBuffer&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Optional compression (LZ4/zlib/zstd/Oodle)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The preparation stage is crucial for achieving good compression ratios; this section will cover some techniques that can be used to improve the results.&lt;/p&gt; 
&lt;p&gt;The index codec targets 1 byte per triangle as a best case; on real-world data, it&#39;s typical to achieve 1-1.2 bytes per triangle. To reach this, the data needs to be optimized for vertex cache and vertex fetch. Optimizations that do not disrupt triangle locality (such as overdraw) are safe to use in between. To reduce the data size further, it&#39;s possible to use &lt;code&gt;meshopt_optimizeVertexCacheStrip&lt;/code&gt; instead of &lt;code&gt;meshopt_optimizeVertexCache&lt;/code&gt; when optimizing for vertex cache. This trades off some efficiency in vertex transform for smaller vertex and index data.&lt;/p&gt; 
&lt;p&gt;When referenced vertex indices are not sequential, the index codec will use around 2 bytes per index. This can happen when the referenced vertices are a sparse subset of the vertex buffer, such as when encoding LODs. General-purpose compression can be especially helpful in this case.&lt;/p&gt; 
&lt;p&gt;The vertex codec tries to take advantage of the inherent locality of sequential vertices and identify bit patterns that repeat in consecutive vertices. Typically, vertex cache + vertex fetch provides a reasonably local vertex traversal order; without an index buffer, it is recommended to sort vertices spatially to improve the compression ratio. It is crucial to correctly specify the stride when encoding vertex data; however, it does not matter whether the vertices are interleaved or deinterleaved, as the codecs perform full byte deinterleaving internally.&lt;/p&gt; 
&lt;p&gt;For optimal compression results, the values must be quantized to small integers. It can be valuable to use bit counts that are not multiples of 8. For example, instead of using 16 bits to represent texture coordinates, use 12-bit integers and divide by 4095 in the shader. Alternatively, using half-precision floats can often achieve good results. For single-precision floating-point data, it&#39;s recommended to use &lt;code&gt;meshopt_quantizeFloat&lt;/code&gt; to remove entropy from the lower bits of the mantissa. Due to current limitations of the codec, the bit count needs to be 15 (23-8) for good results (7 can be used for more extreme compression). For normal or tangent vectors, using octahedral encoding is recommended over three components as it reduces redundancy. Similarly to other quantized values, consider using 10-12 bits per component instead of 16.&lt;/p&gt; 
&lt;p&gt;When data is bit packed, using v1 vertex codec (via &lt;code&gt;meshopt_encodeVertexVersion(1)&lt;/code&gt;) and specifying compression level 3 (&lt;code&gt;meshopt_encodeVertexBufferLevel&lt;/code&gt;) can improve the compression further by redistributing bits between components. Note that v1 vertex codec is recommended regardless, as it improves compression ratios and decoding performance even absent bit packing.&lt;/p&gt; 
&lt;p&gt;To further leverage the inherent structure of some data, the preparation stage can use filters that encode and decode the data in a lossy manner. This is similar to quantization but can be used without having to change the shader code. After decoding, the filter transformation needs to be reversed. For native game engine pipelines, it is usually more optimal to carefully prequantize and pretransform the vertex data, but sometimes (for example when serializing data in glTF format) this is not a practical option and filters are more practical. This library provides three filters:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Octahedral filter (&lt;code&gt;meshopt_encodeFilterOct&lt;/code&gt;/&lt;code&gt;meshopt_decodeFilterOct&lt;/code&gt;) encodes quantized (snorm) normal or tangent vectors using octahedral encoding. Any number of bits &amp;lt;= 16 can be used with 4 bytes or 8 bytes per vector.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Quaternion filter (&lt;code&gt;meshopt_encodeFilterQuat&lt;/code&gt;/&lt;code&gt;meshopt_decodeFilterQuat&lt;/code&gt;) encodes quantized (snorm) quaternion vectors; this can be used to encode rotations or tangent frames. Any number of bits between 4 and 16 can be used with 8 bytes per vector.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Exponential filter (&lt;code&gt;meshopt_encodeFilterExp&lt;/code&gt;/&lt;code&gt;meshopt_decodeFilterExp&lt;/code&gt;) encodes single-precision floating-point vectors; this can be used to encode arbitrary floating-point data more efficiently. In addition to an arbitrary bit count (&amp;lt;= 24), the filter takes a &quot;mode&quot; parameter that allows specifying how the exponent sharing is performed to trade off compression ratio and quality:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;meshopt_EncodeExpSeparate&lt;/code&gt; does not share exponents and results in the largest output&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;meshopt_EncodeExpSharedVector&lt;/code&gt; shares exponents between different components of the same vector&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;meshopt_EncodeExpSharedComponent&lt;/code&gt; shares exponents between the same component in different vectors&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;meshopt_EncodeExpClamped&lt;/code&gt; does not share exponents but clamps the exponent range to reduce exponent entropy&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Note that all filters are lossy and require the data to be deinterleaved with one attribute per stream; this faciliates efficient SIMD implementation of filter decoders, allowing the overall decompression speed to be closer to that of the raw codec.&lt;/p&gt; 
&lt;h2&gt;Triangle strip conversion&lt;/h2&gt; 
&lt;p&gt;On most hardware, indexed triangle lists are the most efficient way to drive the GPU. However, in some cases triangle strips might prove beneficial:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;On some older GPUs, triangle strips may be a bit more efficient to render&lt;/li&gt; 
 &lt;li&gt;On extremely memory constrained systems, index buffers for triangle strips could save a bit of memory&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This library provides an algorithm for converting a vertex cache optimized triangle list to a triangle strip:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;std::vector&amp;lt;unsigned int&amp;gt; strip(meshopt_stripifyBound(index_count));
unsigned int restart_index = ~0u;
size_t strip_size = meshopt_stripify(&amp;amp;strip[0], indices, index_count, vertex_count, restart_index);
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Typically you should expect triangle strips to have ~50-60% of indices compared to triangle lists (~1.5-1.8 indices per triangle) and have ~5% worse ACMR. Note that triangle strips can be stitched with or without restart index support. Using restart indices can result in ~10% smaller index buffers, but on some GPUs restart indices may result in decreased performance.&lt;/p&gt; 
&lt;p&gt;To reduce the triangle strip size further, it&#39;s recommended to use &lt;code&gt;meshopt_optimizeVertexCacheStrip&lt;/code&gt; instead of &lt;code&gt;meshopt_optimizeVertexCache&lt;/code&gt; when optimizing for vertex cache. This trades off some efficiency in vertex transform for smaller index buffers.&lt;/p&gt; 
&lt;h2&gt;Deinterleaved geometry&lt;/h2&gt; 
&lt;p&gt;All of the examples above assume that geometry is represented as a single vertex buffer and a single index buffer. This requires storing all vertex attributes - position, normal, texture coordinate, skinning weights etc. - in a single contiguous struct. However, in some cases using multiple vertex streams may be preferable. In particular, if some passes require only positional data - such as depth pre-pass or shadow map - then it may be beneficial to split it from the rest of the vertex attributes to make sure the bandwidth use during these passes is optimal. On some mobile GPUs a position-only attribute stream also improves efficiency of tiling algorithms.&lt;/p&gt; 
&lt;p&gt;Most of the functions in this library either only need the index buffer (such as vertex cache optimization) or only need positional information (such as overdraw optimization). However, several tasks require knowledge about all vertex attributes.&lt;/p&gt; 
&lt;p&gt;For indexing, &lt;code&gt;meshopt_generateVertexRemap&lt;/code&gt; assumes that there&#39;s just one vertex stream; when multiple vertex streams are used, it&#39;s necessary to use &lt;code&gt;meshopt_generateVertexRemapMulti&lt;/code&gt; as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;meshopt_Stream streams[] = {
    {&amp;amp;unindexed_pos[0], sizeof(float) * 3, sizeof(float) * 3},
    {&amp;amp;unindexed_nrm[0], sizeof(float) * 3, sizeof(float) * 3},
    {&amp;amp;unindexed_uv[0], sizeof(float) * 2, sizeof(float) * 2},
};

std::vector&amp;lt;unsigned int&amp;gt; remap(index_count);
size_t vertex_count = meshopt_generateVertexRemapMulti(&amp;amp;remap[0], NULL, index_count, index_count, streams, sizeof(streams) / sizeof(streams[0]));
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After this &lt;code&gt;meshopt_remapVertexBuffer&lt;/code&gt; needs to be called once for each vertex stream to produce the correctly reindexed stream. For shadow indexing, similarly &lt;code&gt;meshopt_generateShadowIndexBufferMulti&lt;/code&gt; is available as a replacement.&lt;/p&gt; 
&lt;p&gt;Instead of calling &lt;code&gt;meshopt_optimizeVertexFetch&lt;/code&gt; for reordering vertices in a single vertex buffer for efficiency, calling &lt;code&gt;meshopt_optimizeVertexFetchRemap&lt;/code&gt; and then calling &lt;code&gt;meshopt_remapVertexBuffer&lt;/code&gt; for each stream again is recommended.&lt;/p&gt; 
&lt;p&gt;Finally, when compressing vertex data, &lt;code&gt;meshopt_encodeVertexBuffer&lt;/code&gt; should be used on each vertex stream separately - this allows the encoder to best utilize corellation between attribute values for different vertices.&lt;/p&gt; 
&lt;h2&gt;Simplification&lt;/h2&gt; 
&lt;p&gt;All algorithms presented so far don&#39;t affect visual appearance at all, with the exception of quantization that has minimal controlled impact. However, fundamentally the most effective way at reducing the rendering or transmission cost of a mesh is to make the mesh simpler.&lt;/p&gt; 
&lt;p&gt;This library provides two simplification algorithms that reduce the number of triangles in the mesh. Given a vertex and an index buffer, they generate a second index buffer that uses existing vertices in the vertex buffer. This index buffer can be used directly for rendering with the original vertex buffer (preferably after vertex cache optimization using &lt;code&gt;meshopt_optimizeVertexCache&lt;/code&gt;), or a new compact vertex/index buffer can be generated using &lt;code&gt;meshopt_optimizeVertexFetch&lt;/code&gt; that uses the optimal number and order of vertices.&lt;/p&gt; 
&lt;p&gt;The first simplification algorithm, &lt;code&gt;meshopt_simplify&lt;/code&gt;, follows the topology of the original mesh in an attempt to preserve attribute seams, borders and overall appearance. For meshes with inconsistent topology or many seams, such as faceted meshes, it can result in simplifier getting &quot;stuck&quot; and not being able to simplify the mesh fully. Therefore it&#39;s critical that identical vertices are &quot;welded&quot; together, that is, the input vertex buffer does not contain duplicates. Additionally, it may be worthwhile to weld the vertices without taking into account vertex attributes that aren&#39;t critical and can be rebuilt later.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;float threshold = 0.2f;
size_t target_index_count = size_t(index_count * threshold);
float target_error = 1e-2f;

std::vector&amp;lt;unsigned int&amp;gt; lod(index_count);
float lod_error = 0.f;
lod.resize(meshopt_simplify(&amp;amp;lod[0], indices, index_count, &amp;amp;vertices[0].x, vertex_count, sizeof(Vertex),
    target_index_count, target_error, /* options= */ 0, &amp;amp;lod_error));
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Target error is an approximate measure of the deviation from the original mesh using distance normalized to &lt;code&gt;[0..1]&lt;/code&gt; range (e.g. &lt;code&gt;1e-2f&lt;/code&gt; means that simplifier will try to maintain the error to be below 1% of the mesh extents). Note that the simplifier attempts to produce the requested number of indices at minimal error, but because of topological restrictions and error limit it is not guaranteed to reach the target index count and can stop earlier.&lt;/p&gt; 
&lt;p&gt;To disable the error limit, &lt;code&gt;target_error&lt;/code&gt; can be set to &lt;code&gt;FLT_MAX&lt;/code&gt;. This makes it more likely that the simplifier will reach the target index count, but it may produce a mesh that looks significantly different from the original, so using the resulting error to control viewing distance would be required. Conversely, setting &lt;code&gt;target_index_count&lt;/code&gt; to 0 will simplify the input mesh as much as possible within the specified error limit; this can be useful for generating LODs that should look good at a given viewing distance.&lt;/p&gt; 
&lt;p&gt;The second simplification algorithm, &lt;code&gt;meshopt_simplifySloppy&lt;/code&gt;, doesn&#39;t follow the topology of the original mesh. This means that it doesn&#39;t preserve attribute seams or borders, but it can collapse internal details that are too small to matter better because it can merge mesh features that are topologically disjoint but spatially close.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;float threshold = 0.2f;
size_t target_index_count = size_t(index_count * threshold);
float target_error = 1e-1f;

std::vector&amp;lt;unsigned int&amp;gt; lod(index_count);
float lod_error = 0.f;
lod.resize(meshopt_simplifySloppy(&amp;amp;lod[0], indices, index_count, &amp;amp;vertices[0].x, vertex_count, sizeof(Vertex),
    target_index_count, target_error, &amp;amp;lod_error));
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This algorithm will not stop early due to topology restrictions but can still do so if target index count can&#39;t be reached without introducing an error larger than target. It is 5-6x faster than &lt;code&gt;meshopt_simplify&lt;/code&gt; when simplification ratio is large, and is able to reach ~20M triangles/sec on a desktop CPU (&lt;code&gt;meshopt_simplify&lt;/code&gt; works at ~3M triangles/sec).&lt;/p&gt; 
&lt;p&gt;Both algorithms can also return the resulting normalized deviation that can be used to choose the correct level of detail based on screen size or solid angle; the error can be converted to object space by multiplying by the scaling factor returned by &lt;code&gt;meshopt_simplifyScale&lt;/code&gt;. For example, given a mesh with a precomputed LOD and a prescaled error, the screen-space normalized error can be computed and used for LOD selection:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;// lod_factor can be 1 or can be adjusted for more or less aggressive LOD selection
float d = max(0, distance(camera_position, mesh_center) - mesh_radius);
float e = d * (tan(camera_fovy / 2) * 2 / screen_height); // 1px in mesh space
bool lod_ok = e * lod_factor &amp;gt;= lod_error;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;When a sequence of LOD meshes is generated that all use the original vertex buffer, care must be taken to order vertices optimally to not penalize mobile GPU architectures that are only capable of transforming a sequential vertex buffer range. It&#39;s recommended in this case to first optimize each LOD for vertex cache, then assemble all LODs in one large index buffer starting from the coarsest LOD (the one with fewest triangles), and call &lt;code&gt;meshopt_optimizeVertexFetch&lt;/code&gt; on the final large index buffer. This will make sure that coarser LODs require a smaller vertex range and are efficient wrt vertex fetch and transform.&lt;/p&gt; 
&lt;h2&gt;Advanced simplification&lt;/h2&gt; 
&lt;p&gt;The main simplification algorithm, &lt;code&gt;meshopt_simplify&lt;/code&gt;, exposes additional options and functions that can be used to control the simplification process in more detail.&lt;/p&gt; 
&lt;p&gt;For basic customization, a number of options can be passed via &lt;code&gt;options&lt;/code&gt; bitmask that adjust the behavior of the simplifier:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;meshopt_SimplifyLockBorder&lt;/code&gt; restricts the simplifier from collapsing edges that are on the border of the mesh. This can be useful for simplifying mesh subsets independently, so that the LODs can be combined without introducing cracks.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;meshopt_SimplifyErrorAbsolute&lt;/code&gt; changes the error metric from relative to absolute both for the input error limit as well as for the resulting error. This can be used instead of &lt;code&gt;meshopt_simplifyScale&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;meshopt_SimplifySparse&lt;/code&gt; improves simplification performance assuming input indices are a sparse subset of the mesh. This can be useful when simplifying small mesh subsets independently, and is intended to be used for meshlet simplification. For consistency, it is recommended to use absolute errors when sparse simplification is desired, as this flag changes the meaning of the relative errors.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;meshopt_SimplifyPrune&lt;/code&gt; allows the simplifier to remove isolated components regardless of the topological restrictions inside the component. This is generally recommended for full-mesh simplification as it can improve quality and reduce triangle count; note that with this option, triangles connected to locked vertices may be removed as part of their component.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;While &lt;code&gt;meshopt_simplify&lt;/code&gt; is aware of attribute discontinuities by default (and infers them through the supplied index buffer) and tries to preserve them, it can be useful to provide information about attribute values. This allows the simplifier to take attribute error into account which can improve shading (by using vertex normals), texture deformation (by using texture coordinates), and may be necessary to preserve vertex colors when textures are not used in the first place. This can be done by using a variant of the simplification function that takes attribute values and weight factors, &lt;code&gt;meshopt_simplifyWithAttributes&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;const float nrm_weight = 0.5f;
const float attr_weights[3] = {nrm_weight, nrm_weight, nrm_weight};

std::vector&amp;lt;unsigned int&amp;gt; lod(index_count);
float lod_error = 0.f;
lod.resize(meshopt_simplifyWithAttributes(&amp;amp;lod[0], indices, index_count, &amp;amp;vertices[0].x, vertex_count, sizeof(Vertex),
    &amp;amp;vertices[0].nx, sizeof(Vertex), attr_weights, 3, /* vertex_lock= */ NULL,
    target_index_count, target_error, /* options= */ 0, &amp;amp;lod_error));
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The attributes are passed as a separate buffer (in the example above it&#39;s a subset of the same vertex buffer) and should be stored as consecutive floats; attribute weights are used to control the importance of each attribute in the simplification process. For normalized attributes like normals and vertex colors, a weight around 1.0 is usually appropriate; internally, a change of &lt;code&gt;1/weight&lt;/code&gt; in attribute value over a distance &lt;code&gt;d&lt;/code&gt; is approximately equivalent to a change of &lt;code&gt;d&lt;/code&gt; in position. Using higher weights may be appropriate to preserve attribute quality at the cost of position quality. If the attribute has a different scale (e.g. unnormalized vertex colors in [0..255] range), the weight should be divided by the scaling factor (1/255 in this example).&lt;/p&gt; 
&lt;p&gt;Both the target error and the resulting error combine positional error and attribute error, so the error can be used to control the LOD while taking attribute quality into account, assuming carefully chosen weights.&lt;/p&gt; 
&lt;p&gt;When using &lt;code&gt;meshopt_simplifyWithAttributes&lt;/code&gt;, it is also possible to lock certain vertices by providing a &lt;code&gt;vertex_lock&lt;/code&gt; array that contains a boolean value for each vertex in the mesh. This can be useful to preserve certain vertices, such as the boundary of the mesh, with more control than &lt;code&gt;meshopt_SimplifyLockBorder&lt;/code&gt; option provides.&lt;/p&gt; 
&lt;p&gt;Simplification currently assumes that the input mesh is using the same material for all triangles. If the mesh uses multiple materials, it is possible to split the mesh into subsets based on the material and simplify each subset independently, using &lt;code&gt;meshopt_SimplifyLockBorder&lt;/code&gt; or &lt;code&gt;vertex_lock&lt;/code&gt; to preserve material boundaries; however, this limits the collapses and as a result may reduce the resulting quality. An alternative approach is to encode information about the material into the vertex buffer, ensuring that all three vertices referencing the same triangle have the same material ID; this may require duplicating vertices on the boundary between materials. After this, simplification can be performed as usual, and after simplification per-triangle material information can be computed from the vertex material IDs. There is no need to inform the simplifier of the value of the material ID: the implicit boundaries created by duplicating vertices with conflicting material IDs will be preserved automatically.&lt;/p&gt; 
&lt;h2&gt;Point cloud simplification&lt;/h2&gt; 
&lt;p&gt;In addition to triangle mesh simplification, this library provides a function to simplify point clouds. The algorithm reduces the point cloud to a specified number of points while preserving the overall appearance, and can optionally take per-point colors into account:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;const float color_weight = 1;
std::vector&amp;lt;unsigned int&amp;gt; indices(target_count);
indices.resize(meshopt_simplifyPoints(&amp;amp;indices[0], &amp;amp;points[0].x, points.size(), sizeof(Point),
    &amp;amp;points[0].r, sizeof(Point), color_weight, target_count));
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The resulting indices can be used to render the simplified point cloud; to reduce the memory footprint, the point cloud can be reindexed to create an array of points from the indices.&lt;/p&gt; 
&lt;h2&gt;Mesh shading&lt;/h2&gt; 
&lt;p&gt;Modern GPUs are beginning to deviate from the traditional rasterization model. NVidia GPUs starting from Turing and AMD GPUs starting from RDNA2 provide a new programmable geometry pipeline that, instead of being built around index buffers and vertex shaders, is built around mesh shaders - a new shader type that allows to provide a batch of work to the rasterizer.&lt;/p&gt; 
&lt;p&gt;Using mesh shaders in context of traditional mesh rendering provides an opportunity to use a variety of optimization techniques, starting from more efficient vertex reuse, using various forms of culling (e.g. cluster frustum or occlusion culling) and in-memory compression to maximize the utilization of GPU hardware. Beyond traditional rendering mesh shaders provide a richer programming model that can synthesize new geometry more efficiently than common alternatives such as geometry shaders. Mesh shading can be accessed via Vulkan or Direct3D 12 APIs; please refer to &lt;a href=&quot;https://developer.nvidia.com/blog/introduction-turing-mesh-shaders/&quot;&gt;Introduction to Turing Mesh Shaders&lt;/a&gt; and &lt;a href=&quot;https://devblogs.microsoft.com/directx/coming-to-directx-12-mesh-shaders-and-amplification-shaders-reinventing-the-geometry-pipeline/&quot;&gt;Mesh Shaders and Amplification Shaders: Reinventing the Geometry Pipeline&lt;/a&gt; for additional information.&lt;/p&gt; 
&lt;p&gt;To use mesh shaders for conventional rendering efficiently, geometry needs to be converted into a series of meshlets; each meshlet represents a small subset of the original mesh and comes with a small set of vertices and a separate micro-index buffer that references vertices in the meshlet. This information can be directly fed to the rasterizer from the mesh shader. This library provides algorithms to create meshlet data for a mesh, and - assuming geometry is static - can compute bounding information that can be used to perform cluster culling, a technique that can reject a meshlet if it&#39;s invisible on screen.&lt;/p&gt; 
&lt;p&gt;To generate meshlet data, this library provides &lt;code&gt;meshopt_buildMeshlets&lt;/code&gt; algorithm, which tries to balance topological efficiency (by maximizing vertex reuse inside meshlets) with culling efficiency (by minimizing meshlet radius and triangle direction divergence) and produces GPU-friendly data. As an alternative (that can be useful for load-time processing), &lt;code&gt;meshopt_buildMeshletsScan&lt;/code&gt; can create the meshlet data using a vertex cache-optimized index buffer as a starting point by greedily aggregating consecutive triangles until they go over the meshlet limits. &lt;code&gt;meshopt_buildMeshlets&lt;/code&gt; is recommended for offline data processing even if cone culling is not used.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;const size_t max_vertices = 64;
const size_t max_triangles = 124;
const float cone_weight = 0.0f;

size_t max_meshlets = meshopt_buildMeshletsBound(indices.size(), max_vertices, max_triangles);
std::vector&amp;lt;meshopt_Meshlet&amp;gt; meshlets(max_meshlets);
std::vector&amp;lt;unsigned int&amp;gt; meshlet_vertices(max_meshlets * max_vertices);
std::vector&amp;lt;unsigned char&amp;gt; meshlet_triangles(max_meshlets * max_triangles * 3);

size_t meshlet_count = meshopt_buildMeshlets(meshlets.data(), meshlet_vertices.data(), meshlet_triangles.data(), indices.data(),
    indices.size(), &amp;amp;vertices[0].x, vertices.size(), sizeof(Vertex), max_vertices, max_triangles, cone_weight);
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To generate the meshlet data, &lt;code&gt;max_vertices&lt;/code&gt; and &lt;code&gt;max_triangles&lt;/code&gt; need to be set within limits supported by the hardware; for NVidia the values of 64 and 124 are recommended (&lt;code&gt;max_triangles&lt;/code&gt; must be divisible by 4 so 124 is the value closest to official NVidia&#39;s recommended 126). &lt;code&gt;cone_weight&lt;/code&gt; should be left as 0 if cluster cone culling is not used, and set to a value between 0 and 1 to balance cone culling efficiency with other forms of culling like frustum or occlusion culling (&lt;code&gt;0.25&lt;/code&gt; is a reasonable default).&lt;/p&gt; 
&lt;p&gt;Each resulting meshlet refers to a portion of &lt;code&gt;meshlet_vertices&lt;/code&gt; and &lt;code&gt;meshlet_triangles&lt;/code&gt; arrays; the arrays are overallocated for the worst case so it&#39;s recommended to trim them before saving them as an asset / uploading them to the GPU:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;const meshopt_Meshlet&amp;amp; last = meshlets[meshlet_count - 1];

meshlet_vertices.resize(last.vertex_offset + last.vertex_count);
meshlet_triangles.resize(last.triangle_offset + ((last.triangle_count * 3 + 3) &amp;amp; ~3));
meshlets.resize(meshlet_count);
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;However depending on the application other strategies of storing the data can be useful; for example, &lt;code&gt;meshlet_vertices&lt;/code&gt; serves as indices into the original vertex buffer but it might be worthwhile to generate a mini vertex buffer for each meshlet to remove the extra indirection when accessing vertex data, or it might be desirable to compress vertex data as vertices in each meshlet are likely to be very spatially coherent.&lt;/p&gt; 
&lt;p&gt;For optimal performance, it is recommended to further optimize each meshlet in isolation for better triangle and vertex locality by calling &lt;code&gt;meshopt_optimizeMeshlet&lt;/code&gt; on vertex and index data like so:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;meshopt_optimizeMeshlet(&amp;amp;meshlet_vertices[m.vertex_offset], &amp;amp;meshlet_triangles[m.triangle_offset], m.triangle_count, m.vertex_count);
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Different applications will choose different strategies for rendering meshlets; on a GPU capable of mesh shading, meshlets can be rendered directly; for example, a basic GLSL shader for &lt;code&gt;VK_EXT_mesh_shader&lt;/code&gt; extension could look like this (parts omitted for brevity):&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-glsl&quot;&gt;layout(binding = 0) readonly buffer Meshlets { Meshlet meshlets[]; };
layout(binding = 1) readonly buffer MeshletVertices { uint meshlet_vertices[]; };
layout(binding = 2) readonly buffer MeshletTriangles { uint8_t meshlet_triangles[]; };

void main() {
    Meshlet meshlet = meshlets[gl_WorkGroupID.x];
    SetMeshOutputsEXT(meshlet.vertex_count, meshlet.triangle_count);

    for (uint i = gl_LocalInvocationIndex; i &amp;lt; meshlet.vertex_count; i += gl_WorkGroupSize.x) {
        uint index = meshlet_vertices[meshlet.vertex_offset + i];
        gl_MeshVerticesEXT[i].gl_Position = world_view_projection * vec4(vertex_positions[index], 1);
    }

    for (uint i = gl_LocalInvocationIndex; i &amp;lt; meshlet.triangle_count; i += gl_WorkGroupSize.x) {
        uint offset = meshlet.triangle_offset + i * 3;
        gl_PrimitiveTriangleIndicesEXT[i] = uvec3(
            meshlet_triangles[offset], meshlet_triangles[offset + 1], meshlet_triangles[offset + 2]);
    }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After generating the meshlet data, it&#39;s also possible to generate extra data for each meshlet that can be saved and used at runtime to perform cluster culling, where each meshlet can be discarded if it&#39;s guaranteed to be invisible. To generate the data, &lt;code&gt;meshlet_computeMeshletBounds&lt;/code&gt; can be used:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;meshopt_Bounds bounds = meshopt_computeMeshletBounds(&amp;amp;meshlet_vertices[m.vertex_offset], &amp;amp;meshlet_triangles[m.triangle_offset],
    m.triangle_count, &amp;amp;vertices[0].x, vertices.size(), sizeof(Vertex));
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The resulting &lt;code&gt;bounds&lt;/code&gt; values can be used to perform frustum or occlusion culling using the bounding sphere, or cone culling using the cone axis/angle (which will reject the entire meshlet if all triangles are guaranteed to be back-facing from the camera point of view):&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;if (dot(normalize(cone_apex - camera_position), cone_axis) &amp;gt;= cone_cutoff) reject();
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Efficiency analyzers&lt;/h2&gt; 
&lt;p&gt;While the only way to get precise performance data is to measure performance on the target GPU, it can be valuable to measure the impact of these optimization in a GPU-independent manner. To this end, the library provides analyzers for all three major optimization routines. For each optimization there is a corresponding analyze function, like &lt;code&gt;meshopt_analyzeOverdraw&lt;/code&gt;, that returns a struct with statistics.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;meshopt_analyzeVertexCache&lt;/code&gt; returns vertex cache statistics. The common metric to use is ACMR - average cache miss ratio, which is the ratio of the total number of vertex invocations to the triangle count. The worst-case ACMR is 3 (GPU has to process 3 vertices for each triangle); on regular grids the optimal ACMR approaches 0.5. On real meshes it usually is in [0.5..1.5] range depending on the amount of vertex splits. One other useful metric is ATVR - average transformed vertex ratio - which represents the ratio of vertex shader invocations to the total vertices, and has the best case of 1.0 regardless of mesh topology (each vertex is transformed once).&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;meshopt_analyzeVertexFetch&lt;/code&gt; returns vertex fetch statistics. The main metric it uses is overfetch - the ratio between the number of bytes read from the vertex buffer to the total number of bytes in the vertex buffer. Assuming non-redundant vertex buffers, the best case is 1.0 - each byte is fetched once.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;meshopt_analyzeOverdraw&lt;/code&gt; returns overdraw statistics. The main metric it uses is overdraw - the ratio between the number of pixel shader invocations to the total number of covered pixels, as measured from several different orthographic cameras. The best case for overdraw is 1.0 - each pixel is shaded once.&lt;/p&gt; 
&lt;p&gt;Note that all analyzers use approximate models for the relevant GPU units, so the numbers you will get as the result are only a rough approximation of the actual performance.&lt;/p&gt; 
&lt;h2&gt;Specialized processing&lt;/h2&gt; 
&lt;p&gt;In addition to the core optimization techniques, the library provides several specialized algorithms for specific rendering techniques and pipeline optimizations that require a particular configuration of vertex and index data.&lt;/p&gt; 
&lt;h3&gt;Geometry shader adjacency&lt;/h3&gt; 
&lt;p&gt;For algorithms that use geometry shaders and require adjacency information, this library can generate an index buffer with adjacency data:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;std::vector&amp;lt;unsigned int&amp;gt; adjacency(indices.size() * 2);
meshopt_generateAdjacencyIndexBuffer(&amp;amp;adjacency[0], &amp;amp;indices[0], indices.size(), &amp;amp;vertices[0].x, vertices.size(), sizeof(Vertex));
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This creates an index buffer suitable for rendering with triangle-with-adjacency topology, providing 3 extra vertices per triangle that represent vertices opposite to each triangle&#39;s edge. This data can be used to compute silhouettes and perform other types of local geometric processing in geometry shaders. To render the mesh with adjacency data, the index buffer should be used with &lt;code&gt;D3D_PRIMITIVE_TOPOLOGY_TRIANGLELIST_ADJ&lt;/code&gt;/&lt;code&gt;VK_PRIMITIVE_TOPOLOGY_TRIANGLE_LIST_WITH_ADJACENCY&lt;/code&gt;/&lt;code&gt;GL_TRIANGLES_ADJACENCY&lt;/code&gt; topology.&lt;/p&gt; 
&lt;p&gt;Note that the use of geometry shaders may have a performance impact on some GPUs; in some cases alternative implementation strategies may be more efficient.&lt;/p&gt; 
&lt;h3&gt;Tessellation with displacement mapping&lt;/h3&gt; 
&lt;p&gt;For hardware tessellation with crack-free displacement mapping, this library can generate a special index buffer that supports PN-AEN tessellation:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;std::vector&amp;lt;unsigned int&amp;gt; tess(indices.size() * 4);
meshopt_generateTessellationIndexBuffer(&amp;amp;tess[0], &amp;amp;indices[0], indices.size(), &amp;amp;vertices[0].x, vertices.size(), sizeof(Vertex));
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This generates a 12-vertex patch for each input triangle with the following layout:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;0, 1, 2: original triangle vertices&lt;/li&gt; 
 &lt;li&gt;3, 4: opposing edge for edge 0, 1&lt;/li&gt; 
 &lt;li&gt;5, 6: opposing edge for edge 1, 2&lt;/li&gt; 
 &lt;li&gt;7, 8: opposing edge for edge 2, 0&lt;/li&gt; 
 &lt;li&gt;9, 10, 11: dominant vertices for corners 0, 1, 2&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This allows the use of hardware tessellation to implement PN-AEN and/or displacement mapping without cracks along UV seams or normal discontinuities. To render the mesh, the index buffer should be used with &lt;code&gt;D3D_PRIMITIVE_TOPOLOGY_12_CONTROL_POINT_PATCHLIST&lt;/code&gt;/&lt;code&gt;VK_PRIMITIVE_TOPOLOGY_PATCH_LIST&lt;/code&gt; (&lt;code&gt;patchControlPoints=12&lt;/code&gt;) topology. For more details please refer to the following papers: &lt;a href=&quot;https://developer.download.nvidia.com/whitepapers/2010/PN-AEN-Triangles-Whitepaper.pdf&quot;&gt;Crack-Free Point-Normal Triangles using Adjacent Edge Normals&lt;/a&gt;, &lt;a href=&quot;https://www.nvidia.com/content/pdf/gdc2011/john_mcdonald.pdf&quot;&gt;Tessellation on Any Budget&lt;/a&gt; and &lt;a href=&quot;https://developer.download.nvidia.com/assets/gamedev/files/gdc12/GDC12_DUDASH_MyTessellationHasCracks.pdf&quot;&gt;My Tessellation Has Cracks!&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Visibility buffers&lt;/h3&gt; 
&lt;p&gt;To render geometry into visibility buffers, access to primitive index in fragment shader is required. While it is possible to use &lt;code&gt;SV_PrimitiveID&lt;/code&gt;/&lt;code&gt;gl_PrimitiveID&lt;/code&gt; in the fragment shader, this can result in suboptimal performance on some GPUs (notably, AMD RDNA1 and all NVidia GPUs), and may not be supported on mobile or console hardware. Using mesh shaders to generate primitive IDs is efficient but requires hardware support that is not universally available. To work around these limitations, this library provides a way to generate a special index buffer that uses provoking vertex to encode primitive IDs:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;std::vector&amp;lt;unsigned int&amp;gt; provoke(indices.size());
std::vector&amp;lt;unsigned int&amp;gt; reorder(vertices.size() + indices.size() / 3);
reorder.resize(meshopt_generateProvokingIndexBuffer(&amp;amp;provoke[0], &amp;amp;reorder[0], &amp;amp;indices[0], indices.size(), vertices.size()));
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This generates a special index buffer along with a reorder table that satisfies two constraints:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;provoke[3 * tri] == tri&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;reorder[provoke[x]]&lt;/code&gt; refers to the original triangle vertices&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;To render the mesh with provoking vertex data, the application should use &lt;code&gt;provoke&lt;/code&gt; as an index buffer and a vertex shader that passes vertex index (&lt;code&gt;SV_VertexID&lt;/code&gt;/&lt;code&gt;gl_VertexIndex&lt;/code&gt;) via a &lt;code&gt;flat&lt;/code&gt;/&lt;code&gt;nointerpolation&lt;/code&gt; attribute to the fragment shader as a primitive index, and loads vertex data manually by computing the real vertex index based on &lt;code&gt;reorder&lt;/code&gt; table (&lt;code&gt;reorder[gl_VertexIndex]&lt;/code&gt;). For more details please refer to &lt;a href=&quot;https://advances.realtimerendering.com/s2024/content/Hable/Advances_SIGGRAPH_2024_VisibilityVRS-SIGGRAPH_Advances_2024.pptx&quot;&gt;Variable Rate Shading with Visibility Buffer Rendering&lt;/a&gt;; naturally, this technique does not require VRS.&lt;/p&gt; 
&lt;p&gt;Note: This assumes the provoking vertex is the first vertex of a triangle, which is true for all graphics APIs except OpenGL/WebGL. For OpenGL/WebGL, you may need to rotate each triangle (abc -&amp;gt; bca) in the resulting index buffer, or use the &lt;code&gt;glProvokingVertex&lt;/code&gt; function (OpenGL 3.2+) or &lt;code&gt;WEBGL_provoking_vertex&lt;/code&gt; extension (WebGL2) to change the provoking vertex convention. For WebGL2, this is highly recommended to avoid a variety of emulation slowdowns that happen by default if &lt;code&gt;flat&lt;/code&gt; attributes are used, such as an implicit use of geometry shaders.&lt;/p&gt; 
&lt;h2&gt;Memory management&lt;/h2&gt; 
&lt;p&gt;Many algorithms allocate temporary memory to store intermediate results or accelerate processing. The amount of memory allocated is a function of various input parameters such as vertex count and index count. By default memory is allocated using &lt;code&gt;operator new&lt;/code&gt; and &lt;code&gt;operator delete&lt;/code&gt;; if these operators are overloaded by the application, the overloads will be used instead. Alternatively it&#39;s possible to specify custom allocation/deallocation functions using &lt;code&gt;meshopt_setAllocator&lt;/code&gt;, e.g.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;meshopt_setAllocator(malloc, free);
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Note that the library expects the allocation function to either throw in case of out-of-memory (in which case the exception will propagate to the caller) or abort, so technically the use of &lt;code&gt;malloc&lt;/code&gt; above isn&#39;t safe. If you want to handle out-of-memory errors without using C++ exceptions, you can use &lt;code&gt;setjmp&lt;/code&gt;/&lt;code&gt;longjmp&lt;/code&gt; instead.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Vertex and index decoders (&lt;code&gt;meshopt_decodeVertexBuffer&lt;/code&gt;, &lt;code&gt;meshopt_decodeIndexBuffer&lt;/code&gt;, &lt;code&gt;meshopt_decodeIndexSequence&lt;/code&gt;) do not allocate memory and work completely within the buffer space provided via arguments.&lt;/p&gt; 
&lt;p&gt;All functions have bounded stack usage that does not exceed 32 KB for any algorithms.&lt;/p&gt; 
&lt;h2&gt;Experimental APIs&lt;/h2&gt; 
&lt;p&gt;Several algorithms provided by this library are marked as &quot;experimental&quot;; this status is reflected in the comments as well as the annotation &lt;code&gt;MESHOPTIMIZER_EXPERIMENTAL&lt;/code&gt; for each function.&lt;/p&gt; 
&lt;p&gt;APIs that are not experimental (annotated with &lt;code&gt;MESHOPTIMIZER_API&lt;/code&gt;) are considered stable, which means that library updates will not break compatibility: existing calls should compile (API compatibility), existing binaries should link (ABI compatibility), and existing behavior should not change significantly (for example, floating point parameters will have similar behavior). This does not mean that the output of the algorithms will be identical: future versions may improve the algorithms and produce different results.&lt;/p&gt; 
&lt;p&gt;APIs that &lt;em&gt;are&lt;/em&gt; experimental may have their interface change, both in ways that will cause existing calls to not compile, and in ways that may compile but have significantly different behavior (e.g., changes in parameter order, meaning, valid ranges). Experimental APIs may also, in rare cases, be removed from future library versions. It is recommended to carefully read release notes when updating the library if experimental APIs are in use. Some experimental APIs may also lack documentation in this README.&lt;/p&gt; 
&lt;p&gt;Applications may configure the library to change the attributes of experimental APIs, for example defining &lt;code&gt;MESHOPTIMIZER_EXPERIMENTAL&lt;/code&gt; as &lt;code&gt;__attribute__((deprecated))&lt;/code&gt; will emit compiler warnings when experimental APIs are used.&lt;/p&gt; 
&lt;p&gt;Currently, the following APIs are experimental, with the functions marked with &lt;code&gt;*&lt;/code&gt; being likely to become stable in the future with no changes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;meshopt_buildMeshletsFlex&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;meshopt_computeSphereBounds&lt;/code&gt;*&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;meshopt_encodeVertexBufferLevel&lt;/code&gt;*&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;meshopt_generateProvokingIndexBuffer&lt;/code&gt;*&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;meshopt_partitionClusters&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;meshopt_simplifySloppy&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;meshopt_spatialSortTriangles&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This library is available to anybody free of charge, under the terms of MIT License (see LICENSE.md).&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ZLMediaKit/ZLMediaKit</title>
      <link>https://github.com/ZLMediaKit/ZLMediaKit</link>
      <description>&lt;p&gt;WebRTC/RTSP/RTMP/HTTP/HLS/HTTP-FLV/WebSocket-FLV/HTTP-TS/HTTP-fMP4/WebSocket-TS/WebSocket-fMP4/GB28181/SRT server and client framework based on C++11&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/ZLMediaKit/ZLMediaKit/master/www/logo.png&quot; alt=&quot;logo&quot;&gt;&lt;/p&gt; 
&lt;p&gt;简体中文 | &lt;a href=&quot;https://raw.githubusercontent.com/ZLMediaKit/ZLMediaKit/master/README_en.md&quot;&gt;English&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;一个基于C++11的高性能运营级流媒体服务框架&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/ZLMediaKit/ZLMediaKit/raw/master/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/license-MIT-green.svg?sanitize=true&quot; alt=&quot;&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://en.cppreference.com/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/language-c++-red.svg?sanitize=true&quot; alt=&quot;&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/ZLMediaKit/ZLMediaKit&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/platform-linux%20%7C%20macos%20%7C%20windows-blue.svg?sanitize=true&quot; alt=&quot;&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/ZLMediaKit/ZLMediaKit/pulls&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/PRs-welcome-yellow.svg?sanitize=true&quot; alt=&quot;&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/ZLMediaKit/ZLMediaKit&quot;&gt;&lt;img src=&quot;https://github.com/ZLMediaKit/ZLMediaKit/actions/workflows/android.yml/badge.svg?sanitize=true&quot; alt=&quot;&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/ZLMediaKit/ZLMediaKit&quot;&gt;&lt;img src=&quot;https://github.com/ZLMediaKit/ZLMediaKit/actions/workflows/linux.yml/badge.svg?sanitize=true&quot; alt=&quot;&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/ZLMediaKit/ZLMediaKit&quot;&gt;&lt;img src=&quot;https://github.com/ZLMediaKit/ZLMediaKit/actions/workflows/macos.yml/badge.svg?sanitize=true&quot; alt=&quot;&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/ZLMediaKit/ZLMediaKit&quot;&gt;&lt;img src=&quot;https://github.com/ZLMediaKit/ZLMediaKit/actions/workflows/windows.yml/badge.svg?sanitize=true&quot; alt=&quot;&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://hub.docker.com/r/zlmediakit/zlmediakit/tags&quot;&gt;&lt;img src=&quot;https://github.com/ZLMediaKit/ZLMediaKit/actions/workflows/docker.yml/badge.svg?sanitize=true&quot; alt=&quot;&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://hub.docker.com/r/zlmediakit/zlmediakit/tags&quot;&gt;&lt;img src=&quot;https://img.shields.io/docker/pulls/zlmediakit/zlmediakit&quot; alt=&quot;&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;项目特点&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;基于C++11开发，避免使用裸指针，代码稳定可靠，性能优越。&lt;/li&gt; 
 &lt;li&gt;支持多种协议(RTSP/RTMP/HLS/HTTP-FLV/WebSocket-FLV/GB28181/HTTP-TS/WebSocket-TS/HTTP-fMP4/WebSocket-fMP4/MP4/WebRTC),支持协议互转。&lt;/li&gt; 
 &lt;li&gt;使用多路复用/多线程/异步网络IO模式开发，并发性能优越，支持海量客户端连接。&lt;/li&gt; 
 &lt;li&gt;代码经过长期大量的稳定性、性能测试，已经在线上商用验证已久。&lt;/li&gt; 
 &lt;li&gt;支持linux、macos、ios、android、windows全平台。&lt;/li&gt; 
 &lt;li&gt;支持x86、arm、risc-v、mips、龙芯、申威等指令集平台。&lt;/li&gt; 
 &lt;li&gt;支持画面秒开、极低延时(&lt;a href=&quot;https://github.com/ZLMediaKit/ZLMediaKit/wiki/%E5%BB%B6%E6%97%B6%E6%B5%8B%E8%AF%95&quot;&gt;500毫秒内，最低可达100毫秒&lt;/a&gt;)。&lt;/li&gt; 
 &lt;li&gt;提供完善的标准&lt;a href=&quot;https://github.com/ZLMediaKit/ZLMediaKit/tree/master/api/include&quot;&gt;C API&lt;/a&gt;,可以作SDK用，或供其他语言调用。&lt;/li&gt; 
 &lt;li&gt;提供完整的&lt;a href=&quot;https://github.com/ZLMediaKit/ZLMediaKit/tree/master/server&quot;&gt;MediaServer&lt;/a&gt;服务器，可以免开发直接部署为商用服务器。&lt;/li&gt; 
 &lt;li&gt;提供完善的&lt;a href=&quot;https://github.com/ZLMediaKit/ZLMediaKit/wiki/MediaServer%E6%94%AF%E6%8C%81%E7%9A%84HTTP-API&quot;&gt;restful api&lt;/a&gt;以及&lt;a href=&quot;https://github.com/ZLMediaKit/ZLMediaKit/wiki/MediaServer%E6%94%AF%E6%8C%81%E7%9A%84HTTP-HOOK-API&quot;&gt;web hook&lt;/a&gt;，支持丰富的业务逻辑。&lt;/li&gt; 
 &lt;li&gt;打通了视频监控协议栈与直播协议栈，对RTSP/RTMP支持都很完善。&lt;/li&gt; 
 &lt;li&gt;功能完善，支持集群、按需转协议、按需推拉流、先播后推、断连续推等功能。&lt;/li&gt; 
 &lt;li&gt;极致性能，单机10W级别播放器，100Gb/s级别io带宽能力。&lt;/li&gt; 
 &lt;li&gt;极致体验，&lt;a href=&quot;https://github.com/ZLMediaKit/ZLMediaKit/wiki/ZLMediakit%E7%8B%AC%E5%AE%B6%E7%89%B9%E6%80%A7%E4%BB%8B%E7%BB%8D&quot;&gt;独家特性&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ZLMediaKit/ZLMediaKit/issues/511&quot;&gt;谁在使用zlmediakit?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;全面支持ipv6网络&lt;/li&gt; 
 &lt;li&gt;支持多轨道模式(一个流中多个视频/音频)&lt;/li&gt; 
 &lt;li&gt;全协议支持H264/H265/AAC/G711/OPUS/MP3，部分支持VP8/VP9/AV1/JPEG/MP3/H266/ADPCM/SVAC/G722/G723/G729&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;项目定位&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;移动嵌入式跨平台流媒体解决方案。&lt;/li&gt; 
 &lt;li&gt;商用级流媒体服务器。&lt;/li&gt; 
 &lt;li&gt;网络编程二次开发SDK。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;功能清单&lt;/h2&gt; 
&lt;h3&gt;功能一览&lt;/h3&gt; 
&lt;img width=&quot;749&quot; alt=&quot;image&quot; src=&quot;https://github.com/user-attachments/assets/8cf5911b-4603-4aa0-8e24-0acb0c616a82&quot;&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;RTSP[S]&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;RTSP[S] 服务器，支持RTMP/MP4/HLS转RTSP[S],支持亚马逊echo show这样的设备&lt;/li&gt; 
   &lt;li&gt;RTSP[S] 播放器，支持RTSP代理，支持生成静音音频&lt;/li&gt; 
   &lt;li&gt;RTSP[S] 推流客户端与服务器&lt;/li&gt; 
   &lt;li&gt;支持 &lt;code&gt;rtp over udp&lt;/code&gt; &lt;code&gt;rtp over tcp&lt;/code&gt; &lt;code&gt;rtp over http&lt;/code&gt; &lt;code&gt;rtp组播&lt;/code&gt; 四种RTP传输方式&lt;/li&gt; 
   &lt;li&gt;服务器/客户端完整支持Basic/Digest方式的登录鉴权，全异步可配置化的鉴权接口&lt;/li&gt; 
   &lt;li&gt;支持H265编码&lt;/li&gt; 
   &lt;li&gt;服务器支持RTSP推流(包括&lt;code&gt;rtp over udp&lt;/code&gt; &lt;code&gt;rtp over tcp&lt;/code&gt;方式)&lt;/li&gt; 
   &lt;li&gt;支持H264/H265/AAC/G711/OPUS/MJPEG/MP3编码，其他编码能转发但不能转协议&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;RTMP[S]&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;RTMP[S] 播放服务器，支持RTSP/MP4/HLS转RTMP&lt;/li&gt; 
   &lt;li&gt;RTMP[S] 发布服务器，支持录制发布流&lt;/li&gt; 
   &lt;li&gt;RTMP[S] 播放器，支持RTMP代理，支持生成静音音频&lt;/li&gt; 
   &lt;li&gt;RTMP[S] 推流客户端&lt;/li&gt; 
   &lt;li&gt;支持http[s]-flv直播服务器&lt;/li&gt; 
   &lt;li&gt;支持http[s]-flv直播播放器&lt;/li&gt; 
   &lt;li&gt;支持websocket-flv直播&lt;/li&gt; 
   &lt;li&gt;支持H264/H265/AAC/G711/OPUS/MP3编码，其他编码能转发但不能转协议&lt;/li&gt; 
   &lt;li&gt;支持&lt;a href=&quot;https://github.com/ksvc/FFmpeg/wiki&quot;&gt;RTMP-H265&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;支持&lt;a href=&quot;https://github.com/ZLMediaKit/ZLMediaKit/wiki/RTMP%E5%AF%B9H265%E5%92%8COPUS%E7%9A%84%E6%94%AF%E6%8C%81&quot;&gt;RTMP-OPUS&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;支持&lt;a href=&quot;https://github.com/veovera/enhanced-rtmp&quot;&gt;enhanced-rtmp(H265)&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;HLS&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;支持HLS文件(mpegts/fmp4)生成，自带HTTP文件服务器&lt;/li&gt; 
   &lt;li&gt;通过cookie追踪技术，可以模拟HLS播放为长连接，可以实现HLS按需拉流、播放统计等业务&lt;/li&gt; 
   &lt;li&gt;支持HLS播发器，支持拉流HLS转rtsp/rtmp/mp4&lt;/li&gt; 
   &lt;li&gt;支持H264/H265/AAC/G711/OPUS/MP3编码&lt;/li&gt; 
   &lt;li&gt;支持多轨道模式&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;TS&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;支持http[s]-ts直播&lt;/li&gt; 
   &lt;li&gt;支持ws[s]-ts直播&lt;/li&gt; 
   &lt;li&gt;支持H264/H265/AAC/G711/OPUS/MP3编码&lt;/li&gt; 
   &lt;li&gt;支持多轨道模式&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;fMP4&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;支持http[s]-fmp4直播&lt;/li&gt; 
   &lt;li&gt;支持ws[s]-fmp4直播&lt;/li&gt; 
   &lt;li&gt;支持H264/H265/AAC/G711/OPUS/MJPEG/MP3编码&lt;/li&gt; 
   &lt;li&gt;支持多轨道模式&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;HTTP[S]与WebSocket&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;服务器支持&lt;code&gt;目录索引生成&lt;/code&gt;,&lt;code&gt;文件下载&lt;/code&gt;,&lt;code&gt;表单提交请求&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;客户端提供&lt;code&gt;文件下载器(支持断点续传)&lt;/code&gt;,&lt;code&gt;接口请求器&lt;/code&gt;,&lt;code&gt;文件上传器&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;完整HTTP API服务器，可以作为web后台开发框架&lt;/li&gt; 
   &lt;li&gt;支持跨域访问&lt;/li&gt; 
   &lt;li&gt;支持http客户端、服务器cookie&lt;/li&gt; 
   &lt;li&gt;支持WebSocket服务器和客户端&lt;/li&gt; 
   &lt;li&gt;支持http文件访问鉴权&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;GB28181与RTP推流&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;支持UDP/TCP RTP(PS/TS/ES)推流服务器，可以转换成RTSP/RTMP/HLS等协议&lt;/li&gt; 
   &lt;li&gt;支持RTSP/RTMP/HLS等协议转rtp推流客户端，支持TCP/UDP模式，提供相应restful api，支持主动被动方式&lt;/li&gt; 
   &lt;li&gt;支持H264/H265/AAC/G711/OPUS/MP3编码&lt;/li&gt; 
   &lt;li&gt;支持es/ps/ts/ehome rtp推流&lt;/li&gt; 
   &lt;li&gt;支持es/ps rtp转推&lt;/li&gt; 
   &lt;li&gt;支持GB28181主动拉流模式&lt;/li&gt; 
   &lt;li&gt;支持双向语音对讲&lt;/li&gt; 
   &lt;li&gt;支持多轨道模式&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;MP4点播与录制&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;支持录制为FLV/HLS/MP4&lt;/li&gt; 
   &lt;li&gt;RTSP/RTMP/HTTP-FLV/WS-FLV支持MP4文件点播，支持seek&lt;/li&gt; 
   &lt;li&gt;支持H264/H265/AAC/G711/OPUS/MP3编码&lt;/li&gt; 
   &lt;li&gt;支持多轨道模式&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;WebRTC&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;支持WebRTC推流，支持转其他协议&lt;/li&gt; 
   &lt;li&gt;支持WebRTC播放，支持其他协议转WebRTC&lt;/li&gt; 
   &lt;li&gt;支持双向echo test&lt;/li&gt; 
   &lt;li&gt;支持simulcast推流&lt;/li&gt; 
   &lt;li&gt;支持上下行rtx/nack丢包重传&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;支持单端口、多线程、客户端网络连接迁移(开源界唯一)&lt;/strong&gt;。&lt;/li&gt; 
   &lt;li&gt;支持TWCC rtcp动态调整码率&lt;/li&gt; 
   &lt;li&gt;支持remb/pli/sr/rr rtcp&lt;/li&gt; 
   &lt;li&gt;支持rtp扩展解析&lt;/li&gt; 
   &lt;li&gt;支持GOP缓冲，webrtc播放秒开&lt;/li&gt; 
   &lt;li&gt;支持datachannel&lt;/li&gt; 
   &lt;li&gt;支持webrtc over tcp模式&lt;/li&gt; 
   &lt;li&gt;优秀的nack、jitter buffer算法, 抗丢包能力卓越&lt;/li&gt; 
   &lt;li&gt;支持whip/whep协议&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ZLMediaKit/ZLMediaKit/master/srt/srt.md&quot;&gt;SRT支持&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;其他&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;支持丰富的restful api以及web hook事件&lt;/li&gt; 
   &lt;li&gt;支持简单的telnet调试&lt;/li&gt; 
   &lt;li&gt;支持配置文件热加载&lt;/li&gt; 
   &lt;li&gt;支持流量统计、推拉流鉴权等事件&lt;/li&gt; 
   &lt;li&gt;支持虚拟主机,可以隔离不同域名&lt;/li&gt; 
   &lt;li&gt;支持按需拉流，无人观看自动关断拉流&lt;/li&gt; 
   &lt;li&gt;支持先播放后推流，提高及时推流画面打开率&lt;/li&gt; 
   &lt;li&gt;提供完整强大的c api sdk&lt;/li&gt; 
   &lt;li&gt;支持FFmpeg拉流代理任意格式的流&lt;/li&gt; 
   &lt;li&gt;支持http api生成并返回实时截图&lt;/li&gt; 
   &lt;li&gt;支持按需解复用、转协议，当有人观看时才开启转协议，降低cpu占用率&lt;/li&gt; 
   &lt;li&gt;支持溯源模式的集群部署，溯源方式支持rtsp/rtmp/hls/http-ts, 边沿站支持hls, 源站支持多个(采用round robin方式溯源)&lt;/li&gt; 
   &lt;li&gt;rtsp/rtmp/webrtc推流异常断开后，可以在超时时间内重连推流，播放器无感知&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;闭源专业版&lt;/h2&gt; 
&lt;p&gt;在最新开源代码的基础，新增以下闭源专业版，详询邮箱：&lt;a href=&quot;mailto:1213642868@qq.com&quot;&gt;1213642868@qq.com&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;转码版本&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;1、音视频间任意转码(包括h265/h264/opus/g711/aac/g722/g722.1/mp3/svac等。&lt;/li&gt; 
   &lt;li&gt;2、基于配置文件的转码，支持设置比特率，codec类型等参数。&lt;/li&gt; 
   &lt;li&gt;3、基于http api的动态增减转码，支持设置比特率，分辨率倍数，codec类型、滤镜等参数。&lt;/li&gt; 
   &lt;li&gt;4、支持硬件、软件自适应转码。&lt;/li&gt; 
   &lt;li&gt;5、支持按需转码，有人观看才转码，支持透明转码模式，业务无需感知转码的存在，业务代码无需做任何调整。&lt;/li&gt; 
   &lt;li&gt;6、支持负载过高时，转码主动降低帧率且不花屏。&lt;/li&gt; 
   &lt;li&gt;7、支持滤镜，支持添加osd文本以及logo角标等能力。&lt;/li&gt; 
   &lt;li&gt;8、支持全GPU硬件编解码与滤镜，防止显存与内存频繁拷贝。&lt;/li&gt; 
   &lt;li&gt;9、支持视频全GPU(cuda)推理插件，支持人员、车辆等目标AI识别。&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;JT1078部标版本&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;1、支持接收jt1078推流转其他协议；自适应音视频共享seq和单独seq模式。&lt;/li&gt; 
   &lt;li&gt;2、新增支持jt1078级联，支持jt1078对讲。&lt;/li&gt; 
   &lt;li&gt;3、jt1078相关接口和用法与GB28181用法一致，保持兼容。&lt;/li&gt; 
   &lt;li&gt;4、支持h264/h265/g711/aac/mp3/g721/g722/g723/g729/g726/adpcm等编码。&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;IPTV版本&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;1、支持rtsp-ts/hls/http-ts/rtp组播/udp组播拉流转协议，支持ts透传模式，无需解复用转rtsp-ts/hls/http-ts/srt协议。&lt;/li&gt; 
   &lt;li&gt;2、支持接收rtsp-ts/srt推流，支持ts透传模式，无需解复用转rtsp-ts/hls/http-ts/srt协议。&lt;/li&gt; 
   &lt;li&gt;3、上述功能同时支持解复用ts为es流再转rtsp/rtmp/flv/http-ts/hls/hls-fmp4/mp4/fmp4/webrtc等协议。&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;VP9/AV1版本&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;全面新增支持av1/vp9编码，rtmp/rtsp/ts/ps/hls/mp4/fmp4等协议全面支持av1/vp9。&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;编译以及测试&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;编译前务必仔细参考wiki:&lt;a href=&quot;https://github.com/ZLMediaKit/ZLMediaKit/wiki/%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B&quot;&gt;快速开始&lt;/a&gt;操作!!!&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;怎么使用&lt;/h2&gt; 
&lt;p&gt;你有三种方法使用ZLMediaKit，分别是：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;1、使用c api，作为sdk使用，请参考&lt;a href=&quot;https://github.com/ZLMediaKit/ZLMediaKit/tree/master/api/include&quot;&gt;这里&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;2、作为独立的流媒体服务器使用，不想做c/c++开发的，可以参考 &lt;a href=&quot;https://github.com/ZLMediaKit/ZLMediaKit/wiki/MediaServer%E6%94%AF%E6%8C%81%E7%9A%84HTTP-API&quot;&gt;restful api&lt;/a&gt; 和 &lt;a href=&quot;https://github.com/ZLMediaKit/ZLMediaKit/wiki/MediaServer%E6%94%AF%E6%8C%81%E7%9A%84HTTP-HOOK-API&quot;&gt;web hook&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;3、如果想做c/c++开发，添加业务逻辑增加功能，可以参考这里的&lt;a href=&quot;https://github.com/ZLMediaKit/ZLMediaKit/tree/master/tests&quot;&gt;测试程序&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;二进制文件下载&lt;/h2&gt; 
&lt;p&gt;zlmediakit采用 github action 持续集成自动编译打包上传编译产出包，请在&lt;a href=&quot;https://github.com/ZLMediaKit/ZLMediaKit/issues/483&quot;&gt;issue列表&lt;/a&gt;下载最新sdk库文件以及可执行文件。&lt;/p&gt; 
&lt;h2&gt;Docker 镜像&lt;/h2&gt; 
&lt;p&gt;你可以从Docker Hub下载已经编译好的镜像并启动它：&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;#此镜像为github action 持续集成自动编译推送，跟代码(master分支)保持最新状态
docker run -id -p 1935:1935 -p 8080:80 -p 8443:443 -p 8554:554 -p 10000:10000 -p 10000:10000/udp -p 8000:8000/udp -p 9000:9000/udp zlmediakit/zlmediakit:master
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;你也可以根据Dockerfile编译镜像：&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;bash build_docker_images.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;合作项目&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;视频管理平台&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/648540858/wvp-GB28181-pro&quot;&gt;wvp-GB28181-pro&lt;/a&gt; java实现的开箱即用的GB28181协议视频平台&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/chatop2020/AKStream&quot;&gt;AKStream&lt;/a&gt; c#实现的全功能的软NVR接口/GB28181平台&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/any12345com/BXC_SipServer&quot;&gt;BXC_SipServer&lt;/a&gt; c++实现的国标GB28181流媒体信令服务器&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/panjjo/gosip&quot;&gt;gosip&lt;/a&gt; golang实现的GB28181服务器&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/tsingeye/FreeEhome&quot;&gt;FreeEhome&lt;/a&gt; golang实现的海康ehome服务器&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;播放器&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/numberwolf/h265web.js&quot;&gt;h265web.js&lt;/a&gt; 基于wasm支持H265的播放器，支持本项目多种专属协议&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/langhuihui/jessibuca&quot;&gt;jessibuca&lt;/a&gt; 基于wasm支持H265的播放器&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/v354412101/wsPlayer&quot;&gt;wsPlayer&lt;/a&gt; 基于MSE的websocket-fmp4播放器&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/any12345com/BXC_gb28181Player&quot;&gt;BXC_gb28181Player&lt;/a&gt; C++开发的支持国标GB28181协议的视频流播放器&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/leo94666/RTCPlayer&quot;&gt;RTCPlayer&lt;/a&gt; 一个基于Android客户端的的RTC播放器&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;WEB管理网站&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/1002victor/zlm_webassist&quot;&gt;zlm_webassist&lt;/a&gt; 本项目配套的前后端分离web管理项目&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/langmansh/AKStreamNVR&quot;&gt;AKStreamNVR&lt;/a&gt; 前后端分离web项目,支持webrtc播放&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;SDK&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/lunasaw/zlm-spring-boot-starter&quot;&gt;spring-boot-starter&lt;/a&gt; 本项目hook和rest接口starter&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/lidaofu-hub/j_zlm_sdk&quot;&gt;java sdk&lt;/a&gt; 本项目c sdk完整java包装库&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/malegend/ZLMediaKit.Autogen&quot;&gt;c# sdk&lt;/a&gt; 本项目c sdk完整c#包装库&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/metartc/metaRTC&quot;&gt;metaRTC&lt;/a&gt; 全国产纯c webrtc sdk&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;其他项目(已停止更新)&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://gitee.com/hfwudao/GB28181_Node_Http&quot;&gt;NodeJS实现的GB28181平台&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://gitee.com/kkkkk5G/MediaServerUI&quot;&gt;基于ZLMediaKit主线的管理WEB网站 &lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/chenxiaolei/ZLMediaKit_NVR_UI&quot;&gt;基于ZLMediaKit分支的管理WEB网站&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/MingZhuLiu/ZLMediaServerManagent&quot;&gt;一个非常漂亮的可视化后台管理系统&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/hctym1995/ZLM_ApiDemo&quot;&gt;基于C SDK实现的推流客户端&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/chengxiaosheng/ZLMediaKit.HttpApi&quot;&gt;C#版本的Http API与Hook&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/MingZhuLiu/ZLMediaKit.DotNetCore.Sdk&quot;&gt;DotNetCore的RESTful客户端&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;授权协议&lt;/h2&gt; 
&lt;p&gt;本项目自有代码使用宽松的MIT协议，在保留版权信息的情况下可以自由应用于各自商用、非商业的项目。 但是本项目也零碎的使用了一些其他的&lt;a href=&quot;https://github.com/ZLMediaKit/ZLMediaKit/wiki/%E4%BB%A3%E7%A0%81%E4%BE%9D%E8%B5%96%E4%B8%8E%E7%89%88%E6%9D%83%E5%A3%B0%E6%98%8E&quot;&gt;开源代码&lt;/a&gt;，在商用的情况下请自行替代或剔除； 由于使用本项目而产生的商业纠纷或侵权行为一概与本项目及开发者无关，请自行承担法律风险。 在使用本项目代码时，也应该在授权协议中同时表明本项目依赖的第三方库的协议。&lt;/p&gt; 
&lt;h2&gt;联系方式&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;邮箱：&lt;a href=&quot;mailto:1213642868@qq.com&quot;&gt;1213642868@qq.com&lt;/a&gt;(本项目相关或流媒体相关问题请走issue流程，否则恕不邮件答复)&lt;/li&gt; 
 &lt;li&gt;请关注微信公众号获取最新消息推送：&lt;/li&gt; 
&lt;/ul&gt; 
&lt;img src=&quot;https://user-images.githubusercontent.com/11495632/232451702-4c50bc72-84d8-4c94-af2b-57290088ba7a.png&quot; width=&quot;15%&quot;&gt; 
&lt;ul&gt; 
 &lt;li&gt;也可以自愿有偿加入知识星球咨询、获取资料以及加入微信技术群：&lt;/li&gt; 
&lt;/ul&gt; 
&lt;img src=&quot;https://user-images.githubusercontent.com/11495632/231946329-aa8517b0-3cf5-49cf-8c75-a93ed58cb9d2.png&quot; width=&quot;30%&quot;&gt; 
&lt;h2&gt;怎么提问？&lt;/h2&gt; 
&lt;p&gt;如果要对项目有相关疑问，建议您这么做：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;1、仔细看下readme、wiki，如果有必要可以查看下issue.&lt;/li&gt; 
 &lt;li&gt;2、如果您的问题还没解决，可以提issue.&lt;/li&gt; 
 &lt;li&gt;3、如果需要获取更及时贴心的技术支持，可以有偿加入&lt;a href=&quot;https://github.com/ZLMediaKit/ZLMediaKit/issues/2364&quot;&gt;知识星球&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;特别感谢&lt;/h2&gt; 
&lt;p&gt;本项目采用了&lt;a href=&quot;https://github.com/ireader&quot;&gt;老陈&lt;/a&gt; 的 &lt;a href=&quot;https://github.com/ireader/media-server&quot;&gt;media-server&lt;/a&gt; 库， 本项目的 ts/fmp4/mp4/ps 容器格式的复用解复用都依赖media-server库。在实现本项目诸多功能时，老陈多次给予了无私热情关键的帮助， 特此对他表示诚挚的感谢！&lt;/p&gt; 
&lt;h2&gt;致谢&lt;/h2&gt; 
&lt;p&gt;感谢以下各位对本项目包括但不限于代码贡献、问题反馈、资金捐赠等各种方式的支持！以下排名不分先后：&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/ireader&quot;&gt;老陈&lt;/a&gt; &lt;a href=&quot;https://github.com/gemfield&quot;&gt;Gemfield&lt;/a&gt; &lt;a href=&quot;https://github.com/nanguantong2&quot;&gt;南冠彤&lt;/a&gt; &lt;a href=&quot;https://github.com/tsingeye&quot;&gt;凹凸慢&lt;/a&gt; &lt;a href=&quot;https://github.com/chenxiaolei&quot;&gt;chenxiaolei&lt;/a&gt; &lt;a href=&quot;https://github.com/zqsong&quot;&gt;史前小虫&lt;/a&gt; &lt;a href=&quot;https://github.com/baiyfcu&quot;&gt;清涩绿茶&lt;/a&gt; &lt;a href=&quot;https://github.com/3503207480&quot;&gt;3503207480&lt;/a&gt; &lt;a href=&quot;https://github.com/DroidChow&quot;&gt;DroidChow&lt;/a&gt; &lt;a href=&quot;https://github.com/HuoQiShuai&quot;&gt;阿塞&lt;/a&gt; &lt;a href=&quot;https://github.com/ChinaCCF&quot;&gt;火宣&lt;/a&gt; &lt;a href=&quot;https://github.com/JerryLinGd&quot;&gt;γ瑞γミ&lt;/a&gt; &lt;a href=&quot;https://www.linkingvision.com/&quot;&gt;linkingvision&lt;/a&gt; &lt;a href=&quot;https://github.com/taotaobujue2008&quot;&gt;茄子&lt;/a&gt; &lt;a href=&quot;mailto:409257224@qq.com&quot;&gt;好心情&lt;/a&gt; &lt;a href=&quot;https://github.com/MingZhuLiu&quot;&gt;浮沉&lt;/a&gt; &lt;a href=&quot;https://github.com/wasphin&quot;&gt;Xiaofeng Wang&lt;/a&gt; &lt;a href=&quot;https://github.com/doodoocoder&quot;&gt;doodoocoder&lt;/a&gt; &lt;a href=&quot;https://github.com/Colibrow&quot;&gt;qingci&lt;/a&gt; &lt;a href=&quot;https://github.com/swwheihei&quot;&gt;swwheihei&lt;/a&gt; &lt;a href=&quot;https://gitee.com/kkkkk5G&quot;&gt;KKKKK5G&lt;/a&gt; &lt;a href=&quot;mailto:zhouweimin@supremind.com&quot;&gt;Zhou Weimin&lt;/a&gt; &lt;a href=&quot;https://github.com/jim-king-2000&quot;&gt;Jim Jin&lt;/a&gt; &lt;a href=&quot;mailto:392293307@qq.com&quot;&gt;西瓜丶&lt;/a&gt; &lt;a href=&quot;https://github.com/MingZhuLiu&quot;&gt;MingZhuLiu&lt;/a&gt; &lt;a href=&quot;https://github.com/chengxiaosheng&quot;&gt;chengxiaosheng&lt;/a&gt; &lt;a href=&quot;mailto:2381267071@qq.com&quot;&gt;big panda&lt;/a&gt; &lt;a href=&quot;https://github.com/tanningzhong&quot;&gt;tanningzhong&lt;/a&gt; &lt;a href=&quot;https://github.com/hctym1995&quot;&gt;hctym1995&lt;/a&gt; &lt;a href=&quot;https://gitee.com/kingyuanyuan&quot;&gt;hewenyuan&lt;/a&gt; &lt;a href=&quot;mailto:sunhui200475@163.com&quot;&gt;sunhui&lt;/a&gt; &lt;a href=&quot;mailto:fangpengcheng@bilibili.com&quot;&gt;mirs&lt;/a&gt; &lt;a href=&quot;mailto:kevin__cheng@outlook.com&quot;&gt;Kevin Cheng&lt;/a&gt; &lt;a href=&quot;mailto:root@oopy.org&quot;&gt;Liu Jiang&lt;/a&gt; &lt;a href=&quot;https://github.com/alongl&quot;&gt;along&lt;/a&gt; &lt;a href=&quot;mailto:xpy66swsry@gmail.com&quot;&gt;qingci&lt;/a&gt; &lt;a href=&quot;mailto:zh.ghlong@qq.com&quot;&gt;lyg1949&lt;/a&gt; &lt;a href=&quot;mailto:zh.ghlong@qq.com&quot;&gt;zhlong&lt;/a&gt; &lt;a href=&quot;mailto:3503207480@qq.com&quot;&gt;大裤衩&lt;/a&gt; &lt;a href=&quot;mailto:droid.chow@gmail.com&quot;&gt;droid.chow&lt;/a&gt; &lt;a href=&quot;https://github.com/musicwood&quot;&gt;陈晓林&lt;/a&gt; &lt;a href=&quot;https://github.com/CharleyWangHZ&quot;&gt;CharleyWangHZ&lt;/a&gt; &lt;a href=&quot;https://github.com/johzzy&quot;&gt;Johnny&lt;/a&gt; &lt;a href=&quot;https://github.com/DoubleX69&quot;&gt;DoubleX69&lt;/a&gt; &lt;a href=&quot;https://github.com/lawrencehj&quot;&gt;lawrencehj&lt;/a&gt; &lt;a href=&quot;mailto:xyyangkun@163.com&quot;&gt;yangkun&lt;/a&gt; &lt;a href=&quot;mailto:holychaossword@hotmail.com&quot;&gt;Xinghua Zhao&lt;/a&gt; &lt;a href=&quot;https://github.com/brokensword2018&quot;&gt;hejilin&lt;/a&gt; &lt;a href=&quot;https://github.com/rqb500&quot;&gt;rqb500&lt;/a&gt; &lt;a href=&quot;https://github.com/alexliyu7352&quot;&gt;Alex&lt;/a&gt; &lt;a href=&quot;https://github.com/Dw9&quot;&gt;Dw9&lt;/a&gt; &lt;a href=&quot;mailto:mingyuejingque@gmail.com&quot;&gt;明月惊鹊&lt;/a&gt; &lt;a href=&quot;mailto:2958580318@qq.com&quot;&gt;cgm&lt;/a&gt; &lt;a href=&quot;mailto:1724010622@qq.com&quot;&gt;hejilin&lt;/a&gt; &lt;a href=&quot;mailto:liyu7352@gmail.com&quot;&gt;alexliyu7352&lt;/a&gt; &lt;a href=&quot;mailto:2958580318@qq.com&quot;&gt;cgm&lt;/a&gt; &lt;a href=&quot;https://github.com/HaoruiWang&quot;&gt;haorui wang&lt;/a&gt; &lt;a href=&quot;mailto:joshuafc@foxmail.com&quot;&gt;joshuafc&lt;/a&gt; &lt;a href=&quot;https://github.com/JayChen0519&quot;&gt;JayChen0519&lt;/a&gt; &lt;a href=&quot;mailto:zuoxue@qq.com&quot;&gt;zx&lt;/a&gt; &lt;a href=&quot;mailto:wangcker@163.com&quot;&gt;wangcker&lt;/a&gt; &lt;a href=&quot;mailto:wp@zafu.edu.cn&quot;&gt;WuPeng&lt;/a&gt; &lt;a href=&quot;https://github.com/starry&quot;&gt;starry&lt;/a&gt; &lt;a href=&quot;https://github.com/mtdxc&quot;&gt;mtdxc&lt;/a&gt; &lt;a href=&quot;https://github.com/hugangfeng333&quot;&gt;胡刚风&lt;/a&gt; &lt;a href=&quot;https://github.com/zhao85&quot;&gt;zhao85&lt;/a&gt; &lt;a href=&quot;https://github.com/dreamisdream&quot;&gt;dreamisdream&lt;/a&gt; &lt;a href=&quot;https://github.com/dcan123&quot;&gt;dingcan&lt;/a&gt; &lt;a href=&quot;https://github.com/duiniuluantanqin&quot;&gt;Haibo Chen&lt;/a&gt; &lt;a href=&quot;https://gitee.com/leon14631&quot;&gt;Leon&lt;/a&gt; &lt;a href=&quot;https://github.com/custompal&quot;&gt;custompal&lt;/a&gt; &lt;a href=&quot;https://github.com/PioLing&quot;&gt;PioLing&lt;/a&gt; &lt;a href=&quot;https://github.com/ZSC714725&quot;&gt;KevinZang&lt;/a&gt; &lt;a href=&quot;https://github.com/gongluck&quot;&gt;gongluck&lt;/a&gt; &lt;a href=&quot;https://github.com/a-ucontrol&quot;&gt;a-ucontrol&lt;/a&gt; &lt;a href=&quot;https://github.com/TalusL&quot;&gt;TalusL&lt;/a&gt; &lt;a href=&quot;https://github.com/AHAOAHA&quot;&gt;ahaooahaz&lt;/a&gt; &lt;a href=&quot;https://github.com/TempoTian&quot;&gt;TempoTian&lt;/a&gt; &lt;a href=&quot;https://github.com/yjkhtddx&quot;&gt;Derek Liu&lt;/a&gt; &lt;a href=&quot;https://github.com/ljx0305&quot;&gt;ljx0305&lt;/a&gt; &lt;a href=&quot;https://github.com/zhu410289616&quot;&gt;朱如洪 &lt;/a&gt; &lt;a href=&quot;https://github.com/1461521844lijin&quot;&gt;lijin&lt;/a&gt; &lt;a href=&quot;https://github.com/PioLing&quot;&gt;PioLing&lt;/a&gt; &lt;a href=&quot;https://github.com/BackT0TheFuture&quot;&gt;BackT0TheFuture&lt;/a&gt; &lt;a href=&quot;https://github.com/perara&quot;&gt;perara&lt;/a&gt; &lt;a href=&quot;https://github.com/codeRATny&quot;&gt;codeRATny&lt;/a&gt; &lt;a href=&quot;https://github.com/dengjfzh&quot;&gt;dengjfzh&lt;/a&gt; &lt;a href=&quot;https://github.com/ixingqiao&quot;&gt;百鸣&lt;/a&gt; &lt;a href=&quot;https://github.com/xuandu&quot;&gt;fruit Juice&lt;/a&gt; &lt;a href=&quot;https://github.com/tbago&quot;&gt;tbago&lt;/a&gt; &lt;a href=&quot;https://github.com/Luosh&quot;&gt;Luosh&lt;/a&gt; &lt;a href=&quot;https://github.com/linxiaoyan&quot;&gt;linxiaoyan87&lt;/a&gt; &lt;a href=&quot;https://github.com/mc373906408&quot;&gt;waken&lt;/a&gt; &lt;a href=&quot;https://github.com/Deepslient&quot;&gt;Deepslient&lt;/a&gt; &lt;a href=&quot;https://github.com/rayjay214&quot;&gt;imp_rayjay&lt;/a&gt; &lt;a href=&quot;https://github.com/ArmstrongCN&quot;&gt;ArmstrongCN&lt;/a&gt; &lt;a href=&quot;https://github.com/leibnewton&quot;&gt;leibnewton&lt;/a&gt; &lt;a href=&quot;https://github.com/1002victor&quot;&gt;1002victor&lt;/a&gt; &lt;a href=&quot;https://github.com/xyyangkun&quot;&gt;Grin&lt;/a&gt; &lt;a href=&quot;https://github.com/xbpeng121&quot;&gt;xbpeng121&lt;/a&gt; &lt;a href=&quot;https://github.com/lvchenyun&quot;&gt;lvchenyun&lt;/a&gt; &lt;a href=&quot;https://github.com/Fummowo&quot;&gt;Fummowo&lt;/a&gt; &lt;a href=&quot;https://github.com/JHYoung1034&quot;&gt;Jovial Young &lt;/a&gt; &lt;a href=&quot;https://github.com/yujitai&quot;&gt;yujitai&lt;/a&gt; &lt;a href=&quot;https://github.com/kisChang&quot;&gt;KisChang&lt;/a&gt; &lt;a href=&quot;https://github.com/zjx94&quot;&gt;zjx94&lt;/a&gt; &lt;a href=&quot;https://github.com/blueskiner&quot;&gt;LeiZhi.Mai &lt;/a&gt; &lt;a href=&quot;https://github.com/nashiracn&quot;&gt;JiaHao&lt;/a&gt; &lt;a href=&quot;https://github.com/chdahuzi&quot;&gt;chdahuzi&lt;/a&gt; &lt;a href=&quot;https://github.com/snysmtx&quot;&gt;snysmtx&lt;/a&gt; &lt;a href=&quot;https://github.com/SetoKaiba&quot;&gt;SetoKaiba&lt;/a&gt; &lt;a href=&quot;https://github.com/sandro-qiang&quot;&gt;sandro-qiang&lt;/a&gt; &lt;a href=&quot;https://github.com/themactep&quot;&gt;Paul Philippov&lt;/a&gt; &lt;a href=&quot;https://github.com/zhang-chuanfeng&quot;&gt;张传峰&lt;/a&gt; &lt;a href=&quot;https://github.com/lidaofu-hub&quot;&gt;lidaofu-hub&lt;/a&gt; &lt;a href=&quot;https://github.com/huangcaichun&quot;&gt;huangcaichun&lt;/a&gt; &lt;a href=&quot;https://github.com/jamesZHANG500&quot;&gt;jamesZHANG500&lt;/a&gt; &lt;a href=&quot;https://github.com/wdl1697454803&quot;&gt;weidelong&lt;/a&gt; &lt;a href=&quot;https://github.com/linshangqiang&quot;&gt;小强先生&lt;/a&gt; &lt;a href=&quot;https://github.com/leo94666&quot;&gt;李之阳&lt;/a&gt; &lt;a href=&quot;https://github.com/sgzed&quot;&gt;sgzed&lt;/a&gt; &lt;a href=&quot;https://github.com/foobra&quot;&gt;gaoshan&lt;/a&gt; &lt;a href=&quot;https://github.com/zhang2349&quot;&gt;zhang2349&lt;/a&gt; &lt;a href=&quot;https://github.com/BenLocal&quot;&gt;benshi&lt;/a&gt; &lt;a href=&quot;https://github.com/autoantwort&quot;&gt;autoantwort&lt;/a&gt; &lt;a href=&quot;https://github.com/u7ko4&quot;&gt;u7ko4&lt;/a&gt; &lt;a href=&quot;https://github.com/Tsubaki-01&quot;&gt;WengQiang&lt;/a&gt; &lt;a href=&quot;https://github.com/wEnchanters&quot;&gt;wEnchanters&lt;/a&gt; &lt;a href=&quot;https://github.com/sbkyy&quot;&gt;sbkyy&lt;/a&gt; &lt;a href=&quot;https://github.com/wuxingzhong&quot;&gt;wuxingzhong&lt;/a&gt; &lt;a href=&quot;https://github.com/286897655&quot;&gt;286897655&lt;/a&gt; &lt;a href=&quot;https://github.com/ss002012&quot;&gt;ss002012&lt;/a&gt; &lt;a href=&quot;https://github.com/a839419160&quot;&gt;a839419160&lt;/a&gt; &lt;a href=&quot;https://github.com/oldma3095&quot;&gt;oldma3095&lt;/a&gt; &lt;a href=&quot;https://github.com/watersounds&quot;&gt;Dary&lt;/a&gt; &lt;a href=&quot;https://github.com/neesonqk&quot;&gt;N.z&lt;/a&gt; &lt;a href=&quot;https://github.com/callinglove&quot;&gt;yanggs&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;同时感谢JetBrains对开源项目的支持，本项目使用CLion开发与调试：&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://jb.gg/OpenSourceSupport&quot;&gt;&lt;img src=&quot;https://resources.jetbrains.com/storage/products/company/brand/logos/CLion.svg?sanitize=true&quot; alt=&quot;JetBrains&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;使用案例&lt;/h2&gt; 
&lt;p&gt;本项目已经得到不少公司和个人开发者的认可，据作者不完全统计， 使用本项目的公司包括知名的互联网巨头、国内排名前列的云服务公司、多家知名的AI独角兽公司， 以及一系列中小型公司。使用者可以通过在 &lt;a href=&quot;https://github.com/ZLMediaKit/ZLMediaKit/issues/511&quot;&gt;issue&lt;/a&gt; 上粘贴公司的大名和相关项目介绍为本项目背书，感谢支持！&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>google-ai-edge/mediapipe</title>
      <link>https://github.com/google-ai-edge/mediapipe</link>
      <description>&lt;p&gt;Cross-platform, customizable ML solutions for live and streaming media.&lt;/p&gt;&lt;hr&gt;&lt;hr&gt; 
&lt;h2&gt;layout: forward target: &lt;a href=&quot;https://developers.google.com/mediapipe&quot;&gt;https://developers.google.com/mediapipe&lt;/a&gt; title: Home nav_order: 1&lt;/h2&gt; 
&lt;hr&gt; 
&lt;p&gt;&lt;strong&gt;Attention:&lt;/strong&gt; &lt;em&gt;We have moved to &lt;a href=&quot;https://developers.google.com/mediapipe&quot;&gt;https://developers.google.com/mediapipe&lt;/a&gt; as the primary developer documentation site for MediaPipe as of April 3, 2023.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://developers.google.com/static/mediapipe/images/home/hero_01_1920.png&quot; alt=&quot;MediaPipe&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Attention&lt;/strong&gt;: MediaPipe Solutions Preview is an early release. &lt;a href=&quot;https://developers.google.com/mediapipe/solutions/about#notice&quot;&gt;Learn more&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;On-device machine learning for everyone&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Delight your customers with innovative machine learning features. MediaPipe contains everything that you need to customize and deploy to mobile (Android, iOS), web, desktop, edge devices, and IoT, effortlessly.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://goo.gle/mediapipe-studio&quot;&gt;See demos&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://developers.google.com/mediapipe/solutions&quot;&gt;Learn more&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Get started&lt;/h2&gt; 
&lt;p&gt;You can get started with MediaPipe Solutions by by checking out any of the developer guides for &lt;a href=&quot;https://developers.google.com/mediapipe/solutions/vision/object_detector&quot;&gt;vision&lt;/a&gt;, &lt;a href=&quot;https://developers.google.com/mediapipe/solutions/text/text_classifier&quot;&gt;text&lt;/a&gt;, and &lt;a href=&quot;https://developers.google.com/mediapipe/solutions/audio/audio_classifier&quot;&gt;audio&lt;/a&gt; tasks. If you need help setting up a development environment for use with MediaPipe Tasks, check out the setup guides for &lt;a href=&quot;https://developers.google.com/mediapipe/solutions/setup_android&quot;&gt;Android&lt;/a&gt;, &lt;a href=&quot;https://developers.google.com/mediapipe/solutions/setup_web&quot;&gt;web apps&lt;/a&gt;, and &lt;a href=&quot;https://developers.google.com/mediapipe/solutions/setup_python&quot;&gt;Python&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Solutions&lt;/h2&gt; 
&lt;p&gt;MediaPipe Solutions provides a suite of libraries and tools for you to quickly apply artificial intelligence (AI) and machine learning (ML) techniques in your applications. You can plug these solutions into your applications immediately, customize them to your needs, and use them across multiple development platforms. MediaPipe Solutions is part of the MediaPipe &lt;a href=&quot;https://github.com/google/mediapipe&quot;&gt;open source project&lt;/a&gt;, so you can further customize the solutions code to meet your application needs.&lt;/p&gt; 
&lt;p&gt;These libraries and resources provide the core functionality for each MediaPipe Solution:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;MediaPipe Tasks&lt;/strong&gt;: Cross-platform APIs and libraries for deploying solutions. &lt;a href=&quot;https://developers.google.com/mediapipe/solutions/tasks&quot;&gt;Learn more&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MediaPipe models&lt;/strong&gt;: Pre-trained, ready-to-run models for use with each solution.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;These tools let you customize and evaluate solutions:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;MediaPipe Model Maker&lt;/strong&gt;: Customize models for solutions with your data. &lt;a href=&quot;https://developers.google.com/mediapipe/solutions/model_maker&quot;&gt;Learn more&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MediaPipe Studio&lt;/strong&gt;: Visualize, evaluate, and benchmark solutions in your browser. &lt;a href=&quot;https://developers.google.com/mediapipe/solutions/studio&quot;&gt;Learn more&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Legacy solutions&lt;/h3&gt; 
&lt;p&gt;We have ended support for &lt;a href=&quot;https://developers.google.com/mediapipe/solutions/guide#legacy&quot;&gt;these MediaPipe Legacy Solutions&lt;/a&gt; as of March 1, 2023. All other MediaPipe Legacy Solutions will be upgraded to a new MediaPipe Solution. See the &lt;a href=&quot;https://developers.google.com/mediapipe/solutions/guide#legacy&quot;&gt;Solutions guide&lt;/a&gt; for details. The &lt;a href=&quot;https://github.com/google/mediapipe/tree/master/mediapipe&quot;&gt;code repository&lt;/a&gt; and prebuilt binaries for all MediaPipe Legacy Solutions will continue to be provided on an as-is basis.&lt;/p&gt; 
&lt;p&gt;For more on the legacy solutions, see the &lt;a href=&quot;https://github.com/google/mediapipe/tree/master/docs/solutions&quot;&gt;documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Framework&lt;/h2&gt; 
&lt;p&gt;To start using MediaPipe Framework, &lt;a href=&quot;https://developers.google.com/mediapipe/framework/getting_started/install&quot;&gt;install MediaPipe Framework&lt;/a&gt; and start building example applications in C++, Android, and iOS.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://developers.google.com/mediapipe/framework&quot;&gt;MediaPipe Framework&lt;/a&gt; is the low-level component used to build efficient on-device machine learning pipelines, similar to the premade MediaPipe Solutions.&lt;/p&gt; 
&lt;p&gt;Before using MediaPipe Framework, familiarize yourself with the following key &lt;a href=&quot;https://developers.google.com/mediapipe/framework/framework_concepts/overview.md&quot;&gt;Framework concepts&lt;/a&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://developers.google.com/mediapipe/framework/framework_concepts/packets.md&quot;&gt;Packets&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://developers.google.com/mediapipe/framework/framework_concepts/graphs.md&quot;&gt;Graphs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://developers.google.com/mediapipe/framework/framework_concepts/calculators.md&quot;&gt;Calculators&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Community&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://mediapipe.page.link/joinslack&quot;&gt;Slack community&lt;/a&gt; for MediaPipe users.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://groups.google.com/forum/#!forum/mediapipe&quot;&gt;Discuss&lt;/a&gt; - General community discussion around MediaPipe.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://mediapipe.page.link/awesome-mediapipe&quot;&gt;Awesome MediaPipe&lt;/a&gt; - A curated list of awesome MediaPipe related frameworks, libraries and software.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions. Please follow these &lt;a href=&quot;https://github.com/google/mediapipe/raw/master/CONTRIBUTING.md&quot;&gt;guidelines&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;We use GitHub issues for tracking requests and bugs. Please post questions to the MediaPipe Stack Overflow with a &lt;code&gt;mediapipe&lt;/code&gt; tag.&lt;/p&gt; 
&lt;h2&gt;Resources&lt;/h2&gt; 
&lt;h3&gt;Publications&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://developers.googleblog.com/2021/07/bringing-artworks-to-life-with-ar.html&quot;&gt;Bringing artworks to life with AR&lt;/a&gt; in Google Developers Blog&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://developers.googleblog.com/2021/05/control-your-mirru-prosthesis-with-mediapipe-hand-tracking.html&quot;&gt;Prosthesis control via Mirru App using MediaPipe hand tracking&lt;/a&gt; in Google Developers Blog&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://developers.googleblog.com/2021/04/signall-sdk-sign-language-interface-using-mediapipe-now-available.html&quot;&gt;SignAll SDK: Sign language interface using MediaPipe is now available for developers&lt;/a&gt; in Google Developers Blog&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://ai.googleblog.com/2020/12/mediapipe-holistic-simultaneous-face.html&quot;&gt;MediaPipe Holistic - Simultaneous Face, Hand and Pose Prediction, on Device&lt;/a&gt; in Google AI Blog&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://ai.googleblog.com/2020/10/background-features-in-google-meet.html&quot;&gt;Background Features in Google Meet, Powered by Web ML&lt;/a&gt; in Google AI Blog&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://developers.googleblog.com/2020/09/mediapipe-3d-face-transform.html&quot;&gt;MediaPipe 3D Face Transform&lt;/a&gt; in Google Developers Blog&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://developers.googleblog.com/2020/08/instant-motion-tracking-with-mediapipe.html&quot;&gt;Instant Motion Tracking With MediaPipe&lt;/a&gt; in Google Developers Blog&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://ai.googleblog.com/2020/08/on-device-real-time-body-pose-tracking.html&quot;&gt;BlazePose - On-device Real-time Body Pose Tracking&lt;/a&gt; in Google AI Blog&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://ai.googleblog.com/2020/08/mediapipe-iris-real-time-iris-tracking.html&quot;&gt;MediaPipe Iris: Real-time Eye Tracking and Depth Estimation&lt;/a&gt; in Google AI Blog&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://developers.googleblog.com/2020/04/mediapipe-knift-template-based-feature-matching.html&quot;&gt;MediaPipe KNIFT: Template-based feature matching&lt;/a&gt; in Google Developers Blog&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://developers.googleblog.com/2020/03/alfred-camera-smart-camera-features-using-mediapipe.html&quot;&gt;Alfred Camera: Smart camera features using MediaPipe&lt;/a&gt; in Google Developers Blog&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://ai.googleblog.com/2020/03/real-time-3d-object-detection-on-mobile.html&quot;&gt;Real-Time 3D Object Detection on Mobile Devices with MediaPipe&lt;/a&gt; in Google AI Blog&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://ai.googleblog.com/2020/02/autoflip-open-source-framework-for.html&quot;&gt;AutoFlip: An Open Source Framework for Intelligent Video Reframing&lt;/a&gt; in Google AI Blog&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://developers.googleblog.com/2020/01/mediapipe-on-web.html&quot;&gt;MediaPipe on the Web&lt;/a&gt; in Google Developers Blog&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://developers.googleblog.com/2019/12/object-detection-and-tracking-using-mediapipe.html&quot;&gt;Object Detection and Tracking using MediaPipe&lt;/a&gt; in Google Developers Blog&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://ai.googleblog.com/2019/08/on-device-real-time-hand-tracking-with.html&quot;&gt;On-Device, Real-Time Hand Tracking with MediaPipe&lt;/a&gt; in Google AI Blog&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1906.08172&quot;&gt;MediaPipe: A Framework for Building Perception Pipelines&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Videos&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/c/MediaPipe&quot;&gt;YouTube Channel&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>microsoft/BitNet</title>
      <link>https://github.com/microsoft/BitNet</link>
      <description>&lt;p&gt;Official inference framework for 1-bit LLMs&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;bitnet.cpp&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://opensource.org/licenses/MIT&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/license-MIT-blue.svg?sanitize=true&quot; alt=&quot;License: MIT&quot;&gt;&lt;/a&gt; &lt;img src=&quot;https://img.shields.io/badge/version-1.0-blue&quot; alt=&quot;version&quot;&gt;&lt;/p&gt; 
&lt;p&gt;bitnet.cpp is the official inference framework for 1-bit LLMs (e.g., BitNet b1.58). It offers a suite of optimized kernels, that support &lt;strong&gt;fast&lt;/strong&gt; and &lt;strong&gt;lossless&lt;/strong&gt; inference of 1.58-bit models on CPU (with NPU and GPU support coming next).&lt;/p&gt; 
&lt;p&gt;The first release of bitnet.cpp is to support inference on CPUs. bitnet.cpp achieves speedups of &lt;strong&gt;1.37x&lt;/strong&gt; to &lt;strong&gt;5.07x&lt;/strong&gt; on ARM CPUs, with larger models experiencing greater performance gains. Additionally, it reduces energy consumption by &lt;strong&gt;55.4%&lt;/strong&gt; to &lt;strong&gt;70.0%&lt;/strong&gt;, further boosting overall efficiency. On x86 CPUs, speedups range from &lt;strong&gt;2.37x&lt;/strong&gt; to &lt;strong&gt;6.17x&lt;/strong&gt; with energy reductions between &lt;strong&gt;71.9%&lt;/strong&gt; to &lt;strong&gt;82.2%&lt;/strong&gt;. Furthermore, bitnet.cpp can run a 100B BitNet b1.58 model on a single CPU, achieving speeds comparable to human reading (5-7 tokens per second), significantly enhancing the potential for running LLMs on local devices. Please refer to the &lt;a href=&quot;https://arxiv.org/abs/2410.16144&quot;&gt;technical report&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;img src=&quot;https://raw.githubusercontent.com/microsoft/BitNet/main/assets/m2_performance.jpg&quot; alt=&quot;m2_performance&quot; width=&quot;800&quot;&gt; 
&lt;img src=&quot;https://raw.githubusercontent.com/microsoft/BitNet/main/assets/intel_performance.jpg&quot; alt=&quot;m2_performance&quot; width=&quot;800&quot;&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;The tested models are dummy setups used in a research context to demonstrate the inference performance of bitnet.cpp.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Demo&lt;/h2&gt; 
&lt;p&gt;A demo of bitnet.cpp running a BitNet b1.58 3B model on Apple M2:&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/user-attachments/assets/7f46b736-edec-4828-b809-4be780a3e5b1&quot;&gt;https://github.com/user-attachments/assets/7f46b736-edec-4828-b809-4be780a3e5b1&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;What&#39;s New:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;02/18/2025 &lt;a href=&quot;https://arxiv.org/abs/2502.11880&quot;&gt;Bitnet.cpp: Efficient Edge Inference for Ternary LLMs&lt;/a&gt; &lt;img src=&quot;https://img.shields.io/badge/NEW-red&quot; alt=&quot;NEW&quot;&gt;&lt;/li&gt; 
 &lt;li&gt;11/08/2024 &lt;a href=&quot;https://arxiv.org/abs/2411.04965&quot;&gt;BitNet a4.8: 4-bit Activations for 1-bit LLMs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;10/21/2024 &lt;a href=&quot;https://arxiv.org/abs/2410.16144&quot;&gt;1-bit AI Infra: Part 1.1, Fast and Lossless BitNet b1.58 Inference on CPUs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;10/17/2024 bitnet.cpp 1.0 released.&lt;/li&gt; 
 &lt;li&gt;03/21/2024 &lt;a href=&quot;https://github.com/microsoft/unilm/raw/master/bitnet/The-Era-of-1-bit-LLMs__Training_Tips_Code_FAQ.pdf&quot;&gt;The-Era-of-1-bit-LLMs__Training_Tips_Code_FAQ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;02/27/2024 &lt;a href=&quot;https://arxiv.org/abs/2402.17764&quot;&gt;The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;10/17/2023 &lt;a href=&quot;https://arxiv.org/abs/2310.11453&quot;&gt;BitNet: Scaling 1-bit Transformers for Large Language Models&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;p&gt;This project is based on the &lt;a href=&quot;https://github.com/ggerganov/llama.cpp&quot;&gt;llama.cpp&lt;/a&gt; framework. We would like to thank all the authors for their contributions to the open-source community. Also, bitnet.cpp&#39;s kernels are built on top of the Lookup Table methodologies pioneered in &lt;a href=&quot;https://github.com/microsoft/T-MAC/&quot;&gt;T-MAC&lt;/a&gt;. For inference of general low-bit LLMs beyond ternary models, we recommend using T-MAC.&lt;/p&gt; 
&lt;h2&gt;Supported Models&lt;/h2&gt; 
&lt;p&gt;❗️&lt;strong&gt;We use existing 1-bit LLMs available on &lt;a href=&quot;https://huggingface.co/&quot;&gt;Hugging Face&lt;/a&gt; to demonstrate the inference capabilities of bitnet.cpp. These models are neither trained nor released by Microsoft. We hope the release of bitnet.cpp will inspire the development of 1-bit LLMs in large-scale settings in terms of model size and training tokens.&lt;/strong&gt;&lt;/p&gt; 
&lt;table&gt;  
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;th rowspan=&quot;2&quot;&gt;Model&lt;/th&gt; 
   &lt;th rowspan=&quot;2&quot;&gt;Parameters&lt;/th&gt; 
   &lt;th rowspan=&quot;2&quot;&gt;CPU&lt;/th&gt; 
   &lt;th colspan=&quot;3&quot;&gt;Kernel&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;th&gt;I2_S&lt;/th&gt; 
   &lt;th&gt;TL1&lt;/th&gt; 
   &lt;th&gt;TL2&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan=&quot;2&quot;&gt;&lt;a href=&quot;https://huggingface.co/1bitLLM/bitnet_b1_58-large&quot;&gt;bitnet_b1_58-large&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan=&quot;2&quot;&gt;0.7B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan=&quot;2&quot;&gt;&lt;a href=&quot;https://huggingface.co/1bitLLM/bitnet_b1_58-3B&quot;&gt;bitnet_b1_58-3B&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan=&quot;2&quot;&gt;3.3B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan=&quot;2&quot;&gt;&lt;a href=&quot;https://huggingface.co/HF1BitLLM/Llama3-8B-1.58-100B-tokens&quot;&gt;Llama3-8B-1.58-100B-tokens&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan=&quot;2&quot;&gt;8.0B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan=&quot;2&quot;&gt;&lt;a href=&quot;https://huggingface.co/collections/tiiuae/falcon3-67605ae03578be86e4e87026&quot;&gt;Falcon3 Family&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan=&quot;2&quot;&gt;1B-10B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h3&gt;Requirements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;python&amp;gt;=3.9&lt;/li&gt; 
 &lt;li&gt;cmake&amp;gt;=3.22&lt;/li&gt; 
 &lt;li&gt;clang&amp;gt;=18 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;For Windows users, install &lt;a href=&quot;https://visualstudio.microsoft.com/downloads/&quot;&gt;Visual Studio 2022&lt;/a&gt;. In the installer, toggle on at least the following options(this also automatically installs the required additional tools like CMake):&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Desktop-development with C++&lt;/li&gt; 
     &lt;li&gt;C++-CMake Tools for Windows&lt;/li&gt; 
     &lt;li&gt;Git for Windows&lt;/li&gt; 
     &lt;li&gt;C++-Clang Compiler for Windows&lt;/li&gt; 
     &lt;li&gt;MS-Build Support for LLVM-Toolset (clang)&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;For Debian/Ubuntu users, you can download with &lt;a href=&quot;https://apt.llvm.org/&quot;&gt;Automatic installation script&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;bash -c &quot;$(wget -O - https://apt.llvm.org/llvm.sh)&quot;&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;conda (highly recommend)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Build from source&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] If you are using Windows, please remember to always use a Developer Command Prompt / PowerShell for VS2022 for the following commands&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ol&gt; 
 &lt;li&gt;Clone the repo&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;git clone --recursive https://github.com/microsoft/BitNet.git
cd BitNet
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start=&quot;2&quot;&gt; 
 &lt;li&gt;Install the dependencies&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# (Recommended) Create a new conda environment
conda create -n bitnet-cpp python=3.9
conda activate bitnet-cpp

pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start=&quot;3&quot;&gt; 
 &lt;li&gt;Build the project&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# Download the model from Hugging Face, convert it to quantized gguf format, and build the project
python setup_env.py --hf-repo tiiuae/Falcon3-7B-Instruct-1.58bit -q i2_s

# Or you can manually download the model and run with local path
huggingface-cli download tiiuae/Falcon3-7B-Instruct-1.58bit --local-dir models/Falcon3-7B-Instruct-1.58bit
python setup_env.py -md models/Falcon3-7B-Instruct-1.58bit -q i2_s
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;
usage: setup_env.py [-h] [--hf-repo {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens,tiiuae/Falcon3-1B-Instruct-1.58bit,tiiuae/Falcon3-3B-Instruct-1.58bit,tiiuae/Falcon3-7B-Instruct-1.58bit,tiiuae/Falcon3-10B-Instruct-1.58bit}] [--model-dir MODEL_DIR] [--log-dir LOG_DIR] [--quant-type {i2_s,tl1}] [--quant-embd]
                    [--use-pretuned]

Setup the environment for running inference

optional arguments:
  -h, --help            show this help message and exit
  --hf-repo {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens,tiiuae/Falcon3-1B-Instruct-1.58bit,tiiuae/Falcon3-3B-Instruct-1.58bit,tiiuae/Falcon3-7B-Instruct-1.58bit,tiiuae/Falcon3-10B-Instruct-1.58bit}, -hr {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens,tiiuae/Falcon3-1B-Instruct-1.58bit,tiiuae/Falcon3-3B-Instruct-1.58bit,tiiuae/Falcon3-7B-Instruct-1.58bit,tiiuae/Falcon3-10B-Instruct-1.58bit}
                        Model used for inference
  --model-dir MODEL_DIR, -md MODEL_DIR
                        Directory to save/load the model
  --log-dir LOG_DIR, -ld LOG_DIR
                        Directory to save the logging info
  --quant-type {i2_s,tl1}, -q {i2_s,tl1}
                        Quantization type
  --quant-embd          Quantize the embeddings to f16
  --use-pretuned, -p    Use the pretuned kernel parameters
&lt;/pre&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;h3&gt;Basic usage&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# Run inference with the quantized model
python run_inference.py -m models/Falcon3-7B-Instruct-1.58bit/ggml-model-i2_s.gguf -p &quot;You are a helpful assistant&quot; -cnv
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;
usage: run_inference.py [-h] [-m MODEL] [-n N_PREDICT] -p PROMPT [-t THREADS] [-c CTX_SIZE] [-temp TEMPERATURE] [-cnv]

Run inference

optional arguments:
  -h, --help            show this help message and exit
  -m MODEL, --model MODEL
                        Path to model file
  -n N_PREDICT, --n-predict N_PREDICT
                        Number of tokens to predict when generating text
  -p PROMPT, --prompt PROMPT
                        Prompt to generate text from
  -t THREADS, --threads THREADS
                        Number of threads to use
  -c CTX_SIZE, --ctx-size CTX_SIZE
                        Size of the prompt context
  -temp TEMPERATURE, --temperature TEMPERATURE
                        Temperature, a hyperparameter that controls the randomness of the generated text
  -cnv, --conversation  Whether to enable chat mode or not (for instruct models.)
                        (When this option is turned on, the prompt specified by -p will be used as the system prompt.)
&lt;/pre&gt; 
&lt;h3&gt;Benchmark&lt;/h3&gt; 
&lt;p&gt;We provide scripts to run the inference benchmark providing a model.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;usage: e2e_benchmark.py -m MODEL [-n N_TOKEN] [-p N_PROMPT] [-t THREADS]  
   
Setup the environment for running the inference  
   
required arguments:  
  -m MODEL, --model MODEL  
                        Path to the model file. 
   
optional arguments:  
  -h, --help  
                        Show this help message and exit. 
  -n N_TOKEN, --n-token N_TOKEN  
                        Number of generated tokens. 
  -p N_PROMPT, --n-prompt N_PROMPT  
                        Prompt to generate text from. 
  -t THREADS, --threads THREADS  
                        Number of threads to use. 
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Here&#39;s a brief explanation of each argument:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;-m&lt;/code&gt;, &lt;code&gt;--model&lt;/code&gt;: The path to the model file. This is a required argument that must be provided when running the script.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-n&lt;/code&gt;, &lt;code&gt;--n-token&lt;/code&gt;: The number of tokens to generate during the inference. It is an optional argument with a default value of 128.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-p&lt;/code&gt;, &lt;code&gt;--n-prompt&lt;/code&gt;: The number of prompt tokens to use for generating text. This is an optional argument with a default value of 512.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-t&lt;/code&gt;, &lt;code&gt;--threads&lt;/code&gt;: The number of threads to use for running the inference. It is an optional argument with a default value of 2.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-h&lt;/code&gt;, &lt;code&gt;--help&lt;/code&gt;: Show the help message and exit. Use this argument to display usage information.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;python utils/e2e_benchmark.py -m /path/to/model -n 200 -p 256 -t 4  
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This command would run the inference benchmark using the model located at &lt;code&gt;/path/to/model&lt;/code&gt;, generating 200 tokens from a 256 token prompt, utilizing 4 threads.&lt;/p&gt; 
&lt;p&gt;For the model layout that do not supported by any public model, we provide scripts to generate a dummy model with the given model layout, and run the benchmark on your machine:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python utils/generate-dummy-bitnet-model.py models/bitnet_b1_58-large --outfile models/dummy-bitnet-125m.tl1.gguf --outtype tl1 --model-size 125M

# Run benchmark with the generated model, use -m to specify the model path, -p to specify the prompt processed, -n to specify the number of token to generate
python utils/e2e_benchmark.py -m models/dummy-bitnet-125m.tl1.gguf -p 512 -n 128
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>uxlfoundation/oneTBB</title>
      <link>https://github.com/uxlfoundation/oneTBB</link>
      <description>&lt;p&gt;oneAPI Threading Building Blocks (oneTBB)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;oneAPI Threading Building Blocks (oneTBB) &lt;img align=&quot;right&quot; width=&quot;200&quot; height=&quot;100&quot; src=&quot;https://raw.githubusercontent.com/uxlfoundation/artwork/e98f1a7a3d305c582d02c5f532e41487b710d470/foundation/uxl-foundation-logo-horizontal-color.svg?sanitize=true&quot;&gt;&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/uxlfoundation/oneTBB/master/LICENSE.txt&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/license-Apache_2.0-green.svg?sanitize=true&quot; alt=&quot;Apache License Version 2.0&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/uxlfoundation/oneTBB/actions/workflows/ci.yml?query=branch%3Amaster&quot;&gt;&lt;img src=&quot;https://github.com/uxlfoundation/oneTBB/actions/workflows/ci.yml/badge.svg?sanitize=true&quot; alt=&quot;oneTBB CI&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/uxlfoundation/oneTBB/discussions&quot;&gt;&lt;img src=&quot;https://badgen.net/badge/join%20the%20discussion/on%20github/blue?icon=github&quot; alt=&quot;Join the community on GitHub Discussions&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://www.bestpractices.dev/projects/9125&quot;&gt;&lt;img src=&quot;https://www.bestpractices.dev/projects/9125/badge&quot; alt=&quot;OpenSSF Best Practices&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://securityscorecards.dev/viewer/?uri=github.com/uxlfoundation/oneTBB&quot;&gt;&lt;img src=&quot;https://api.securityscorecards.dev/projects/github.com/uxlfoundation/oneTBB/badge&quot; alt=&quot;OpenSSF Scorecard&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://gurubase.io/g/onetbb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Gurubase-Ask%20oneTBB%20Guru-006BFF&quot; alt=&quot;Gurubase&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://securityscorecards.dev/viewer/?uri=github.com/oneapi-src/oneTBB&quot;&gt;&lt;img src=&quot;https://api.securityscorecards.dev/projects/github.com/oneapi-src/oneTBB/badge&quot; alt=&quot;OpenSSF Scorecard&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://scan.coverity.com/projects/oneapi-src-onetbb&quot;&gt;&lt;img src=&quot;https://img.shields.io/coverity/scan/30373.svg?sanitize=true&quot; alt=&quot;Coverity Scan Build Status&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;oneTBB is a flexible C++ library that simplifies the work of adding parallelism to complex applications, even if you are not a threading expert.&lt;/p&gt; 
&lt;p&gt;The library lets you easily write parallel programs that take full advantage of the multi-core performance. Such programs are portable, composable and have a future-proof scalability. oneTBB provides you with functions, interfaces, and classes to parallelize and scale the code. All you have to do is to use the templates.&lt;/p&gt; 
&lt;p&gt;The library differs from typical threading packages in the following ways:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;oneTBB enables you to specify logical parallelism instead of threads.&lt;/li&gt; 
 &lt;li&gt;oneTBB targets threading for performance.&lt;/li&gt; 
 &lt;li&gt;oneTBB is compatible with other threading packages.&lt;/li&gt; 
 &lt;li&gt;oneTBB emphasizes scalable, data parallel programming.&lt;/li&gt; 
 &lt;li&gt;oneTBB relies on generic programming.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Refer to oneTBB &lt;a href=&quot;https://raw.githubusercontent.com/uxlfoundation/oneTBB/master/examples&quot;&gt;examples&lt;/a&gt; and &lt;a href=&quot;https://github.com/oneapi-src/oneAPI-samples/tree/master/Libraries/oneTBB&quot;&gt;samples&lt;/a&gt; to see how you can use the library.&lt;/p&gt; 
&lt;p&gt;oneTBB is a part of the &lt;a href=&quot;http://www.uxlfoundation.org&quot;&gt;UXL Foundation&lt;/a&gt; and is an implementation of &lt;a href=&quot;https://oneapi.io&quot;&gt;oneAPI specification&lt;/a&gt;.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;&lt;em&gt;NOTE:&lt;/em&gt;&lt;/strong&gt; Threading Building Blocks (TBB) is now called oneAPI Threading Building Blocks (oneTBB) to highlight that the tool is a part of the oneAPI ecosystem.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Release Information&lt;/h2&gt; 
&lt;p&gt;See &lt;a href=&quot;https://raw.githubusercontent.com/uxlfoundation/oneTBB/master/RELEASE_NOTES.md&quot;&gt;Release Notes&lt;/a&gt; and &lt;a href=&quot;https://raw.githubusercontent.com/uxlfoundation/oneTBB/master/SYSTEM_REQUIREMENTS.md&quot;&gt;System Requirements&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://spec.oneapi.com/versions/latest/elements/oneTBB/source/nested-index.html&quot;&gt;oneTBB Specification&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://uxlfoundation.github.io/oneTBB&quot;&gt;oneTBB Developer Guide and Reference&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://uxlfoundation.github.io/oneTBB/main/tbb_userguide/Migration_Guide.html&quot;&gt;Migrating from TBB to oneTBB&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/uxlfoundation/oneTBB/master/cmake/README.md&quot;&gt;README for the CMake build system&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/uxlfoundation/oneTBB/master/VERSIONING.md&quot;&gt;oneTBB Versioning Policies&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://uxlfoundation.github.io/oneTBB/main/intro/testing_approach.html&quot;&gt;oneTBB Testing Approach&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/uxlfoundation/oneTBB/master/Bazel.md&quot;&gt;Basic support for the Bazel build system&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/uxlfoundation/oneTBB/discussions&quot;&gt;oneTBB Discussions&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/uxlfoundation/oneTBB/master/WASM_Support.md&quot;&gt;WASM Support&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;See &lt;a href=&quot;https://raw.githubusercontent.com/uxlfoundation/oneTBB/master/INSTALL.md&quot;&gt;Installation from Sources&lt;/a&gt; to learn how to install oneTBB.&lt;/p&gt; 
&lt;h2&gt;Governance&lt;/h2&gt; 
&lt;p&gt;The oneTBB project is governed by the UXL Foundation. You can get involved in this project in following ways:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Join the &lt;a href=&quot;https://github.com/uxlfoundation/foundation/tree/main?tab=readme-ov-file#working-groups&quot;&gt;Open Source and Specification Working Group&lt;/a&gt; meetings.&lt;/li&gt; 
 &lt;li&gt;Join the mailing lists for the &lt;a href=&quot;https://lists.uxlfoundation.org/g/main/subgroups&quot;&gt;UXL Foundation&lt;/a&gt; to receive meetings schedule and latest updates.&lt;/li&gt; 
 &lt;li&gt;Contribute to oneTBB project or oneTBB specification. Read &lt;a href=&quot;https://raw.githubusercontent.com/uxlfoundation/oneTBB/master/CONTRIBUTING.md&quot;&gt;CONTRIBUTING&lt;/a&gt; for more information.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Support&lt;/h2&gt; 
&lt;p&gt;See our &lt;a href=&quot;https://raw.githubusercontent.com/uxlfoundation/oneTBB/master/SUPPORT.md&quot;&gt;documentation&lt;/a&gt; to learn how to request help.&lt;/p&gt; 
&lt;h2&gt;How to Contribute&lt;/h2&gt; 
&lt;p&gt;We&amp;nbsp;welcome&amp;nbsp;community&amp;nbsp;contributions,&amp;nbsp;so&amp;nbsp;check&amp;nbsp;our&amp;nbsp;&lt;a href=&quot;https://raw.githubusercontent.com/uxlfoundation/oneTBB/master/CONTRIBUTING.md&quot;&gt;Contributing&amp;nbsp;Guidelines&lt;/a&gt; to&amp;nbsp;learn&amp;nbsp;more.&lt;/p&gt; 
&lt;p&gt;Use GitHub Issues for feature requests, bug reports, and minor inquiries. For broader questions and development-related discussions, use GitHub Discussions.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;oneAPI Threading Building Blocks is licensed under &lt;a href=&quot;https://raw.githubusercontent.com/uxlfoundation/oneTBB/master/LICENSE.txt&quot;&gt;Apache License, Version 2.0&lt;/a&gt;. By its terms, contributions submitted to the project are also done under that license.&lt;/p&gt; 
&lt;hr&gt; 
&lt;p&gt;* All names and brands may be claimed as the property of others.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>baldurk/renderdoc</title>
      <link>https://github.com/baldurk/renderdoc</link>
      <description>&lt;p&gt;RenderDoc is a stand-alone graphics debugging tool.&lt;/p&gt;&lt;hr&gt;&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/661798/36482670-f81601c0-170b-11e8-8adb-2365b346ac27.png&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/baldurk/renderdoc/v1.x/LICENSE.md&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/license-MIT-blue.svg?sanitize=true&quot; alt=&quot;MIT licensed&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/baldurk/renderdoc/actions&quot;&gt;&lt;img src=&quot;https://github.com/baldurk/renderdoc/workflows/CI/badge.svg?branch=v1.x&amp;amp;event=push&quot; alt=&quot;CI&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/baldurk/renderdoc/v1.x/docs/CODE_OF_CONDUCT.md&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg?sanitize=true&quot; alt=&quot;Contributor Covenant&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;RenderDoc is a frame-capture based graphics debugger, currently available for Vulkan, D3D11, D3D12, OpenGL, and OpenGL ES development on Windows, Linux, Android, and Nintendo Switch™. It is completely open-source under the MIT license.&lt;/p&gt; 
&lt;p&gt;RenderDoc is intended for debugging your own programs only. Any discussion of capturing programs that you did not create will not be allowed in any official public RenderDoc setting, including the issue tracker, discord, or via email. For example this includes capturing commercial games that you did not create, or capturing Google Maps or Google Earth. Note: Capturing projects you created that use a third party engine like Unreal or Unity, or open source and free projects is completely fine and supported.&lt;/p&gt; 
&lt;p&gt;If you have any questions, suggestions or problems or you can &lt;a href=&quot;https://github.com/baldurk/renderdoc/issues/new/choose&quot;&gt;create an issue&lt;/a&gt; here on github, &lt;a href=&quot;mailto:baldurk@baldurk.org&quot;&gt;email me directly&lt;/a&gt; or come into &lt;a href=&quot;https://webchat.oftc.net/?channels=renderdoc&quot;&gt;IRC&lt;/a&gt; or &lt;a href=&quot;https://discord.gg/ahq6yRB&quot;&gt;Discord&lt;/a&gt; to discuss it.&lt;/p&gt; 
&lt;p&gt;To install on windows run the appropriate installer for your OS (&lt;a href=&quot;https://renderdoc.org/stable/latest/RenderDoc_latest_64.msi&quot;&gt;64-bit&lt;/a&gt; | &lt;a href=&quot;https://renderdoc.org/stable/latest/RenderDoc_latest_32.msi&quot;&gt;32-bit&lt;/a&gt;) or download the portable zip from the &lt;a href=&quot;https://renderdoc.org/builds&quot;&gt;builds page&lt;/a&gt;. The 64-bit windows build fully supports capturing from 32-bit programs. On linux only 64-bit x86 is supported - there is a precompiled &lt;a href=&quot;https://renderdoc.org/stable/latest/renderdoc_latest.tar.gz&quot;&gt;binary tarball&lt;/a&gt; available, or your distribution may package it. If not you can &lt;a href=&quot;https://raw.githubusercontent.com/baldurk/renderdoc/v1.x/docs/CONTRIBUTING/Compiling.md&quot;&gt;build from source&lt;/a&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Downloads&lt;/strong&gt;: Stable and nightly builds: &lt;a href=&quot;https://renderdoc.org/builds&quot;&gt;https://renderdoc.org/builds&lt;/a&gt; ( &lt;a href=&quot;https://renderdoc.org/symbols&quot;&gt;Symbol server&lt;/a&gt; )&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Documentation&lt;/strong&gt;: &lt;a href=&quot;https://renderdoc.org/docs&quot;&gt;HTML online&lt;/a&gt;, &lt;a href=&quot;https://renderdoc.org/docs/renderdoc.chm&quot;&gt;CHM in builds&lt;/a&gt;, &lt;a href=&quot;https://www.youtube.com/user/baldurkarlsson&quot;&gt;Videos&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Contact&lt;/strong&gt;: &lt;a href=&quot;mailto:baldurk@baldurk.org&quot;&gt;baldurk@baldurk.org&lt;/a&gt;, &lt;a href=&quot;https://webchat.oftc.net/?channels=renderdoc&quot;&gt;#renderdoc on OFTC IRC&lt;/a&gt;, &lt;a href=&quot;https://discord.gg/ahq6yRB&quot;&gt;Discord server&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Code of Conduct&lt;/strong&gt;: &lt;a href=&quot;https://raw.githubusercontent.com/baldurk/renderdoc/v1.x/docs/CODE_OF_CONDUCT.md&quot;&gt;Contributor Covenant&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Information for contributors&lt;/strong&gt;: &lt;a href=&quot;https://raw.githubusercontent.com/baldurk/renderdoc/v1.x/docs/CONTRIBUTING.md&quot;&gt;All contribution information&lt;/a&gt;, &lt;a href=&quot;https://raw.githubusercontent.com/baldurk/renderdoc/v1.x/docs/CONTRIBUTING/Compiling.md&quot;&gt;Compilation instructions&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Community extensions&lt;/strong&gt;: &lt;a href=&quot;https://github.com/baldurk/renderdoc-contrib&quot;&gt;Extensions repository&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Screenshots&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;a href=&quot;https://renderdoc.org/fp/screen1.jpg&quot;&gt; &lt;img src=&quot;https://renderdoc.org/fp/ts_screen1.jpg?2&quot; alt=&quot;Texture view&quot;&gt; &lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href=&quot;https://renderdoc.org/fp/screen2.png&quot;&gt; &lt;img src=&quot;https://renderdoc.org/fp/ts_screen2.jpg?2&quot; alt=&quot;Pixel history &amp;amp; shader debug&quot;&gt; &lt;/a&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://renderdoc.org/fp/screen3.png&quot;&gt; &lt;img src=&quot;https://renderdoc.org/fp/ts_screen3.jpg?2&quot; alt=&quot;Mesh viewer&quot;&gt; &lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://renderdoc.org/fp/screen4.png&quot;&gt; &lt;img src=&quot;https://renderdoc.org/fp/ts_screen4.jpg?2&quot; alt=&quot;Pipeline viewer &amp;amp; constants&quot;&gt; &lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;API Support&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th&gt;Windows&lt;/th&gt; 
   &lt;th&gt;Linux&lt;/th&gt; 
   &lt;th&gt;Android&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Vulkan&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenGL ES 2.0 - 3.2&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenGL 3.2 - 4.6 Core&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;D3D11 &amp;amp; D3D12&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenGL 1.0 - 2.0 Compat&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✖&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✖&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;D3D9 &amp;amp; 10&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✖&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Metal&lt;/td&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;ul&gt; 
 &lt;li&gt;Nintendo Switch™ support is distributed separately for authorized developers as part of the NintendoSDK. For more information, consult the Nintendo Developer Portal.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Downloads&lt;/h2&gt; 
&lt;p&gt;There are &lt;a href=&quot;https://renderdoc.org/builds&quot;&gt;binary releases&lt;/a&gt; available, built from the release targets. If you just want to use the program and you ended up here, this is what you want :).&lt;/p&gt; 
&lt;p&gt;It&#39;s recommended that if you&#39;re new you start with the stable builds. Nightly builds are available every day from the &lt;a href=&quot;https://renderdoc.org/builds#nightly&quot;&gt;v1.x branch here&lt;/a&gt; if you need it, but correspondingly may be less stable.&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;The text documentation is available &lt;a href=&quot;https://renderdoc.org/docs/&quot;&gt;online for the latest stable version&lt;/a&gt;, as well as in &lt;a href=&quot;https://renderdoc.org/docs/renderdoc.chm&quot;&gt;renderdoc.chm&lt;/a&gt; in any build. It&#39;s built from &lt;a href=&quot;https://raw.githubusercontent.com/baldurk/renderdoc/v1.x/docs&quot;&gt;restructured text with sphinx&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;As mentioned above there are some &lt;a href=&quot;https://www.youtube.com/user/baldurkarlsson&quot;&gt;youtube videos&lt;/a&gt; showing the use of some basic features and an introduction/overview.&lt;/p&gt; 
&lt;p&gt;There is also a great presentation by &lt;a href=&quot;https://twitter.com/Icetigris&quot;&gt;@Icetigris&lt;/a&gt; which goes into some details of how RenderDoc can be used in real world situations: &lt;a href=&quot;https://docs.google.com/presentation/d/1LQUMIld4SGoQVthnhT1scoA3k4Sg0as14G4NeSiSgFU/edit#slide=id.p&quot;&gt;slides are up here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;RenderDoc is released under the MIT license, see &lt;a href=&quot;https://raw.githubusercontent.com/baldurk/renderdoc/v1.x/LICENSE.md&quot;&gt;LICENSE.md&lt;/a&gt; for full text as well as 3rd party library acknowledgements.&lt;/p&gt; 
&lt;h2&gt;Compiling&lt;/h2&gt; 
&lt;p&gt;Building RenderDoc is fairly straight forward on most platforms. See &lt;a href=&quot;https://raw.githubusercontent.com/baldurk/renderdoc/v1.x/docs/CONTRIBUTING/Compiling.md&quot;&gt;Compiling.md&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;Contributing &amp;amp; Development&lt;/h2&gt; 
&lt;p&gt;I&#39;ve added some notes on how to contribute, as well as where to get started looking through the code in &lt;a href=&quot;https://raw.githubusercontent.com/baldurk/renderdoc/v1.x/docs/CONTRIBUTING/Developing-Change.md&quot;&gt;Developing-Change.md&lt;/a&gt;. All contribution information is available under &lt;a href=&quot;https://raw.githubusercontent.com/baldurk/renderdoc/v1.x/docs/CONTRIBUTING.md&quot;&gt;CONTRIBUTING.md&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>google/highway</title>
      <link>https://github.com/google/highway</link>
      <description>&lt;p&gt;Performance-portable, length-agnostic SIMD with runtime dispatch&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Efficient and performance-portable vector software&lt;/h1&gt; 
&lt;p&gt;Highway is a C++ library that provides portable SIMD/vector intrinsics.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://google.github.io/highway/en/master/&quot;&gt;Documentation&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Previously licensed under Apache 2, now dual-licensed as Apache 2 / BSD-3.&lt;/p&gt; 
&lt;h2&gt;Why&lt;/h2&gt; 
&lt;p&gt;We are passionate about high-performance software. We see major untapped potential in CPUs (servers, mobile, desktops). Highway is for engineers who want to reliably and economically push the boundaries of what is possible in software.&lt;/p&gt; 
&lt;h2&gt;How&lt;/h2&gt; 
&lt;p&gt;CPUs provide SIMD/vector instructions that apply the same operation to multiple data items. This can reduce energy usage e.g. &lt;em&gt;fivefold&lt;/em&gt; because fewer instructions are executed. We also often see &lt;em&gt;5-10x&lt;/em&gt; speedups.&lt;/p&gt; 
&lt;p&gt;Highway makes SIMD/vector programming practical and workable according to these guiding principles:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Does what you expect&lt;/strong&gt;: Highway is a C++ library with carefully-chosen functions that map well to CPU instructions without extensive compiler transformations. The resulting code is more predictable and robust to code changes/compiler updates than autovectorization.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Works on widely-used platforms&lt;/strong&gt;: Highway supports five architectures; the same application code can target various instruction sets, including those with &#39;scalable&#39; vectors (size unknown at compile time). Highway only requires C++11 and supports four families of compilers. If you would like to use Highway on other platforms, please raise an issue.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Flexible to deploy&lt;/strong&gt;: Applications using Highway can run on heterogeneous clouds or client devices, choosing the best available instruction set at runtime. Alternatively, developers may choose to target a single instruction set without any runtime overhead. In both cases, the application code is the same except for swapping &lt;code&gt;HWY_STATIC_DISPATCH&lt;/code&gt; with &lt;code&gt;HWY_DYNAMIC_DISPATCH&lt;/code&gt; plus one line of code. See also @kfjahnke&#39;s &lt;a href=&quot;https://github.com/kfjahnke/zimt/raw/multi_isa/examples/multi_isa_example/multi_simd_isa.md&quot;&gt;introduction to dispatching&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Suitable for a variety of domains&lt;/strong&gt;: Highway provides an extensive set of operations, used for image processing (floating-point), compression, video analysis, linear algebra, cryptography, sorting and random generation. We recognise that new use-cases may require additional ops and are happy to add them where it makes sense (e.g. no performance cliffs on some architectures). If you would like to discuss, please file an issue.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Rewards data-parallel design&lt;/strong&gt;: Highway provides tools such as Gather, MaskedLoad, and FixedTag to enable speedups for legacy data structures. However, the biggest gains are unlocked by designing algorithms and data structures for scalable vectors. Helpful techniques include batching, structure-of-array layouts, and aligned/padded allocations.&lt;/p&gt; 
&lt;p&gt;We recommend these resources for getting started:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;http://const.me/articles/simd/simd.pdf&quot;&gt;SIMD for C++ Developers&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://en.algorithmica.org/hpc/&quot;&gt;Algorithms for Modern Hardware&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://agner.org/optimize/optimizing_cpp.pdf&quot;&gt;Optimizing software in C++&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://stackoverflow.blog/2020/07/08/improving-performance-with-simd-intrinsics-in-three-use-cases/&quot;&gt;Improving performance with SIMD intrinsics in three use cases&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;p&gt;Online demos using Compiler Explorer:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://gcc.godbolt.org/z/KM3ben7ET&quot;&gt;multiple targets with dynamic dispatch&lt;/a&gt; (more complicated, but flexible and uses best available SIMD)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://gcc.godbolt.org/z/rGnjMevKG&quot;&gt;single target using -m flags&lt;/a&gt; (simpler, but requires/only uses the instruction set enabled by compiler flags)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We observe that Highway is referenced in the following open source projects, found via sourcegraph.com. Most are GitHub repositories. If you would like to add your project or link to it directly, feel free to raise an issue or contact us via the below email.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Audio: &lt;a href=&quot;https://github.com/google/zimtohrli&quot;&gt;Zimtohrli perceptual metric&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Browsers: Chromium (+Vivaldi), Firefox (+floorp / foxhound / librewolf / Waterfox)&lt;/li&gt; 
 &lt;li&gt;Computational biology: &lt;a href=&quot;https://github.com/bnprks/BPCells&quot;&gt;RNA analysis&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Computer graphics: &lt;a href=&quot;https://github.com/rools/voxl&quot;&gt;Sparse voxel renderer&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Cryptography: google/distributed_point_functions, google/shell-encryption&lt;/li&gt; 
 &lt;li&gt;Data structures: bkille/BitLib&lt;/li&gt; 
 &lt;li&gt;Image codecs: eustas/2im, &lt;a href=&quot;https://github.com/GrokImageCompression/grok&quot;&gt;Grok JPEG 2000&lt;/a&gt;, &lt;a href=&quot;https://github.com/libjxl/libjxl&quot;&gt;JPEG XL&lt;/a&gt;, &lt;a href=&quot;https://github.com/osamu620/JPEGenc&quot;&gt;JPEGenc&lt;/a&gt;, &lt;a href=&quot;https://github.com/google/jpegli&quot;&gt;Jpegli&lt;/a&gt;, OpenHTJ2K&lt;/li&gt; 
 &lt;li&gt;Image processing: cloudinary/ssimulacra2, m-ab-s/media-autobuild_suite, &lt;a href=&quot;https://github.com/libvips/libvips&quot;&gt;libvips&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Image viewers: AlienCowEatCake/ImageViewer, diffractor/diffractor, mirillis/jpegxl-wic, &lt;a href=&quot;https://bitbucket.org/kfj/pv/&quot;&gt;Lux panorama/image viewer&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Information retrieval: &lt;a href=&quot;https://github.com/iresearch-toolkit/iresearch&quot;&gt;iresearch database index&lt;/a&gt;, michaeljclark/zvec, &lt;a href=&quot;https://github.com/varchar-io/nebula&quot;&gt;nebula interactive analytics / OLAP&lt;/a&gt;, &lt;a href=&quot;https://github.com/google-research/google-research/tree/7a269cb2ce0ae1db591fe11b62cbc0be7d72532a/scann&quot;&gt;ScaNN Scalable Nearest Neighbors&lt;/a&gt;, &lt;a href=&quot;https://github.com/1yefuwang1/vectorlite/&quot;&gt;vectorlite vector search&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Machine learning: &lt;a href=&quot;https://github.com/google/gemma.cpp&quot;&gt;gemma.cpp&lt;/a&gt;, Tensorflow, Numpy, zpye/SimpleInfer&lt;/li&gt; 
 &lt;li&gt;Robotics: &lt;a href=&quot;https://github.com/RobotLocomotion/drake&quot;&gt;MIT Model-Based Design and Verification&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Other&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.mnm-team.org/pub/Fopras/rock23/&quot;&gt;Evaluation of C++ SIMD Libraries&lt;/a&gt;: &quot;Highway excelled with a strong performance across multiple SIMD extensions [..]. Thus, Highway may currently be the most suitable SIMD library for many software projects.&quot;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/kfjahnke/zimt&quot;&gt;zimt&lt;/a&gt;: C++11 template library to process n-dimensional arrays with multi-threaded SIMD code&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/google/highway/tree/master/hwy/contrib/sort&quot;&gt;vectorized Quicksort&lt;/a&gt; (&lt;a href=&quot;https://arxiv.org/abs/2205.05982&quot;&gt;paper&lt;/a&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If you&#39;d like to get Highway, in addition to cloning from this GitHub repository or using it as a Git submodule, you can also find it in the following package managers or repositories:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;alpinelinux&lt;/li&gt; 
 &lt;li&gt;conan-io&lt;/li&gt; 
 &lt;li&gt;conda-forge&lt;/li&gt; 
 &lt;li&gt;DragonFlyBSD,&lt;/li&gt; 
 &lt;li&gt;fd00/yacp&lt;/li&gt; 
 &lt;li&gt;freebsd&lt;/li&gt; 
 &lt;li&gt;getsolus/packages&lt;/li&gt; 
 &lt;li&gt;ghostbsd&lt;/li&gt; 
 &lt;li&gt;microsoft/vcpkg&lt;/li&gt; 
 &lt;li&gt;MidnightBSD&lt;/li&gt; 
 &lt;li&gt;MSYS2&lt;/li&gt; 
 &lt;li&gt;NetBSD&lt;/li&gt; 
 &lt;li&gt;openSUSE&lt;/li&gt; 
 &lt;li&gt;opnsense&lt;/li&gt; 
 &lt;li&gt;Xilinx/Vitis_Libraries&lt;/li&gt; 
 &lt;li&gt;xmake-io/xmake-repo&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See also the list at &lt;a href=&quot;https://repology.org/project/highway-simd-library/versions&quot;&gt;https://repology.org/project/highway-simd-library/versions&lt;/a&gt; .&lt;/p&gt; 
&lt;h2&gt;Current status&lt;/h2&gt; 
&lt;h3&gt;Targets&lt;/h3&gt; 
&lt;p&gt;Highway supports 24 targets, listed in alphabetical order of platform:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Any: &lt;code&gt;EMU128&lt;/code&gt;, &lt;code&gt;SCALAR&lt;/code&gt;;&lt;/li&gt; 
 &lt;li&gt;Armv7+: &lt;code&gt;NEON_WITHOUT_AES&lt;/code&gt;, &lt;code&gt;NEON&lt;/code&gt;, &lt;code&gt;NEON_BF16&lt;/code&gt;, &lt;code&gt;SVE&lt;/code&gt;, &lt;code&gt;SVE2&lt;/code&gt;, &lt;code&gt;SVE_256&lt;/code&gt;, &lt;code&gt;SVE2_128&lt;/code&gt;;&lt;/li&gt; 
 &lt;li&gt;IBM Z: &lt;code&gt;Z14&lt;/code&gt;, &lt;code&gt;Z15&lt;/code&gt;;&lt;/li&gt; 
 &lt;li&gt;POWER: &lt;code&gt;PPC8&lt;/code&gt; (v2.07), &lt;code&gt;PPC9&lt;/code&gt; (v3.0), &lt;code&gt;PPC10&lt;/code&gt; (v3.1B, not yet supported due to compiler bugs, see #1207; also requires QEMU 7.2);&lt;/li&gt; 
 &lt;li&gt;RISC-V: &lt;code&gt;RVV&lt;/code&gt; (1.0);&lt;/li&gt; 
 &lt;li&gt;WebAssembly: &lt;code&gt;WASM&lt;/code&gt;, &lt;code&gt;WASM_EMU256&lt;/code&gt; (a 2x unrolled version of wasm128, enabled if &lt;code&gt;HWY_WANT_WASM2&lt;/code&gt; is defined. This will remain supported until it is potentially superseded by a future version of WASM.);&lt;/li&gt; 
 &lt;li&gt;x86: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;SSE2&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;SSSE3&lt;/code&gt; (~Intel Core)&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;SSE4&lt;/code&gt; (~Nehalem, also includes AES + CLMUL).&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;AVX2&lt;/code&gt; (~Haswell, also includes BMI2 + F16 + FMA)&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;AVX3&lt;/code&gt; (~Skylake, AVX-512F/BW/CD/DQ/VL)&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;AVX3_DL&lt;/code&gt; (~Icelake, includes &lt;code&gt;BitAlg&lt;/code&gt; + &lt;code&gt;CLMUL&lt;/code&gt; + &lt;code&gt;GFNI&lt;/code&gt; + &lt;code&gt;VAES&lt;/code&gt; + &lt;code&gt;VBMI&lt;/code&gt; + &lt;code&gt;VBMI2&lt;/code&gt; + &lt;code&gt;VNNI&lt;/code&gt; + &lt;code&gt;VPOPCNT&lt;/code&gt;),&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;AVX3_ZEN4&lt;/code&gt; (AVX3_DL plus BF16, optimized for AMD Zen4; requires opt-in by defining &lt;code&gt;HWY_WANT_AVX3_ZEN4&lt;/code&gt; if compiling for static dispatch, but enabled by default for runtime dispatch),&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;AVX3_SPR&lt;/code&gt; (~Sapphire Rapids, includes AVX-512FP16)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Our policy is that unless otherwise specified, targets will remain supported as long as they can be (cross-)compiled with currently supported Clang or GCC, and tested using QEMU. If the target can be compiled with LLVM trunk and tested using our version of QEMU without extra flags, then it is eligible for inclusion in our continuous testing infrastructure. Otherwise, the target will be manually tested before releases with selected versions/configurations of Clang and GCC.&lt;/p&gt; 
&lt;p&gt;SVE was initially tested using farm_sve (see acknowledgments).&lt;/p&gt; 
&lt;h3&gt;Versioning&lt;/h3&gt; 
&lt;p&gt;Highway releases aim to follow the semver.org system (MAJOR.MINOR.PATCH), incrementing MINOR after backward-compatible additions and PATCH after backward-compatible fixes. We recommend using releases (rather than the Git tip) because they are tested more extensively, see below.&lt;/p&gt; 
&lt;p&gt;The current version 1.0 signals an increased focus on backwards compatibility. Applications using documented functionality will remain compatible with future updates that have the same major version number.&lt;/p&gt; 
&lt;h3&gt;Testing&lt;/h3&gt; 
&lt;p&gt;Continuous integration tests build with a recent version of Clang (running on native x86, or QEMU for RISC-V and Arm) and MSVC 2019 (v19.28, running on native x86).&lt;/p&gt; 
&lt;p&gt;Before releases, we also test on x86 with Clang and GCC, and Armv7/8 via GCC cross-compile. See the &lt;a href=&quot;https://raw.githubusercontent.com/google/highway/master/g3doc/release_testing_process.md&quot;&gt;testing process&lt;/a&gt; for details.&lt;/p&gt; 
&lt;h3&gt;Related modules&lt;/h3&gt; 
&lt;p&gt;The &lt;code&gt;contrib&lt;/code&gt; directory contains SIMD-related utilities: an image class with aligned rows, a math library (16 functions already implemented, mostly trigonometry), and functions for computing dot products and sorting.&lt;/p&gt; 
&lt;h3&gt;Other libraries&lt;/h3&gt; 
&lt;p&gt;If you only require x86 support, you may also use Agner Fog&#39;s &lt;a href=&quot;https://github.com/vectorclass&quot;&gt;VCL vector class library&lt;/a&gt;. It includes many functions including a complete math library.&lt;/p&gt; 
&lt;p&gt;If you have existing code using x86/NEON intrinsics, you may be interested in &lt;a href=&quot;https://github.com/simd-everywhere/simde&quot;&gt;SIMDe&lt;/a&gt;, which emulates those intrinsics using other platforms&#39; intrinsics or autovectorization.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;This project uses CMake to generate and build. In a Debian-based system you can install it via:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sudo apt install cmake
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Highway&#39;s unit tests use &lt;a href=&quot;https://github.com/google/googletest&quot;&gt;googletest&lt;/a&gt;. By default, Highway&#39;s CMake downloads this dependency at configuration time. You can avoid this by setting the &lt;code&gt;HWY_SYSTEM_GTEST&lt;/code&gt; CMake variable to ON and installing gtest separately:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sudo apt install libgtest-dev
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, you can define &lt;code&gt;HWY_TEST_STANDALONE=1&lt;/code&gt; and remove all occurrences of &lt;code&gt;gtest_main&lt;/code&gt; in each BUILD file, then tests avoid the dependency on GUnit.&lt;/p&gt; 
&lt;p&gt;Running cross-compiled tests requires support from the OS, which on Debian is provided by the &lt;code&gt;qemu-user-binfmt&lt;/code&gt; package.&lt;/p&gt; 
&lt;p&gt;To build Highway as a shared or static library (depending on BUILD_SHARED_LIBS), the standard CMake workflow can be used:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;mkdir -p build &amp;amp;&amp;amp; cd build
cmake ..
make -j &amp;amp;&amp;amp; make test
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or you can run &lt;code&gt;run_tests.sh&lt;/code&gt; (&lt;code&gt;run_tests.bat&lt;/code&gt; on Windows).&lt;/p&gt; 
&lt;p&gt;Bazel is also supported for building, but it is not as widely used/tested.&lt;/p&gt; 
&lt;p&gt;When building for Armv7, a limitation of current compilers requires you to add &lt;code&gt;-DHWY_CMAKE_ARM7:BOOL=ON&lt;/code&gt; to the CMake command line; see #834 and #1032. We understand that work is underway to remove this limitation.&lt;/p&gt; 
&lt;p&gt;Building on 32-bit x86 is not officially supported, and AVX2/3 are disabled by default there. Note that johnplatts has successfully built and run the Highway tests on 32-bit x86, including AVX2/3, on GCC 7/8 and Clang 8/11/12. On Ubuntu 22.04, Clang 11 and 12, but not later versions, require extra compiler flags &lt;code&gt;-m32 -isystem /usr/i686-linux-gnu/include&lt;/code&gt;. Clang 10 and earlier require the above plus &lt;code&gt;-isystem /usr/i686-linux-gnu/include/c++/12/i686-linux-gnu&lt;/code&gt;. See #1279.&lt;/p&gt; 
&lt;h2&gt;Building highway - Using vcpkg&lt;/h2&gt; 
&lt;p&gt;highway is now available in &lt;a href=&quot;https://github.com/Microsoft/vcpkg&quot;&gt;vcpkg&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;vcpkg install highway
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The highway port in vcpkg is kept up to date by Microsoft team members and community contributors. If the version is out of date, please &lt;a href=&quot;https://github.com/Microsoft/vcpkg&quot;&gt;create an issue or pull request&lt;/a&gt; on the vcpkg repository.&lt;/p&gt; 
&lt;h2&gt;Quick start&lt;/h2&gt; 
&lt;p&gt;You can use the &lt;code&gt;benchmark&lt;/code&gt; inside examples/ as a starting point.&lt;/p&gt; 
&lt;p&gt;A &lt;a href=&quot;https://raw.githubusercontent.com/google/highway/master/g3doc/quick_reference.md&quot;&gt;quick-reference page&lt;/a&gt; briefly lists all operations and their parameters, and the &lt;a href=&quot;https://raw.githubusercontent.com/google/highway/master/g3doc/instruction_matrix.pdf&quot;&gt;instruction_matrix&lt;/a&gt; indicates the number of instructions per operation.&lt;/p&gt; 
&lt;p&gt;The &lt;a href=&quot;https://raw.githubusercontent.com/google/highway/master/g3doc/faq.md&quot;&gt;FAQ&lt;/a&gt; answers questions about portability, API design and where to find more information.&lt;/p&gt; 
&lt;p&gt;We recommend using full SIMD vectors whenever possible for maximum performance portability. To obtain them, pass a &lt;code&gt;ScalableTag&amp;lt;float&amp;gt;&lt;/code&gt; (or equivalently &lt;code&gt;HWY_FULL(float)&lt;/code&gt;) tag to functions such as &lt;code&gt;Zero/Set/Load&lt;/code&gt;. There are two alternatives for use-cases requiring an upper bound on the lanes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;For up to &lt;code&gt;N&lt;/code&gt; lanes, specify &lt;code&gt;CappedTag&amp;lt;T, N&amp;gt;&lt;/code&gt; or the equivalent &lt;code&gt;HWY_CAPPED(T, N)&lt;/code&gt;. The actual number of lanes will be &lt;code&gt;N&lt;/code&gt; rounded down to the nearest power of two, such as 4 if &lt;code&gt;N&lt;/code&gt; is 5, or 8 if &lt;code&gt;N&lt;/code&gt; is 8. This is useful for data structures such as a narrow matrix. A loop is still required because vectors may actually have fewer than &lt;code&gt;N&lt;/code&gt; lanes.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;For exactly a power of two &lt;code&gt;N&lt;/code&gt; lanes, specify &lt;code&gt;FixedTag&amp;lt;T, N&amp;gt;&lt;/code&gt;. The largest supported &lt;code&gt;N&lt;/code&gt; depends on the target, but is guaranteed to be at least &lt;code&gt;16/sizeof(T)&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Due to ADL restrictions, user code calling Highway ops must either:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Reside inside &lt;code&gt;namespace hwy { namespace HWY_NAMESPACE {&lt;/code&gt;; or&lt;/li&gt; 
 &lt;li&gt;prefix each op with an alias such as &lt;code&gt;namespace hn = hwy::HWY_NAMESPACE; hn::Add()&lt;/code&gt;; or&lt;/li&gt; 
 &lt;li&gt;add using-declarations for each op used: &lt;code&gt;using hwy::HWY_NAMESPACE::Add;&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Additionally, each function that calls Highway ops (such as &lt;code&gt;Load&lt;/code&gt;) must either be prefixed with &lt;code&gt;HWY_ATTR&lt;/code&gt;, OR reside between &lt;code&gt;HWY_BEFORE_NAMESPACE()&lt;/code&gt; and &lt;code&gt;HWY_AFTER_NAMESPACE()&lt;/code&gt;. Lambda functions currently require &lt;code&gt;HWY_ATTR&lt;/code&gt; before their opening brace.&lt;/p&gt; 
&lt;p&gt;Do not use namespace-scope nor &lt;code&gt;static&lt;/code&gt; initializers for SIMD vectors because this can cause SIGILL when using runtime dispatch and the compiler chooses an initializer compiled for a target not supported by the current CPU. Instead, constants initialized via &lt;code&gt;Set&lt;/code&gt; should generally be local (const) variables.&lt;/p&gt; 
&lt;p&gt;The entry points into code using Highway differ slightly depending on whether they use static or dynamic dispatch. In both cases, we recommend that the top-level function receives one or more pointers to arrays, rather than target-specific vector types.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;For static dispatch, &lt;code&gt;HWY_TARGET&lt;/code&gt; will be the best available target among &lt;code&gt;HWY_BASELINE_TARGETS&lt;/code&gt;, i.e. those allowed for use by the compiler (see &lt;a href=&quot;https://raw.githubusercontent.com/google/highway/master/g3doc/quick_reference.md&quot;&gt;quick-reference&lt;/a&gt;). Functions inside &lt;code&gt;HWY_NAMESPACE&lt;/code&gt; can be called using &lt;code&gt;HWY_STATIC_DISPATCH(func)(args)&lt;/code&gt; within the same module they are defined in. You can call the function from other modules by wrapping it in a regular function and declaring the regular function in a header.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;For dynamic dispatch, a table of function pointers is generated via the &lt;code&gt;HWY_EXPORT&lt;/code&gt; macro that is used by &lt;code&gt;HWY_DYNAMIC_DISPATCH(func)(args)&lt;/code&gt; to call the best function pointer for the current CPU&#39;s supported targets. A module is automatically compiled for each target in &lt;code&gt;HWY_TARGETS&lt;/code&gt; (see &lt;a href=&quot;https://raw.githubusercontent.com/google/highway/master/g3doc/quick_reference.md&quot;&gt;quick-reference&lt;/a&gt;) if &lt;code&gt;HWY_TARGET_INCLUDE&lt;/code&gt; is defined and &lt;code&gt;foreach_target.h&lt;/code&gt; is included. Note that the first invocation of &lt;code&gt;HWY_DYNAMIC_DISPATCH&lt;/code&gt;, or each call to the pointer returned by the first invocation of &lt;code&gt;HWY_DYNAMIC_POINTER&lt;/code&gt;, involves some CPU detection overhead. You can prevent this by calling the following before any invocation of &lt;code&gt;HWY_DYNAMIC_*&lt;/code&gt;: &lt;code&gt;hwy::GetChosenTarget().Update(hwy::SupportedTargets());&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See also a separate &lt;a href=&quot;https://github.com/kfjahnke/zimt/raw/multi_isa/examples/multi_isa_example/multi_simd_isa.md&quot;&gt;introduction to dynamic dispatch&lt;/a&gt; by @kfjahnke.&lt;/p&gt; 
&lt;p&gt;When using dynamic dispatch, &lt;code&gt;foreach_target.h&lt;/code&gt; is included from translation units (.cc files), not headers. Headers containing vector code shared between several translation units require a special include guard, for example the following taken from &lt;code&gt;examples/skeleton-inl.h&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;#if defined(HIGHWAY_HWY_EXAMPLES_SKELETON_INL_H_) == defined(HWY_TARGET_TOGGLE)
#ifdef HIGHWAY_HWY_EXAMPLES_SKELETON_INL_H_
#undef HIGHWAY_HWY_EXAMPLES_SKELETON_INL_H_
#else
#define HIGHWAY_HWY_EXAMPLES_SKELETON_INL_H_
#endif

#include &quot;hwy/highway.h&quot;
// Your vector code
#endif
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;By convention, we name such headers &lt;code&gt;-inl.h&lt;/code&gt; because their contents (often function templates) are usually inlined.&lt;/p&gt; 
&lt;h2&gt;Compiler flags&lt;/h2&gt; 
&lt;p&gt;Applications should be compiled with optimizations enabled. Without inlining SIMD code may slow down by factors of 10 to 100. For clang and GCC, &lt;code&gt;-O2&lt;/code&gt; is generally sufficient.&lt;/p&gt; 
&lt;p&gt;For MSVC, we recommend compiling with &lt;code&gt;/Gv&lt;/code&gt; to allow non-inlined functions to pass vector arguments in registers. If intending to use the AVX2 target together with half-width vectors (e.g. for &lt;code&gt;PromoteTo&lt;/code&gt;), it is also important to compile with &lt;code&gt;/arch:AVX2&lt;/code&gt;. This seems to be the only way to reliably generate VEX-encoded SSE instructions on MSVC. Sometimes MSVC generates VEX-encoded SSE instructions, if they are mixed with AVX, but not always, see &lt;a href=&quot;https://developercommunity.visualstudio.com/t/10618264&quot;&gt;DevCom-10618264&lt;/a&gt;. Otherwise, mixing VEX-encoded AVX2 instructions and non-VEX SSE may cause severe performance degradation. Unfortunately, with &lt;code&gt;/arch:AVX2&lt;/code&gt; option, the resulting binary will then require AVX2. Note that no such flag is needed for clang and GCC because they support target-specific attributes, which we use to ensure proper VEX code generation for AVX2 targets.&lt;/p&gt; 
&lt;h2&gt;Strip-mining loops&lt;/h2&gt; 
&lt;p&gt;When vectorizing a loop, an important question is whether and how to deal with a number of iterations (&#39;trip count&#39;, denoted &lt;code&gt;count&lt;/code&gt;) that does not evenly divide the vector size &lt;code&gt;N = Lanes(d)&lt;/code&gt;. For example, it may be necessary to avoid writing past the end of an array.&lt;/p&gt; 
&lt;p&gt;In this section, let &lt;code&gt;T&lt;/code&gt; denote the element type and &lt;code&gt;d = ScalableTag&amp;lt;T&amp;gt;&lt;/code&gt;. Assume the loop body is given as a function &lt;code&gt;template&amp;lt;bool partial, class D&amp;gt; void LoopBody(D d, size_t index, size_t max_n)&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;&quot;Strip-mining&quot; is a technique for vectorizing a loop by transforming it into an outer loop and inner loop, such that the number of iterations in the inner loop matches the vector width. Then, the inner loop is replaced with vector operations.&lt;/p&gt; 
&lt;p&gt;Highway offers several strategies for loop vectorization:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Ensure all inputs/outputs are padded. Then the (outer) loop is simply&lt;/p&gt; &lt;pre&gt;&lt;code&gt;for (size_t i = 0; i &amp;lt; count; i += N) LoopBody&amp;lt;false&amp;gt;(d, i, 0);
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here, the template parameter and second function argument are not needed.&lt;/p&gt; &lt;p&gt;This is the preferred option, unless &lt;code&gt;N&lt;/code&gt; is in the thousands and vector operations are pipelined with long latencies. This was the case for supercomputers in the 90s, but nowadays ALUs are cheap and we see most implementations split vectors into 1, 2 or 4 parts, so there is little cost to processing entire vectors even if we do not need all their lanes. Indeed this avoids the (potentially large) cost of predication or partial loads/stores on older targets, and does not duplicate code.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Process whole vectors and include previously processed elements in the last vector:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;for (size_t i = 0; i &amp;lt; count; i += N) LoopBody&amp;lt;false&amp;gt;(d, HWY_MIN(i, count - N), 0);
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This is the second preferred option provided that &lt;code&gt;count &amp;gt;= N&lt;/code&gt; and &lt;code&gt;LoopBody&lt;/code&gt; is idempotent. Some elements might be processed twice, but a single code path and full vectorization is usually worth it. Even if &lt;code&gt;count &amp;lt; N&lt;/code&gt;, it usually makes sense to pad inputs/outputs up to &lt;code&gt;N&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Use the &lt;code&gt;Transform*&lt;/code&gt; functions in hwy/contrib/algo/transform-inl.h. This takes care of the loop and remainder handling and you simply define a generic lambda function (C++14) or functor which receives the current vector from the input/output array, plus optionally vectors from up to two extra input arrays, and returns the value to write to the input/output array.&lt;/p&gt; &lt;p&gt;Here is an example implementing the BLAS function SAXPY (&lt;code&gt;alpha * x + y&lt;/code&gt;):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Transform1(d, x, n, y, [](auto d, const auto v, const auto v1) HWY_ATTR {
  return MulAdd(Set(d, alpha), v, v1);
});
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Process whole vectors as above, followed by a scalar loop:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;size_t i = 0;
for (; i + N &amp;lt;= count; i += N) LoopBody&amp;lt;false&amp;gt;(d, i, 0);
for (; i &amp;lt; count; ++i) LoopBody&amp;lt;false&amp;gt;(CappedTag&amp;lt;T, 1&amp;gt;(), i, 0);
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The template parameter and second function arguments are again not needed.&lt;/p&gt; &lt;p&gt;This avoids duplicating code, and is reasonable if &lt;code&gt;count&lt;/code&gt; is large. If &lt;code&gt;count&lt;/code&gt; is small, the second loop may be slower than the next option.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Process whole vectors as above, followed by a single call to a modified &lt;code&gt;LoopBody&lt;/code&gt; with masking:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;size_t i = 0;
for (; i + N &amp;lt;= count; i += N) {
  LoopBody&amp;lt;false&amp;gt;(d, i, 0);
}
if (i &amp;lt; count) {
  LoopBody&amp;lt;true&amp;gt;(d, i, count - i);
}
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now the template parameter and third function argument can be used inside &lt;code&gt;LoopBody&lt;/code&gt; to non-atomically &#39;blend&#39; the first &lt;code&gt;num_remaining&lt;/code&gt; lanes of &lt;code&gt;v&lt;/code&gt; with the previous contents of memory at subsequent locations: &lt;code&gt;BlendedStore(v, FirstN(d, num_remaining), d, pointer);&lt;/code&gt;. Similarly, &lt;code&gt;MaskedLoad(FirstN(d, num_remaining), d, pointer)&lt;/code&gt; loads the first &lt;code&gt;num_remaining&lt;/code&gt; elements and returns zero in other lanes.&lt;/p&gt; &lt;p&gt;This is a good default when it is infeasible to ensure vectors are padded, but is only safe &lt;code&gt;#if !HWY_MEM_OPS_MIGHT_FAULT&lt;/code&gt;! In contrast to the scalar loop, only a single final iteration is needed. The increased code size from two loop bodies is expected to be worthwhile because it avoids the cost of masking in all but the final iteration.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Additional resources&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google/highway/master/g3doc/highway_intro.pdf&quot;&gt;Highway introduction (slides)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google/highway/master/g3doc/instruction_matrix.pdf&quot;&gt;Overview of instructions per operation on different architectures&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google/highway/master/g3doc/design_philosophy.md&quot;&gt;Design philosophy and comparison&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google/highway/master/g3doc/impl_details.md&quot;&gt;Implementation details&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Acknowledgments&lt;/h2&gt; 
&lt;p&gt;We have used &lt;a href=&quot;https://gitlab.inria.fr/bramas/farm-sve&quot;&gt;farm-sve&lt;/a&gt; by Berenger Bramas; it has proved useful for checking the SVE port on an x86 development machine.&lt;/p&gt; 
&lt;p&gt;This is not an officially supported Google product. Contact: &lt;a href=&quot;mailto:janwas@google.com&quot;&gt;janwas@google.com&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Haivision/srt</title>
      <link>https://github.com/Haivision/srt</link>
      <description>&lt;p&gt;Secure, Reliable, Transport&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Secure Reliable Transport (SRT) Protocol&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Haivision/srt/master/#what-is-srt&quot;&gt;About SRT&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/Haivision/srt/master/#features&quot;&gt;Features&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/Haivision/srt/master/#getting-started-with-srt&quot;&gt;Getting Started&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/Haivision/srt/master/#build-instructions&quot;&gt;Build Instructions&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/Haivision/srt/master/#sample-applications-and-tools&quot;&gt;Sample Apps and Tools&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/Haivision/srt/master/#contributing&quot;&gt;Contribute&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/Haivision/srt/master/#license&quot;&gt;License&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/Haivision/srt/master/#release-history&quot;&gt;Releases&lt;/a&gt;&lt;/p&gt; 
&lt;p align=&quot;left&quot;&gt; &lt;a href=&quot;http://srtalliance.org/&quot;&gt; &lt;img alt=&quot;SRT&quot; src=&quot;http://www.srtalliance.org/wp-content/uploads/SRT_text_hor_logo_grey.png&quot; width=&quot;500&quot;&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Haivision/srt/master/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/License-MPLv2.0-blue&quot; alt=&quot;License: MPLv2.0&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/Haivision/srt/releases&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/release/Haivision/srt.svg?sanitize=true&quot; alt=&quot;Latest release&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://sonarcloud.io/project/overview?id=srt&quot;&gt;&lt;img src=&quot;https://sonarcloud.io/api/project_badges/measure?project=srt&amp;amp;metric=alert_status&quot; alt=&quot;Quality Gate Status&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://codecov.io/gh/haivision/srt&quot;&gt;&lt;img src=&quot;https://codecov.io/gh/haivision/srt/branch/master/graph/badge.svg?sanitize=true&quot; alt=&quot;codecov&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://travis-ci.org/Haivision/srt&quot;&gt;&lt;img src=&quot;https://img.shields.io/travis/Haivision/srt/master.svg?label=Linux/macOS&quot; alt=&quot;Build Status Linux and macOS&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://ci.appveyor.com/project/Haivision/srt&quot;&gt;&lt;img src=&quot;https://img.shields.io/appveyor/ci/Haivision/srt/master.svg?label=Windows&quot; alt=&quot;Build Status Windows&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://repology.org/project/srt/versions&quot;&gt;&lt;img src=&quot;https://repology.org/badge/version-for-repo/ubuntu_23_04/srt.svg?sanitize=true&quot; alt=&quot;Ubuntu 23.04&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://repology.org/project/srt/versions&quot;&gt;&lt;img src=&quot;https://repology.org/badge/version-for-repo/fedora_37/srt.svg?sanitize=true&quot; alt=&quot;Fedora 37&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://packages.debian.org/testing/libs/libsrt1.5-gnutls&quot;&gt;&lt;img src=&quot;https://badges.debian.net/badges/debian/testing/libsrt1.5-gnutls/version.svg?sanitize=true&quot; alt=&quot;Debian&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://repology.org/project/srt/versions&quot;&gt;&lt;img src=&quot;https://repology.org/badge/version-for-repo/homebrew/srt.svg?sanitize=true&quot; alt=&quot;Homebrew&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://repology.org/project/srt/versions&quot;&gt;&lt;img src=&quot;https://repology.org/badge/version-for-repo/vcpkg/srt.svg?sanitize=true&quot; alt=&quot;Vcpkg&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://conan.io/center/recipes/srt&quot;&gt;&lt;img src=&quot;https://img.shields.io/conan/v/srt&quot; alt=&quot;ConanCenter&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;What is SRT?&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Secure Reliable Transport (SRT)&lt;/strong&gt; is a transport protocol for ultra low (sub-second) latency live video and audio streaming, as well as for generic bulk data transfer[^1]. SRT is available as an open-source technology with the code on GitHub, a published &lt;a href=&quot;https://datatracker.ietf.org/doc/html/draft-sharabayko-srt-01&quot;&gt;Internet Draft&lt;/a&gt;, and a growing &lt;a href=&quot;https://www.srtalliance.org/&quot;&gt;community of SRT users&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;SRT is applied to contribution and distribution endpoints as part of a video stream workflow to deliver the best quality and lowest latency video at all times.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;S&lt;/strong&gt;ecure&lt;/td&gt; 
   &lt;td&gt;Encrypts video streams&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;R&lt;/strong&gt;eliable&lt;/td&gt; 
   &lt;td&gt;Recovers from severe packet loss&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;T&lt;/strong&gt;ransport&lt;/td&gt; 
   &lt;td&gt;Dynamically adapts to changing network conditions&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;In live streaming configurations, the SRT protocol maintains a constant end-to-end latency. This allows the live stream&#39;s signal characteristics to be recreated on the receiver side, reducing the need for buffering. As packets are streamed from source to destination, SRT detects and adapts to real-time network conditions between the two endpoints. It helps compensate for jitter and bandwidth fluctuations due to congestion over noisy networks.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://datatracker.ietf.org/doc/html/draft-sharabayko-srt-01#section-6&quot;&gt;SRT implements AES encryption&lt;/a&gt; to protect the payload of the media streams, and offers various error recovery mechanisms for minimizing the packet loss that is typical of Internet connections, of which Automatic Repeat reQuest (ARQ) is the primary method. With ARQ, when a receiver detects that a packet is missing it sends an alert to the sender requesting retransmission of this missing packet. &lt;a href=&quot;https://raw.githubusercontent.com/Haivision/srt/master/docs/features/packet-filtering-and-fec.md&quot;&gt;Forward Error Correction (FEC)&lt;/a&gt; and &lt;a href=&quot;https://raw.githubusercontent.com/Haivision/srt/master/docs/features/bonding-quick-start.md&quot;&gt;Connection Bonding&lt;/a&gt;, which adds seamless stream protection and hitless failover, are also supported by the protocol.&lt;/p&gt; 
&lt;p align=&quot;right&quot;&gt;&lt;em&gt;To learn more about the protocol subscribe to the &lt;a href=&quot;https://medium.com/innovation-labs-blog/tagged/secure-reliable-transport&quot;&gt;Innovation Labs Blog&lt;/a&gt; on &amp;nbsp;&lt;img alt=&quot;slack logo&quot; src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/0/0d/Medium_%28website%29_logo.svg/500px-Medium_%28website%29_logo.svg.png&quot; width=&quot;80&quot;&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;p align=&quot;right&quot;&gt;&lt;em&gt;To ask a question &lt;a href=&quot;https://slackin-srtalliance.azurewebsites.net&quot;&gt;join the conversation&lt;/a&gt; in the &lt;b&gt;#development&lt;/b&gt; channel on &amp;nbsp;&lt;a href=&quot;https://srtalliance.slack.com&quot;&gt;&lt;img alt=&quot;slack logo&quot; src=&quot;https://github.com/stevomatthews/srt/raw/master/docs/images/Slack_RGB2.svg?sanitize=true&quot; width=&quot;60&quot;&gt;&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;span&gt;👇&lt;/span&gt; Click on the ► button to expand a feature description.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;details&gt; 
 &lt;summary&gt;Pristine Quality and Reliability&lt;/summary&gt; 
 &lt;p&gt; &lt;/p&gt;
 &lt;p&gt;No matter how unreliable your network, SRT can recover from severe packet loss and jitter, ensuring the integrity and quality of your video streams.&lt;/p&gt; 
 &lt;p&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Low Latency&lt;/summary&gt; 
 &lt;p&gt; &lt;/p&gt;
 &lt;p&gt;SRT’s stream error correction is configurable to accommodate a user’s deployment conditions. Leveraging real-time IP communications development to extend traditional network error recovery practices, SRT delivers media with significantly lower latency than TCP/IP, while offering the speed of UDP transmission with greatly improved reliability.&lt;/p&gt; 
 &lt;p&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Content Agnostic&lt;/summary&gt; 
 &lt;p&gt; &lt;/p&gt;
 &lt;p&gt;Unlike some other streaming protocols that only support specific video and audio formats, SRT is payload agnostic. Because SRT operates at the network transport level, acting as a wrapper around your content, it can transport any type of video format, codec, resolution, or frame rate.&lt;/p&gt; 
 &lt;p&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Easy Firewall Traversal with Rendezvous Mode&lt;/summary&gt; 
 &lt;p&gt; &lt;/p&gt;
 &lt;p&gt;The handshaking process used by SRT supports outbound connections without the potential risks and dangers of permanent exterior ports being opened in a firewall, thereby maintaining corporate LAN security policies and minimizing the need for IT intervention.&lt;/p&gt; 
 &lt;p&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;a href=&quot;https://datatracker.ietf.org/doc/html/draft-sharabayko-srt-01#section-6&quot;&gt;AES Encryption&lt;/a&gt;&lt;/summary&gt; 
 &lt;p&gt; &lt;/p&gt;
 &lt;p&gt;Using 128/192/256-bit AES encryption trusted by governments and organizations around the world, SRT ensures that valuable content is protected end-to-end from contribution to distribution so that no unauthorized parties can listen.&lt;/p&gt; 
 &lt;p&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Haivision/srt/master/docs/features/packet-filtering-and-fec.md&quot;&gt;Forward Error Correction (FEC) and Packet Filter API&lt;/a&gt;&lt;/summary&gt; 
 &lt;p&gt; &lt;/p&gt;
 &lt;p&gt;&lt;a href=&quot;https://github.com/Haivision/srt/releases/tag/v1.4.0&quot;&gt;SRT 1.4&lt;/a&gt; sees the introduction of the &lt;em&gt;packet filter API&lt;/em&gt;. This mechanism allows custom processing to be performed on network packets on the sender side before they are sent, and on the receiver side once received from the network. The API allows users to write their own plugin, thereby extending the SRT protocol&#39;s capabilities even further with all kinds of different packet filtering. Users can manipulate the resulting packet filter data in any way, such as for custom encryption, packet inspection, or accessing data before it is sent.&lt;/p&gt; 
 &lt;p&gt;The first plugin created as an example of what can be achieved with the packet filter API is for Forward Error Correction (FEC) which, in certain use cases, can offer slightly lower latency than Automatic Repeat reQuest (ARQ). This plugin allows three different modes:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;ARQ only – retransmits lost packets,&lt;/li&gt; 
  &lt;li&gt;FEC only – provides the overhead needed for FEC recovery on the receiver side,&lt;/li&gt; 
  &lt;li&gt;FEC and ARQ – retransmits lost packets that FEC fails to recover.&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Haivision/srt/master/docs/features/bonding-quick-start.md&quot;&gt;Connection Bonding&lt;/a&gt;&lt;/summary&gt; 
 &lt;p&gt; &lt;/p&gt;
 &lt;p&gt;Similar to SMPTE-2022-7 over managed networks, Connection Bonding adds seamless stream protection and hitless failover to the SRT protocol. This technology relies on more than one IP network path to prevent disruption to live video streams in the event of network congestion or outages, maintaining continuity of service.&lt;/p&gt; 
 &lt;p&gt;This is accomplished using the &lt;a href=&quot;https://raw.githubusercontent.com/Haivision/srt/master/docs/features/socket-groups.md&quot;&gt;socket groups&lt;/a&gt; introduced in &lt;a href=&quot;https://github.com/Haivision/srt/releases/tag/v1.5.0&quot;&gt;SRT v1.5&lt;/a&gt;. The general concept of socket groups means having a group that contains multiple sockets, where one operation for sending one data signal is applied to the group. Single sockets inside the group will take over this operation and do what is necessary to deliver the signal to the receiver.&lt;/p&gt; 
 &lt;p&gt;Two modes are supported:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Haivision/srt/master/docs/features/socket-groups.md#1-broadcast&quot;&gt;Broadcast&lt;/a&gt; - In &lt;em&gt;Broadcast&lt;/em&gt; mode, data is sent redundantly over all the member links in a group. If one of the links fails or experiences network jitter and/or packet loss, the missing data will be received over another link in the group. Redundant packets are simply discarded at the receiver side.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Haivision/srt/master/docs/features/bonding-main-backup.md&quot;&gt;Main/Backup&lt;/a&gt; - In &lt;em&gt;Main/Backup&lt;/em&gt; mode, only one (main) link at a time is used for data transmission while other (backup) connections are on standby to ensure the transmission will continue if the main link fails. The goal of Main/Backup mode is to identify a potential link break before it happens, thus providing a time window within which to seamlessly switch to one of the backup links.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Haivision/srt/master/docs/features/access-control.md&quot;&gt;Access Control (Stream ID)&lt;/a&gt;&lt;/summary&gt; 
 &lt;p&gt; &lt;/p&gt;
 &lt;p&gt;Access Control enables the upstream application to assign a Stream ID to individual SRT streams. By using a unique Stream ID, either automatically generated or customized, the upstream application can send multiple SRT streams to a single IP address and UDP port. The Stream IDs can then be used by a receiver to identify and differentiate between ingest streams, apply user password access methods, and in some cases even apply automation based on the naming of the Stream ID. For example, contribution could be sent to a video production workflow and monitoring to a monitoring service.&lt;/p&gt; 
 &lt;p&gt;For broadcasters, Stream ID is key to replacing&amp;nbsp;RTMP&amp;nbsp;for ingesting video streams, especially&amp;nbsp;HEVC/H.265 content, into cloud service or CDNs that have a single IP socket (address + port) open for incoming video.&lt;/p&gt; 
 &lt;p&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;Getting Started with SRT&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Haivision/srt/master/docs#srt-api-documents&quot;&gt;The SRT API&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://datatracker.ietf.org/doc/html/draft-sharabayko-srt-01&quot;&gt;IETF Internet Draft&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Haivision/srt/master/docs#sample-applications&quot;&gt;Sample Apps&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;Reference documentation for the SRT library API&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;The SRT Protocol Internet Draft&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Instructions for using test apps (&lt;code&gt;srt-live-transmit&lt;/code&gt;, &lt;code&gt;srt-file-transmit&lt;/code&gt;, etc.)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/Haivision/srt/files/2489142/SRT_Protocol_TechnicalOverview_DRAFT_2018-10-17.pdf&quot;&gt;SRT Technical Overview&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://www.srtalliance.org/srt-deployment-guide/&quot;&gt;SRT Deployment Guide&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://srtlab.github.io/srt-cookbook&quot;&gt;SRT CookBook&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;Early draft technical overview (precursor to the Internet Draft)&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;A comprehensive overview of the protocol with deployment guidelines&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Development notes on the SRT protocol&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://medium.com/innovation-labs-blog/tagged/secure-reliable-transport&quot;&gt;Innovation Labs Blog&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://www.youtube.com/channel/UCr35JJ32jKKWIYymR1PTdpA&quot;&gt;SRTLab YouTube Channel&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://srtalliance.slack.com&quot;&gt;Slack&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;The blog on Medium with SRT-related technical articles&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Technical YouTube channel with useful videos&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Slack channels to get the latest updates and ask questions &lt;br&gt;&lt;a href=&quot;https://slackin-srtalliance.azurewebsites.net/&quot;&gt;Join SRT Alliance on Slack&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Additional Documentation&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Haivision/srt/master/docs/misc/why-srt-was-created.md&quot;&gt;Why SRT?&lt;/a&gt; - A brief history and rationale for SRT by Marc Cymontkowski.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.haivision.com/resources/white-paper/srt-versus-rtmp/&quot;&gt;RTMP vs. SRT: Comparing Latency and Maximum Bandwidth&lt;/a&gt; White Paper.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Haivision/srt/master/docs#documentation-overview&quot;&gt;Documentation on GitHub&lt;/a&gt; with SRT API documents, features decsriptions, etc.&lt;/li&gt; 
 &lt;li&gt;The SRT Protocol Internet Draft: &lt;a href=&quot;https://datatracker.ietf.org/doc/draft-sharabayko-srt/&quot;&gt;Datatracker&lt;/a&gt; | &lt;a href=&quot;https://datatracker.ietf.org/doc/html/draft-sharabayko-srt-01&quot;&gt;Latest Version&lt;/a&gt; | &lt;a href=&quot;https://haivision.github.io/srt-rfc/draft-sharabayko-srt.html&quot;&gt;Latest Working Copy&lt;/a&gt; | &lt;a href=&quot;https://github.com/Haivision/srt-rfc&quot;&gt;GitHub Repo&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Build Instructions&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Haivision/srt/master/docs/build/build-linux.md&quot;&gt;Linux (Ubuntu/CentOS)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/Haivision/srt/master/docs/build/build-win.md&quot;&gt;Windows&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/Haivision/srt/master/docs/build/build-macOS.md&quot;&gt;macOS&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/Haivision/srt/master/docs/build/build-iOS.md&quot;&gt;iOS&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/Haivision/srt/master/docs/build/build-android.md&quot;&gt;Android&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/Haivision/srt/master/docs/build/package-managers.md&quot;&gt;Package Managers&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Requirements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;C++03 or above compliant compiler.&lt;/li&gt; 
 &lt;li&gt;CMake 2.8.12 or above as a build system.&lt;/li&gt; 
 &lt;li&gt;OpenSSL 1.1 to enable encryption, otherwise build with &lt;a href=&quot;https://raw.githubusercontent.com/Haivision/srt/master/docs/build/build-options.md#enable_encryption&quot;&gt;&lt;code&gt;-DENABLE_ENCRYPTION=OFF&lt;/code&gt;&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Multithreading is provided by either of the following: 
  &lt;ul&gt; 
   &lt;li&gt;C++11: standard library (&lt;code&gt;std&lt;/code&gt; by &lt;a href=&quot;https://raw.githubusercontent.com/Haivision/srt/master/docs/build/build-options.md#enable_stdcxx_sync&quot;&gt;&lt;code&gt;-DENABLE_STDCXX_SYNC=ON&lt;/code&gt;&lt;/a&gt; CMake option),&lt;/li&gt; 
   &lt;li&gt;C++03: Pthreads (for POSIX systems it&#39;s built in, for Windows there is a ported library).&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Tcl 8.5 is optional and is used by &lt;code&gt;./configure&lt;/code&gt; script. Otherwise, use CMake directly.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Build Options&lt;/h3&gt; 
&lt;p&gt;For detailed descriptions of the build system and options, please read the &lt;a href=&quot;https://raw.githubusercontent.com/Haivision/srt/master/docs/build/build-options.md&quot;&gt;SRT Build Options&lt;/a&gt; document.&lt;/p&gt; 
&lt;h2&gt;Sample Applications and Tools&lt;/h2&gt; 
&lt;p&gt;The current repo provides &lt;a href=&quot;https://raw.githubusercontent.com/Haivision/srt/master/apps&quot;&gt;sample applications&lt;/a&gt; and &lt;a href=&quot;https://raw.githubusercontent.com/Haivision/srt/master/examples&quot;&gt;code examples&lt;/a&gt; that demonstrate the usage of the SRT library API. Among them are &lt;a href=&quot;https://raw.githubusercontent.com/Haivision/srt/master/apps/srt-live-transmit.cpp&quot;&gt;&lt;code&gt;srt-live-transmit&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;https://raw.githubusercontent.com/Haivision/srt/master/apps/srt-file-transmit.cpp&quot;&gt;&lt;code&gt;srt-file-transmit&lt;/code&gt;&lt;/a&gt;, and other applications. The respective documentation can be found &lt;a href=&quot;https://raw.githubusercontent.com/Haivision/srt/master/docs#sample-applications&quot;&gt;here&lt;/a&gt;. Note that all samples are provided for instructional purposes, and should not be used in a production environment.&lt;/p&gt; 
&lt;p&gt;The &lt;a href=&quot;https://github.com/maxsharabayko/srt-xtransmit&quot;&gt;&lt;code&gt;srt-xtransmit&lt;/code&gt;&lt;/a&gt; utility is actively used for internal testing and performance evaluation. Among other features it supports dummy payload generation, traffic routings, and connection bonding. Additional details are available in the &lt;a href=&quot;https://github.com/maxsharabayko/srt-xtransmit&quot;&gt;&lt;code&gt;srt-xtransmit&lt;/code&gt;&lt;/a&gt; repo itself.&lt;/p&gt; 
&lt;p&gt;Python tools that might be useful during development are:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/mbakholdina/srt-stats-plotting&quot;&gt;&lt;code&gt;srt-stats-plotting&lt;/code&gt;&lt;/a&gt; - A script designed to plot graphs based on SRT &lt;code&gt;.csv&lt;/code&gt; statistics.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/mbakholdina/lib-tcpdump-processing&quot;&gt;&lt;code&gt;lib-tcpdump-processing&lt;/code&gt;&lt;/a&gt; - A library designed to process &lt;code&gt;.pcap(ng)&lt;/code&gt; &lt;a href=&quot;https://www.tcpdump.org/&quot;&gt;tcpdump&lt;/a&gt; or &lt;a href=&quot;https://www.wireshark.org/&quot;&gt;Wireshark&lt;/a&gt; trace files and extract SRT packets of interest for further analysis.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/mbakholdina/lib-srt-utils&quot;&gt;&lt;code&gt;lib-srt-utils&lt;/code&gt;&lt;/a&gt; - A Python library containing supporting code for running SRT tests based on an experiment configuration.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Anyone is welcome to contribute. If you decide to get involved, please take a moment to review the guidelines:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Haivision/srt/master/docs/dev/developers-guide.md&quot;&gt;SRT Developer&#39;s Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Haivision/srt/master/CONTRIBUTING.md&quot;&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Haivision/srt/master/docs/dev/making-srt-better.md&quot;&gt;Reporting Issues&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For information on contributing to the &lt;a href=&quot;https://datatracker.ietf.org/doc/html/draft-sharabayko-srt-01&quot;&gt;Internet Draft&lt;/a&gt; or to submit issues please go to the following &lt;a href=&quot;https://github.com/Haivision/srt-rfc&quot;&gt;repo&lt;/a&gt;. The repo for contributing in &lt;a href=&quot;https://srtlab.github.io/srt-cookbook/&quot;&gt;SRT CookBook&lt;/a&gt; can be found &lt;a href=&quot;https://github.com/SRTLab/srt-cookbook/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;By contributing code to the SRT project, you agree to license your contribution under the &lt;a href=&quot;https://raw.githubusercontent.com/Haivision/srt/master/LICENSE&quot;&gt;MPLv2.0 License&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Release History&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Haivision/srt/releases&quot;&gt;Release notes&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Haivision/srt/master/docs/dev/developers-guide.md#versioning&quot;&gt;SRT versioning&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;[^1]: The term “live streaming” refers to continuous data transmission (MPEG-TS or equivalent) with latency management. Live streaming based on segmentation and transmission of files like in the HTTP Live Streaming (HLS) protocol (as described in RFC8216) is not part of this use case. File transmission in either message or buffer mode should be considered in this case. See &lt;a href=&quot;https://datatracker.ietf.org/doc/html/draft-sharabayko-srt-01#section-7&quot;&gt;Section 7. Best Practices and Configuration Tips for Data Transmission via SRT&lt;/a&gt; of the Internet Draft for details. Note that SRT is content agnostic, meaning that any type of data can be transmitted via its payload.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>facebook/folly</title>
      <link>https://github.com/facebook/folly</link>
      <description>&lt;p&gt;An open-source C++ library developed and used at Facebook.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Folly: Facebook Open-source Library&lt;/h1&gt; 
&lt;a href=&quot;https://opensource.facebook.com/support-ukraine&quot;&gt; &lt;img src=&quot;https://img.shields.io/badge/Support-Ukraine-FFD500?style=flat&amp;amp;labelColor=005BBB&quot; alt=&quot;Support Ukraine - Help Provide Humanitarian Aid to Ukraine.&quot;&gt; &lt;/a&gt; 
&lt;h1&gt;What is &lt;code&gt;folly&lt;/code&gt;?&lt;/h1&gt; 
&lt;img src=&quot;https://raw.githubusercontent.com/facebook/folly/main/static/logo.svg?sanitize=true&quot; alt=&quot;Logo Folly&quot; width=&quot;15%&quot; align=&quot;right&quot;&gt; 
&lt;p&gt;Folly (acronymed loosely after Facebook Open Source Library) is a library of C++17 components designed with practicality and efficiency in mind. &lt;strong&gt;Folly contains a variety of core library components used extensively at Facebook&lt;/strong&gt;. In particular, it&#39;s often a dependency of Facebook&#39;s other open source C++ efforts and place where those projects can share code.&lt;/p&gt; 
&lt;p&gt;It complements (as opposed to competing against) offerings such as Boost and of course &lt;code&gt;std&lt;/code&gt;. In fact, we embark on defining our own component only when something we need is either not available, or does not meet the needed performance profile. We endeavor to remove things from folly if or when &lt;code&gt;std&lt;/code&gt; or Boost obsoletes them.&lt;/p&gt; 
&lt;p&gt;Performance concerns permeate much of Folly, sometimes leading to designs that are more idiosyncratic than they would otherwise be (see e.g. &lt;code&gt;PackedSyncPtr.h&lt;/code&gt;, &lt;code&gt;SmallLocks.h&lt;/code&gt;). Good performance at large scale is a unifying theme in all of Folly.&lt;/p&gt; 
&lt;h2&gt;Check it out in the intro video&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=Wr_IfOICYSs&quot;&gt;&lt;img src=&quot;https://img.youtube.com/vi/Wr_IfOICYSs/0.jpg&quot; alt=&quot;Explain Like I’m 5: Folly&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Logical Design&lt;/h1&gt; 
&lt;p&gt;Folly is a collection of relatively independent components, some as simple as a few symbols. There is no restriction on internal dependencies, meaning that a given folly module may use any other folly components.&lt;/p&gt; 
&lt;p&gt;All symbols are defined in the top-level namespace &lt;code&gt;folly&lt;/code&gt;, except of course macros. Macro names are ALL_UPPERCASE and should be prefixed with &lt;code&gt;FOLLY_&lt;/code&gt;. Namespace &lt;code&gt;folly&lt;/code&gt; defines other internal namespaces such as &lt;code&gt;internal&lt;/code&gt; or &lt;code&gt;detail&lt;/code&gt;. User code should not depend on symbols in those namespaces.&lt;/p&gt; 
&lt;h1&gt;Physical Design&lt;/h1&gt; 
&lt;p&gt;At the top level Folly uses the classic &quot;stuttering&quot; scheme &lt;code&gt;folly/folly&lt;/code&gt; used by Boost and others. The first directory serves as an installation root of the library (with possible versioning a la &lt;code&gt;folly-1.0/&lt;/code&gt;), and the second is to distinguish the library when including files, e.g. &lt;code&gt;#include &amp;lt;folly/FBString.h&amp;gt;&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;The directory structure is flat (mimicking the namespace structure), i.e. we don&#39;t have an elaborate directory hierarchy (it is possible this will change in future versions). The subdirectory &lt;code&gt;experimental&lt;/code&gt; contains files that are used inside folly and possibly at Facebook but not considered stable enough for client use. Your code should not use files in &lt;code&gt;folly/experimental&lt;/code&gt; lest it may break when you update Folly.&lt;/p&gt; 
&lt;p&gt;The &lt;code&gt;folly/folly/test&lt;/code&gt; subdirectory includes the unittests for all components, usually named &lt;code&gt;ComponentXyzTest.cpp&lt;/code&gt; for each &lt;code&gt;ComponentXyz.*&lt;/code&gt;. The &lt;code&gt;folly/folly/docs&lt;/code&gt; directory contains documentation.&lt;/p&gt; 
&lt;h1&gt;What&#39;s in it?&lt;/h1&gt; 
&lt;p&gt;Because of folly&#39;s fairly flat structure, the best way to see what&#39;s in it is to look at the headers in &lt;a href=&quot;https://github.com/facebook/folly/tree/main/folly&quot;&gt;top level &lt;code&gt;folly/&lt;/code&gt; directory&lt;/a&gt;. You can also check the &lt;a href=&quot;https://raw.githubusercontent.com/facebook/folly/main/folly/docs&quot;&gt;&lt;code&gt;docs&lt;/code&gt; folder&lt;/a&gt; for documentation, starting with the &lt;a href=&quot;https://raw.githubusercontent.com/facebook/folly/main/folly/docs/Overview.md&quot;&gt;overview&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Folly is published on GitHub at &lt;a href=&quot;https://github.com/facebook/folly&quot;&gt;https://github.com/facebook/folly&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Build Notes&lt;/h1&gt; 
&lt;p&gt;Because folly does not provide any ABI compatibility guarantees from commit to commit, we generally recommend building folly as a static library.&lt;/p&gt; 
&lt;p&gt;folly supports gcc (5.1+), clang, or MSVC. It should run on Linux (x86-32, x86-64, and ARM), iOS, macOS, and Windows (x86-64). The CMake build is only tested on some of these platforms; at a minimum, we aim to support macOS and Linux (on the latest Ubuntu LTS release or newer.)&lt;/p&gt; 
&lt;h2&gt;&lt;code&gt;getdeps.py&lt;/code&gt;&lt;/h2&gt; 
&lt;p&gt;This script is used by many of Meta&#39;s OSS tools. It will download and build all of the necessary dependencies first, and will then invoke cmake etc to build folly. This will help ensure that you build with relevant versions of all of the dependent libraries, taking into account what versions are installed locally on your system.&lt;/p&gt; 
&lt;p&gt;It&#39;s written in python so you&#39;ll need python3.6 or later on your PATH. It works on Linux, macOS and Windows.&lt;/p&gt; 
&lt;p&gt;The settings for folly&#39;s cmake build are held in its getdeps manifest &lt;code&gt;build/fbcode_builder/manifests/folly&lt;/code&gt;, which you can edit locally if desired.&lt;/p&gt; 
&lt;h3&gt;Dependencies&lt;/h3&gt; 
&lt;p&gt;If on Linux or MacOS (with homebrew installed) you can install system dependencies to save building them:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;# Clone the repo
git clone https://github.com/facebook/folly
# Install dependencies
cd folly
sudo ./build/fbcode_builder/getdeps.py install-system-deps --recursive
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you&#39;d like to see the packages before installing them:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;./build/fbcode_builder/getdeps.py install-system-deps --dry-run --recursive
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;On other platforms or if on Linux and without system dependencies &lt;code&gt;getdeps.py&lt;/code&gt; will mostly download and build them for you during the build step.&lt;/p&gt; 
&lt;p&gt;Some of the dependencies &lt;code&gt;getdeps.py&lt;/code&gt; uses and installs are:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;a version of boost compiled with C++14 support.&lt;/li&gt; 
 &lt;li&gt;googletest is required to build and run folly&#39;s tests.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Build&lt;/h3&gt; 
&lt;p&gt;This script will download and build all of the necessary dependencies first, and will then invoke cmake etc to build folly. This will help ensure that you build with relevant versions of all of the dependent libraries, taking into account what versions are installed locally on your system.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;getdeps.py&lt;/code&gt; currently requires python 3.6+ to be on your path.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;getdeps.py&lt;/code&gt; will invoke cmake etc.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;# Clone the repo
git clone https://github.com/facebook/folly
cd folly
# Build, using system dependencies if available
python3 ./build/fbcode_builder/getdeps.py --allow-system-packages build
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;It puts output in its scratch area:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;installed/folly/lib/libfolly.a&lt;/code&gt;: Library&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can also specify a &lt;code&gt;--scratch-path&lt;/code&gt; argument to control the location of the scratch directory used for the build. You can find the default scratch install location from logs or with &lt;code&gt;python3 ./build/fbcode_builder/getdeps.py show-inst-dir&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;There are also &lt;code&gt;--install-dir&lt;/code&gt; and &lt;code&gt;--install-prefix&lt;/code&gt; arguments to provide some more fine-grained control of the installation directories. However, given that folly provides no compatibility guarantees between commits we generally recommend building and installing the libraries to a temporary location, and then pointing your project&#39;s build at this temporary location, rather than installing folly in the traditional system installation directories. e.g., if you are building with CMake you can use the &lt;code&gt;CMAKE_PREFIX_PATH&lt;/code&gt; variable to allow CMake to find folly in this temporary installation directory when building your project.&lt;/p&gt; 
&lt;p&gt;If you want to invoke &lt;code&gt;cmake&lt;/code&gt; again to iterate, there is a helpful &lt;code&gt;run_cmake.py&lt;/code&gt; script output in the scratch build directory. You can find the scratch build directory from logs or with &lt;code&gt;python3 ./build/fbcode_builder/getdeps.py show-build-dir&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Run tests&lt;/h3&gt; 
&lt;p&gt;By default &lt;code&gt;getdeps.py&lt;/code&gt; will build the tests for folly. To run them:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;cd folly
python3 ./build/fbcode_builder/getdeps.py --allow-system-packages test
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;&lt;code&gt;build.sh&lt;/code&gt;/&lt;code&gt;build.bat&lt;/code&gt; wrapper&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;build.sh&lt;/code&gt; can be used on Linux and MacOS, on Windows use the &lt;code&gt;build.bat&lt;/code&gt; script instead. Its a wrapper around &lt;code&gt;getdeps.py&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Build with cmake directly&lt;/h2&gt; 
&lt;p&gt;If you don&#39;t want to let getdeps invoke cmake for you then by default, building the tests is disabled as part of the CMake &lt;code&gt;all&lt;/code&gt; target. To build the tests, specify &lt;code&gt;-DBUILD_TESTS=ON&lt;/code&gt; to CMake at configure time.&lt;/p&gt; 
&lt;p&gt;NB if you want to invoke &lt;code&gt;cmake&lt;/code&gt; again to iterate on a &lt;code&gt;getdeps.py&lt;/code&gt; build, there is a helpful &lt;code&gt;run_cmake.py&lt;/code&gt; script output in the scratch-path build directory. You can find the scratch build directory from logs or with &lt;code&gt;python3 ./build/fbcode_builder/getdeps.py show-build-dir&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Running tests with ctests also works if you cd to the build dir, e.g. &lt;code&gt;(cd $(python3 ./build/fbcode_builder/getdeps.py show-build-dir) &amp;amp;&amp;amp; ctest)&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Finding dependencies in non-default locations&lt;/h3&gt; 
&lt;p&gt;If you have boost, gtest, or other dependencies installed in a non-default location, you can use the &lt;code&gt;CMAKE_INCLUDE_PATH&lt;/code&gt; and &lt;code&gt;CMAKE_LIBRARY_PATH&lt;/code&gt; variables to make CMAKE look also look for header files and libraries in non-standard locations. For example, to also search the directories &lt;code&gt;/alt/include/path1&lt;/code&gt; and &lt;code&gt;/alt/include/path2&lt;/code&gt; for header files and the directories &lt;code&gt;/alt/lib/path1&lt;/code&gt; and &lt;code&gt;/alt/lib/path2&lt;/code&gt; for libraries, you can invoke &lt;code&gt;cmake&lt;/code&gt; as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;cmake \
  -DCMAKE_INCLUDE_PATH=/alt/include/path1:/alt/include/path2 \
  -DCMAKE_LIBRARY_PATH=/alt/lib/path1:/alt/lib/path2 ...
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Ubuntu LTS, CentOS Stream, Fedora&lt;/h2&gt; 
&lt;p&gt;Use the &lt;code&gt;getdeps.py&lt;/code&gt; approach above. We test in CI on Ubuntu LTS, and occasionally on other distros.&lt;/p&gt; 
&lt;p&gt;If you find the set of system packages is not quite right for your chosen distro, you can specify distro version specific overrides in the dependency manifests (e.g. &lt;a href=&quot;https://github.com/facebook/folly/raw/main/build/fbcode_builder/manifests/boost&quot;&gt;https://github.com/facebook/folly/blob/main/build/fbcode_builder/manifests/boost&lt;/a&gt; ). You could probably make it work on most recent Ubuntu/Debian or Fedora/Redhat derived distributions.&lt;/p&gt; 
&lt;p&gt;At time of writing (Dec 2021) there is a build break on GCC 11.x based systems in lang_badge_test. If you don&#39;t need badge functionality you can work around by commenting it out from CMakeLists.txt (unfortunately fbthrift does need it)&lt;/p&gt; 
&lt;h2&gt;Windows (Vcpkg)&lt;/h2&gt; 
&lt;p&gt;Note that many tests are disabled for folly Windows builds, you can see them in the log from the cmake configure step, or by looking for WINDOWS_DISABLED in &lt;code&gt;CMakeLists.txt&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;That said, &lt;code&gt;getdeps.py&lt;/code&gt; builds work on Windows and are tested in CI.&lt;/p&gt; 
&lt;p&gt;If you prefer, you can try Vcpkg. folly is available in &lt;a href=&quot;https://github.com/Microsoft/vcpkg#vcpkg&quot;&gt;Vcpkg&lt;/a&gt; and releases may be built via &lt;code&gt;vcpkg install folly:x64-windows&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;You may also use &lt;code&gt;vcpkg install folly:x64-windows --head&lt;/code&gt; to build against &lt;code&gt;main&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;macOS&lt;/h2&gt; 
&lt;p&gt;&lt;code&gt;getdeps.py&lt;/code&gt; builds work on macOS and are tested in CI, however if you prefer, you can try one of the macOS package managers&lt;/p&gt; 
&lt;h3&gt;Homebrew&lt;/h3&gt; 
&lt;p&gt;folly is available as a Formula and releases may be built via &lt;code&gt;brew install folly&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;You may also use &lt;code&gt;folly/build/bootstrap-osx-homebrew.sh&lt;/code&gt; to build against &lt;code&gt;main&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;  ./folly/build/bootstrap-osx-homebrew.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will create a build directory &lt;code&gt;_build&lt;/code&gt; in the top-level.&lt;/p&gt; 
&lt;h3&gt;MacPorts&lt;/h3&gt; 
&lt;p&gt;Install the required packages from MacPorts:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;  sudo port install \
    boost \
    cmake \
    gflags \
    git \
    google-glog \
    libevent \
    libtool \
    lz4 \
    lzma \
    openssl \
    snappy \
    xz \
    zlib
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Download and install double-conversion:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;  git clone https://github.com/google/double-conversion.git
  cd double-conversion
  cmake -DBUILD_SHARED_LIBS=ON .
  make
  sudo make install
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Download and install folly with the parameters listed below:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;  git clone https://github.com/facebook/folly.git
  cd folly
  mkdir _build
  cd _build
  cmake ..
  make
  sudo make install
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>ggml-org/llama.cpp</title>
      <link>https://github.com/ggml-org/llama.cpp</link>
      <description>&lt;p&gt;LLM inference in C/C++&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;llama.cpp&lt;/h1&gt; 
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/1991296/230134379-7181e485-c521-4d23-a0d6-f7b3b61ba524.png&quot; alt=&quot;llama&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://opensource.org/licenses/MIT&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/license-MIT-blue.svg?sanitize=true&quot; alt=&quot;License: MIT&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/actions/workflows/server.yml&quot;&gt;&lt;img src=&quot;https://github.com/ggml-org/llama.cpp/actions/workflows/server.yml/badge.svg?sanitize=true&quot; alt=&quot;Server&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/users/ggerganov/projects/7&quot;&gt;Roadmap&lt;/a&gt; / &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/discussions/3471&quot;&gt;Project status&lt;/a&gt; / &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/discussions/205&quot;&gt;Manifesto&lt;/a&gt; / &lt;a href=&quot;https://github.com/ggml-org/ggml&quot;&gt;ggml&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Inference of Meta&#39;s &lt;a href=&quot;https://arxiv.org/abs/2302.13971&quot;&gt;LLaMA&lt;/a&gt; model (and others) in pure C/C++&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] New &lt;code&gt;llama.cpp&lt;/code&gt; package location: &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/pkgs/container/llama.cpp&quot;&gt;ggml-org/llama.cpp&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;Update your container URLs to: &lt;code&gt;ghcr.io/ggml-org/llama.cpp&lt;/code&gt;&lt;/p&gt; 
 &lt;p&gt;More info: &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/discussions/11801&quot;&gt;https://github.com/ggml-org/llama.cpp/discussions/11801&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Recent API changes&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ggml-org/llama.cpp/issues/9289&quot;&gt;Changelog for &lt;code&gt;libllama&lt;/code&gt; API&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ggml-org/llama.cpp/issues/9291&quot;&gt;Changelog for &lt;code&gt;llama-server&lt;/code&gt; REST API&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Hot topics&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;How to use &lt;a href=&quot;https://developer.apple.com/documentation/metal/mtlresidencyset?language=objc&quot;&gt;MTLResidencySet&lt;/a&gt; to keep the GPU memory active?&lt;/strong&gt; &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/pull/11427&quot;&gt;https://github.com/ggml-org/llama.cpp/pull/11427&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;VS Code extension for FIM completions:&lt;/strong&gt; &lt;a href=&quot;https://github.com/ggml-org/llama.vscode&quot;&gt;https://github.com/ggml-org/llama.vscode&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Universal &lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/function-calling.md&quot;&gt;tool call support&lt;/a&gt; in &lt;code&gt;llama-server&lt;/code&gt; &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/pull/9639&quot;&gt;https://github.com/ggml-org/llama.cpp/pull/9639&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Vim/Neovim plugin for FIM completions: &lt;a href=&quot;https://github.com/ggml-org/llama.vim&quot;&gt;https://github.com/ggml-org/llama.vim&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Introducing GGUF-my-LoRA &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/discussions/10123&quot;&gt;https://github.com/ggml-org/llama.cpp/discussions/10123&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Hugging Face Inference Endpoints now support GGUF out of the box! &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/discussions/9669&quot;&gt;https://github.com/ggml-org/llama.cpp/discussions/9669&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Hugging Face GGUF editor: &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/discussions/9268&quot;&gt;discussion&lt;/a&gt; | &lt;a href=&quot;https://huggingface.co/spaces/CISCai/gguf-editor&quot;&gt;tool&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr&gt; 
&lt;h2&gt;Description&lt;/h2&gt; 
&lt;p&gt;The main goal of &lt;code&gt;llama.cpp&lt;/code&gt; is to enable LLM inference with minimal setup and state-of-the-art performance on a wide range of hardware - locally and in the cloud.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Plain C/C++ implementation without any dependencies&lt;/li&gt; 
 &lt;li&gt;Apple silicon is a first-class citizen - optimized via ARM NEON, Accelerate and Metal frameworks&lt;/li&gt; 
 &lt;li&gt;AVX, AVX2, AVX512 and AMX support for x86 architectures&lt;/li&gt; 
 &lt;li&gt;1.5-bit, 2-bit, 3-bit, 4-bit, 5-bit, 6-bit, and 8-bit integer quantization for faster inference and reduced memory use&lt;/li&gt; 
 &lt;li&gt;Custom CUDA kernels for running LLMs on NVIDIA GPUs (support for AMD GPUs via HIP and Moore Threads MTT GPUs via MUSA)&lt;/li&gt; 
 &lt;li&gt;Vulkan and SYCL backend support&lt;/li&gt; 
 &lt;li&gt;CPU+GPU hybrid inference to partially accelerate models larger than the total VRAM capacity&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The &lt;code&gt;llama.cpp&lt;/code&gt; project is the main playground for developing new features for the &lt;a href=&quot;https://github.com/ggml-org/ggml&quot;&gt;ggml&lt;/a&gt; library.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Models&lt;/summary&gt; 
 &lt;p&gt;Typically finetunes of the base models below are supported as well.&lt;/p&gt; 
 &lt;p&gt;Instructions for adding support for new models: &lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/development/HOWTO-add-model.md&quot;&gt;HOWTO-add-model.md&lt;/a&gt;&lt;/p&gt; 
 &lt;h4&gt;Text-only&lt;/h4&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; LLaMA 🦙&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; LLaMA 2 🦙🦙&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; LLaMA 3 🦙🦙🦙&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;a href=&quot;https://huggingface.co/mistralai/Mistral-7B-v0.1&quot;&gt;Mistral 7B&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;a href=&quot;https://huggingface.co/models?search=mistral-ai/Mixtral&quot;&gt;Mixtral MoE&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;a href=&quot;https://huggingface.co/databricks/dbrx-instruct&quot;&gt;DBRX&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;a href=&quot;https://huggingface.co/models?search=tiiuae/falcon&quot;&gt;Falcon&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;a href=&quot;https://github.com/ymcui/Chinese-LLaMA-Alpaca&quot;&gt;Chinese LLaMA / Alpaca&lt;/a&gt; and &lt;a href=&quot;https://github.com/ymcui/Chinese-LLaMA-Alpaca-2&quot;&gt;Chinese LLaMA-2 / Alpaca-2&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;a href=&quot;https://github.com/bofenghuang/vigogne&quot;&gt;Vigogne (French)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/pull/5423&quot;&gt;BERT&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;a href=&quot;https://bair.berkeley.edu/blog/2023/04/03/koala/&quot;&gt;Koala&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;a href=&quot;https://huggingface.co/models?search=baichuan-inc/Baichuan&quot;&gt;Baichuan 1 &amp;amp; 2&lt;/a&gt; + &lt;a href=&quot;https://huggingface.co/hiyouga/baichuan-7b-sft&quot;&gt;derivations&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;a href=&quot;https://huggingface.co/models?search=BAAI/Aquila&quot;&gt;Aquila 1 &amp;amp; 2&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/pull/3187&quot;&gt;Starcoder models&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;a href=&quot;https://huggingface.co/smallcloudai/Refact-1_6B-fim&quot;&gt;Refact&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/pull/3417&quot;&gt;MPT&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/pull/3553&quot;&gt;Bloom&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;a href=&quot;https://huggingface.co/models?search=01-ai/Yi&quot;&gt;Yi models&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;a href=&quot;https://huggingface.co/stabilityai&quot;&gt;StableLM models&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;a href=&quot;https://huggingface.co/models?search=deepseek-ai/deepseek&quot;&gt;Deepseek models&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;a href=&quot;https://huggingface.co/models?search=Qwen/Qwen&quot;&gt;Qwen models&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/pull/3557&quot;&gt;PLaMo-13B&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;a href=&quot;https://huggingface.co/models?search=microsoft/phi&quot;&gt;Phi models&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/pull/11003&quot;&gt;PhiMoE&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;a href=&quot;https://huggingface.co/gpt2&quot;&gt;GPT-2&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/pull/5118&quot;&gt;Orion 14B&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;a href=&quot;https://huggingface.co/models?search=internlm2&quot;&gt;InternLM2&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;a href=&quot;https://github.com/WisdomShell/codeshell&quot;&gt;CodeShell&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;a href=&quot;https://ai.google.dev/gemma&quot;&gt;Gemma&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;a href=&quot;https://github.com/state-spaces/mamba&quot;&gt;Mamba&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;a href=&quot;https://huggingface.co/keyfan/grok-1-hf&quot;&gt;Grok-1&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;a href=&quot;https://huggingface.co/models?search=xverse&quot;&gt;Xverse&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;a href=&quot;https://huggingface.co/models?search=CohereForAI/c4ai-command-r&quot;&gt;Command-R models&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;a href=&quot;https://huggingface.co/models?search=sea-lion&quot;&gt;SEA-LION&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;a href=&quot;https://huggingface.co/GritLM/GritLM-7B&quot;&gt;GritLM-7B&lt;/a&gt; + &lt;a href=&quot;https://huggingface.co/GritLM/GritLM-8x7B&quot;&gt;GritLM-8x7B&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;a href=&quot;https://allenai.org/olmo&quot;&gt;OLMo&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;a href=&quot;https://allenai.org/olmo&quot;&gt;OLMo 2&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;a href=&quot;https://huggingface.co/allenai/OLMoE-1B-7B-0924&quot;&gt;OLMoE&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;a href=&quot;https://huggingface.co/collections/ibm-granite/granite-code-models-6624c5cec322e4c148c8b330&quot;&gt;Granite models&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;a href=&quot;https://github.com/EleutherAI/gpt-neox&quot;&gt;GPT-NeoX&lt;/a&gt; + &lt;a href=&quot;https://github.com/EleutherAI/pythia&quot;&gt;Pythia&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;a href=&quot;https://huggingface.co/collections/Snowflake/arctic-66290090abe542894a5ac520&quot;&gt;Snowflake-Arctic MoE&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;a href=&quot;https://huggingface.co/models?search=Smaug&quot;&gt;Smaug&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;a href=&quot;https://huggingface.co/LumiOpen/Poro-34B&quot;&gt;Poro 34B&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;a href=&quot;https://huggingface.co/1bitLLM&quot;&gt;Bitnet b1.58 models&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;a href=&quot;https://huggingface.co/models?search=flan-t5&quot;&gt;Flan T5&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;a href=&quot;https://huggingface.co/collections/apple/openelm-instruct-models-6619ad295d7ae9f868b759ca&quot;&gt;Open Elm models&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;a href=&quot;https://huggingface.co/THUDM/chatglm3-6b&quot;&gt;ChatGLM3-6b&lt;/a&gt; + &lt;a href=&quot;https://huggingface.co/THUDM/glm-4-9b&quot;&gt;ChatGLM4-9b&lt;/a&gt; + &lt;a href=&quot;https://huggingface.co/THUDM/glm-edge-1.5b-chat&quot;&gt;GLMEdge-1.5b&lt;/a&gt; + &lt;a href=&quot;https://huggingface.co/THUDM/glm-edge-4b-chat&quot;&gt;GLMEdge-4b&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;a href=&quot;https://huggingface.co/collections/HuggingFaceTB/smollm-6695016cad7167254ce15966&quot;&gt;SmolLM&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;a href=&quot;https://huggingface.co/LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct&quot;&gt;EXAONE-3.0-7.8B-Instruct&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;a href=&quot;https://huggingface.co/collections/tiiuae/falconmamba-7b-66b9a580324dd1598b0f6d4a&quot;&gt;FalconMamba Models&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;a href=&quot;https://huggingface.co/inceptionai/jais-13b-chat&quot;&gt;Jais&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;a href=&quot;https://huggingface.co/collections/speakleash/bielik-11b-v23-66ee813238d9b526a072408a&quot;&gt;Bielik-11B-v2.3&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;a href=&quot;https://github.com/BlinkDL/RWKV-LM&quot;&gt;RWKV-6&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;a href=&quot;https://huggingface.co/recursal/QRWKV6-32B-Instruct-Preview-v0.1&quot;&gt;QRWKV-6&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;a href=&quot;https://huggingface.co/ai-sage/GigaChat-20B-A3B-instruct&quot;&gt;GigaChat-20B-A3B&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h4&gt;Multimodal&lt;/h4&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;a href=&quot;https://huggingface.co/collections/liuhaotian/llava-15-653aac15d994e992e2677a7e&quot;&gt;LLaVA 1.5 models&lt;/a&gt;, &lt;a href=&quot;https://huggingface.co/collections/liuhaotian/llava-16-65b9e40155f60fd046a5ccf2&quot;&gt;LLaVA 1.6 models&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;a href=&quot;https://huggingface.co/models?search=SkunkworksAI/Bakllava&quot;&gt;BakLLaVA&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;a href=&quot;https://huggingface.co/NousResearch/Obsidian-3B-V0.5&quot;&gt;Obsidian&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;a href=&quot;https://huggingface.co/models?search=Lin-Chen/ShareGPT4V&quot;&gt;ShareGPT4V&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;a href=&quot;https://huggingface.co/models?search=mobileVLM&quot;&gt;MobileVLM 1.7B/3B models&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;a href=&quot;https://huggingface.co/models?search=Yi-VL&quot;&gt;Yi-VL&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;a href=&quot;https://huggingface.co/models?search=MiniCPM&quot;&gt;Mini CPM&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;a href=&quot;https://huggingface.co/vikhyatk/moondream2&quot;&gt;Moondream&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;a href=&quot;https://github.com/BAAI-DCAI/Bunny&quot;&gt;Bunny&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;a href=&quot;https://huggingface.co/models?search=glm-edge&quot;&gt;GLM-EDGE&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;a href=&quot;https://huggingface.co/collections/Qwen/qwen2-vl-66cee7455501d7126940800d&quot;&gt;Qwen2-VL&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Bindings&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Python: &lt;a href=&quot;https://github.com/abetlen/llama-cpp-python&quot;&gt;abetlen/llama-cpp-python&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Go: &lt;a href=&quot;https://github.com/go-skynet/go-llama.cpp&quot;&gt;go-skynet/go-llama.cpp&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Node.js: &lt;a href=&quot;https://github.com/withcatai/node-llama-cpp&quot;&gt;withcatai/node-llama-cpp&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;JS/TS (llama.cpp server client): &lt;a href=&quot;https://modelfusion.dev/integration/model-provider/llamacpp&quot;&gt;lgrammel/modelfusion&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;JS/TS (Programmable Prompt Engine CLI): &lt;a href=&quot;https://github.com/offline-ai/cli&quot;&gt;offline-ai/cli&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;JavaScript/Wasm (works in browser): &lt;a href=&quot;https://github.com/tangledgroup/llama-cpp-wasm&quot;&gt;tangledgroup/llama-cpp-wasm&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Typescript/Wasm (nicer API, available on npm): &lt;a href=&quot;https://github.com/ngxson/wllama&quot;&gt;ngxson/wllama&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Ruby: &lt;a href=&quot;https://github.com/yoshoku/llama_cpp.rb&quot;&gt;yoshoku/llama_cpp.rb&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Rust (more features): &lt;a href=&quot;https://github.com/edgenai/llama_cpp-rs&quot;&gt;edgenai/llama_cpp-rs&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Rust (nicer API): &lt;a href=&quot;https://github.com/mdrokz/rust-llama.cpp&quot;&gt;mdrokz/rust-llama.cpp&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Rust (more direct bindings): &lt;a href=&quot;https://github.com/utilityai/llama-cpp-rs&quot;&gt;utilityai/llama-cpp-rs&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Rust (automated build from crates.io): &lt;a href=&quot;https://github.com/ShelbyJenkins/llm_client&quot;&gt;ShelbyJenkins/llm_client&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;C#/.NET: &lt;a href=&quot;https://github.com/SciSharp/LLamaSharp&quot;&gt;SciSharp/LLamaSharp&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;C#/VB.NET (more features - community license): &lt;a href=&quot;https://docs.lm-kit.com/lm-kit-net/index.html&quot;&gt;LM-Kit.NET&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Scala 3: &lt;a href=&quot;https://github.com/donderom/llm4s&quot;&gt;donderom/llm4s&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Clojure: &lt;a href=&quot;https://github.com/phronmophobic/llama.clj&quot;&gt;phronmophobic/llama.clj&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;React Native: &lt;a href=&quot;https://github.com/mybigday/llama.rn&quot;&gt;mybigday/llama.rn&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Java: &lt;a href=&quot;https://github.com/kherud/java-llama.cpp&quot;&gt;kherud/java-llama.cpp&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zig: &lt;a href=&quot;https://github.com/Deins/llama.cpp.zig&quot;&gt;deins/llama.cpp.zig&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Flutter/Dart: &lt;a href=&quot;https://github.com/netdur/llama_cpp_dart&quot;&gt;netdur/llama_cpp_dart&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Flutter: &lt;a href=&quot;https://github.com/xuegao-tzx/Fllama&quot;&gt;xuegao-tzx/Fllama&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;PHP (API bindings and features built on top of llama.cpp): &lt;a href=&quot;https://github.com/distantmagic/resonance&quot;&gt;distantmagic/resonance&lt;/a&gt; &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/pull/6326&quot;&gt;(more info)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Guile Scheme: &lt;a href=&quot;https://savannah.nongnu.org/projects/guile-llama-cpp&quot;&gt;guile_llama_cpp&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Swift &lt;a href=&quot;https://github.com/srgtuszy/llama-cpp-swift&quot;&gt;srgtuszy/llama-cpp-swift&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Swift &lt;a href=&quot;https://github.com/ShenghaiWang/SwiftLlama&quot;&gt;ShenghaiWang/SwiftLlama&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Delphi &lt;a href=&quot;https://github.com/Embarcadero/llama-cpp-delphi&quot;&gt;Embarcadero/llama-cpp-delphi&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;UIs&lt;/summary&gt; 
 &lt;p&gt;&lt;em&gt;(to have a project listed here, it should clearly state that it depends on &lt;code&gt;llama.cpp&lt;/code&gt;)&lt;/em&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/yaroslavyaroslav/OpenAI-sublime-text&quot;&gt;AI Sublime Text plugin&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/cztomsik/ava&quot;&gt;cztomsik/ava&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/alexpinel/Dot&quot;&gt;Dot&lt;/a&gt; (GPL)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/ylsdamxssjxxdd/eva&quot;&gt;eva&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/iohub/coLLaMA&quot;&gt;iohub/collama&lt;/a&gt; (Apache-2.0)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/janhq/jan&quot;&gt;janhq/jan&lt;/a&gt; (AGPL)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/johnbean393/Sidekick&quot;&gt;johnbean393/Sidekick&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/zhouwg/kantv?tab=readme-ov-file&quot;&gt;KanTV&lt;/a&gt; (Apache-2.0)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/firatkiral/kodibot&quot;&gt;KodiBot&lt;/a&gt; (GPL)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/ggml-org/llama.vim&quot;&gt;llama.vim&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/abgulati/LARS&quot;&gt;LARS&lt;/a&gt; (AGPL)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/vietanhdev/llama-assistant&quot;&gt;Llama Assistant&lt;/a&gt; (GPL)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/guinmoon/LLMFarm?tab=readme-ov-file&quot;&gt;LLMFarm&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/undreamai/LLMUnity&quot;&gt;LLMUnity&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://lmstudio.ai/&quot;&gt;LMStudio&lt;/a&gt; (proprietary)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/mudler/LocalAI&quot;&gt;LocalAI&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/LostRuins/koboldcpp&quot;&gt;LostRuins/koboldcpp&lt;/a&gt; (AGPL)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://mindmac.app&quot;&gt;MindMac&lt;/a&gt; (proprietary)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/MindWorkAI/AI-Studio&quot;&gt;MindWorkAI/AI-Studio&lt;/a&gt; (FSL-1.1-MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/Mobile-Artificial-Intelligence/maid&quot;&gt;Mobile-Artificial-Intelligence/maid&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/Mozilla-Ocho/llamafile&quot;&gt;Mozilla-Ocho/llamafile&lt;/a&gt; (Apache-2.0)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/nat/openplayground&quot;&gt;nat/openplayground&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/nomic-ai/gpt4all&quot;&gt;nomic-ai/gpt4all&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/ollama/ollama&quot;&gt;ollama/ollama&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/oobabooga/text-generation-webui&quot;&gt;oobabooga/text-generation-webui&lt;/a&gt; (AGPL)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/a-ghorbani/pocketpal-ai&quot;&gt;PocketPal AI&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/psugihara/FreeChat&quot;&gt;psugihara/FreeChat&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/ptsochantaris/emeltal&quot;&gt;ptsochantaris/emeltal&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/pythops/tenere&quot;&gt;pythops/tenere&lt;/a&gt; (AGPL)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/containers/ramalama&quot;&gt;ramalama&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/semperai/amica&quot;&gt;semperai/amica&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/withcatai/catai&quot;&gt;withcatai/catai&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/blackhole89/autopen&quot;&gt;Autopen&lt;/a&gt; (GPL)&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Tools&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/akx/ggify&quot;&gt;akx/ggify&lt;/a&gt; – download PyTorch models from HuggingFace Hub and convert them to GGML&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/akx/ollama-dl&quot;&gt;akx/ollama-dl&lt;/a&gt; – download models from the Ollama library to be used directly with llama.cpp&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/crashr/gppm&quot;&gt;crashr/gppm&lt;/a&gt; – launch llama.cpp instances utilizing NVIDIA Tesla P40 or P100 GPUs with reduced idle power consumption&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/gpustack/gguf-parser-go/tree/main/cmd/gguf-parser&quot;&gt;gpustack/gguf-parser&lt;/a&gt; - review/check the GGUF file and estimate the memory usage&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://marketplace.unity.com/packages/tools/generative-ai/styled-lines-llama-cpp-model-292902&quot;&gt;Styled Lines&lt;/a&gt; (proprietary licensed, async wrapper of inference part for game development in Unity3d with pre-built Mobile and Web platform wrappers and a model example)&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Infrastructure&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/distantmagic/paddler&quot;&gt;Paddler&lt;/a&gt; - Stateful load balancer custom-tailored for llama.cpp&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/gpustack/gpustack&quot;&gt;GPUStack&lt;/a&gt; - Manage GPU clusters for running LLMs&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/onicai/llama_cpp_canister&quot;&gt;llama_cpp_canister&lt;/a&gt; - llama.cpp as a smart contract on the Internet Computer, using WebAssembly&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/mostlygeek/llama-swap&quot;&gt;llama-swap&lt;/a&gt; - transparent proxy that adds automatic model switching with llama-server&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/kalavai-net/kalavai-client&quot;&gt;Kalavai&lt;/a&gt; - Crowdsource end to end LLM deployment at any scale&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/InftyAI/llmaz&quot;&gt;llmaz&lt;/a&gt; - ☸️ Easy, advanced inference platform for large language models on Kubernetes.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Games&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/MorganRO8/Lucys_Labyrinth&quot;&gt;Lucy&#39;s Labyrinth&lt;/a&gt; - A simple maze game where agents controlled by an AI model will try to trick you.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;Supported backends&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Backend&lt;/th&gt; 
   &lt;th&gt;Target devices&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/build.md#metal-build&quot;&gt;Metal&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Apple Silicon&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/build.md#blas-build&quot;&gt;BLAS&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;All&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/backend/BLIS.md&quot;&gt;BLIS&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;All&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/backend/SYCL.md&quot;&gt;SYCL&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Intel and Nvidia GPU&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/build.md#musa&quot;&gt;MUSA&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Moore Threads MTT GPU&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/build.md#cuda&quot;&gt;CUDA&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Nvidia GPU&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/build.md#hip&quot;&gt;HIP&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;AMD GPU&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/build.md#vulkan&quot;&gt;Vulkan&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;GPU&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/build.md#cann&quot;&gt;CANN&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Ascend NPU&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/backend/OPENCL.md&quot;&gt;OpenCL&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Adreno GPU&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Building the project&lt;/h2&gt; 
&lt;p&gt;The main product of this project is the &lt;code&gt;llama&lt;/code&gt; library. Its C-style interface can be found in &lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/include/llama.h&quot;&gt;include/llama.h&lt;/a&gt;. The project also includes many example programs and tools using the &lt;code&gt;llama&lt;/code&gt; library. The examples range from simple, minimal code snippets to sophisticated sub-projects such as an OpenAI-compatible HTTP server. Possible methods for obtaining the binaries:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Clone this repository and build locally, see &lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/build.md&quot;&gt;how to build&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;On MacOS or Linux, install &lt;code&gt;llama.cpp&lt;/code&gt; via &lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/install.md&quot;&gt;brew, flox or nix&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Use a Docker image, see &lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/docker.md&quot;&gt;documentation for Docker&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Download pre-built binaries from &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/releases&quot;&gt;releases&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Obtaining and quantizing models&lt;/h2&gt; 
&lt;p&gt;The &lt;a href=&quot;https://huggingface.co&quot;&gt;Hugging Face&lt;/a&gt; platform hosts a &lt;a href=&quot;https://huggingface.co/models?library=gguf&amp;amp;sort=trending&quot;&gt;number of LLMs&lt;/a&gt; compatible with &lt;code&gt;llama.cpp&lt;/code&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://huggingface.co/models?library=gguf&amp;amp;sort=trending&quot;&gt;Trending&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://huggingface.co/models?sort=trending&amp;amp;search=llama+gguf&quot;&gt;LLaMA&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can either manually download the GGUF file or directly use any &lt;code&gt;llama.cpp&lt;/code&gt;-compatible models from Hugging Face by using this CLI argument: &lt;code&gt;-hf &amp;lt;user&amp;gt;/&amp;lt;model&amp;gt;[:quant]&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;After downloading a model, use the CLI tools to run it locally - see below.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;llama.cpp&lt;/code&gt; requires the model to be stored in the &lt;a href=&quot;https://github.com/ggml-org/ggml/raw/master/docs/gguf.md&quot;&gt;GGUF&lt;/a&gt; file format. Models in other data formats can be converted to GGUF using the &lt;code&gt;convert_*.py&lt;/code&gt; Python scripts in this repo.&lt;/p&gt; 
&lt;p&gt;The Hugging Face platform provides a variety of online tools for converting, quantizing and hosting models with &lt;code&gt;llama.cpp&lt;/code&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Use the &lt;a href=&quot;https://huggingface.co/spaces/ggml-org/gguf-my-repo&quot;&gt;GGUF-my-repo space&lt;/a&gt; to convert to GGUF format and quantize model weights to smaller sizes&lt;/li&gt; 
 &lt;li&gt;Use the &lt;a href=&quot;https://huggingface.co/spaces/ggml-org/gguf-my-lora&quot;&gt;GGUF-my-LoRA space&lt;/a&gt; to convert LoRA adapters to GGUF format (more info: &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/discussions/10123&quot;&gt;https://github.com/ggml-org/llama.cpp/discussions/10123&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;Use the &lt;a href=&quot;https://huggingface.co/spaces/CISCai/gguf-editor&quot;&gt;GGUF-editor space&lt;/a&gt; to edit GGUF meta data in the browser (more info: &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/discussions/9268&quot;&gt;https://github.com/ggml-org/llama.cpp/discussions/9268&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;Use the &lt;a href=&quot;https://ui.endpoints.huggingface.co/&quot;&gt;Inference Endpoints&lt;/a&gt; to directly host &lt;code&gt;llama.cpp&lt;/code&gt; in the cloud (more info: &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/discussions/9669&quot;&gt;https://github.com/ggml-org/llama.cpp/discussions/9669&lt;/a&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;To learn more about model quantization, &lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/examples/quantize/README.md&quot;&gt;read this documentation&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/examples/main&quot;&gt;&lt;code&gt;llama-cli&lt;/code&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;h4&gt;A CLI tool for accessing and experimenting with most of &lt;code&gt;llama.cpp&lt;/code&gt;&#39;s functionality.&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt; 
  &lt;details open&gt; 
   &lt;summary&gt;Run in conversation mode&lt;/summary&gt; 
   &lt;p&gt;Models with a built-in chat template will automatically activate conversation mode. If this doesn&#39;t occur, you can manually enable it by adding &lt;code&gt;-cnv&lt;/code&gt; and specifying a suitable chat template with &lt;code&gt;--chat-template NAME&lt;/code&gt;&lt;/p&gt; 
   &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;llama-cli -m model.gguf

# &amp;gt; hi, who are you?
# Hi there! I&#39;m your helpful assistant! I&#39;m an AI-powered chatbot designed to assist and provide information to users like you. I&#39;m here to help answer your questions, provide guidance, and offer support on a wide range of topics. I&#39;m a friendly and knowledgeable AI, and I&#39;m always happy to help with anything you need. What&#39;s on your mind, and how can I assist you today?
#
# &amp;gt; what is 1+1?
# Easy peasy! The answer to 1+1 is... 2!
&lt;/code&gt;&lt;/pre&gt; 
  &lt;/details&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;details&gt; 
   &lt;summary&gt;Run in conversation mode with custom chat template&lt;/summary&gt; 
   &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# use the &quot;chatml&quot; template (use -h to see the list of supported templates)
llama-cli -m model.gguf -cnv --chat-template chatml

# use a custom template
llama-cli -m model.gguf -cnv --in-prefix &#39;User: &#39; --reverse-prompt &#39;User:&#39;
&lt;/code&gt;&lt;/pre&gt; 
  &lt;/details&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;details&gt; 
   &lt;summary&gt;Run simple text completion&lt;/summary&gt; 
   &lt;p&gt;To disable conversation mode explicitly, use &lt;code&gt;-no-cnv&lt;/code&gt;&lt;/p&gt; 
   &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;llama-cli -m model.gguf -p &quot;I believe the meaning of life is&quot; -n 128 -no-cnv

# I believe the meaning of life is to find your own truth and to live in accordance with it. For me, this means being true to myself and following my passions, even if they don&#39;t align with societal expectations. I think that&#39;s what I love about yoga – it&#39;s not just a physical practice, but a spiritual one too. It&#39;s about connecting with yourself, listening to your inner voice, and honoring your own unique journey.
&lt;/code&gt;&lt;/pre&gt; 
  &lt;/details&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;details&gt; 
   &lt;summary&gt;Constrain the output with a custom grammar&lt;/summary&gt; 
   &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;llama-cli -m model.gguf -n 256 --grammar-file grammars/json.gbnf -p &#39;Request: schedule a call at 8pm; Command:&#39;

# {&quot;appointmentTime&quot;: &quot;8pm&quot;, &quot;appointmentDetails&quot;: &quot;schedule a a call&quot;}
&lt;/code&gt;&lt;/pre&gt; 
   &lt;p&gt;The &lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/grammars/&quot;&gt;grammars/&lt;/a&gt; folder contains a handful of sample grammars. To write your own, check out the &lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/grammars/README.md&quot;&gt;GBNF Guide&lt;/a&gt;.&lt;/p&gt; 
   &lt;p&gt;For authoring more complex JSON grammars, check out &lt;a href=&quot;https://grammar.intrinsiclabs.ai/&quot;&gt;https://grammar.intrinsiclabs.ai/&lt;/a&gt;&lt;/p&gt; 
  &lt;/details&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/examples/server&quot;&gt;&lt;code&gt;llama-server&lt;/code&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;h4&gt;A lightweight, &lt;a href=&quot;https://github.com/openai/openai-openapi&quot;&gt;OpenAI API&lt;/a&gt; compatible, HTTP server for serving LLMs.&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt; 
  &lt;details open&gt; 
   &lt;summary&gt;Start a local HTTP server with default configuration on port 8080&lt;/summary&gt; 
   &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;llama-server -m model.gguf --port 8080

# Basic web UI can be accessed via browser: http://localhost:8080
# Chat completion endpoint: http://localhost:8080/v1/chat/completions
&lt;/code&gt;&lt;/pre&gt; 
  &lt;/details&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;details&gt; 
   &lt;summary&gt;Support multiple-users and parallel decoding&lt;/summary&gt; 
   &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# up to 4 concurrent requests, each with 4096 max context
llama-server -m model.gguf -c 16384 -np 4
&lt;/code&gt;&lt;/pre&gt; 
  &lt;/details&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;details&gt; 
   &lt;summary&gt;Enable speculative decoding&lt;/summary&gt; 
   &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# the draft.gguf model should be a small variant of the target model.gguf
llama-server -m model.gguf -md draft.gguf
&lt;/code&gt;&lt;/pre&gt; 
  &lt;/details&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;details&gt; 
   &lt;summary&gt;Serve an embedding model&lt;/summary&gt; 
   &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# use the /embedding endpoint
llama-server -m model.gguf --embedding --pooling cls -ub 8192
&lt;/code&gt;&lt;/pre&gt; 
  &lt;/details&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;details&gt; 
   &lt;summary&gt;Serve a reranking model&lt;/summary&gt; 
   &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# use the /reranking endpoint
llama-server -m model.gguf --reranking
&lt;/code&gt;&lt;/pre&gt; 
  &lt;/details&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;details&gt; 
   &lt;summary&gt;Constrain all outputs with a grammar&lt;/summary&gt; 
   &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# custom grammar
llama-server -m model.gguf --grammar-file grammar.gbnf

# JSON
llama-server -m model.gguf --grammar-file grammars/json.gbnf
&lt;/code&gt;&lt;/pre&gt; 
  &lt;/details&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/examples/perplexity&quot;&gt;&lt;code&gt;llama-perplexity&lt;/code&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;h4&gt;A tool for measuring the perplexity &lt;a href=&quot;%5Bhttps://huggingface.co/docs/transformers/perplexity%5D(https://huggingface.co/docs/transformers/perplexity)&quot;&gt;^1&lt;/a&gt; (and other quality metrics) of a model over a given text.&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt; 
  &lt;details open&gt; 
   &lt;summary&gt;Measure the perplexity over a text file&lt;/summary&gt; 
   &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;llama-perplexity -m model.gguf -f file.txt

# [1]15.2701,[2]5.4007,[3]5.3073,[4]6.2965,[5]5.8940,[6]5.6096,[7]5.7942,[8]4.9297, ...
# Final estimate: PPL = 5.4007 +/- 0.67339
&lt;/code&gt;&lt;/pre&gt; 
  &lt;/details&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;details&gt; 
   &lt;summary&gt;Measure KL divergence&lt;/summary&gt; 
   &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# TODO
&lt;/code&gt;&lt;/pre&gt; 
  &lt;/details&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/examples/llama-bench&quot;&gt;&lt;code&gt;llama-bench&lt;/code&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;h4&gt;Benchmark the performance of the inference for various parameters.&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt; 
  &lt;details open&gt; 
   &lt;summary&gt;Run default benchmark&lt;/summary&gt; 
   &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;llama-bench -m model.gguf

# Output:
# | model               |       size |     params | backend    | threads |          test |                  t/s |
# | ------------------- | ---------: | ---------: | ---------- | ------: | ------------: | -------------------: |
# | qwen2 1.5B Q4_0     | 885.97 MiB |     1.54 B | Metal,BLAS |      16 |         pp512 |      5765.41 ± 20.55 |
# | qwen2 1.5B Q4_0     | 885.97 MiB |     1.54 B | Metal,BLAS |      16 |         tg128 |        197.71 ± 0.81 |
#
# build: 3e0ba0e60 (4229)
&lt;/code&gt;&lt;/pre&gt; 
  &lt;/details&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/examples/run&quot;&gt;&lt;code&gt;llama-run&lt;/code&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;h4&gt;A comprehensive example for running &lt;code&gt;llama.cpp&lt;/code&gt; models. Useful for inferencing. Used with RamaLama &lt;a href=&quot;%5BRamaLama%5D(https://github.com/containers/ramalama)&quot;&gt;^3&lt;/a&gt;.&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt; 
  &lt;details&gt; 
   &lt;summary&gt;Run a model with a specific prompt (by default it&#39;s pulled from Ollama registry)&lt;/summary&gt; 
   &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;llama-run granite-code
&lt;/code&gt;&lt;/pre&gt; 
  &lt;/details&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/examples/simple&quot;&gt;&lt;code&gt;llama-simple&lt;/code&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;h4&gt;A minimal example for implementing apps with &lt;code&gt;llama.cpp&lt;/code&gt;. Useful for developers.&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt; 
  &lt;details&gt; 
   &lt;summary&gt;Basic text completion&lt;/summary&gt; 
   &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;llama-simple -m model.gguf

# Hello my name is Kaitlyn and I am a 16 year old girl. I am a junior in high school and I am currently taking a class called &quot;The Art of
&lt;/code&gt;&lt;/pre&gt; 
  &lt;/details&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Contributors can open PRs&lt;/li&gt; 
 &lt;li&gt;Collaborators can push to branches in the &lt;code&gt;llama.cpp&lt;/code&gt; repo and merge PRs into the &lt;code&gt;master&lt;/code&gt; branch&lt;/li&gt; 
 &lt;li&gt;Collaborators will be invited based on contributions&lt;/li&gt; 
 &lt;li&gt;Any help with managing issues, PRs and projects is very appreciated!&lt;/li&gt; 
 &lt;li&gt;See &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22&quot;&gt;good first issues&lt;/a&gt; for tasks suitable for first contributions&lt;/li&gt; 
 &lt;li&gt;Read the &lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/CONTRIBUTING.md&quot;&gt;CONTRIBUTING.md&lt;/a&gt; for more information&lt;/li&gt; 
 &lt;li&gt;Make sure to read this: &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/discussions/205&quot;&gt;Inference at the edge&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;A bit of backstory for those who are interested: &lt;a href=&quot;https://changelog.com/podcast/532&quot;&gt;Changelog podcast&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Other documentation&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/examples/main/README.md&quot;&gt;main (cli)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/examples/server/README.md&quot;&gt;server&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/grammars/README.md&quot;&gt;GBNF grammars&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Development documentation&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/build.md&quot;&gt;How to build&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/docker.md&quot;&gt;Running on Docker&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/android.md&quot;&gt;Build on Android&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/development/token_generation_performance_tips.md&quot;&gt;Performance troubleshooting&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ggml-org/llama.cpp/wiki/GGML-Tips-&amp;amp;-Tricks&quot;&gt;GGML tips &amp;amp; tricks&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Seminal papers and background on the models&lt;/h4&gt; 
&lt;p&gt;If your issue is with model generation quality, then please at least scan the following links and papers to understand the limitations of LLaMA models. This is especially important when choosing an appropriate model size and appreciating both the significant and subtle differences between LLaMA models and ChatGPT:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;LLaMA: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://ai.facebook.com/blog/large-language-model-llama-meta-ai/&quot;&gt;Introducing LLaMA: A foundational, 65-billion-parameter large language model&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2302.13971&quot;&gt;LLaMA: Open and Efficient Foundation Language Models&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;GPT-3 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;&gt;Language Models are Few-Shot Learners&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;GPT-3.5 / InstructGPT / ChatGPT: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://openai.com/research/instruction-following&quot;&gt;Aligning language models to follow instructions&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2203.02155&quot;&gt;Training language models to follow instructions with human feedback&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Completions&lt;/h2&gt; 
&lt;p&gt;Command-line completion is available for some environments.&lt;/p&gt; 
&lt;h4&gt;Bash Completion&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ build/bin/llama-cli --completion-bash &amp;gt; ~/.llama-completion.bash
$ source ~/.llama-completion.bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Optionally this can be added to your &lt;code&gt;.bashrc&lt;/code&gt; or &lt;code&gt;.bash_profile&lt;/code&gt; to load it automatically. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-console&quot;&gt;$ echo &quot;source ~/.llama-completion.bash&quot; &amp;gt;&amp;gt; ~/.bashrc
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;References&lt;/h2&gt;</description>
    </item>
    
    <item>
      <title>mlc-ai/xgrammar</title>
      <link>https://github.com/mlc-ai/xgrammar</link>
      <description>&lt;p&gt;Fast, Flexible and Portable Structured Generation&lt;/p&gt;&lt;hr&gt;&lt;div align=&quot;center&quot; id=&quot;top&quot;&gt; 
 &lt;h1&gt;XGrammar&lt;/h1&gt; 
 &lt;p&gt;&lt;a href=&quot;https://xgrammar.mlc.ai/docs/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/docs-latest-green&quot; alt=&quot;Documentation&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/mlc-ai/xgrammar/raw/main/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/license-apache_2-blue&quot; alt=&quot;License&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://pypi.org/project/xgrammar&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/v/xgrammar&quot; alt=&quot;PyPI&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Efficient, Flexible and Portable Structured Generation&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/mlc-ai/xgrammar/main/#get-started&quot;&gt;Get Started&lt;/a&gt; | &lt;a href=&quot;https://xgrammar.mlc.ai/docs/&quot;&gt;Documentation&lt;/a&gt; | &lt;a href=&quot;https://blog.mlc.ai/2024/11/22/achieving-efficient-flexible-portable-structured-generation-with-xgrammar&quot;&gt;Blogpost&lt;/a&gt; | &lt;a href=&quot;https://arxiv.org/abs/2411.15100&quot;&gt;Technical Report&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;[2025/02] XGrammar has been officially integrated into &lt;a href=&quot;https://docs.modular.com/max/serve/structured-output&quot;&gt;Modular&#39;s MAX&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[2025/01] XGrammar has been officially integrated into &lt;a href=&quot;https://github.com/NVIDIA/TensorRT-LLM&quot;&gt;TensorRT-LLM&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;[2024/12] XGrammar has been officially integrated into &lt;a href=&quot;https://github.com/vllm-project/vllm&quot;&gt;vLLM&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;[2024/12] We presented research talks on XGrammar at CMU Catalyst, Berkeley SkyLab, MIT HANLAB, THU IIIS, SJTU, Ant Group, SGLang Meetup, Qingke AI, Camel AI. The slides can be found &lt;a href=&quot;https://docs.google.com/presentation/d/1iS7tu2EV4IKRWDaR0F3YD7ubrNqtGYUStSskceneelc/edit?usp=sharing&quot;&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;[2024/11] XGrammar has been officially integrated into &lt;a href=&quot;https://github.com/sgl-project/sglang&quot;&gt;SGLang&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;[2024/11] XGrammar has been officially integrated into &lt;a href=&quot;https://github.com/mlc-ai/mlc-llm&quot;&gt;MLC-LLM&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;[2024/11] We officially released XGrammar v0.1.0!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Overview&lt;/h2&gt; 
&lt;p&gt;XGrammar is an open-source library for efficient, flexible, and portable structured generation. It supports general context-free grammar to enable a broad range of structures while bringing careful system optimizations to enable fast executions. XGrammar features a minimal and portable C++ backend that can be easily integrated into multiple environments and frameworks, and is co-designed with the LLM inference engine and enables zero-overhead structured generation in LLM inference.&lt;/p&gt; 
&lt;h2&gt;Get Started&lt;/h2&gt; 
&lt;p&gt;Please visit our &lt;a href=&quot;https://xgrammar.mlc.ai/docs/&quot;&gt;documentation&lt;/a&gt; to get started with XGrammar.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://xgrammar.mlc.ai/docs/start/install&quot;&gt;Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://xgrammar.mlc.ai/docs/start/quick_start&quot;&gt;Quick start&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>OpenNMT/CTranslate2</title>
      <link>https://github.com/OpenNMT/CTranslate2</link>
      <description>&lt;p&gt;Fast inference engine for Transformer models&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/OpenNMT/CTranslate2/actions?query=workflow%3ACI&quot;&gt;&lt;img src=&quot;https://github.com/OpenNMT/CTranslate2/workflows/CI/badge.svg?sanitize=true&quot; alt=&quot;CI&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://badge.fury.io/py/ctranslate2&quot;&gt;&lt;img src=&quot;https://badge.fury.io/py/ctranslate2.svg?sanitize=true&quot; alt=&quot;PyPI version&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://opennmt.net/CTranslate2/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/docs-latest-blue.svg?sanitize=true&quot; alt=&quot;Documentation&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://gitter.im/OpenNMT/CTranslate2?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&quot;&gt;&lt;img src=&quot;https://badges.gitter.im/OpenNMT/CTranslate2.svg?sanitize=true&quot; alt=&quot;Gitter&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://forum.opennmt.net/&quot;&gt;&lt;img src=&quot;https://img.shields.io/discourse/status?server=https%3A%2F%2Fforum.opennmt.net%2F&quot; alt=&quot;Forum&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;CTranslate2&lt;/h1&gt; 
&lt;p&gt;CTranslate2 is a C++ and Python library for efficient inference with Transformer models.&lt;/p&gt; 
&lt;p&gt;The project implements a custom runtime that applies many performance optimization techniques such as weights quantization, layers fusion, batch reordering, etc., to &lt;a href=&quot;https://raw.githubusercontent.com/OpenNMT/CTranslate2/master/#benchmarks&quot;&gt;accelerate and reduce the memory usage&lt;/a&gt; of Transformer models on CPU and GPU.&lt;/p&gt; 
&lt;p&gt;The following model types are currently supported:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Encoder-decoder models: Transformer base/big, M2M-100, NLLB, BART, mBART, Pegasus, T5, Whisper&lt;/li&gt; 
 &lt;li&gt;Decoder-only models: GPT-2, GPT-J, GPT-NeoX, OPT, BLOOM, MPT, Llama, Mistral, Gemma, CodeGen, GPTBigCode, Falcon, Qwen2&lt;/li&gt; 
 &lt;li&gt;Encoder-only models: BERT, DistilBERT, XLM-RoBERTa&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Compatible models should be first converted into an optimized model format. The library includes converters for multiple frameworks:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://opennmt.net/CTranslate2/guides/opennmt_py.html&quot;&gt;OpenNMT-py&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://opennmt.net/CTranslate2/guides/opennmt_tf.html&quot;&gt;OpenNMT-tf&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://opennmt.net/CTranslate2/guides/fairseq.html&quot;&gt;Fairseq&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://opennmt.net/CTranslate2/guides/marian.html&quot;&gt;Marian&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://opennmt.net/CTranslate2/guides/opus_mt.html&quot;&gt;OPUS-MT&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://opennmt.net/CTranslate2/guides/transformers.html&quot;&gt;Transformers&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The project is production-oriented and comes with &lt;a href=&quot;https://opennmt.net/CTranslate2/versioning.html&quot;&gt;backward compatibility guarantees&lt;/a&gt;, but it also includes experimental features related to model compression and inference acceleration.&lt;/p&gt; 
&lt;h2&gt;Key features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Fast and efficient execution on CPU and GPU&lt;/strong&gt;&lt;br&gt;The execution &lt;a href=&quot;https://raw.githubusercontent.com/OpenNMT/CTranslate2/master/#benchmarks&quot;&gt;is significantly faster and requires less resources&lt;/a&gt; than general-purpose deep learning frameworks on supported models and tasks thanks to many advanced optimizations: layer fusion, padding removal, batch reordering, in-place operations, caching mechanism, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Quantization and reduced precision&lt;/strong&gt;&lt;br&gt;The model serialization and computation support weights with &lt;a href=&quot;https://opennmt.net/CTranslate2/quantization.html&quot;&gt;reduced precision&lt;/a&gt;: 16-bit floating points (FP16), 16-bit brain floating points (BF16), 16-bit integers (INT16), 8-bit integers (INT8) and AWQ quantization (INT4).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multiple CPU architectures support&lt;/strong&gt;&lt;br&gt;The project supports x86-64 and AArch64/ARM64 processors and integrates multiple backends that are optimized for these platforms: &lt;a href=&quot;https://software.intel.com/content/www/us/en/develop/tools/oneapi/components/onemkl.html&quot;&gt;Intel MKL&lt;/a&gt;, &lt;a href=&quot;https://github.com/oneapi-src/oneDNN&quot;&gt;oneDNN&lt;/a&gt;, &lt;a href=&quot;https://www.openblas.net/&quot;&gt;OpenBLAS&lt;/a&gt;, &lt;a href=&quot;https://github.com/google/ruy&quot;&gt;Ruy&lt;/a&gt;, and &lt;a href=&quot;https://developer.apple.com/documentation/accelerate&quot;&gt;Apple Accelerate&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Automatic CPU detection and code dispatch&lt;/strong&gt;&lt;br&gt;One binary can include multiple backends (e.g. Intel MKL and oneDNN) and instruction set architectures (e.g. AVX, AVX2) that are automatically selected at runtime based on the CPU information.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Parallel and asynchronous execution&lt;/strong&gt;&lt;br&gt;Multiple batches can be processed in parallel and asynchronously using multiple GPUs or CPU cores.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Dynamic memory usage&lt;/strong&gt;&lt;br&gt;The memory usage changes dynamically depending on the request size while still meeting performance requirements thanks to caching allocators on both CPU and GPU.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Lightweight on disk&lt;/strong&gt;&lt;br&gt;Quantization can make the models 4 times smaller on disk with minimal accuracy loss.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Simple integration&lt;/strong&gt;&lt;br&gt;The project has few dependencies and exposes simple APIs in &lt;a href=&quot;https://opennmt.net/CTranslate2/python/overview.html&quot;&gt;Python&lt;/a&gt; and C++ to cover most integration needs.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Configurable and interactive decoding&lt;/strong&gt;&lt;br&gt;&lt;a href=&quot;https://opennmt.net/CTranslate2/decoding.html&quot;&gt;Advanced decoding features&lt;/a&gt; allow autocompleting a partial sequence and returning alternatives at a specific location in the sequence.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Support tensor parallelism for distributed inference&lt;/strong&gt;&lt;br&gt;Very large model can be split into multiple GPUs. Following this &lt;a href=&quot;https://raw.githubusercontent.com/OpenNMT/CTranslate2/master/docs/parallel.md#model-and-tensor-parallelism&quot;&gt;documentation&lt;/a&gt; to set up the required environment.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Some of these features are difficult to achieve with standard deep learning frameworks and are the motivation for this project.&lt;/p&gt; 
&lt;h2&gt;Installation and usage&lt;/h2&gt; 
&lt;p&gt;CTranslate2 can be installed with pip:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install ctranslate2
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The Python module is used to convert models and can translate or generate text with few lines of code:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;translator = ctranslate2.Translator(translation_model_path)
translator.translate_batch(tokens)

generator = ctranslate2.Generator(generation_model_path)
generator.generate_batch(start_tokens)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See the &lt;a href=&quot;https://opennmt.net/CTranslate2&quot;&gt;documentation&lt;/a&gt; for more information and examples.&lt;/p&gt; 
&lt;h2&gt;Benchmarks&lt;/h2&gt; 
&lt;p&gt;We translate the En-&amp;gt;De test set &lt;em&gt;newstest2014&lt;/em&gt; with multiple models:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://opennmt.net/Models-tf/#translation&quot;&gt;OpenNMT-tf WMT14&lt;/a&gt;: a base Transformer trained with OpenNMT-tf on the WMT14 dataset (4.5M lines)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://opennmt.net/Models-py/#translation&quot;&gt;OpenNMT-py WMT14&lt;/a&gt;: a base Transformer trained with OpenNMT-py on the WMT14 dataset (4.5M lines)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Helsinki-NLP/OPUS-MT-train/tree/master/models/en-de#opus-2020-02-26zip&quot;&gt;OPUS-MT&lt;/a&gt;: a base Transformer trained with Marian on all OPUS data available on 2020-02-26 (81.9M lines)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The benchmark reports the number of target tokens generated per second (higher is better). The results are aggregated over multiple runs. See the &lt;a href=&quot;https://raw.githubusercontent.com/OpenNMT/CTranslate2/master/tools/benchmark&quot;&gt;benchmark scripts&lt;/a&gt; for more details and reproduce these numbers.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Please note that the results presented below are only valid for the configuration used during this benchmark: absolute and relative performance may change with different settings.&lt;/strong&gt;&lt;/p&gt; 
&lt;h4&gt;CPU&lt;/h4&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th&gt;Tokens per second&lt;/th&gt; 
   &lt;th&gt;Max. memory&lt;/th&gt; 
   &lt;th&gt;BLEU&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;OpenNMT-tf WMT14 model&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenNMT-tf 2.31.0 (with TensorFlow 2.11.0)&lt;/td&gt; 
   &lt;td&gt;209.2&lt;/td&gt; 
   &lt;td&gt;2653MB&lt;/td&gt; 
   &lt;td&gt;26.93&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;OpenNMT-py WMT14 model&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenNMT-py 3.0.4 (with PyTorch 1.13.1)&lt;/td&gt; 
   &lt;td&gt;275.8&lt;/td&gt; 
   &lt;td&gt;2012MB&lt;/td&gt; 
   &lt;td&gt;26.77&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;- int8&lt;/td&gt; 
   &lt;td&gt;323.3&lt;/td&gt; 
   &lt;td&gt;1359MB&lt;/td&gt; 
   &lt;td&gt;26.72&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;CTranslate2 3.6.0&lt;/td&gt; 
   &lt;td&gt;658.8&lt;/td&gt; 
   &lt;td&gt;849MB&lt;/td&gt; 
   &lt;td&gt;26.77&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;- int16&lt;/td&gt; 
   &lt;td&gt;733.0&lt;/td&gt; 
   &lt;td&gt;672MB&lt;/td&gt; 
   &lt;td&gt;26.82&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;- int8&lt;/td&gt; 
   &lt;td&gt;860.2&lt;/td&gt; 
   &lt;td&gt;529MB&lt;/td&gt; 
   &lt;td&gt;26.78&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;- int8 + vmap&lt;/td&gt; 
   &lt;td&gt;1126.2&lt;/td&gt; 
   &lt;td&gt;598MB&lt;/td&gt; 
   &lt;td&gt;26.64&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;OPUS-MT model&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Transformers 4.26.1 (with PyTorch 1.13.1)&lt;/td&gt; 
   &lt;td&gt;147.3&lt;/td&gt; 
   &lt;td&gt;2332MB&lt;/td&gt; 
   &lt;td&gt;27.90&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Marian 1.11.0&lt;/td&gt; 
   &lt;td&gt;344.5&lt;/td&gt; 
   &lt;td&gt;7605MB&lt;/td&gt; 
   &lt;td&gt;27.93&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;- int16&lt;/td&gt; 
   &lt;td&gt;330.2&lt;/td&gt; 
   &lt;td&gt;5901MB&lt;/td&gt; 
   &lt;td&gt;27.65&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;- int8&lt;/td&gt; 
   &lt;td&gt;355.8&lt;/td&gt; 
   &lt;td&gt;4763MB&lt;/td&gt; 
   &lt;td&gt;27.27&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;CTranslate2 3.6.0&lt;/td&gt; 
   &lt;td&gt;525.0&lt;/td&gt; 
   &lt;td&gt;721MB&lt;/td&gt; 
   &lt;td&gt;27.92&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;- int16&lt;/td&gt; 
   &lt;td&gt;596.1&lt;/td&gt; 
   &lt;td&gt;660MB&lt;/td&gt; 
   &lt;td&gt;27.53&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;- int8&lt;/td&gt; 
   &lt;td&gt;696.1&lt;/td&gt; 
   &lt;td&gt;516MB&lt;/td&gt; 
   &lt;td&gt;27.65&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Executed with 4 threads on a &lt;a href=&quot;https://aws.amazon.com/ec2/instance-types/c5/&quot;&gt;&lt;em&gt;c5.2xlarge&lt;/em&gt;&lt;/a&gt; Amazon EC2 instance equipped with an Intel(R) Xeon(R) Platinum 8275CL CPU.&lt;/p&gt; 
&lt;h4&gt;GPU&lt;/h4&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th&gt;Tokens per second&lt;/th&gt; 
   &lt;th&gt;Max. GPU memory&lt;/th&gt; 
   &lt;th&gt;Max. CPU memory&lt;/th&gt; 
   &lt;th&gt;BLEU&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;OpenNMT-tf WMT14 model&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenNMT-tf 2.31.0 (with TensorFlow 2.11.0)&lt;/td&gt; 
   &lt;td&gt;1483.5&lt;/td&gt; 
   &lt;td&gt;3031MB&lt;/td&gt; 
   &lt;td&gt;3122MB&lt;/td&gt; 
   &lt;td&gt;26.94&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;OpenNMT-py WMT14 model&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenNMT-py 3.0.4 (with PyTorch 1.13.1)&lt;/td&gt; 
   &lt;td&gt;1795.2&lt;/td&gt; 
   &lt;td&gt;2973MB&lt;/td&gt; 
   &lt;td&gt;3099MB&lt;/td&gt; 
   &lt;td&gt;26.77&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;FasterTransformer 5.3&lt;/td&gt; 
   &lt;td&gt;6979.0&lt;/td&gt; 
   &lt;td&gt;2402MB&lt;/td&gt; 
   &lt;td&gt;1131MB&lt;/td&gt; 
   &lt;td&gt;26.77&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;- float16&lt;/td&gt; 
   &lt;td&gt;8592.5&lt;/td&gt; 
   &lt;td&gt;1360MB&lt;/td&gt; 
   &lt;td&gt;1135MB&lt;/td&gt; 
   &lt;td&gt;26.80&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;CTranslate2 3.6.0&lt;/td&gt; 
   &lt;td&gt;6634.7&lt;/td&gt; 
   &lt;td&gt;1261MB&lt;/td&gt; 
   &lt;td&gt;953MB&lt;/td&gt; 
   &lt;td&gt;26.77&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;- int8&lt;/td&gt; 
   &lt;td&gt;8567.2&lt;/td&gt; 
   &lt;td&gt;1005MB&lt;/td&gt; 
   &lt;td&gt;807MB&lt;/td&gt; 
   &lt;td&gt;26.85&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;- float16&lt;/td&gt; 
   &lt;td&gt;10990.7&lt;/td&gt; 
   &lt;td&gt;941MB&lt;/td&gt; 
   &lt;td&gt;807MB&lt;/td&gt; 
   &lt;td&gt;26.77&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;- int8 + float16&lt;/td&gt; 
   &lt;td&gt;8725.4&lt;/td&gt; 
   &lt;td&gt;813MB&lt;/td&gt; 
   &lt;td&gt;800MB&lt;/td&gt; 
   &lt;td&gt;26.83&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;OPUS-MT model&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Transformers 4.26.1 (with PyTorch 1.13.1)&lt;/td&gt; 
   &lt;td&gt;1022.9&lt;/td&gt; 
   &lt;td&gt;4097MB&lt;/td&gt; 
   &lt;td&gt;2109MB&lt;/td&gt; 
   &lt;td&gt;27.90&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Marian 1.11.0&lt;/td&gt; 
   &lt;td&gt;3241.0&lt;/td&gt; 
   &lt;td&gt;3381MB&lt;/td&gt; 
   &lt;td&gt;2156MB&lt;/td&gt; 
   &lt;td&gt;27.92&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;- float16&lt;/td&gt; 
   &lt;td&gt;3962.4&lt;/td&gt; 
   &lt;td&gt;3239MB&lt;/td&gt; 
   &lt;td&gt;1976MB&lt;/td&gt; 
   &lt;td&gt;27.94&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;CTranslate2 3.6.0&lt;/td&gt; 
   &lt;td&gt;5876.4&lt;/td&gt; 
   &lt;td&gt;1197MB&lt;/td&gt; 
   &lt;td&gt;754MB&lt;/td&gt; 
   &lt;td&gt;27.92&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;- int8&lt;/td&gt; 
   &lt;td&gt;7521.9&lt;/td&gt; 
   &lt;td&gt;1005MB&lt;/td&gt; 
   &lt;td&gt;792MB&lt;/td&gt; 
   &lt;td&gt;27.79&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;- float16&lt;/td&gt; 
   &lt;td&gt;9296.7&lt;/td&gt; 
   &lt;td&gt;909MB&lt;/td&gt; 
   &lt;td&gt;814MB&lt;/td&gt; 
   &lt;td&gt;27.90&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;- int8 + float16&lt;/td&gt; 
   &lt;td&gt;8362.7&lt;/td&gt; 
   &lt;td&gt;813MB&lt;/td&gt; 
   &lt;td&gt;766MB&lt;/td&gt; 
   &lt;td&gt;27.90&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Executed with CUDA 11 on a &lt;a href=&quot;https://aws.amazon.com/ec2/instance-types/g5/&quot;&gt;&lt;em&gt;g5.xlarge&lt;/em&gt;&lt;/a&gt; Amazon EC2 instance equipped with a NVIDIA A10G GPU (driver version: 510.47.03).&lt;/p&gt; 
&lt;h2&gt;Additional resources&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://opennmt.net/CTranslate2&quot;&gt;Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://forum.opennmt.net&quot;&gt;Forum&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://gitter.im/OpenNMT/CTranslate2&quot;&gt;Gitter&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
  </channel>
</rss>
