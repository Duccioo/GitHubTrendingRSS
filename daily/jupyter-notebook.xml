<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>GitHub Jupyter Notebook Daily Trending</title>
    <description>Daily Trending of Jupyter Notebook in GitHub</description>
    <pubDate>Sun, 16 Mar 2025 01:32:12 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>dusty-nv/jetson-containers</title>
      <link>https://github.com/dusty-nv/jetson-containers</link>
      <description>&lt;p&gt;Machine Learning Containers for NVIDIA Jetson and JetPack-L4T&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&quot;https://www.jetson-ai-lab.com&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/docs/docs/images/header_blueprint_rainbow.jpg&quot; alt=&quot;a header for a software project about building containers for AI and machine learning&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Machine Learning Containers for Jetson and JetPack&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/l4t/l4t-pytorch&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/actions/workflow/status/dusty-nv/jetson-containers/l4t-pytorch_jp51.yml?label=l4t-pytorch&quot; alt=&quot;l4t-pytorch&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/l4t/l4t-tensorflow&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/actions/workflow/status/dusty-nv/jetson-containers/l4t-tensorflow-tf2_jp51.yml?label=l4t-tensorflow&quot; alt=&quot;l4t-tensorflow&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/l4t/l4t-ml&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/actions/workflow/status/dusty-nv/jetson-containers/l4t-ml_jp51.yml?label=l4t-ml&quot; alt=&quot;l4t-ml&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/l4t/l4t-diffusion&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/actions/workflow/status/dusty-nv/jetson-containers/l4t-diffusion_jp51.yml?label=l4t-diffusion&quot; alt=&quot;l4t-diffusion&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/l4t/l4t-text-generation&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/actions/workflow/status/dusty-nv/jetson-containers/l4t-text-generation_jp60.yml?label=l4t-text-generation&quot; alt=&quot;l4t-text-generation&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Modular container build system that provides the latest &lt;a href=&quot;http://jetson.webredirect.org/&quot;&gt;&lt;strong&gt;AI/ML packages&lt;/strong&gt;&lt;/a&gt; for &lt;a href=&quot;https://developer.nvidia.com/embedded-computing&quot;&gt;NVIDIA Jetson&lt;/a&gt; &lt;span&gt;ðŸš€&lt;/span&gt;&lt;span&gt;ðŸ¤–&lt;/span&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;br&gt; Ubuntu 24.04 containers for JetPack 6 are now available (with CUDA support)&lt;/p&gt; 
 &lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;code&gt;LSB_RELEASE=24.04 jetson-containers build pytorch:2.6&lt;/code&gt;&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;code&gt;jetson-containers run dustynv/pytorch:2.6-r36.4.0-cu128-24.04&lt;/code&gt;&lt;/p&gt; 
 &lt;p&gt;See the &lt;strong&gt;&lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/docs/build.md#2404-containers&quot;&gt;&lt;code&gt;Ubuntu 24.04&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt; section of the docs for details and a list of available containers ðŸ¤—&lt;br&gt; Thanks to all our active contributors from &lt;strong&gt;&lt;a href=&quot;https://discord.gg/BmqNSK4886&quot;&gt;&lt;code&gt;Discord&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt; for their help with the ongoing builds.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ML&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/pytorch&quot;&gt;&lt;code&gt;pytorch&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/ml/tensorflow&quot;&gt;&lt;code&gt;tensorflow&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/ml/jax&quot;&gt;&lt;code&gt;jax&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/ml/onnxruntime&quot;&gt;&lt;code&gt;onnxruntime&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/multimedia/deepstream&quot;&gt;&lt;code&gt;deepstream&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/multimedia/holoscan&quot;&gt;&lt;code&gt;holoscan&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/ml/ctranslate2&quot;&gt;&lt;code&gt;CTranslate2&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/ml/jupyterlab&quot;&gt;&lt;code&gt;JupyterLab&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;LLM&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/llm/sglang&quot;&gt;&lt;code&gt;SGLang&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/llm/vllm&quot;&gt;&lt;code&gt;vLLM&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/llm/mlc&quot;&gt;&lt;code&gt;MLC&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/llm/awq&quot;&gt;&lt;code&gt;AWQ&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/llm/transformers&quot;&gt;&lt;code&gt;transformers&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/llm/text-generation-webui&quot;&gt;&lt;code&gt;text-generation-webui&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/llm/ollama&quot;&gt;&lt;code&gt;ollama&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/llm/llama_cpp&quot;&gt;&lt;code&gt;llama.cpp&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/llm/llama-factory&quot;&gt;&lt;code&gt;llama-factory&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/llm/exllama&quot;&gt;&lt;code&gt;exllama&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/llm/auto_gptq&quot;&gt;&lt;code&gt;AutoGPTQ&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/llm/flash-attention&quot;&gt;&lt;code&gt;FlashAttention&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/llm/deepspeed&quot;&gt;&lt;code&gt;DeepSpeed&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/llm/bitsandbytes&quot;&gt;&lt;code&gt;bitsandbytes&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/llm/xformers&quot;&gt;&lt;code&gt;xformers&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;VLM&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/vlm/llava&quot;&gt;&lt;code&gt;llava&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/vlm/llama-vision&quot;&gt;&lt;code&gt;llama-vision&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/vlm/vila&quot;&gt;&lt;code&gt;VILA&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/vlm/lita&quot;&gt;&lt;code&gt;LITA&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/llm/nano_llm&quot;&gt;&lt;code&gt;NanoLLM&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/vlm/shape-llm&quot;&gt;&lt;code&gt;ShapeLLM&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/vlm/prismatic&quot;&gt;&lt;code&gt;Prismatic&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/vlm/xtuner&quot;&gt;&lt;code&gt;xtuner&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;VIT&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/vit/nanoowl&quot;&gt;&lt;code&gt;NanoOWL&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/vit/nanosam&quot;&gt;&lt;code&gt;NanoSAM&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/vit/sam&quot;&gt;&lt;code&gt;Segment Anything (SAM)&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/vit/tam&quot;&gt;&lt;code&gt;Track Anything (TAM)&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/vit/clip_trt&quot;&gt;&lt;code&gt;clip_trt&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;RAG&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/rag/llama-index&quot;&gt;&lt;code&gt;llama-index&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/rag/langchain&quot;&gt;&lt;code&gt;langchain&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/rag/jetson-copilot&quot;&gt;&lt;code&gt;jetson-copilot&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/vectordb/nanodb&quot;&gt;&lt;code&gt;NanoDB&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/vectordb/faiss&quot;&gt;&lt;code&gt;FAISS&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/ml/rapids/raft&quot;&gt;&lt;code&gt;RAFT&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;L4T&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/l4t/l4t-pytorch&quot;&gt;&lt;code&gt;l4t-pytorch&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/l4t/l4t-tensorflow&quot;&gt;&lt;code&gt;l4t-tensorflow&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/l4t/l4t-ml&quot;&gt;&lt;code&gt;l4t-ml&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/l4t/l4t-diffusion&quot;&gt;&lt;code&gt;l4t-diffusion&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/l4t/l4t-text-generation&quot;&gt;&lt;code&gt;l4t-text-generation&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;CUDA&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/cuda/cupy&quot;&gt;&lt;code&gt;cupy&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/cuda/cuda-python&quot;&gt;&lt;code&gt;cuda-python&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/cuda/pycuda&quot;&gt;&lt;code&gt;pycuda&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/numba&quot;&gt;&lt;code&gt;numba&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/opencv&quot;&gt;&lt;code&gt;opencv:cuda&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/ml/rapids/cudf&quot;&gt;&lt;code&gt;cudf&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/ml/rapids/cuml&quot;&gt;&lt;code&gt;cuml&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Robotics&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/robots/cosmos&quot;&gt;&lt;code&gt;Cosmos&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/robots/genesis&quot;&gt;&lt;code&gt;Genesis&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/ros&quot;&gt;&lt;code&gt;ROS&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/robots/lerobot&quot;&gt;&lt;code&gt;LeRobot&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/robots/openvla&quot;&gt;&lt;code&gt;OpenVLA&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/robots/3d_diffusion_policy&quot;&gt;&lt;code&gt;3D Diffusion Policy&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/robots/crossformer&quot;&gt;&lt;code&gt;Crossformer&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/robots/mimicgen&quot;&gt;&lt;code&gt;MimicGen&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/robots/opendronemap&quot;&gt;&lt;code&gt;OpenDroneMap&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/hardware/zed&quot;&gt;&lt;code&gt;ZED&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Graphics&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/diffusion/stable-diffusion-webui&quot;&gt;&lt;code&gt;stable-diffusion-webui&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/diffusion/comfyui&quot;&gt;&lt;code&gt;comfyui&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/nerf/nerfstudio&quot;&gt;&lt;code&gt;nerfstudio&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/nerf/meshlab&quot;&gt;&lt;code&gt;meshlab&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/nerf/pixsfm&quot;&gt;&lt;code&gt;pixsfm&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/nerf/gsplat&quot;&gt;&lt;code&gt;gsplat&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Mamba&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/mamba/mamba&quot;&gt;&lt;code&gt;mamba&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/mamba/mambavision&quot;&gt;&lt;code&gt;mambavision&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/mamba/cobra&quot;&gt;&lt;code&gt;cobra&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/mamba/dimba&quot;&gt;&lt;code&gt;dimba&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/mamba/videomambasuite&quot;&gt;&lt;code&gt;videomambasuite&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Speech&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/speech/whisper&quot;&gt;&lt;code&gt;whisper&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/speech/whisper_trt&quot;&gt;&lt;code&gt;whisper_trt&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/speech/piper-tts&quot;&gt;&lt;code&gt;piper&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/speech/riva-client&quot;&gt;&lt;code&gt;riva&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/speech/audiocraft&quot;&gt;&lt;code&gt;audiocraft&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/speech/voicecraft&quot;&gt;&lt;code&gt;voicecraft&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/speech/xtts&quot;&gt;&lt;code&gt;xtts&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Home/IoT&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/smart-home/homeassistant-core&quot;&gt;&lt;code&gt;homeassistant-core&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/smart-home/wyoming/wyoming-whisper&quot;&gt;&lt;code&gt;wyoming-whisper&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/smart-home/wyoming/openwakeword&quot;&gt;&lt;code&gt;wyoming-openwakeword&lt;/code&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/smart-home/wyoming/piper&quot;&gt;&lt;code&gt;wyoming-piper&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;See the &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages&quot;&gt;&lt;strong&gt;&lt;code&gt;packages&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt; directory for the full list, including pre-built container images for JetPack/L4T.&lt;/p&gt; 
&lt;p&gt;Using the included tools, you can easily combine packages together for building your own containers. Want to run ROS2 with PyTorch and Transformers? No problem - just do the &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/docs/setup.md&quot;&gt;system setup&lt;/a&gt;, and build it on your Jetson:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ jetson-containers build --name=my_container pytorch transformers ros:humble-desktop
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;There are shortcuts for running containers too - this will pull or build a &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/l4t/l4t-pytorch&quot;&gt;&lt;code&gt;l4t-pytorch&lt;/code&gt;&lt;/a&gt; image that&#39;s compatible:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ jetson-containers run $(autotag l4t-pytorch)
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;sup&gt;&lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/docs/run.md&quot;&gt;&lt;code&gt;jetson-containers run&lt;/code&gt;&lt;/a&gt; launches &lt;a href=&quot;https://docs.docker.com/engine/reference/commandline/run/&quot;&gt;&lt;code&gt;docker run&lt;/code&gt;&lt;/a&gt; with some added defaults (like &lt;code&gt;--runtime nvidia&lt;/code&gt;, mounted &lt;code&gt;/data&lt;/code&gt; cache and devices)&lt;/sup&gt;&lt;br&gt; &lt;sup&gt;&lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/docs/run.md#autotag&quot;&gt;&lt;code&gt;autotag&lt;/code&gt;&lt;/a&gt; finds a container image that&#39;s compatible with your version of JetPack/L4T - either locally, pulled from a registry, or by building it.&lt;/sup&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;If you look at any package&#39;s readme (like &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/l4t/l4t-pytorch&quot;&gt;&lt;code&gt;l4t-pytorch&lt;/code&gt;&lt;/a&gt;), it will have detailed instructions for running it.&lt;/p&gt; 
&lt;h4&gt;Changing CUDA Versions&lt;/h4&gt; 
&lt;p&gt;You can rebuild the container stack for different versions of CUDA by setting the &lt;code&gt;CUDA_VERSION&lt;/code&gt; variable:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;CUDA_VERSION=12.4 jetson-containers build transformers
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;It will then go off and either pull or build all the dependencies needed, including PyTorch and other packages that would be time-consuming to compile. There is a &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/docs/build.md#pip-server&quot;&gt;Pip server&lt;/a&gt; that caches the wheels to accelerate builds. You can also request specific versions of cuDNN, TensorRT, Python, and PyTorch with similar environment variables like &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/docs/build.md#changing-versions&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.jetson-ai-lab.com&quot;&gt;&lt;img align=&quot;right&quot; width=&quot;200&quot; height=&quot;200&quot; src=&quot;https://nvidia-ai-iot.github.io/jetson-generative-ai-playground/images/JON_Gen-AI-panels.png&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages&quot;&gt;Package List&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/docs/packages.md&quot;&gt;Package Definitions&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/docs/setup.md&quot;&gt;System Setup&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/docs/build.md&quot;&gt;Building Containers&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/docs/run.md&quot;&gt;Running Containers&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Check out the tutorials at the &lt;a href=&quot;https://www.jetson-ai-lab.com&quot;&gt;&lt;strong&gt;Jetson Generative AI Lab&lt;/strong&gt;&lt;/a&gt;!&lt;/p&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;Refer to the &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/docs/setup.md&quot;&gt;System Setup&lt;/a&gt; page for tips about setting up your Docker daemon and memory/storage tuning.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# install the container tools
git clone https://github.com/dusty-nv/jetson-containers
bash jetson-containers/install.sh

# automatically pull &amp;amp; run any container
jetson-containers run $(autotag l4t-pytorch)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or you can manually run a &lt;a href=&quot;https://hub.docker.com/r/dustynv&quot;&gt;container image&lt;/a&gt; of your choice without using the helper scripts above:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sudo docker run --runtime nvidia -it --rm --network=host dustynv/l4t-pytorch:r36.2.0
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Looking for the old jetson-containers? See the &lt;a href=&quot;https://github.com/dusty-nv/jetson-containers/tree/legacy&quot;&gt;&lt;code&gt;legacy&lt;/code&gt;&lt;/a&gt; branch.&lt;/p&gt; 
&lt;h2&gt;Gallery&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=UOjqF3YCGkY&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/docs/docs/images/llamaspeak_llava_clip.gif&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=9ObzbbBTbcc&quot;&gt;Multimodal Voice Chat with LLaVA-1.5 13B on NVIDIA Jetson AGX Orin&lt;/a&gt; (container: &lt;a href=&quot;https://dusty-nv.github.io/NanoLLM/&quot;&gt;&lt;code&gt;NanoLLM&lt;/code&gt;&lt;/a&gt;)&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;br&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=hswNSZTvEFE&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/docs/docs/images/llamaspeak_70b_yt.jpg&quot; width=&quot;800px&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=wzLHAgDxMjQ&quot;&gt;Interactive Voice Chat with Llama-2-70B on NVIDIA Jetson AGX Orin&lt;/a&gt; (container: &lt;a href=&quot;https://dusty-nv.github.io/NanoLLM/&quot;&gt;&lt;code&gt;NanoLLM&lt;/code&gt;&lt;/a&gt;)&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;br&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=OJT-Ax0CkhU&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/docs/docs/images/nanodb_tennis.jpg&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=wzLHAgDxMjQ&quot;&gt;Realtime Multimodal VectorDB on NVIDIA Jetson&lt;/a&gt; (container: &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/vectordb/nanodb&quot;&gt;&lt;code&gt;nanodb&lt;/code&gt;&lt;/a&gt;)&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;br&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.jetson-ai-lab.com/tutorial_nanoowl.html&quot;&gt;&lt;img src=&quot;https://github.com/NVIDIA-AI-IOT/nanoowl/raw/main/assets/jetson_person_2x.gif&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;a href=&quot;https://www.jetson-ai-lab.com/tutorial_nanoowl.html&quot;&gt;NanoOWL - Open Vocabulary Object Detection ViT&lt;/a&gt; (container: &lt;a href=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/vit/nanoowl&quot;&gt;&lt;code&gt;nanoowl&lt;/code&gt;&lt;/a&gt;)&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=w48i8FmVvLA&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/docs/docs/images/live_llava.gif&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;a href=&quot;https://youtu.be/X-OXxPiUTuU&quot;&gt;Live Llava on Jetson AGX Orin&lt;/a&gt; (container: &lt;a href=&quot;https://dusty-nv.github.io/NanoLLM/&quot;&gt;&lt;code&gt;NanoLLM&lt;/code&gt;&lt;/a&gt;)&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=wZq7ynbgRoE&quot;&gt;&lt;img width=&quot;640px&quot; src=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/docs/docs/images/live_llava_bear.jpg&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;a href=&quot;https://youtu.be/X-OXxPiUTuU&quot;&gt;Live Llava 2.0 - VILA + Multimodal NanoDB on Jetson Orin&lt;/a&gt; (container: &lt;a href=&quot;https://dusty-nv.github.io/NanoLLM/&quot;&gt;&lt;code&gt;NanoLLM&lt;/code&gt;&lt;/a&gt;)&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.jetson-ai-lab.com/tutorial_slm.html&quot;&gt;&lt;img src=&quot;https://www.jetson-ai-lab.com/images/slm_console.gif&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;a href=&quot;https://www.jetson-ai-lab.com/tutorial_slm.html&quot;&gt;Small Language Models (SLM) on Jetson Orin Nano&lt;/a&gt; (container: &lt;a href=&quot;https://dusty-nv.github.io/NanoLLM/&quot;&gt;&lt;code&gt;NanoLLM&lt;/code&gt;&lt;/a&gt;)&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.jetson-ai-lab.com/tutorial_nano-vlm.html#video-sequences&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/dusty-nv/jetson-containers/docs/docs/images/video_vila_wildfire.gif&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;a href=&quot;https://www.jetson-ai-lab.com/tutorial_nano-vlm.html#video-sequences&quot;&gt;Realtime Video Vision/Language Model with VILA1.5-3b&lt;/a&gt; (container: &lt;a href=&quot;https://dusty-nv.github.io/NanoLLM/&quot;&gt;&lt;code&gt;NanoLLM&lt;/code&gt;&lt;/a&gt;)&lt;/p&gt; 
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>ageron/handson-ml2</title>
      <link>https://github.com/ageron/handson-ml2</link>
      <description>&lt;p&gt;A series of Jupyter notebooks that walk you through the fundamentals of Machine Learning and Deep Learning in Python using Scikit-Learn, Keras and TensorFlow 2.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Machine Learning Notebooks&lt;/h1&gt; 
&lt;h1&gt;âš  The 3rd edition of my book will be released in October 2022. The notebooks are available at &lt;a href=&quot;https://github.com/ageron/handson-ml3&quot;&gt;ageron/handson-ml3&lt;/a&gt; and contain more up-to-date code.&lt;/h1&gt; 
&lt;p&gt;This project aims at teaching you the fundamentals of Machine Learning in python. It contains the example code and solutions to the exercises in the second edition of my O&#39;Reilly book &lt;a href=&quot;https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/&quot;&gt;Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow&lt;/a&gt;:&lt;/p&gt; 
&lt;img src=&quot;https://images-na.ssl-images-amazon.com/images/I/51aqYc1QyrL._SX379_BO1,204,203,200_.jpg&quot; title=&quot;book&quot; width=&quot;150&quot;&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: If you are looking for the first edition notebooks, check out &lt;a href=&quot;https://github.com/ageron/handson-ml&quot;&gt;ageron/handson-ml&lt;/a&gt;. For the third edition, check out &lt;a href=&quot;https://github.com/ageron/handson-ml3&quot;&gt;ageron/handson-ml3&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;h3&gt;Want to play with these notebooks online without having to install anything?&lt;/h3&gt; 
&lt;p&gt;Use any of the following services (I recommended Colab or Kaggle, since they offer free GPUs and TPUs).&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;WARNING&lt;/strong&gt;: &lt;em&gt;Please be aware that these services provide temporary environments: anything you do will be deleted after a while, so make sure you download any data you care about.&lt;/em&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://colab.research.google.com/github/ageron/handson-ml2/blob/master/&quot; target=&quot;_parent&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://homl.info/kaggle/&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://mybinder.org/v2/gh/ageron/handson-ml2/HEAD?filepath=%2Findex.ipynb&quot;&gt;&lt;img src=&quot;https://mybinder.org/badge_logo.svg?sanitize=true&quot; alt=&quot;Launch binder&quot;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://homl.info/deepnote/&quot;&gt;&lt;img src=&quot;https://deepnote.com/buttons/launch-in-deepnote-small.svg?sanitize=true&quot; alt=&quot;Launch in Deepnote&quot;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Just want to quickly look at some notebooks, without executing any code?&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://nbviewer.jupyter.org/github/ageron/handson-ml2/blob/master/index.ipynb&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jupyter/design/master/logos/Badges/nbviewer_badge.svg?sanitize=true&quot; alt=&quot;Render nbviewer&quot;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/ageron/handson-ml2/raw/master/index.ipynb&quot;&gt;github.com&#39;s notebook viewer&lt;/a&gt; also works but it&#39;s not ideal: it&#39;s slower, the math equations are not always displayed correctly, and large notebooks often fail to open.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Want to run this project using a Docker image?&lt;/h3&gt; 
&lt;p&gt;Read the &lt;a href=&quot;https://github.com/ageron/handson-ml2/tree/master/docker&quot;&gt;Docker instructions&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Want to install this project on your own machine?&lt;/h3&gt; 
&lt;p&gt;Start by installing &lt;a href=&quot;https://www.anaconda.com/distribution/&quot;&gt;Anaconda&lt;/a&gt; (or &lt;a href=&quot;https://docs.conda.io/en/latest/miniconda.html&quot;&gt;Miniconda&lt;/a&gt;), &lt;a href=&quot;https://git-scm.com/downloads&quot;&gt;git&lt;/a&gt;, and if you have a TensorFlow-compatible GPU, install the &lt;a href=&quot;https://www.nvidia.com/Download/index.aspx&quot;&gt;GPU driver&lt;/a&gt;, as well as the appropriate version of CUDA and cuDNN (see TensorFlow&#39;s documentation for more details).&lt;/p&gt; 
&lt;p&gt;Next, clone this project by opening a terminal and typing the following commands (do not type the first &lt;code&gt;$&lt;/code&gt; signs on each line, they just indicate that these are terminal commands):&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/ageron/handson-ml2.git
$ cd handson-ml2
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Next, run the following commands:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ conda env create -f environment.yml
$ conda activate tf2
$ python -m ipykernel install --user --name=python3
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Finally, start Jupyter:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ jupyter notebook
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you need further instructions, read the &lt;a href=&quot;https://raw.githubusercontent.com/ageron/handson-ml2/master/INSTALL.md&quot;&gt;detailed installation instructions&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;FAQ&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;Which Python version should I use?&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;I recommend Python 3.8. If you follow the installation instructions above, that&#39;s the version you will get. Most code will work with other versions of Python 3, but some libraries do not support Python 3.9 or 3.10 yet, which is why I recommend Python 3.8.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;I&#39;m getting an error when I call &lt;code&gt;load_housing_data()&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Make sure you call &lt;code&gt;fetch_housing_data()&lt;/code&gt; &lt;em&gt;before&lt;/em&gt; you call &lt;code&gt;load_housing_data()&lt;/code&gt;. If you&#39;re getting an HTTP error, make sure you&#39;re running the exact same code as in the notebook (copy/paste it if needed). If the problem persists, please check your network configuration.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;I&#39;m getting an SSL error on MacOSX&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;You probably need to install the SSL certificates (see this &lt;a href=&quot;https://stackoverflow.com/questions/27835619/urllib-and-ssl-certificate-verify-failed-error&quot;&gt;StackOverflow question&lt;/a&gt;). If you downloaded Python from the official website, then run &lt;code&gt;/Applications/Python\ 3.8/Install\ Certificates.command&lt;/code&gt; in a terminal (change &lt;code&gt;3.8&lt;/code&gt; to whatever version you installed). If you installed Python using MacPorts, run &lt;code&gt;sudo port install curl-ca-bundle&lt;/code&gt; in a terminal.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;I&#39;ve installed this project locally. How do I update it to the latest version?&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;See &lt;a href=&quot;https://raw.githubusercontent.com/ageron/handson-ml2/master/INSTALL.md&quot;&gt;INSTALL.md&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;How do I update my Python libraries to the latest versions, when using Anaconda?&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;See &lt;a href=&quot;https://raw.githubusercontent.com/ageron/handson-ml2/master/INSTALL.md&quot;&gt;INSTALL.md&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Contributors&lt;/h2&gt; 
&lt;p&gt;I would like to thank everyone &lt;a href=&quot;https://github.com/ageron/handson-ml2/graphs/contributors&quot;&gt;who contributed to this project&lt;/a&gt;, either by providing useful feedback, filing issues or submitting Pull Requests. Special thanks go to Haesun Park and Ian Beauregard who reviewed every notebook and submitted many PRs, including help on some of the exercise solutions. Thanks as well to Steven Bunkley and Ziembla who created the &lt;code&gt;docker&lt;/code&gt; directory, and to github user SuperYorio who helped on some exercise solutions.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>langchain-ai/langchain</title>
      <link>https://github.com/langchain-ai/langchain</link>
      <description>&lt;p&gt;ðŸ¦œðŸ”— Build context-aware reasoning applications&lt;/p&gt;&lt;hr&gt;&lt;picture&gt; 
 &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;docs/static/img/logo-dark.svg&quot;&gt; 
 &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;docs/static/img/logo-light.svg&quot;&gt; 
 &lt;img alt=&quot;LangChain Logo&quot; src=&quot;https://raw.githubusercontent.com/langchain-ai/langchain/master/docs/static/img/logo-dark.svg?sanitize=true&quot; width=&quot;80%&quot;&gt; 
&lt;/picture&gt; 
&lt;div&gt; 
 &lt;br&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/langchain-ai/langchain/releases&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/release/langchain-ai/langchain?style=flat-square&quot; alt=&quot;Release Notes&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/langchain-ai/langchain/actions/workflows/check_diffs.yml&quot;&gt;&lt;img src=&quot;https://github.com/langchain-ai/langchain/actions/workflows/check_diffs.yml/badge.svg?sanitize=true&quot; alt=&quot;CI&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://opensource.org/licenses/MIT&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/l/langchain-core?style=flat-square&quot; alt=&quot;PyPI - License&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://pypistats.org/packages/langchain-core&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/dm/langchain-core?style=flat-square&quot; alt=&quot;PyPI - Downloads&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://star-history.com/#langchain-ai/langchain&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/langchain-ai/langchain?style=flat-square&quot; alt=&quot;GitHub star chart&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/langchain-ai/langchain/issues&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/issues-raw/langchain-ai/langchain?style=flat-square&quot; alt=&quot;Open Issues&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/langchain-ai/langchain&quot;&gt;&lt;img src=&quot;https://img.shields.io/static/v1?label=Dev%20Containers&amp;amp;message=Open&amp;amp;color=blue&amp;amp;logo=visualstudiocode&amp;amp;style=flat-square&quot; alt=&quot;Open in Dev Containers&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://codespaces.new/langchain-ai/langchain&quot;&gt;&lt;img src=&quot;https://github.com/codespaces/badge.svg?sanitize=true&quot; alt=&quot;Open in GitHub Codespaces&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://twitter.com/langchainai&quot;&gt;&lt;img src=&quot;https://img.shields.io/twitter/url/https/twitter.com/langchainai.svg?style=social&amp;amp;label=Follow%20%40LangChainAI&quot; alt=&quot;Twitter&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Looking for the JS/TS library? Check out &lt;a href=&quot;https://github.com/langchain-ai/langchainjs&quot;&gt;LangChain.js&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;LangChain is a framework for building LLM-powered applications. It helps you chain together interoperable components and third-party integrations to simplify AI application development â€” all while future-proofing decisions as the underlying technology evolves.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install -U langchain
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To learn more about LangChain, check out &lt;a href=&quot;https://python.langchain.com/docs/introduction/&quot;&gt;the docs&lt;/a&gt;. If youâ€™re looking for more advanced customization or agent orchestration, check out &lt;a href=&quot;https://langchain-ai.github.io/langgraph/&quot;&gt;LangGraph&lt;/a&gt;, our framework for building controllable agent workflows.&lt;/p&gt; 
&lt;h2&gt;Why use LangChain?&lt;/h2&gt; 
&lt;p&gt;LangChain helps developers build applications powered by LLMs through a standard interface for models, embeddings, vector stores, and more.&lt;/p&gt; 
&lt;p&gt;Use LangChain for:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Real-time data augmentation&lt;/strong&gt;. Easily connect LLMs to diverse data sources and external / internal systems, drawing from LangChainâ€™s vast library of integrations with model providers, tools, vector stores, retrievers, and more.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Model interoperability&lt;/strong&gt;. Swap models in and out as your engineering team experiments to find the best choice for your applicationâ€™s needs. As the industry frontier evolves, adapt quickly â€” LangChainâ€™s abstractions keep you moving without losing momentum.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;LangChainâ€™s ecosystem&lt;/h2&gt; 
&lt;p&gt;While the LangChain framework can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools when building LLM applications.&lt;/p&gt; 
&lt;p&gt;To improve your LLM application development, pair LangChain with:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;http://www.langchain.com/langsmith&quot;&gt;LangSmith&lt;/a&gt; - Helpful for agent evals and observability. Debug poor-performing LLM app runs, evaluate agent trajectories, gain visibility in production, and improve performance over time.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://langchain-ai.github.io/langgraph/&quot;&gt;LangGraph&lt;/a&gt; - Build agents that can reliably handle complex tasks with LangGraph, our low-level agent orchestration framework. LangGraph offers customizable architecture, long-term memory, and human-in-the-loop workflows â€” and is trusted in production by companies like LinkedIn, Uber, Klarna, and GitLab.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://langchain-ai.github.io/langgraph/concepts/#langgraph-platform&quot;&gt;LangGraph Platform&lt;/a&gt; - Deploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. Discover, reuse, configure, and share agents across teams â€” and iterate quickly with visual prototyping in &lt;a href=&quot;https://langchain-ai.github.io/langgraph/concepts/langgraph_studio/&quot;&gt;LangGraph Studio&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Additional resources&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://python.langchain.com/docs/tutorials/&quot;&gt;Tutorials&lt;/a&gt;: Simple walkthroughs with guided examples on getting started with LangChain.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://python.langchain.com/docs/how_to/&quot;&gt;How-to Guides&lt;/a&gt;: Quick, actionable code snippets for topics such as tool calling, RAG use cases, and more.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://python.langchain.com/docs/concepts/&quot;&gt;Conceptual Guides&lt;/a&gt;: Explanations of key concepts behind the LangChain framework.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://python.langchain.com/api_reference/&quot;&gt;API Reference&lt;/a&gt;: Detailed reference on navigating base packages and integrations for LangChain.&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>fchollet/deep-learning-with-python-notebooks</title>
      <link>https://github.com/fchollet/deep-learning-with-python-notebooks</link>
      <description>&lt;p&gt;Jupyter notebooks for the code samples of the book &quot;Deep Learning with Python&quot;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Companion Jupyter notebooks for the book &quot;Deep Learning with Python&quot;&lt;/h1&gt; 
&lt;p&gt;This repository contains Jupyter notebooks implementing the code samples found in the book &lt;a href=&quot;https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&amp;amp;a_bid=76564dff&quot;&gt;Deep Learning with Python, 2nd Edition (Manning Publications)&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For readability, these notebooks only contain runnable code blocks and section titles, and omit everything else in the book: text paragraphs, figures, and pseudocode. &lt;strong&gt;If you want to be able to follow what&#39;s going on, I recommend reading the notebooks side by side with your copy of the book.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;These notebooks use TensorFlow 2.6.&lt;/p&gt; 
&lt;h2&gt;Table of contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://colab.research.google.com/github/fchollet/deep-learning-with-python-notebooks/blob/master/chapter02_mathematical-building-blocks.ipynb&quot;&gt;Chapter 2: The mathematical building blocks of neural networks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://colab.research.google.com/github/fchollet/deep-learning-with-python-notebooks/blob/master/chapter03_introduction-to-keras-and-tf.ipynb&quot;&gt;Chapter 3: Introduction to Keras and TensorFlow&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://colab.research.google.com/github/fchollet/deep-learning-with-python-notebooks/blob/master/chapter04_getting-started-with-neural-networks.ipynb&quot;&gt;Chapter 4: Getting started with neural networks: classification and regression&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://colab.research.google.com/github/fchollet/deep-learning-with-python-notebooks/blob/master/chapter05_fundamentals-of-ml.ipynb&quot;&gt;Chapter 5: Fundamentals of machine learning&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://colab.research.google.com/github/fchollet/deep-learning-with-python-notebooks/blob/master/chapter07_working-with-keras.ipynb&quot;&gt;Chapter 7: Working with Keras: a deep dive&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://colab.research.google.com/github/fchollet/deep-learning-with-python-notebooks/blob/master/chapter08_intro-to-dl-for-computer-vision.ipynb&quot;&gt;Chapter 8: Introduction to deep learning for computer vision&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Chapter 9: Advanced deep learning for computer vision 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://colab.research.google.com/github/fchollet/deep-learning-with-python-notebooks/blob/master/chapter09_part01_image-segmentation.ipynb&quot;&gt;Part 1: Image segmentation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://colab.research.google.com/github/fchollet/deep-learning-with-python-notebooks/blob/master/chapter09_part02_modern-convnet-architecture-patterns.ipynb&quot;&gt;Part 2: Modern convnet architecture patterns&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://colab.research.google.com/github/fchollet/deep-learning-with-python-notebooks/blob/master/chapter09_part03_interpreting-what-convnets-learn.ipynb&quot;&gt;Part 3: Interpreting what convnets learn&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://colab.research.google.com/github/fchollet/deep-learning-with-python-notebooks/blob/master/chapter10_dl-for-timeseries.ipynb&quot;&gt;Chapter 10: Deep learning for timeseries&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Chapter 11: Deep learning for text 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://colab.research.google.com/github/fchollet/deep-learning-with-python-notebooks/blob/master/chapter11_part01_introduction.ipynb&quot;&gt;Part 1: Introduction&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://colab.research.google.com/github/fchollet/deep-learning-with-python-notebooks/blob/master/chapter11_part02_sequence-models.ipynb&quot;&gt;Part 2: Sequence models&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://colab.research.google.com/github/fchollet/deep-learning-with-python-notebooks/blob/master/chapter11_part03_transformer.ipynb&quot;&gt;Part 3: Transformer&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://colab.research.google.com/github/fchollet/deep-learning-with-python-notebooks/blob/master/chapter11_part04_sequence-to-sequence-learning.ipynb&quot;&gt;Part 4: Sequence-to-sequence learning&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Chapter 12: Generative deep learning 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://colab.research.google.com/github/fchollet/deep-learning-with-python-notebooks/blob/master/chapter12_part01_text-generation.ipynb&quot;&gt;Part 1: Text generation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://colab.research.google.com/github/fchollet/deep-learning-with-python-notebooks/blob/master/chapter12_part02_deep-dream.ipynb&quot;&gt;Part 2: Deep Dream&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://colab.research.google.com/github/fchollet/deep-learning-with-python-notebooks/blob/master/chapter12_part03_neural-style-transfer.ipynb&quot;&gt;Part 3: Neural style transfer&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://colab.research.google.com/github/fchollet/deep-learning-with-python-notebooks/blob/master/chapter12_part04_variational-autoencoders.ipynb&quot;&gt;Part 4: Variational autoencoders&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://colab.research.google.com/github/fchollet/deep-learning-with-python-notebooks/blob/master/chapter12_part05_gans.ipynb&quot;&gt;Part 5: Generative adversarial networks&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://colab.research.google.com/github/fchollet/deep-learning-with-python-notebooks/blob/master/chapter13_best-practices-for-the-real-world.ipynb&quot;&gt;Chapter 13: Best practices for the real world&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://colab.research.google.com/github/fchollet/deep-learning-with-python-notebooks/blob/master/chapter14_conclusions.ipynb&quot;&gt;Chapter 14: Conclusions&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>Pierian-Data/Complete-Python-3-Bootcamp</title>
      <link>https://github.com/Pierian-Data/Complete-Python-3-Bootcamp</link>
      <description>&lt;p&gt;Course Files for Complete Python 3 Bootcamp Course on Udemy&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Complete-Python-3-Bootcamp&lt;/h1&gt; 
&lt;p&gt;Course Files for Complete Python 3 Bootcamp Course on Udemy&lt;/p&gt; 
&lt;p&gt;Copyright(Â©) by Pierian Data Inc.&lt;/p&gt; 
&lt;p&gt;Get it now for 95% off with the link: &lt;a href=&quot;https://www.udemy.com/complete-python-bootcamp/?couponCode=COMPLETE_GITHUB&quot;&gt;https://www.udemy.com/complete-python-bootcamp/?couponCode=COMPLETE_GITHUB&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Thanks!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>roboflow/notebooks</title>
      <link>https://github.com/roboflow/notebooks</link>
      <description>&lt;p&gt;This repository offers a comprehensive collection of tutorials on state-of-the-art computer vision models and techniques. Explore everything from foundational architectures like ResNet to cutting-edge models like YOLO11, RT-DETR, SAM 2, Florence-2, PaliGemma 2, and Qwen2.5VL.&lt;/p&gt;&lt;hr&gt;&lt;div align=&quot;center&quot;&gt; 
 &lt;p&gt; &lt;a align=&quot;center&quot; href=&quot;&quot; target=&quot;_blank&quot;&gt; &lt;img width=&quot;850&quot; src=&quot;https://raw.githubusercontent.com/roboflow/notebooks/main/assets/roboflow-notebooks-banner.png&quot;&gt; &lt;/a&gt; &lt;/p&gt; 
 &lt;br&gt; 
 &lt;p&gt;&lt;a href=&quot;https://github.com/roboflow/notebooks&quot;&gt;notebooks&lt;/a&gt; | &lt;a href=&quot;https://github.com/roboflow/inference&quot;&gt;inference&lt;/a&gt; | &lt;a href=&quot;https://github.com/autodistill/autodistill&quot;&gt;autodistill&lt;/a&gt; | &lt;a href=&quot;https://github.com/roboflow/roboflow-collect&quot;&gt;collect&lt;/a&gt;&lt;/p&gt; 
 &lt;br&gt; 
 &lt;div align=&quot;center&quot;&gt; 
  &lt;a href=&quot;https://youtube.com/roboflow&quot;&gt; &lt;img src=&quot;https://media.roboflow.com/notebooks/template/icons/purple/youtube.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949634652&quot; width=&quot;3%&quot;&gt; &lt;/a&gt; 
  &lt;img src=&quot;https://github.com/SkalskiP/SkalskiP/raw/master/icons/transparent.png&quot; width=&quot;3%&quot;&gt; 
  &lt;a href=&quot;https://roboflow.com&quot;&gt; &lt;img src=&quot;https://media.roboflow.com/notebooks/template/icons/purple/roboflow-app.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949746649&quot; width=&quot;3%&quot;&gt; &lt;/a&gt; 
  &lt;img src=&quot;https://github.com/SkalskiP/SkalskiP/raw/master/icons/transparent.png&quot; width=&quot;3%&quot;&gt; 
  &lt;a href=&quot;https://www.linkedin.com/company/roboflow-ai/&quot;&gt; &lt;img src=&quot;https://media.roboflow.com/notebooks/template/icons/purple/linkedin.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949633691&quot; width=&quot;3%&quot;&gt; &lt;/a&gt; 
  &lt;img src=&quot;https://github.com/SkalskiP/SkalskiP/raw/master/icons/transparent.png&quot; width=&quot;3%&quot;&gt; 
  &lt;a href=&quot;https://docs.roboflow.com&quot;&gt; &lt;img src=&quot;https://media.roboflow.com/notebooks/template/icons/purple/knowledge.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949634511&quot; width=&quot;3%&quot;&gt; &lt;/a&gt; 
  &lt;img src=&quot;https://github.com/SkalskiP/SkalskiP/raw/master/icons/transparent.png&quot; width=&quot;3%&quot;&gt; 
  &lt;a href=&quot;https://discuss.roboflow.com&quot;&gt; &lt;img src=&quot;https://media.roboflow.com/notebooks/template/icons/purple/forum.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949633584&quot; width=&quot;3%&quot;&gt; &lt;img src=&quot;https://github.com/SkalskiP/SkalskiP/raw/master/icons/transparent.png&quot; width=&quot;3%&quot;&gt; &lt;/a&gt;
  &lt;a href=&quot;https://blog.roboflow.com&quot;&gt; &lt;img src=&quot;https://media.roboflow.com/notebooks/template/icons/purple/blog.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949633605&quot; width=&quot;3%&quot;&gt; &lt;/a&gt;  
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;h2&gt;ðŸ‘‹ hello&lt;/h2&gt; 
&lt;p&gt;This repository offers a growing collection of computer vision tutorials. Learn to use SOTA models like YOLOv11, SAM 2, Florence-2, PaliGemma 2, and Qwen2.5-VL for tasks ranging from object detection, segmentation, and pose estimation to data extraction and OCR. Dive in and explore the exciting world of computer vision!&lt;/p&gt; 
&lt;!-- AUTOGENERATED-NOTEBOOKS-TABLE --&gt; 
&lt;!--
   WARNING: DO NOT EDIT THIS TABLE MANUALLY. IT IS AUTOMATICALLY GENERATED.
   HEAD OVER TO CONTRIBUTING.MD FOR MORE DETAILS ON HOW TO MAKE CHANGES PROPERLY.
--&gt; 
&lt;h2&gt;ðŸš€ model tutorials (49 notebooks)&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;strong&gt;notebook&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;strong&gt;open in colab / kaggle / sagemaker studio lab&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;strong&gt;complementary materials&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;strong&gt;repository / paper&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/zero-shot-object-detection-and-segmentation-with-yoloe.ipynb&quot;&gt;Zero-Shot Object Detection and Segmentation with YOLOE&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/zero-shot-object-detection-and-segmentation-with-yoloe.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/zero-shot-object-detection-and-segmentation-with-yoloe.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/THU-MIG/yoloe&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2503.07465&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2503.07465-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov12-object-detection-model.ipynb&quot;&gt;Fine-Tune YOLOv12 on Object Detection Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov12-object-detection-model.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov12-object-detection-model.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/train-yolov12-model&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/sunsmarterjie/yolov12&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2502.12524&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2502.12524-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/zero-shot-object-detection-with-qwen2-5-vl.ipynb&quot;&gt;Zero-Shot Object Detection with Qwen2.5-VL&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/zero-shot-object-detection-with-qwen2-5-vl.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/zero-shot-object-detection-with-qwen2-5-vl.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=xEfh0IR8Fvo&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen2.5-VL&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2502.13923&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2502.13923-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-finetune-qwen2-5-vl-for-json-data-extraction.ipynb&quot;&gt;Fine-Tune Qwen2.5-VL for JSON Data Extraction&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-finetune-qwen2-5-vl-for-json-data-extraction.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-finetune-qwen2-5-vl-for-json-data-extraction.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=xEfh0IR8Fvo&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen2.5-VL&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2502.13923&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2502.13923-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-finetune-paligemma2-on-detection-dataset.ipynb&quot;&gt;Fine-Tune PaliGemma2 on Object Detection Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-finetune-paligemma2-on-detection-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-finetune-paligemma2-on-detection-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/fine-tune-paligemma-2/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/google-research/big_vision/raw/main/big_vision/configs/proj/paligemma/README.md&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2412.03555&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2412.03555-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-finetune-paligemma2-for-json-data-extraction.ipynb&quot;&gt;Fine-Tune PaliGemma2 for JSON Data Extraction&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-finetune-paligemma2-for-json-data-extraction.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-finetune-paligemma2-for-json-data-extraction.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/fine-tune-paligemma-2/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/google-research/big_vision/raw/main/big_vision/configs/proj/paligemma/README.md&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2412.03555&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2412.03555-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-finetune-paligemma2-on-latex-ocr-dataset.ipynb&quot;&gt;Fine-Tune PaliGemma2 for LaTeX OCR&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-finetune-paligemma2-on-latex-ocr-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-finetune-paligemma2-on-latex-ocr-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/fine-tune-paligemma-2/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/google-research/big_vision/raw/main/big_vision/configs/proj/paligemma/README.md&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2412.03555&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2412.03555-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/fine-tune-sam-2.1.ipynb&quot;&gt;Fine-Tune SAM-2.1&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/fine-tune-sam-2.1.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/fine-tune-sam-2.1.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/fine-tune-sam-2-1/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=QnCGcFHZy9s&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/facebookresearch/sam2&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/openai-gpt-4o-fine-tuning.ipynb&quot;&gt;Fine-Tune GPT-4o on Object Detection Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/openai-gpt-4o-fine-tuning.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/openai-gpt-4o-fine-tuning.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/gpt-4o-object-detection/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=6Q6TieCBA4E&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolo11-object-detection-on-custom-dataset.ipynb&quot;&gt;Fine-Tune YOLO11 on Object Detection Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolo11-object-detection-on-custom-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolo11-object-detection-on-custom-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/yolov11-how-to-train-custom-data/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=jE_s4tVgPHA&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/ultralytics/ultralytics&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolo11-instance-segmentation-on-custom-dataset.ipynb&quot;&gt;Fine-Tune YOLO11 on Instance Segmentation Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolo11-instance-segmentation-on-custom-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolo11-instance-segmentation-on-custom-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=jE_s4tVgPHA&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/ultralytics/ultralytics&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-segment-images-with-sam-2.ipynb&quot;&gt;Segment Images with SAM2&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-segment-images-with-sam-2.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-segment-images-with-sam-2.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/what-is-segment-anything-2/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/live/Dv003fTyO-Y&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/facebookresearch/segment-anything-2&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2408.00714&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2408.00714-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-segment-videos-with-sam-2.ipynb&quot;&gt;Segment Videos with SAM2&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-segment-videos-with-sam-2.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-segment-videos-with-sam-2.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/what-is-segment-anything-2/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/live/Dv003fTyO-Y&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/facebookresearch/segment-anything-2&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2408.00714&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2408.00714-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-rt-detr-on-custom-dataset-with-transformers.ipynb&quot;&gt;Fine-Tune RT-DETR on Object Detection Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-rt-detr-on-custom-dataset-with-transformers.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-rt-detr-on-custom-dataset-with-transformers.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/train-rt-detr-custom-dataset-transformers/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/lyuwenyu/RT-DETR&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2304.08069&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2304.08069-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-finetune-florence-2-on-detection-dataset.ipynb&quot;&gt;Fine-Tune Florence-2 on Object Detection Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-finetune-florence-2-on-detection-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-finetune-florence-2-on-detection-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/fine-tune-florence-2-object-detection/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=i3KjYgxNH6w&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.06242&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2311.06242-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-run-different-vision-tasks-with-florence-2.ipynb&quot;&gt;Run Different Vision Tasks with Florence-2&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-run-different-vision-tasks-with-florence-2.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-run-different-vision-tasks-with-florence-2.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/florence-2/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=hj_ybcRdk5Y&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.06242&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2311.06242-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-finetune-paligemma-on-detection-dataset.ipynb&quot;&gt;Fine-Tune PaliGemma on Object Detection Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-finetune-paligemma-on-detection-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-finetune-paligemma-on-detection-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/how-to-fine-tune-paligemma/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=OMBmVInx68M&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/google-research/big_vision/raw/main/big_vision/configs/proj/paligemma/README.md&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2407.07726&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2407.07726-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov10-object-detection-on-custom-dataset.ipynb&quot;&gt;Fine-Tune YOLOv10 on Object Detection Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov10-object-detection-on-custom-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov10-object-detection-on-custom-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/yolov10-how-to-train/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/THU-MIG/yolov10&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2405.14458&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2405.14458-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/zero-shot-object-detection-with-yolo-world.ipynb&quot;&gt;Zero-Shot Object Detection with YOLO-World&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/zero-shot-object-detection-with-yolo-world.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/zero-shot-object-detection-with-yolo-world.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/what-is-yolo-world/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=X7gKBGVz4vs&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/AILab-CVC/YOLO-World&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2401.17270&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2401.17270-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov9-object-detection-on-custom-dataset.ipynb&quot;&gt;Fine-Tune YOLOv9 on Object Detection Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov9-object-detection-on-custom-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov9-object-detection-on-custom-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/train-yolov9-model&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://youtu.be/XHT2c8jT3Bc&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/WongKinYiu/yolov9&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2402.13616&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2402.13616-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-rtmdet-object-detection-on-custom-data.ipynb&quot;&gt;Fine-Tune RTMDet on Object Detection Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-rtmdet-object-detection-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-rtmdet-object-detection-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/how-to-train-rtmdet-on-a-custom-dataset&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://youtu.be/5kgWyo6Sg4E&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/open-mmlab/mmdetection&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2212.07784&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2212.07784-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-segment-anything-with-fast-sam.ipynb&quot;&gt;Segment Images with FastSAM&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-segment-anything-with-fast-sam.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-segment-anything-with-fast-sam.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/how-to-use-fastsam&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://youtu.be/yHNPyqazYYU&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/CASIA-IVA-Lab/FastSAM&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2306.12156&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2306.12156-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolo-nas-on-custom-dataset.ipynb&quot;&gt;Fine-Tune YOLO-NAS on Object Detection Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolo-nas-on-custom-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolo-nas-on-custom-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/yolo-nas-how-to-train-on-custom-dataset&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://youtu.be/V-H3eoPUnA8&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/Deci-AI/super-gradients/raw/master/YOLONAS.md&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-segment-anything-with-sam.ipynb&quot;&gt;Segment Images with Segment Anything Model (SAM)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-segment-anything-with-sam.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-segment-anything-with-sam.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/how-to-use-segment-anything-model-sam&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://youtu.be/D-D6ZmadzPE&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/facebookresearch/segment-anything&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2304.02643&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2304.02643-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/zero-shot-object-detection-with-grounding-dino.ipynb&quot;&gt;Zero-Shot Object Detection with Grounding DINO&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/zero-shot-object-detection-with-grounding-dino.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/zero-shot-object-detection-with-grounding-dino.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/grounding-dino-zero-shot-object-detection&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://youtu.be/cMa77r3YrDk&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/IDEA-Research/GroundingDINO&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2303.05499&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2303.05499-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-huggingface-detr-on-custom-dataset.ipynb&quot;&gt;Fine-Tune DETR Transformer on Object Detection Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-huggingface-detr-on-custom-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-huggingface-detr-on-custom-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://youtu.be/AM8D4j9KoaU&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://youtu.be/AM8D4j9KoaU&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/facebookresearch/detr&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2005.12872&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2005.12872-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/dinov2-classification.ipynb&quot;&gt;Classify Images with DINOv2&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/dinov2-classification.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/dinov2-classification.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/how-to-classify-images-with-dinov2/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/facebookresearch/dinov2/&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2304.07193&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2304.07193-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov8-object-detection-on-custom-dataset.ipynb&quot;&gt;Fine-Tune YOLOv8 on Object Detection Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov8-object-detection-on-custom-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov8-object-detection-on-custom-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/how-to-train-yolov8-on-a-custom-dataset&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://youtu.be/wuZtUMEiKWY&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/ultralytics/ultralytics&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov8-keypoint.ipynb&quot;&gt;Fine-Tune YOLOv8 on Pose Estimation Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov8-keypoint.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov8-keypoint.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/train-a-custom-yolov8-pose-estimation-model/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/ultralytics/ultralytics&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov8-obb.ipynb&quot;&gt;Fine-Tune YOLOv8 on Oriented Bounding Boxes (OBB) Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov8-obb.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov8-obb.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https:/blog.roboflow.com/train-yolov8-obb-model/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/ultralytics/ultralytics&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov8-instance-segmentation-on-custom-dataset.ipynb&quot;&gt;Fine-Tune YOLOv8 on Instance Segmentation Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov8-instance-segmentation-on-custom-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov8-instance-segmentation-on-custom-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/how-to-train-yolov8-instance-segmentation/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://youtu.be/pFiGSrRtaU4&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/ultralytics/ultralytics&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov8-classification-on-custom-dataset.ipynb&quot;&gt;Fine-Tune YOLOv8 on Classification Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov8-classification-on-custom-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov8-classification-on-custom-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/how-to-train-a-yolov8-classification-model/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/ultralytics/ultralytics&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov7-object-detection-on-custom-data.ipynb&quot;&gt;Fine-Tune YOLOv7 on Object Detection Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov7-object-detection-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov7-object-detection-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/yolov7-custom-dataset-training-tutorial&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=5nsmXLyDaU4&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/WongKinYiu/yolov7&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2207.02696&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2207.02696-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov7-instance-segmentation-on-custom-data.ipynb&quot;&gt;Fine-Tune YOLOv7 on Instance Segmentation Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov7-instance-segmentation-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov7-instance-segmentation-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/train-yolov7-instance-segmentation-on-custom-data&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=vFGxM2KLs10&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/WongKinYiu/yolov7&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2207.02696&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2207.02696-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov6-object-detection-on-custom-data.ipynb&quot;&gt;Fine-Tune MT-YOLOv6 on Object Detection Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov6-object-detection-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov6-object-detection-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/how-to-train-yolov6-on-a-custom-dataset&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=fFCWrMFH2UY&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/meituan/YOLOv6&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2209.02976&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2209.02976-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov5-object-detection-on-custom-data.ipynb&quot;&gt;Fine-Tune YOLOv5 on Object Detection Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov5-object-detection-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov5-object-detection-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/how-to-train-yolov5-on-a-custom-dataset&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://youtu.be/watch?v=x0ThXHbtqCQ&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/ultralytics/yolov5&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov5-classification-on-custom-data.ipynb&quot;&gt;Fine-Tune YOLOv5 on Classification Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov5-classification-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov5-classification-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/train-yolov5-classification-custom-data&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=DPjp9Kq4qn8&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/ultralytics/yolov5&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov5-instance-segmentation-on-custom-data.ipynb&quot;&gt;Fine-Tune YOLOv5 on Instance Segmentation Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov5-instance-segmentation-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov5-instance-segmentation-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/train-yolov5-instance-segmentation-custom-dataset&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=vKzfvtEtiYo&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/ultralytics/yolov5&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-detectron2-segmentation-on-custom-data.ipynb&quot;&gt;Fine-Tune Faster RCNN on Instance Segmentation Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-detectron2-segmentation-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-detectron2-segmentation-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/how-to-train-detectron2&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://youtu.be/e8LPflX0nwQ&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/facebookresearch/detectron2&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/1703.06870v3&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-1703.06870v3-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-segformer-segmentation-on-custom-data.ipynb&quot;&gt;Fine-Tune SegFormer on Instance Segmentation Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-segformer-segmentation-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-segformer-segmentation-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/how-to-train-segformer-on-a-custom-dataset-with-pytorch-lightning&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=4HNkBMfw-2o&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/NVlabs/SegFormer&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2105.15203v3&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2105.15203v3-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-vision-transformer-classification-on-custom-data.ipynb&quot;&gt;Fine-Tune ViT on Classification Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-vision-transformer-classification-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-vision-transformer-classification-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/how-to-train-vision-transformer&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=8yRE2Pa-8_I&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/lucidrains/vit-pytorch&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2010.11929&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2010.11929-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-scaled-yolov4-object-detection-on-custom-data.ipynb&quot;&gt;Fine-Tune Scaled-YOLOv4 on Object Detection Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-scaled-yolov4-object-detection-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-scaled-yolov4-object-detection-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/how-to-train-scaled-yolov4&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=rEbpKxZbvIo&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/WongKinYiu/ScaledYOLOv4&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2004.10934&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2004.10934-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolos-huggingface-object-detection-on-custom-data.ipynb&quot;&gt;Fine-Tune YOLOS on Object Detection Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolos-huggingface-object-detection-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolos-huggingface-object-detection-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/train-yolos-transformer-custom-dataset&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=N0V0xxSi6Xc&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/huggingface/transformers&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2106.00666&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2106.00666-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolor-object-detection-on-custom-data.ipynb&quot;&gt;Fine-Tune YOLOR on Object Detection Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolor-object-detection-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolor-object-detection-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/train-yolor-on-a-custom-dataset&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=sZ5DiXDOHEM&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/yolor&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/1506.02640&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-1506.02640-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolox-object-detection-on-custom-data.ipynb&quot;&gt;Fine-Tune YOLOX on Object Detection Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolox-object-detection-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolox-object-detection-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/how-to-train-yolox-on-a-custom-dataset&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=q3RbFbaQQGw&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/Megvii-BaseDetection/YOLOX&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2107.08430&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2107.08430-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-resnet34-classification.ipynb&quot;&gt;Fine-Tune ResNet34 on Classification Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-resnet34-classification.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-resnet34-classification.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/how-to-train-a-custom-resnet34-model&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=93kXzUOiYY4&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-use-openai-clip-classification.ipynb&quot;&gt;Image Classification with OpenAI Clip&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-use-openai-clip-classification.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-use-openai-clip-classification.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/how-to-use-openai-clip&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=8o701AEoZ8I&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/openai/CLIP&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2103.00020-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov4-tiny-object-detection-on-custom-data.ipynb&quot;&gt;Fine-Tune YOLOv4-tiny Darknet on Object Detection Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov4-tiny-object-detection-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov4-tiny-object-detection-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.ai/train-yolov4-tiny-on-custom-data-lighting-fast-detection&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=NTnZgLsk_DA&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/darknet&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2011.04244&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2011.04244-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-train-yolov8-classification-no-labeling.ipynb&quot;&gt;Train a YOLOv8 Classification Model with No Labeling&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-train-yolov8-classification-no-labeling.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-train-yolov8-classification-no-labeling.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/train-classification-model-no-labeling/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/ultralytics/ultralytics&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;ðŸ“¸ computer vision skills (21 notebooks)&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;strong&gt;notebook&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;strong&gt;open in colab / kaggle / sagemaker studio lab&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;strong&gt;complementary materials&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;strong&gt;repository / paper&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/football-ai.ipynb&quot;&gt;Football AI&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/football-ai.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/football-ai.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/camera-calibration-sports-computer-vision/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://youtu.be/aBVGKoNZQUw&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow/sports&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/grounded-sam-2-auto-label.ipynb&quot;&gt;Auto-Annotate Dataset with GroundedSAM 2&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/grounded-sam-2-auto-label.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/grounded-sam-2-auto-label.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/what-is-segment-anything-2&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/facebookresearch/segment-anything-2&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov7-object-detection-on-custom-data-openvino-torch-ort.ipynb&quot;&gt;Run YOLOv7 Object Detection with OpenVINO + TorchORT&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov7-object-detection-on-custom-data-openvino-torch-ort.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov7-object-detection-on-custom-data-openvino-torch-ort.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/accelerate-pytorch-openvino-torch-ort&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/yolov7&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2207.02696&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2207.02696-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-estimate-vehicle-speed-with-computer-vision.ipynb&quot;&gt;Estimate Vehicle Speed with YOLOv8&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-estimate-vehicle-speed-with-computer-vision.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-estimate-vehicle-speed-with-computer-vision.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/estimate-speed-computer-vision/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://youtu.be/uWP6UjDeZvY&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow/supervision/tree/develop/examples/speed_estimation&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-detect-and-count-objects-in-polygon-zone.ipynb&quot;&gt;Detect and Count Objects in Polygon Zone with YOLOv5 / YOLOv8 / Detectron2 + Supervision&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-detect-and-count-objects-in-polygon-zone.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-detect-and-count-objects-in-polygon-zone.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/how-to-count-objects-in-a-zone&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://youtu.be/l_kf9CfZ_8M&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow/supervision&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-track-and-count-vehicles-with-yolov8.ipynb&quot;&gt;Track and Count Vehicles with YOLOv8 + ByteTRACK + Supervision&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-track-and-count-vehicles-with-yolov8.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-track-and-count-vehicles-with-yolov8.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/yolov8-tracking-and-counting/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://youtu.be/OS5qI9YBkfk&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow/supervision&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2110.06864&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2110.06864-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-track-football-players.ipynb&quot;&gt;Football Players Tracking with YOLOv5 + ByteTRACK&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-track-football-players.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-track-football-players.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/track-football-players&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://youtu.be/QCG8QMhga9k&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/ifzhang/ByteTrack&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2110.06864&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2110.06864-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-auto-train-yolov8-model-with-autodistill.ipynb&quot;&gt;Auto Train YOLOv8 Model with Autodistill&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-auto-train-yolov8-model-with-autodistill.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-auto-train-yolov8-model-with-autodistill.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/autodistill&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://youtu.be/gKTYMfwPo4M&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/autodistill/autodistill&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/image_embeddings_analysis_part_1.ipynb&quot;&gt;Image Embeddings Analysis - Part 1&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/image_embeddings_analysis_part_1.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/image_embeddings_analysis_part_1.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://youtu.be/YxJkE6FvGF4&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/openai/CLIP&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2103.00020-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/automated-dataset-annotation-and-evaluation-with-grounding-dino-and-sam.ipynb&quot;&gt;Automated Dataset Annotation and Evaluation with Grounding DINO and SAM&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/automated-dataset-annotation-and-evaluation-with-grounding-dino-and-sam.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/automated-dataset-annotation-and-evaluation-with-grounding-dino-and-sam.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/enhance-image-annotation-with-grounding-dino-and-sam/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://youtu.be/oEQYStnF2l8&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/IDEA-Research/GroundingDINO&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2303.05499&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2303.05499-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/automated-dataset-annotation-and-evaluation-with-grounding-dino.ipynb&quot;&gt;Automated Dataset Annotation and Evaluation with Grounding DINO&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/automated-dataset-annotation-and-evaluation-with-grounding-dino.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/automated-dataset-annotation-and-evaluation-with-grounding-dino.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://youtu.be/C4NqaRBz_Kw&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/IDEA-Research/GroundingDINO&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2303.05499&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2303.05499-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/roboflow_video_inference_with_custom_annotators.ipynb&quot;&gt;Roboflow Video Inference with Custom Annotators&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/roboflow_video_inference_with_custom_annotators.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/roboflow_video_inference_with_custom_annotators.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/custom-annotator-video-inference&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow/inference&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/dino-gpt4v-autodistill.ipynb&quot;&gt;DINO-GPT-4V Object Detection&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/dino-gpt4v-autodistill.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/dino-gpt4v-autodistill.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/dino-gpt-4v/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-segmentation-model-with-no-labeling.ipynb&quot;&gt;Train a Segmentation Model with No Labeling&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-segmentation-model-with-no-labeling.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-segmentation-model-with-no-labeling.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/train-a-segmentation-model-no-labeling/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/autodistill/autodistill&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/dinov2-image-retrieval.ipynb&quot;&gt;DINOv2 Image Retrieval&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/dinov2-image-retrieval.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/dinov2-image-retrieval.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/facebookresearch/dinov2/&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2304.07193&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2304.07193-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/vector-analysis-with-sklearn-and-bokeh.ipynb&quot;&gt;Vector Analysis with Scikit-learn and Bokeh&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/vector-analysis-with-sklearn-and-bokeh.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/vector-analysis-with-sklearn-and-bokeh.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/vector-analysis&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-use-rf100.ipynb&quot;&gt;RF100 Object Detection Model Benchmarking&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-use-rf100.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-use-rf100.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/roboflow-100&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://youtu.be/jIgZMr-PBMo&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/roboflow-100-benchmark&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2211.13523&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2211.13523-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-generate-segmentation-mask-with-roboflow.ipynb&quot;&gt;Create Segmentation Masks with Roboflow&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-generate-segmentation-mask-with-roboflow.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-generate-segmentation-mask-with-roboflow.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/how-to-create-segmentation-masks-with-roboflow&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-use-polygonzone-annotate-and-supervision.ipynb&quot;&gt;How to Use PolygonZone and Roboflow Supervision&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-use-polygonzone-annotate-and-supervision.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-use-polygonzone-annotate-and-supervision.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/polygonzone/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-package-detector-two-labeled-images.ipynb&quot;&gt;Train a Package Detector With Two Labeled Images&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-package-detector-two-labeled-images.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-package-detector-two-labeled-images.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/package-detector/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/autodistill/autodistill-seggpt&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/image-to-image-search-clip-faiss.ipynb&quot;&gt;Image-to-Image Search with CLIP and faiss&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/image-to-image-search-clip-faiss.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/image-to-image-search-clip-faiss.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/clip-image-search-faiss/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;!-- AUTOGENERATED-NOTEBOOKS-TABLE --&gt; 
&lt;h2&gt;ðŸŽ¬ videos&lt;/h2&gt; 
&lt;p&gt;Almost every week we create tutorials showing you the hottest models in Computer Vision. ðŸ”¥ &lt;a href=&quot;https://www.youtube.com/@Roboflow&quot;&gt;Subscribe&lt;/a&gt;, and stay up to date with our latest YouTube videos!&lt;/p&gt; 
&lt;p align=&quot;left&quot;&gt; &lt;a href=&quot;https://youtu.be/CilXrt3S-ws&quot; title=&quot;How to Choose the Best Computer Vision Model for Your Project&quot;&gt;&lt;img src=&quot;https://github.com/roboflow/notebooks/assets/26109316/73a01d3b-cf70-40c3-a5e4-e4bc5be38d42&quot; alt=&quot;How to Choose the Best Computer Vision Model for Your Project&quot; width=&quot;300px&quot; align=&quot;left&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://youtu.be/CilXrt3S-ws&quot; title=&quot;How to Choose the Best Computer Vision Model for Your Project&quot;&gt;&lt;strong&gt;How to Choose the Best Computer Vision Model for Your Project&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt;
&lt;div&gt;
 &lt;strong&gt;Created: 26 May 2023&lt;/strong&gt; | 
 &lt;strong&gt;Updated: 26 May 2023&lt;/strong&gt;
&lt;/div&gt; 
&lt;br&gt; In this video, we will dive into the complexity of choosing the right computer vision model for your unique project. From the importance of high-quality datasets to hardware considerations, interoperability, benchmarking, and licensing issues, this video covers it all... 
&lt;p&gt;&lt;/p&gt; 
&lt;br&gt; 
&lt;p align=&quot;left&quot;&gt; &lt;a href=&quot;https://youtu.be/oEQYStnF2l8&quot; title=&quot;Accelerate Image Annotation with SAM and Grounding DINO&quot;&gt;&lt;img src=&quot;https://github.com/SkalskiP/SkalskiP/assets/26109316/ae1ca38e-40b7-4b35-8582-e8ea5de3806e&quot; alt=&quot;Accelerate Image Annotation with SAM and Grounding DINO&quot; width=&quot;300px&quot; align=&quot;left&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://youtu.be/oEQYStnF2l8&quot; title=&quot;Accelerate Image Annotation with SAM and Grounding DINO&quot;&gt;&lt;strong&gt;Accelerate Image Annotation with SAM and Grounding DINO&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt;
&lt;div&gt;
 &lt;strong&gt;Created: 20 Apr 2023&lt;/strong&gt; | 
 &lt;strong&gt;Updated: 20 Apr 2023&lt;/strong&gt;
&lt;/div&gt; 
&lt;br&gt; Discover how to speed up your image annotation process using Grounding DINO and Segment Anything Model (SAM). Learn how to convert object detection datasets into instance segmentation datasets, and see the potential of using these models to automatically annotate your datasets for real-time detectors like YOLOv8... 
&lt;p&gt;&lt;/p&gt; 
&lt;br&gt; 
&lt;p align=&quot;left&quot;&gt; &lt;a href=&quot;https://youtu.be/D-D6ZmadzPE&quot; title=&quot;SAM - Segment Anything Model by Meta AI: Complete Guide&quot;&gt;&lt;img src=&quot;https://github.com/SkalskiP/SkalskiP/assets/26109316/6913ff11-53c6-4341-8d90-eaff3023c3fd&quot; alt=&quot;SAM - Segment Anything Model by Meta AI: Complete Guide&quot; width=&quot;300px&quot; align=&quot;left&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://youtu.be/D-D6ZmadzPE&quot; title=&quot;SAM - Segment Anything Model by Meta AI: Complete Guide&quot;&gt;&lt;strong&gt;SAM - Segment Anything Model by Meta AI: Complete Guide&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt;
&lt;div&gt;
 &lt;strong&gt;Created: 11 Apr 2023&lt;/strong&gt; | 
 &lt;strong&gt;Updated: 11 Apr 2023&lt;/strong&gt;
&lt;/div&gt; 
&lt;p&gt;&lt;br&gt; Discover the incredible potential of Meta AI&#39;s Segment Anything Model (SAM)! We dive into SAM, an efficient and promptable model for image segmentation, which has revolutionized computer vision tasks. With over 1 billion masks on 11M licensed and privacy-respecting images, SAM&#39;s zero-shot performance is often superior to prior fully supervised results... &lt;/p&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;h2&gt;ðŸ’» run locally&lt;/h2&gt; 
&lt;p&gt;We try to make it as easy as possible to run Roboflow Notebooks in Colab and Kaggle, but if you still want to run them locally, below you will find instructions on how to do it. Remember don&#39;t install your dependencies globally, use &lt;a href=&quot;https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments/&quot;&gt;venv&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-console&quot;&gt;# clone repository and navigate to root directory
git clone git@github.com:roboflow-ai/notebooks.git
cd notebooks

# setup python environment and activate it
python3 -m venv venv
source venv/bin/activate

# install and run jupyter notebook
pip install notebook
jupyter notebook
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;â˜ï¸ run in sagemaker studio lab&lt;/h2&gt; 
&lt;p&gt;You can now open our tutorial notebooks in &lt;a href=&quot;https://aws.amazon.com/sagemaker/studio-lab/&quot;&gt;Amazon SageMaker Studio Lab&lt;/a&gt; - a free machine learning development environment that provides the compute, storage, and securityâ€”all at no costâ€”for anyone to learn and experiment with ML.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align=&quot;center&quot;&gt;Stable Diffusion Image Generation&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;YOLOv5 Custom Dataset Training&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;YOLOv7 Custom Dataset Training&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://studiolab.sagemaker.aws/import/github/roboflow-ai/notebooks/blob/main/notebooks/sagemaker-studiolab/stable-diffusion-image-generation.ipynb&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/sage-maker.svg?sanitize=true&quot; alt=&quot;SageMaker&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://studiolab.sagemaker.aws/import/github/roboflow-ai/notebooks/blob/main/notebooks/sagemaker-studiolab/yolov5-custom-training.ipynb&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/sage-maker.svg?sanitize=true&quot; alt=&quot;SageMaker&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://studiolab.sagemaker.aws/import/github/roboflow-ai/notebooks/blob/main/notebooks/sagemaker-studiolab/yolov7-custom-training.ipynb&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/sage-maker.svg?sanitize=true&quot; alt=&quot;SageMaker&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;ðŸž bugs &amp;amp; ðŸ¦¸ contribution&lt;/h2&gt; 
&lt;p&gt;Computer Vision moves fast! Sometimes our notebooks lag a tad behind the ever-pushing forward libraries. If you notice that any of the notebooks is not working properly, create a &lt;a href=&quot;https://github.com/roboflow-ai/notebooks/issues/new?assignees=&amp;amp;labels=bug%2Ctriage&amp;amp;template=bug-report.yml&quot;&gt;bug report&lt;/a&gt; and let us know.&lt;/p&gt; 
&lt;p&gt;If you have an idea for a new tutorial we should do, create a &lt;a href=&quot;https://github.com/roboflow-ai/notebooks/issues/new?assignees=&amp;amp;labels=enhancement&amp;amp;template=feature-request.yml&quot;&gt;feature request&lt;/a&gt;. We are constantly looking for new ideas. If you feel up to the task and want to create a tutorial yourself, please take a peek at our &lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/CONTRIBUTING.md&quot;&gt;contribution guide&lt;/a&gt;. There you can find all the information you need.&lt;/p&gt; 
&lt;p&gt;We are here for you, so don&#39;t hesitate to &lt;a href=&quot;https://github.com/roboflow-ai/notebooks/discussions&quot;&gt;reach out&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>google-deepmind/gemma</title>
      <link>https://github.com/google-deepmind/gemma</link>
      <description>&lt;p&gt;Gemma open-weight LLM library, from Google DeepMind&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Gemma&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/google-deepmind/gemma/actions/workflows/pytest_and_autopublish.yml&quot;&gt;&lt;img src=&quot;https://github.com/google-deepmind/gemma/actions/workflows/pytest_and_autopublish.yml/badge.svg?sanitize=true&quot; alt=&quot;Unittests&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://badge.fury.io/py/gemma&quot;&gt;&lt;img src=&quot;https://badge.fury.io/py/gemma.svg?sanitize=true&quot; alt=&quot;PyPI version&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://gemma-llm.readthedocs.io/en/latest/?badge=latest&quot;&gt;&lt;img src=&quot;https://readthedocs.org/projects/gemma-llm/badge/?version=latest&quot; alt=&quot;Documentation Status&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://ai.google.dev/gemma&quot;&gt;Gemma&lt;/a&gt; is a family of open-weights Large Language Model (LLM) by &lt;a href=&quot;https://deepmind.google/&quot;&gt;Google DeepMind&lt;/a&gt;, based on Gemini research and technology.&lt;/p&gt; 
&lt;p&gt;This repository contain the implementation of the &lt;a href=&quot;https://pypi.org/project/gemma/&quot;&gt;&lt;code&gt;gemma&lt;/code&gt;&lt;/a&gt; PyPI package. A &lt;a href=&quot;https://github.com/jax-ml/jax&quot;&gt;JAX&lt;/a&gt; library to use and fine-tune Gemma.&lt;/p&gt; 
&lt;p&gt;For examples and uses-cases, see our &lt;a href=&quot;https://gemma-llm.readthedocs.io/&quot;&gt;documentation&lt;/a&gt;. Please report issues and feedback in &lt;a href=&quot;https://github.com/google-deepmind/gemma/issues&quot;&gt;our GitHub&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Install JAX for CPU, GPU or TPU. Follow instructions at &lt;a href=&quot;https://jax.readthedocs.io/en/latest/installation.html&quot;&gt;the JAX website&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;pip install gemma
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Examples&lt;/h3&gt; 
&lt;p&gt;Here is a minimal example to have a multi-turn, multi-modal conversation with Gemma:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from gemma import gm

# Model and parameters
model = gm.nn.Gemma3_4B()
params = gm.ckpts.load_params(gm.ckpts.CheckpointPath.GEMMA3_4B_IT)

# Example of multi-turn conversation
sampler = gm.text.ChatSampler(
    model=model,
    params=params,
    mult_turn=True,
)

prompt = &quot;&quot;&quot;Which of the 2 images do you prefer ?

Image 1: &amp;lt;start_of_image&amp;gt;
Image 2: &amp;lt;start_of_image&amp;gt;

Write your answer as a poem.&quot;&quot;&quot;
out0 = sampler.chat(prompt, images=[image1, image2])

out1 = sampler.chat(&#39;What about the other image ?&#39;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Our documentation contain various Colabs and tutorial, including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://gemma-llm.readthedocs.io/en/latest/colab_sampling.html&quot;&gt;Sampling&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://gemma-llm.readthedocs.io/en/latest/colab_multimodal.html&quot;&gt;Multi-modal&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://gemma-llm.readthedocs.io/en/latest/colab_finetuning.html&quot;&gt;Fine-tuning&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://gemma-llm.readthedocs.io/en/latest/colab_lora_sampling.html&quot;&gt;LoRA&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;...&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Additionally, our &lt;a href=&quot;https://github.com/google-deepmind/gemma/tree/main/examples&quot;&gt;examples/&lt;/a&gt; folder contain additional scripts to fine-tune and sample with Gemma.&lt;/p&gt; 
&lt;h3&gt;Learn more about Gemma&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;To use this library: &lt;a href=&quot;https://gemma-llm.readthedocs.io/&quot;&gt;Gemma documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Technical reports for metrics and model capabilities: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://goo.gle/GemmaReport&quot;&gt;Gemma 1&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://goo.gle/gemma2report&quot;&gt;Gemma 2&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://storage.googleapis.com/deepmind-media/gemma/Gemma3Report.pdf&quot;&gt;Gemma 3&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Other Gemma implementations and doc on the &lt;a href=&quot;https://ai.google.dev/gemma/docs&quot;&gt;Gemma ecosystem&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Downloading the models&lt;/h3&gt; 
&lt;p&gt;To download the model weights. See &lt;a href=&quot;https://gemma-llm.readthedocs.io/en/latest/checkpoints.html&quot;&gt;our documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;System Requirements&lt;/h3&gt; 
&lt;p&gt;Gemma can run on a CPU, GPU and TPU. For GPU, we recommend a 8GB+ RAM on GPU for the 2B checkpoint and 24GB+ RAM on GPU for the 7B checkpoint.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;This is not an official Google product.&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>google-deepmind/mujoco</title>
      <link>https://github.com/google-deepmind/mujoco</link>
      <description>&lt;p&gt;Multi-Joint dynamics with Contact. A general purpose physics simulator.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt; &lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/mujoco/main/#&quot;&gt;&lt;img alt=&quot;MuJoCo&quot; src=&quot;https://raw.githubusercontent.com/google-deepmind/mujoco/main/banner.png&quot; width=&quot;100%&quot;&gt;&lt;/a&gt; &lt;/h1&gt; 
&lt;p&gt; &lt;a href=&quot;https://github.com/google-deepmind/mujoco/actions/workflows/build.yml?query=branch%3Amain&quot; alt=&quot;GitHub Actions&quot;&gt; &lt;img src=&quot;https://img.shields.io/github/actions/workflow/status/google-deepmind/mujoco/build.yml?branch=main&quot;&gt; &lt;/a&gt; &lt;a href=&quot;https://mujoco.readthedocs.io/&quot; alt=&quot;Documentation&quot;&gt; &lt;img src=&quot;https://readthedocs.org/projects/mujoco/badge/?version=latest&quot;&gt; &lt;/a&gt; &lt;a href=&quot;https://github.com/google-deepmind/mujoco/raw/main/LICENSE&quot; alt=&quot;License&quot;&gt; &lt;img src=&quot;https://img.shields.io/github/license/google-deepmind/mujoco&quot;&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;MuJoCo&lt;/strong&gt; stands for &lt;strong&gt;Mu&lt;/strong&gt;lti-&lt;strong&gt;Jo&lt;/strong&gt;int dynamics with &lt;strong&gt;Co&lt;/strong&gt;ntact. It is a general purpose physics engine that aims to facilitate research and development in robotics, biomechanics, graphics and animation, machine learning, and other areas which demand fast and accurate simulation of articulated structures interacting with their environment.&lt;/p&gt; 
&lt;p&gt;This repository is maintained by &lt;a href=&quot;https://www.deepmind.com/&quot;&gt;Google DeepMind&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;MuJoCo has a C API and is intended for researchers and developers. The runtime simulation module is tuned to maximize performance and operates on low-level data structures that are preallocated by the built-in XML compiler. The library includes interactive visualization with a native GUI, rendered in OpenGL. MuJoCo further exposes a large number of utility functions for computing physics-related quantities.&lt;/p&gt; 
&lt;p&gt;We also provide &lt;a href=&quot;https://mujoco.readthedocs.io/en/stable/python.html#python-bindings&quot;&gt;Python bindings&lt;/a&gt; and a plug-in for the &lt;a href=&quot;https://unity.com/&quot;&gt;Unity&lt;/a&gt; game engine.&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;MuJoCo&#39;s documentation can be found at &lt;a href=&quot;https://mujoco.readthedocs.io&quot;&gt;mujoco.readthedocs.io&lt;/a&gt;. Upcoming features due for the next release can be found in the &lt;a href=&quot;https://mujoco.readthedocs.io/en/latest/changelog.html&quot;&gt;changelog&lt;/a&gt; in the &quot;latest&quot; branch.&lt;/p&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;There are two easy ways to get started with MuJoCo:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Run &lt;code&gt;simulate&lt;/code&gt; on your machine.&lt;/strong&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=P83tKA1iz2Y&quot;&gt;This video&lt;/a&gt; shows a screen capture of &lt;code&gt;simulate&lt;/code&gt;, MuJoCo&#39;s native interactive viewer. Follow the steps described in the &lt;a href=&quot;https://mujoco.readthedocs.io/en/latest/programming#getting-started&quot;&gt;Getting Started&lt;/a&gt; section of the documentation to get &lt;code&gt;simulate&lt;/code&gt; running on your machine.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Explore our online IPython notebooks.&lt;/strong&gt; If you are a Python user, you might want to start with our tutorial notebooks running on Google Colab:&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;ul&gt; 
 &lt;li&gt;The &lt;strong&gt;introductory&lt;/strong&gt; tutorial teaches MuJoCo basics: &lt;a href=&quot;https://colab.research.google.com/github/google-deepmind/mujoco/blob/main/python/tutorial.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot;&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;The &lt;strong&gt;rollout&lt;/strong&gt; tutorial shows how to use the multithreaded &lt;code&gt;rollout&lt;/code&gt; module: &lt;a href=&quot;https://colab.research.google.com/github/google-deepmind/mujoco/blob/main/python/rollout.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot;&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;The &lt;strong&gt;LQR&lt;/strong&gt; tutorial synthesizes a linear-quadratic controller, balancing a humanoid on one leg: &lt;a href=&quot;https://colab.research.google.com/github/google-deepmind/mujoco/blob/main/python/LQR.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot;&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;The &lt;strong&gt;least-squares&lt;/strong&gt; tutorial explains how to use the Python-based nonlinear least-squares solver: &lt;a href=&quot;https://colab.research.google.com/github/google-deepmind/mujoco/blob/main/python/least_squares.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot;&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;The &lt;strong&gt;MJX&lt;/strong&gt; tutorial provides usage examples of &lt;a href=&quot;https://mujoco.readthedocs.io/en/stable/mjx.html&quot;&gt;MuJoCo XLA&lt;/a&gt;, a branch of MuJoCo written in JAX: &lt;a href=&quot;https://colab.research.google.com/github/google-deepmind/mujoco/blob/main/mjx/tutorial.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot;&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;The &lt;strong&gt;differentiable physics&lt;/strong&gt; tutorial trains locomotion policies with analytical gradients automatically derived from MuJoCo&#39;s physics step: &lt;a href=&quot;https://colab.research.google.com/github/google-deepmind/mujoco/blob/main/mjx/training_apg.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot;&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h3&gt;Prebuilt binaries&lt;/h3&gt; 
&lt;p&gt;Versioned releases are available as precompiled binaries from the GitHub &lt;a href=&quot;https://github.com/google-deepmind/mujoco/releases&quot;&gt;releases page&lt;/a&gt;, built for Linux (x86-64 and AArch64), Windows (x86-64 only), and macOS (universal). This is the recommended way to use the software.&lt;/p&gt; 
&lt;h3&gt;Building from source&lt;/h3&gt; 
&lt;p&gt;Users who wish to build MuJoCo from source should consult the &lt;a href=&quot;https://mujoco.readthedocs.io/en/latest/programming#building-mujoco-from-source&quot;&gt;build from source&lt;/a&gt; section of the documentation. However, note that the commit at the tip of the &lt;code&gt;main&lt;/code&gt; branch may be unstable.&lt;/p&gt; 
&lt;h3&gt;Python (&amp;gt;= 3.9)&lt;/h3&gt; 
&lt;p&gt;The native Python bindings, which come pre-packaged with a copy of MuJoCo, can be installed from &lt;a href=&quot;https://pypi.org/project/mujoco/&quot;&gt;PyPI&lt;/a&gt; via:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install mujoco
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note that Pre-built Linux wheels target &lt;code&gt;manylinux2014&lt;/code&gt;, see &lt;a href=&quot;https://github.com/pypa/manylinux&quot;&gt;here&lt;/a&gt; for compatible distributions. For more information such as building the bindings from source, see the &lt;a href=&quot;https://mujoco.readthedocs.io/en/stable/python.html#python-bindings&quot;&gt;Python bindings&lt;/a&gt; section of the documentation.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome community engagement: questions, requests for help, bug reports and feature requests. To read more about bug reports, feature requests and more ambitious contributions, please see our &lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/mujoco/main/CONTRIBUTING.md&quot;&gt;contributors guide&lt;/a&gt; and &lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/mujoco/main/STYLEGUIDE.md&quot;&gt;style guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Asking Questions&lt;/h2&gt; 
&lt;p&gt;Questions and requests for help are welcome as a GitHub &lt;a href=&quot;https://github.com/google-deepmind/mujoco/discussions/categories/asking-for-help&quot;&gt;&quot;Asking for Help&quot; Discussion&lt;/a&gt; and should focus on a specific problem or question.&lt;/p&gt; 
&lt;h2&gt;Bug reports and feature requests&lt;/h2&gt; 
&lt;p&gt;GitHub &lt;a href=&quot;https://github.com/google-deepmind/mujoco/issues&quot;&gt;Issues&lt;/a&gt; are reserved for bug reports, feature requests and other development-related subjects.&lt;/p&gt; 
&lt;h2&gt;Related software&lt;/h2&gt; 
&lt;p&gt;MuJoCo is the backbone for numerous environment packages. Below we list several bindings and converters.&lt;/p&gt; 
&lt;h3&gt;Bindings&lt;/h3&gt; 
&lt;p&gt;These packages give users of various languages access to MuJoCo functionality:&lt;/p&gt; 
&lt;h4&gt;First-party bindings:&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://mujoco.readthedocs.io/en/stable/python.html&quot;&gt;Python bindings&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/google-deepmind/dm_control&quot;&gt;dm_control&lt;/a&gt;, Google DeepMind&#39;s related environment stack, includes &lt;a href=&quot;https://github.com/google-deepmind/dm_control/raw/main/dm_control/mjcf/README.md&quot;&gt;PyMJCF&lt;/a&gt;, a module for procedural manipulation of MuJoCo models.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://mujoco.readthedocs.io/en/stable/unity.html&quot;&gt;C# bindings and Unity plug-in&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Third-party bindings:&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;WebAssembly&lt;/strong&gt;: &lt;a href=&quot;https://github.com/zalo/mujoco_wasm&quot;&gt;mujoco_wasm&lt;/a&gt; by &lt;a href=&quot;https://github.com/zalo&quot;&gt;@zalo&lt;/a&gt; with contributions by &lt;a href=&quot;https://github.com/kevinzakka&quot;&gt;@kevinzakka&lt;/a&gt;, based on the &lt;a href=&quot;https://github.com/stillonearth/MuJoCo-WASM&quot;&gt;emscripten build&lt;/a&gt; by &lt;a href=&quot;https://github.com/stillonearth&quot;&gt;@stillonearth&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;span&gt;âž¡&lt;/span&gt; &lt;a href=&quot;https://zalo.github.io/mujoco_wasm/&quot;&gt;Click here&lt;/a&gt; for a live demo of MuJoCo running in your browser.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;MATLAB Simulink&lt;/strong&gt;: &lt;a href=&quot;https://github.com/mathworks-robotics/mujoco-simulink-blockset&quot;&gt;Simulink Blockset for MuJoCo Simulator&lt;/a&gt; by &lt;a href=&quot;https://github.com/vmanoj1996&quot;&gt;Manoj Velmurugan&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Swift&lt;/strong&gt;: &lt;a href=&quot;https://github.com/liuliu/swift-mujoco&quot;&gt;swift-mujoco&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Java&lt;/strong&gt;: &lt;a href=&quot;https://github.com/CommonWealthRobotics/mujoco-java&quot;&gt;mujoco-java&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Julia&lt;/strong&gt;: &lt;a href=&quot;https://github.com/JamieMair/MuJoCo.jl&quot;&gt;MuJoCo.jl&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Converters&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;OpenSim&lt;/strong&gt;: &lt;a href=&quot;https://github.com/MyoHub/myoconverter&quot;&gt;MyoConverter&lt;/a&gt; converts OpenSim models to MJCF.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;SDFormat&lt;/strong&gt;: &lt;a href=&quot;https://github.com/gazebosim/gz-mujoco/&quot;&gt;gz-mujoco&lt;/a&gt; is a two-way SDFormat &amp;lt;-&amp;gt; MJCF conversion tool.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;OBJ&lt;/strong&gt;: &lt;a href=&quot;https://github.com/kevinzakka/obj2mjcf&quot;&gt;obj2mjcf&lt;/a&gt; a script for converting composite OBJ files into a loadable MJCF model.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;onshape&lt;/strong&gt;: &lt;a href=&quot;https://github.com/rhoban/onshape-to-robot&quot;&gt;Onshape to Robot&lt;/a&gt; Converts &lt;a href=&quot;https://www.onshape.com/en/&quot;&gt;onshape&lt;/a&gt; CAD assemblies to MJCF.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you use MuJoCo for published research, please cite:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@inproceedings{todorov2012mujoco,
  title={MuJoCo: A physics engine for model-based control},
  author={Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  booktitle={2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  pages={5026--5033},
  year={2012},
  organization={IEEE},
  doi={10.1109/IROS.2012.6386109}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;License and Disclaimer&lt;/h2&gt; 
&lt;p&gt;Copyright 2021 DeepMind Technologies Limited.&lt;/p&gt; 
&lt;p&gt;Box collision code (&lt;a href=&quot;https://github.com/google-deepmind/mujoco/raw/main/src/engine/engine_collision_box.c&quot;&gt;&lt;code&gt;engine_collision_box.c&lt;/code&gt;&lt;/a&gt;) is Copyright 2016 Svetoslav Kolev.&lt;/p&gt; 
&lt;p&gt;ReStructuredText documents, images, and videos in the &lt;code&gt;doc&lt;/code&gt; directory are made available under the terms of the Creative Commons Attribution 4.0 (CC BY 4.0) license. You may obtain a copy of the License at &lt;a href=&quot;https://creativecommons.org/licenses/by/4.0/legalcode&quot;&gt;https://creativecommons.org/licenses/by/4.0/legalcode&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Source code is licensed under the Apache License, Version 2.0. You may obtain a copy of the License at &lt;a href=&quot;https://www.apache.org/licenses/LICENSE-2.0&quot;&gt;https://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;This is not an officially supported Google product.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>meta-llama/llama-cookbook</title>
      <link>https://github.com/meta-llama/llama-cookbook</link>
      <description>&lt;p&gt;Welcome to the Llama Cookbook! This is your go to guide for Building with Llama: Getting started with Inference, Fine-Tuning, RAG. We also show you how to solve end to end problems using Llama model family and using them on various provider services&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Llama Cookbook: The Official Guide to building with Llama Models&lt;/h1&gt; 
&lt;p&gt;Welcome to the official repository for helping you get started with &lt;a href=&quot;https://github.com/meta-llama/llama-cookbook/tree/main/getting-started/inference/&quot;&gt;inference&lt;/a&gt;, &lt;a href=&quot;https://github.com/meta-llama/llama-cookbook/tree/main/getting-started/finetuning&quot;&gt;fine-tuning&lt;/a&gt; and &lt;a href=&quot;https://github.com/meta-llama/llama-cookbook/tree/main/end-to-end-use-cases&quot;&gt;end-to-end use-cases&lt;/a&gt; of building with the Llama Model family.&lt;/p&gt; 
&lt;p&gt;This repository covers the most popular community approaches, use-cases and the latest recipes for Llama Text and Vision models.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] Popular getting started links:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/meta-llama/llama-cookbook/tree/main/getting-started/build_with_Llama_3_2.ipynb&quot;&gt;Build with Llama Tutorial&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/meta-llama/llama-cookbook/tree/main/getting-started/inference/local_inference/README.md#multimodal-inference&quot;&gt;Multimodal Inference with Llama 3.2 Vision&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/meta-llama/llama-cookbook/tree/main/getting-started/responsible_ai/llama_guard/&quot;&gt;Inferencing using Llama Guard (Safety Model)&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] Popular end to end recipes:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/meta-llama/llama-cookbook/tree/main/end-to-end-use-cases/email_agent/&quot;&gt;Email Agent&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/meta-llama/llama-cookbook/tree/main/end-to-end-use-cases/NotebookLlama/&quot;&gt;NotebookLlama&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/meta-llama/llama-cookbook/tree/main/end-to-end-use-cases/coding/text2sql/&quot;&gt;Text to SQL&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Note: We recently did a refactor of the repo, &lt;a href=&quot;https://github.com/meta-llama/llama-cookbook/tree/archive-main&quot;&gt;archive-main&lt;/a&gt; is a snapshot branch from before the refactor&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Repository Structure:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/meta-llama/llama-cookbook/tree/main/3p-integrations&quot;&gt;3P Integrations&lt;/a&gt;: Getting Started Recipes and End to End Use-Cases from various Llama providers&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/meta-llama/llama-cookbook/tree/main/end-to-end-use-cases&quot;&gt;End to End Use Cases&lt;/a&gt;: As the name suggests, spanning various domains and applications&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/meta-llama/llama-cookbook/tree/main/getting-started/&quot;&gt;Getting Started&lt;/a&gt;: Reference for inferencing, fine-tuning and RAG examples&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/meta-llama/llama-cookbook/tree/main/src/&quot;&gt;src&lt;/a&gt;: Contains the src for the original llama-recipes library along with some FAQs for fine-tuning.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;FAQ:&lt;/h2&gt; 
&lt;h2&gt;FAQ:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; What happened to llama-recipes?&lt;br&gt; &lt;strong&gt;A:&lt;/strong&gt; We recently renamed llama-recipes to llama-cookbook.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; Prompt Template changes for Multi-Modality?&lt;br&gt; &lt;strong&gt;A:&lt;/strong&gt; Llama 3.2 follows the same prompt template as Llama 3.1, with a new special token &lt;code&gt;&amp;lt;|image|&amp;gt;&lt;/code&gt; representing the input image for the multimodal models. More details on the prompt templates for image reasoning, tool-calling, and code interpreter can be found &lt;a href=&quot;https://llama.meta.com/docs/model-cards-and-prompt-formats/llama3_2&quot;&gt;on the documentation website&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; I have some questions for Fine-Tuning, is there a section to address these?&lt;br&gt; &lt;strong&gt;A:&lt;/strong&gt; Checkout the Fine-Tuning FAQ &lt;a href=&quot;https://github.com/meta-llama/llama-cookbook/tree/main/src/docs/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; Some links are broken/folders are missing:&lt;br&gt; &lt;strong&gt;A:&lt;/strong&gt; We recently did a refactor of the repo, &lt;a href=&quot;https://github.com/meta-llama/llama-cookbook/tree/archive-main&quot;&gt;archive-main&lt;/a&gt; is a snapshot branch from before the refactor.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; Where can we find details about the latest models?&lt;br&gt; &lt;strong&gt;A:&lt;/strong&gt; Official &lt;a href=&quot;https://www.llama.com&quot;&gt;Llama models website&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Please read &lt;a href=&quot;https://raw.githubusercontent.com/meta-llama/llama-cookbook/main/CONTRIBUTING.md&quot;&gt;CONTRIBUTING.md&lt;/a&gt; for details on our code of conduct, and the process for submitting pull requests to us.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;!-- markdown-link-check-disable --&gt; 
&lt;p&gt;See the License file for Meta Llama 3.2 &lt;a href=&quot;https://github.com/meta-llama/llama-models/raw/main/models/llama3_2/LICENSE&quot;&gt;here&lt;/a&gt; and Acceptable Use Policy &lt;a href=&quot;https://github.com/meta-llama/llama-models/raw/main/models/llama3_2/USE_POLICY.md&quot;&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;See the License file for Meta Llama 3.1 &lt;a href=&quot;https://github.com/meta-llama/llama-models/raw/main/models/llama3_1/LICENSE&quot;&gt;here&lt;/a&gt; and Acceptable Use Policy &lt;a href=&quot;https://github.com/meta-llama/llama-models/raw/main/models/llama3_1/USE_POLICY.md&quot;&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;See the License file for Meta Llama 3 &lt;a href=&quot;https://github.com/meta-llama/llama-models/raw/main/models/llama3/LICENSE&quot;&gt;here&lt;/a&gt; and Acceptable Use Policy &lt;a href=&quot;https://github.com/meta-llama/llama-models/raw/main/models/llama3/USE_POLICY.md&quot;&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;See the License file for Meta Llama 2 &lt;a href=&quot;https://github.com/meta-llama/llama-models/raw/main/models/llama2/LICENSE&quot;&gt;here&lt;/a&gt; and Acceptable Use Policy &lt;a href=&quot;https://github.com/meta-llama/llama-models/raw/main/models/llama2/USE_POLICY.md&quot;&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;!-- markdown-link-check-enable --&gt;</description>
    </item>
    
    <item>
      <title>SakanaAI/AI-Scientist</title>
      <link>https://github.com/SakanaAI/AI-Scientist</link>
      <description>&lt;p&gt;The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery ðŸ§‘â€ðŸ”¬&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&quot;center&quot;&gt; &lt;a href=&quot;https://github.com/SakanaAI/AI-Scientist/raw/main/docs/logo_2.png&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/SakanaAI/AI-Scientist/main/docs/logo_2.png&quot; width=&quot;215&quot;&gt;&lt;/a&gt;&lt;br&gt; &lt;b&gt;The AI Scientist: Towards Fully Automated&lt;/b&gt;&lt;br&gt; &lt;b&gt;Open-Ended Scientific Discovery ðŸ§‘â€ðŸ”¬&lt;/b&gt;&lt;br&gt; &lt;/h1&gt; 
&lt;p align=&quot;center&quot;&gt; ðŸ“š &lt;a href=&quot;https://arxiv.org/abs/2408.06292&quot;&gt;[Paper]&lt;/a&gt; | ðŸ“ &lt;a href=&quot;https://sakana.ai/ai-scientist/&quot;&gt;[Blog Post]&lt;/a&gt; | ðŸ“‚ &lt;a href=&quot;https://drive.google.com/drive/folders/1G7A0wTqfXVa-cpexjk0oaXakaSJwffEt&quot;&gt;[Drive Folder]&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;One of the grand challenges of artificial intelligence is developing agents capable of conducting scientific research and discovering new knowledge. While frontier models have already been used to aid human scientistsâ€”for example, for brainstorming ideas or writing codeâ€”they still require extensive manual supervision or are heavily constrained to specific tasks.&lt;/p&gt; 
&lt;p&gt;We&#39;re excited to introduce &lt;strong&gt;The AI Scientist&lt;/strong&gt;, the first comprehensive system for fully automatic scientific discovery, enabling Foundation Models such as Large Language Models (LLMs) to perform research independently.&lt;/p&gt; 
&lt;p&gt;We provide all runs and data from our paper &lt;a href=&quot;https://drive.google.com/drive/folders/1G7A0wTqfXVa-cpexjk0oaXakaSJwffEt?usp=sharing&quot;&gt;here&lt;/a&gt;, where we run each base model on each template for approximately 50 ideas. We &lt;em&gt;highly&lt;/em&gt; recommend reading through some of the &lt;a href=&quot;https://drive.google.com/drive/folders/1Mmpz6M1FK4q8e-SewgZcUzdeD0Q2zC39?usp=sharing&quot;&gt;Claude papers&lt;/a&gt; to get a sense of the system&#39;s strengths and weaknesses. Here are some example papers generated by &lt;strong&gt;The AI Scientist&lt;/strong&gt; ðŸ“:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/SakanaAI/AI-Scientist/raw/main/example_papers/adaptive_dual_scale_denoising.pdf&quot;&gt;DualScale Diffusion: Adaptive Feature Balancing for Low-Dimensional Generative Models&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/SakanaAI/AI-Scientist/raw/main/example_papers/grid_based_noise_adaptation.pdf&quot;&gt;Multi-scale Grid Noise Adaptation: Enhancing Diffusion Models For Low-dimensional Data&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/SakanaAI/AI-Scientist/raw/main/example_papers/gan_diffusion.pdf&quot;&gt;GAN-Enhanced Diffusion: Boosting Sample Quality and Diversity&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/SakanaAI/AI-Scientist/tree/main/example_papers/dual_expert_denoiser.pdf&quot;&gt;DualDiff: Enhancing Mode Capture in Low-dimensional Diffusion Models via Dual-expert Denoising&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/SakanaAI/AI-Scientist/raw/main/example_papers/multi_style_adapter.pdf&quot;&gt;StyleFusion: Adaptive Multi-style Generation in Character-Level Language Models&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/SakanaAI/AI-Scientist/tree/main/example_papers/rl_lr_adaptation.pdf&quot;&gt;Adaptive Learning Rates for Transformers via Q-Learning&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/SakanaAI/AI-Scientist/tree/main/example_papers/weight_initialization_grokking.pdf&quot;&gt;Unlocking Grokking: A Comparative Study of Weight Initialization Strategies in Transformer Models&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/SakanaAI/AI-Scientist/tree/main/example_papers/layerwise_lr_grokking.pdf&quot;&gt;Grokking Accelerated: Layer-wise Learning Rates for Transformer Generalization&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/SakanaAI/AI-Scientist/tree/main/example_papers/mdl_grokking_correlation.pdf&quot;&gt;Grokking Through Compression: Unveiling Sudden Generalization via Minimal Description Length&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/SakanaAI/AI-Scientist/tree/main/example_papers/data_augmentation_grokking.pdf&quot;&gt;Accelerating Mathematical Insight: Boosting Grokking Through Strategic Data Augmentation&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt;&lt;br&gt; &lt;strong&gt;Caution!&lt;/strong&gt; This codebase will execute LLM-written code. There are various risks and challenges associated with this autonomy, including the use of potentially dangerous packages, web access, and potential spawning of processes. Use at your own discretion. Please make sure to &lt;a href=&quot;https://raw.githubusercontent.com/SakanaAI/AI-Scientist/main/#containerization&quot;&gt;containerize&lt;/a&gt; and restrict web access appropriately.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;a href=&quot;https://github.com/SakanaAI/AI-Scientist/raw/main/example_papers/adaptive_dual_scale_denoising/adaptive_dual_scale_denoising.pdf&quot;&gt;&lt;img src=&quot;https://github.com/SakanaAI/AI-Scientist/raw/main/docs/anim-ai-scientist.gif&quot; alt=&quot;Adaptive Dual Scale Denoising&quot; width=&quot;80%&quot;&gt; &lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/SakanaAI/AI-Scientist/main/#introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/SakanaAI/AI-Scientist/main/#requirements&quot;&gt;Requirements&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/SakanaAI/AI-Scientist/main/#installation&quot;&gt;Installation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/SakanaAI/AI-Scientist/main/#supported-models-and-api-keys&quot;&gt;Supported Models and API Keys&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/SakanaAI/AI-Scientist/main/#setting-up-the-templates&quot;&gt;Setting Up the Templates&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/SakanaAI/AI-Scientist/main/#nanogpt-template&quot;&gt;NanoGPT Template&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/SakanaAI/AI-Scientist/main/#2d-diffusion-template&quot;&gt;2D Diffusion Template&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/SakanaAI/AI-Scientist/main/#grokking-template&quot;&gt;Grokking Template&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/SakanaAI/AI-Scientist/main/#run-ai-scientist-paper-generation-experiments&quot;&gt;Run AI Scientist Paper Generation Experiments&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/SakanaAI/AI-Scientist/main/#getting-an-llm-generated-paper-review&quot;&gt;Getting an LLM-Generated Paper Review&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/SakanaAI/AI-Scientist/main/#making-your-own-template&quot;&gt;Making Your Own Template&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/SakanaAI/AI-Scientist/main/#community-contributed-templates&quot;&gt;Community-Contributed Templates&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/SakanaAI/AI-Scientist/main/#template-resources&quot;&gt;Template Resources&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/SakanaAI/AI-Scientist/main/#citing-the-ai-scientist&quot;&gt;Citing The AI Scientist&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/SakanaAI/AI-Scientist/main/#frequently-asked-questions&quot;&gt;Frequently Asked Questions&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/SakanaAI/AI-Scientist/main/#containerization&quot;&gt;Containerization&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;We provide three templates, which were used in our paper, covering the following domains: &lt;strong&gt;NanoGPT&lt;/strong&gt;, &lt;strong&gt;2D Diffusion&lt;/strong&gt;, and &lt;strong&gt;Grokking&lt;/strong&gt;. These templates enable The AI Scientist to generate ideas and conduct experiments in these areas. We accept contributions of new templates from the community, but please note that they are not maintained by us. All other templates beyond the three provided are community contributions.&lt;/p&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;p&gt;This code is designed to run on Linux with NVIDIA GPUs using CUDA and PyTorch. Support for other GPU architectures may be possible by following the &lt;a href=&quot;https://pytorch.org/get-started/locally/&quot;&gt;PyTorch guidelines&lt;/a&gt;. The current templates would likely take an infeasible amount of time on CPU-only machines. Running on other operating systems may require significant adjustments.&lt;/p&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;conda create -n ai_scientist python=3.11
conda activate ai_scientist
# Install pdflatex
sudo apt-get install texlive-full

# Install PyPI requirements
pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Installing &lt;code&gt;texlive-full&lt;/code&gt; can take a long time. You may need to &lt;a href=&quot;https://askubuntu.com/questions/956006/pregenerating-context-markiv-format-this-may-take-some-time-takes-forever&quot;&gt;hold Enter&lt;/a&gt; during the installation.&lt;/p&gt; 
&lt;h3&gt;Supported Models and API Keys&lt;/h3&gt; 
&lt;p&gt;We support a wide variety of models, including open-weight and API-only models. In general, we recommend using only frontier models above the capability of the original GPT-4. To see a full list of supported models, see &lt;a href=&quot;https://github.com/SakanaAI/AI-Scientist/raw/main/ai_scientist/llm.py&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h4&gt;OpenAI API (GPT-4o, GPT-4o-mini, o1 models)&lt;/h4&gt; 
&lt;p&gt;By default, this uses the &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; environment variable.&lt;/p&gt; 
&lt;h4&gt;Anthropic API (Claude Sonnet 3.5)&lt;/h4&gt; 
&lt;p&gt;By default, this uses the &lt;code&gt;ANTHROPIC_API_KEY&lt;/code&gt; environment variable.&lt;/p&gt; 
&lt;h5&gt;Claude Models via Bedrock&lt;/h5&gt; 
&lt;p&gt;For Claude models provided by &lt;a href=&quot;https://aws.amazon.com/bedrock/&quot;&gt;Amazon Bedrock&lt;/a&gt;, please install these additional packages:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install anthropic[bedrock]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Next, specify a set of valid &lt;a href=&quot;https://docs.aws.amazon.com/cli/v1/userguide/cli-configure-envvars.html&quot;&gt;AWS Credentials&lt;/a&gt; and the target &lt;a href=&quot;https://docs.aws.amazon.com/bedrock/latest/userguide/bedrock-regions.html&quot;&gt;AWS Region&lt;/a&gt;:&lt;/p&gt; 
&lt;p&gt;Set the environment variables: &lt;code&gt;AWS_ACCESS_KEY_ID&lt;/code&gt;, &lt;code&gt;AWS_SECRET_ACCESS_KEY&lt;/code&gt;, &lt;code&gt;AWS_REGION_NAME&lt;/code&gt;.&lt;/p&gt; 
&lt;h5&gt;Claude Models via Vertex AI&lt;/h5&gt; 
&lt;p&gt;For Claude models provided by &lt;a href=&quot;https://cloud.google.com/model-garden?hl=en&quot;&gt;Vertex AI Model Garden&lt;/a&gt;, please install these additional packages:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install google-cloud-aiplatform
pip install anthropic[vertex]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Next, set up valid authentication for a &lt;a href=&quot;https://cloud.google.com/vertex-ai/docs/authentication&quot;&gt;Google Cloud project&lt;/a&gt;, for example by providing the region and project ID:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;export CLOUD_ML_REGION=&quot;REGION&quot;           # for Model Garden call
export ANTHROPIC_VERTEX_PROJECT_ID=&quot;PROJECT_ID&quot;  # for Model Garden call
export VERTEXAI_LOCATION=&quot;REGION&quot;         # for Aider/LiteLLM call
export VERTEXAI_PROJECT=&quot;PROJECT_ID&quot;      # for Aider/LiteLLM call
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;DeepSeek API (deepseek-chat, deepseek-reasoner)&lt;/h4&gt; 
&lt;p&gt;By default, this uses the &lt;code&gt;DEEPSEEK_API_KEY&lt;/code&gt; environment variable.&lt;/p&gt; 
&lt;h4&gt;OpenRouter API (Llama3.1)&lt;/h4&gt; 
&lt;p&gt;By default, this uses the &lt;code&gt;OPENROUTER_API_KEY&lt;/code&gt; environment variable.&lt;/p&gt; 
&lt;h4&gt;Google Gemini&lt;/h4&gt; 
&lt;p&gt;We support Google Gemini models (e.g., &quot;gemini-1.5-flash&quot;, &quot;gemini-1.5-pro&quot;) via the &lt;a href=&quot;https://pypi.org/project/google-generativeai&quot;&gt;google-generativeai&lt;/a&gt; Python library. By default, it uses the environment variable:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;export GEMINI_API_KEY=&quot;YOUR GEMINI API KEY&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Semantic Scholar API (Literature Search)&lt;/h4&gt; 
&lt;p&gt;Our code can also optionally use a Semantic Scholar API Key (&lt;code&gt;S2_API_KEY&lt;/code&gt;) for higher throughput &lt;a href=&quot;https://www.semanticscholar.org/product/api&quot;&gt;if you have one&lt;/a&gt;, though it should work without it in principle. If you have problems with Semantic Scholar, you can skip the literature search and citation phases of paper generation.&lt;/p&gt; 
&lt;p&gt;Be sure to provide the key for the model used for your runs, e.g.:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;export OPENAI_API_KEY=&quot;YOUR KEY HERE&quot;
export S2_API_KEY=&quot;YOUR KEY HERE&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;OpenAlex API (Literature Search Alternative)&lt;/h4&gt; 
&lt;p&gt;OpenAlex API can be used as an alternative if you do not have a Semantic Scholar API Key. OpenAlex does not require API key.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install pyalex
export OPENALEX_MAIL_ADDRESS=&quot;YOUR EMAIL ADDRESS&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;And specify &lt;code&gt;--engine openalex&lt;/code&gt; when you execute the AI Scientist code.&lt;/p&gt; 
&lt;p&gt;Note that this is experimental for those who do not have a Semantic Scholar API Key.&lt;/p&gt; 
&lt;h2&gt;Setting Up the Templates&lt;/h2&gt; 
&lt;p&gt;This section provides instructions for setting up each of the three templates used in our paper. Before running The AI Scientist experiments, please ensure you have completed the setup steps for the templates you are interested in.&lt;/p&gt; 
&lt;h3&gt;NanoGPT Template&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Description:&lt;/strong&gt; This template investigates transformer-based autoregressive next-token prediction tasks.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Setup Steps:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Prepare the data:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python data/enwik8/prepare.py
python data/shakespeare_char/prepare.py
python data/text8/prepare.py
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Create baseline runs (machine dependent):&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# Set up NanoGPT baseline run
# NOTE: YOU MUST FIRST RUN THE PREPARE SCRIPTS ABOVE!
cd templates/nanoGPT
python experiment.py --out_dir run_0
python plot.py
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;2D Diffusion Template&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Description:&lt;/strong&gt; This template studies improving the performance of diffusion generative models on low-dimensional datasets.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Setup Steps:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install dependencies:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# Set up 2D Diffusion
git clone https://github.com/gregversteeg/NPEET.git
cd NPEET
pip install .
pip install scikit-learn
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Create baseline runs:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# Set up 2D Diffusion baseline run
cd templates/2d_diffusion
python experiment.py --out_dir run_0
python plot.py
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Grokking Template&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Description:&lt;/strong&gt; This template investigates questions about generalization and learning speed in deep neural networks.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Setup Steps:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install dependencies:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# Set up Grokking
pip install einops
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Create baseline runs:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# Set up Grokking baseline run
cd templates/grokking
python experiment.py --out_dir run_0
python plot.py
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Run AI Scientist Paper Generation Experiments&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Please ensure the setup steps above are completed before running these experiments.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;conda activate ai_scientist
# Run the paper generation.
python launch_scientist.py --model &quot;gpt-4o-2024-05-13&quot; --experiment nanoGPT_lite --num-ideas 2
python launch_scientist.py --model &quot;claude-3-5-sonnet-20241022&quot; --experiment nanoGPT_lite --num-ideas 2
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you have more than one GPU, use the &lt;code&gt;--parallel&lt;/code&gt; option to parallelize ideas across multiple GPUs.&lt;/p&gt; 
&lt;h2&gt;Getting an LLM-Generated Paper Review&lt;/h2&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import openai
from ai_scientist.perform_review import load_paper, perform_review

client = openai.OpenAI()
model = &quot;gpt-4o-2024-05-13&quot;

# Load paper from PDF file (raw text)
paper_txt = load_paper(&quot;report.pdf&quot;)

# Get the review dictionary
review = perform_review(
    paper_txt,
    model,
    client,
    num_reflections=5,
    num_fs_examples=1,
    num_reviews_ensemble=5,
    temperature=0.1,
)

# Inspect review results
review[&quot;Overall&quot;]    # Overall score (1-10)
review[&quot;Decision&quot;]   # &#39;Accept&#39; or &#39;Reject&#39;
review[&quot;Weaknesses&quot;] # List of weaknesses (strings)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To run batch analysis:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;cd review_iclr_bench
python iclr_analysis.py --num_reviews 500 --batch_size 100 --num_fs_examples 1 --num_reflections 5 --temperature 0.1 --num_reviews_ensemble 5
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Making Your Own Template&lt;/h2&gt; 
&lt;p&gt;If there is an area of study you would like &lt;strong&gt;The AI Scientist&lt;/strong&gt; to explore, it is straightforward to create your own templates. In general, follow the structure of the existing templates, which consist of:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;experiment.py&lt;/code&gt; â€” This is the main script where the core content is. It takes an argument &lt;code&gt;--out_dir&lt;/code&gt;, which specifies where it should create the folder and save the relevant information from the run.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;plot.py&lt;/code&gt; â€” This script takes the information from the &lt;code&gt;run&lt;/code&gt; folders and creates plots. The code should be clear and easy to edit.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;prompt.json&lt;/code&gt; â€” Put information about your template here.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;seed_ideas.json&lt;/code&gt; â€” Place example ideas here. You can also try to generate ideas without any examples and then pick the best one or two to put here.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;latex/template.tex&lt;/code&gt; â€” We recommend using our LaTeX folder but be sure to replace the pre-loaded citations with ones that you expect to be more relevant.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The key to making new templates work is matching the base filenames and output JSONs to the existing format; everything else is free to change. You should also ensure that the &lt;code&gt;template.tex&lt;/code&gt; file is updated to use the correct citation style / base plots for your template.&lt;/p&gt; 
&lt;h3&gt;Community-Contributed Templates&lt;/h3&gt; 
&lt;p&gt;We welcome community contributions in the form of new templates. While these are not maintained by us, we are delighted to highlight your templates to others. Below, we list community-contributed templates along with links to their pull requests (PRs):&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Infectious Disease Modeling (&lt;code&gt;seir&lt;/code&gt;) - &lt;a href=&quot;https://github.com/SakanaAI/AI-Scientist/pull/137&quot;&gt;PR #137&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Image Classification with MobileNetV3 (&lt;code&gt;mobilenetV3&lt;/code&gt;) - &lt;a href=&quot;https://github.com/SakanaAI/AI-Scientist/pull/141&quot;&gt;PR #141&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Sketch RNN (&lt;code&gt;sketch_rnn&lt;/code&gt;) - &lt;a href=&quot;https://github.com/SakanaAI/AI-Scientist/pull/143&quot;&gt;PR #143&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;AI in Quantum Chemistry (&lt;code&gt;MACE&lt;/code&gt;) - &lt;a href=&quot;https://github.com/SakanaAI/AI-Scientist/pull/157&quot;&gt;PR#157&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Earthquake Prediction (&lt;code&gt;earthquake-prediction&lt;/code&gt;) - &lt;a href=&quot;https://github.com/SakanaAI/AI-Scientist/pull/167&quot;&gt;PR #167&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Tensorial Radiance Fields (&lt;code&gt;tensorf&lt;/code&gt;) - &lt;a href=&quot;https://github.com/SakanaAI/AI-Scientist/pull/175&quot;&gt;PR #175&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;em&gt;This section is reserved for community contributions. Please submit a pull request to add your template to the list! Please describe the template in the PR description, and also show examples of the generated papers.&lt;/em&gt;&lt;/p&gt; 
&lt;h2&gt;Template Resources&lt;/h2&gt; 
&lt;p&gt;We provide three templates, which heavily use code from other repositories, credited below:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;NanoGPT Template&lt;/strong&gt; uses code from &lt;a href=&quot;https://github.com/karpathy/nanoGPT&quot;&gt;NanoGPT&lt;/a&gt; and this &lt;a href=&quot;https://github.com/karpathy/nanoGPT/pull/254&quot;&gt;PR&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;2D Diffusion Template&lt;/strong&gt; uses code from &lt;a href=&quot;https://github.com/tanelp/tiny-diffusion&quot;&gt;tiny-diffusion&lt;/a&gt;, &lt;a href=&quot;https://github.com/lucidrains/ema-pytorch&quot;&gt;ema-pytorch&lt;/a&gt;, and &lt;a href=&quot;https://www.research.autodesk.com/publications/same-stats-different-graphs/&quot;&gt;Datasaur&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Grokking Template&lt;/strong&gt; uses code from &lt;a href=&quot;https://github.com/Sea-Snell/grokking&quot;&gt;Sea-Snell/grokking&lt;/a&gt; and &lt;a href=&quot;https://github.com/danielmamay/grokking&quot;&gt;danielmamay/grokking&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We would like to thank the developers of the open-source models and packages for their contributions and for making their work available.&lt;/p&gt; 
&lt;h2&gt;Citing The AI Scientist&lt;/h2&gt; 
&lt;p&gt;If you use &lt;strong&gt;The AI Scientist&lt;/strong&gt; in your research, please cite it as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@article{lu2024aiscientist,
  title={The {AI} {S}cientist: Towards Fully Automated Open-Ended Scientific Discovery},
  author={Lu, Chris and Lu, Cong and Lange, Robert Tjarko and Foerster, Jakob and Clune, Jeff and Ha, David},
  journal={arXiv preprint arXiv:2408.06292},
  year={2024}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Frequently Asked Questions&lt;/h2&gt; 
&lt;p&gt;We recommend reading our paper first for any questions you have on The AI Scientist.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Why am I missing files when running The AI Scientist?&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Ensure you have completed all the setup and preparation steps before the main experiment script.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Why has a PDF or a review not been generated?&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;The AI Scientist finishes an idea with a success rate that depends on the template, the base foundation model, and the complexity of the idea. We advise referring to our main paper. The highest success rates are observed with Claude Sonnet 3.5. Reviews are best done with GPT-4o; all other models have issues with positivity bias or failure to conform to required outputs.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;What is the cost of each idea generated?&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Typically less than $15 per paper with Claude Sonnet 3.5. We recommend DeepSeek Coder V2 for a much more cost-effective approach. A good place to look for new models is the &lt;a href=&quot;https://aider.chat/docs/leaderboards/&quot;&gt;Aider leaderboard&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;How do I change the base conference format associated with the write-ups?&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Change the base &lt;code&gt;template.tex&lt;/code&gt; files contained within each template.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;How do I run The AI Scientist for different subject fields?&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Please refer to the instructions for different templates. In this current iteration, this is restricted to ideas that can be expressed in code. However, lifting this restriction would represent exciting future work! :)&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;How do I add support for a new foundation model?&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;You may modify &lt;code&gt;ai_scientist/llm.py&lt;/code&gt; to add support for a new foundation model. We do not advise using any model that is significantly weaker than GPT-4 level for &lt;strong&gt;The AI Scientist&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Why do I need to run the baseline runs myself?&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;These appear as &lt;code&gt;run_0&lt;/code&gt; and should be run per machine you execute &lt;strong&gt;The AI Scientist&lt;/strong&gt; on for accurate run-time comparisons due to hardware differences.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;What if I have problems accessing the Semantic Scholar API?&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;We use the Semantic Scholar API to check ideas for novelty and collect citations for the paper write-up. You may be able to skip these phases if you don&#39;t have an API key or the API is slow to access.&lt;/p&gt; 
&lt;h2&gt;Containerization&lt;/h2&gt; 
&lt;p&gt;We include a &lt;a href=&quot;https://github.com/SakanaAI/AI-Scientist/pull/21&quot;&gt;community-contributed&lt;/a&gt; Docker image that may assist with your containerization efforts in &lt;code&gt;experimental/Dockerfile&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;You can use this image like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# Endpoint Script
docker run -e OPENAI_API_KEY=$OPENAI_API_KEY -v `pwd`/templates:/app/AI-Scientist/templates &amp;lt;AI_SCIENTIST_IMAGE&amp;gt; \
  --model gpt-4o-2024-05-13 \
  --experiment 2d_diffusion \
  --num-ideas 2
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# Interactive
docker run -it -e OPENAI_API_KEY=$OPENAI_API_KEY \
  --entrypoint /bin/bash \
  &amp;lt;AI_SCIENTIST_IMAGE&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://star-history.com/#SakanaAI/AI-Scientist&amp;amp;Date&quot;&gt;&lt;img src=&quot;https://api.star-history.com/svg?repos=SakanaAI/AI-Scientist&amp;amp;type=Date&quot; alt=&quot;Star History Chart&quot;&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>MervinPraison/PraisonAI</title>
      <link>https://github.com/MervinPraison/PraisonAI</link>
      <description>&lt;p&gt;PraisonAI is a production-ready Multi AI Agents framework, designed to create AI Agents to automate and solve problems ranging from simple tasks to complex challenges. It provides a low-code solution to streamline the building and management of multi-agent LLM systems, emphasising simplicity, customisation, and effective human-agent collaboration.&lt;/p&gt;&lt;hr&gt;&lt;p align=&quot;center&quot;&gt; 
 &lt;picture&gt; 
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;docs/logo/dark.png&quot;&gt; 
  &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;docs/logo/light.png&quot;&gt; 
  &lt;img alt=&quot;PraisonAI Logo&quot; src=&quot;https://raw.githubusercontent.com/MervinPraison/PraisonAI/main/docs/logo/light.png&quot;&gt; 
 &lt;/picture&gt; &lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;a href=&quot;https://github.com/MervinPraison/PraisonAI&quot;&gt;&lt;img src=&quot;https://static.pepy.tech/badge/PraisonAI&quot; alt=&quot;Total Downloads&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/MervinPraison/PraisonAI&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/v/release/MervinPraison/PraisonAI&quot; alt=&quot;Latest Stable Version&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/MervinPraison/PraisonAI&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/License-MIT-yellow.svg?sanitize=true&quot; alt=&quot;License&quot;&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;h1&gt;Praison AI&lt;/h1&gt; 
 &lt;p&gt;&lt;a href=&quot;https://trendshift.io/repositories/9130&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/9130&quot; alt=&quot;MervinPraison%2FPraisonAI | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;PraisonAI is a production-ready Multi-AI Agents framework with self-reflection, designed to create AI Agents to automate and solve problems ranging from simple tasks to complex challenges. By integrating PraisonAI Agents, AutoGen, and CrewAI into a low-code solution, it streamlines the building and management of multi-agent LLM systems, emphasising simplicity, customisation, and effective human-agent collaboration.&lt;/p&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;a href=&quot;https://docs.praison.ai&quot;&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;https://img.shields.io/badge/%F0%9F%93%9A_Documentation-Visit_docs.praison.ai-blue?style=for-the-badge&amp;amp;logo=bookstack&amp;amp;logoColor=white&quot; alt=&quot;Documentation&quot;&gt; &lt;/p&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ðŸ¤– Automated AI Agents Creation&lt;/li&gt; 
 &lt;li&gt;ðŸ”„ Self Reflection AI Agents&lt;/li&gt; 
 &lt;li&gt;ðŸ§  Reasoning AI Agents&lt;/li&gt; 
 &lt;li&gt;ðŸ‘ï¸ Multi Modal AI Agents&lt;/li&gt; 
 &lt;li&gt;ðŸ¤ Multi Agent Collaboration&lt;/li&gt; 
 &lt;li&gt;ðŸŽ­ AI Agent Workflow&lt;/li&gt; 
 &lt;li&gt;ðŸ“š Add Custom Knowledge&lt;/li&gt; 
 &lt;li&gt;ðŸ§  Agents with Short and Long Term Memory&lt;/li&gt; 
 &lt;li&gt;ðŸ“„ Chat with PDF Agents&lt;/li&gt; 
 &lt;li&gt;ðŸ’» Code Interpreter Agents&lt;/li&gt; 
 &lt;li&gt;ðŸ“š RAG Agents&lt;/li&gt; 
 &lt;li&gt;ðŸ¤” Async &amp;amp; Parallel Processing&lt;/li&gt; 
 &lt;li&gt;ðŸ”„ Auto Agents&lt;/li&gt; 
 &lt;li&gt;ðŸ”¢ Math Agents&lt;/li&gt; 
 &lt;li&gt;ðŸŽ¯ Structured Output Agents&lt;/li&gt; 
 &lt;li&gt;ðŸ”— LangChain Integrated Agents&lt;/li&gt; 
 &lt;li&gt;ðŸ“ž Callback Agents&lt;/li&gt; 
 &lt;li&gt;ðŸ¤ Mini AI Agents&lt;/li&gt; 
 &lt;li&gt;ðŸ› ï¸ 100+ Custom Tools&lt;/li&gt; 
 &lt;li&gt;ðŸ“„ YAML Configuration&lt;/li&gt; 
 &lt;li&gt;ðŸ’¯ 100+ LLM Support&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Using Python Code&lt;/h2&gt; 
&lt;p&gt;Light weight package dedicated for coding:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install praisonaiagents
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;export OPENAI_API_KEY=xxxxxxxxxxxxxxxxxxxxxx
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;1. Single Agent&lt;/h3&gt; 
&lt;p&gt;Create app.py file and add the code below:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from praisonaiagents import Agent
agent = Agent(instructions=&quot;Your are a helpful AI assistant&quot;)
agent.start(&quot;Write a movie script about a robot in Mars&quot;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python app.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. Multi Agents&lt;/h3&gt; 
&lt;p&gt;Create app.py file and add the code below:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from praisonaiagents import Agent, PraisonAIAgents

research_agent = Agent(instructions=&quot;Research about AI&quot;)
summarise_agent = Agent(instructions=&quot;Summarise research agent&#39;s findings&quot;)
agents = PraisonAIAgents(agents=[research_agent, summarise_agent])
agents.start()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python app.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Using No Code&lt;/h2&gt; 
&lt;h3&gt;Auto Mode:&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install praisonai
export OPENAI_API_KEY=xxxxxxxxxxxxxxxxxxxxxx
praisonai --auto create a movie script about Robots in Mars
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Using JavaScript Code&lt;/h2&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;npm install praisonai
export OPENAI_API_KEY=xxxxxxxxxxxxxxxxxxxxxx
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;const { Agent } = require(&#39;praisonai&#39;);
const agent = new Agent({ instructions: &#39;You are a helpful AI assistant&#39; });
agent.start(&#39;Write a movie script about a robot in Mars&#39;);
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/MervinPraison/PraisonAI/main/docs/demo/praisonai-cli-demo.gif&quot; alt=&quot;PraisonAI CLI Demo&quot;&gt;&lt;/p&gt; 
&lt;h2&gt;AI Agents Flow&lt;/h2&gt; 
&lt;pre&gt;&lt;code class=&quot;language-mermaid&quot;&gt;graph LR
    %% Define the main flow
    Start([â–¶ Start]) --&amp;gt; Agent1
    Agent1 --&amp;gt; Process[âš™ Process]
    Process --&amp;gt; Agent2
    Agent2 --&amp;gt; Output([âœ“ Output])
    Process -.-&amp;gt; Agent1
    
    %% Define subgraphs for agents and their tasks
    subgraph Agent1[ ]
        Task1[ðŸ“‹ Task]
        AgentIcon1[ðŸ¤– AI Agent]
        Tools1[ðŸ”§ Tools]
        
        Task1 --- AgentIcon1
        AgentIcon1 --- Tools1
    end
    
    subgraph Agent2[ ]
        Task2[ðŸ“‹ Task]
        AgentIcon2[ðŸ¤– AI Agent]
        Tools2[ðŸ”§ Tools]
        
        Task2 --- AgentIcon2
        AgentIcon2 --- Tools2
    end

    classDef input fill:#8B0000,stroke:#7C90A0,color:#fff
    classDef process fill:#189AB4,stroke:#7C90A0,color:#fff
    classDef tools fill:#2E8B57,stroke:#7C90A0,color:#fff
    classDef transparent fill:none,stroke:none

    class Start,Output,Task1,Task2 input
    class Process,AgentIcon1,AgentIcon2 process
    class Tools1,Tools2 tools
    class Agent1,Agent2 transparent
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;AI Agents with Tools&lt;/h2&gt; 
&lt;p&gt;Create AI agents that can use tools to interact with external systems and perform actions.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-mermaid&quot;&gt;flowchart TB
    subgraph Tools
        direction TB
        T3[Internet Search]
        T1[Code Execution]
        T2[Formatting]
    end

    Input[Input] ---&amp;gt; Agents
    subgraph Agents
        direction LR
        A1[Agent 1]
        A2[Agent 2]
        A3[Agent 3]
    end
    Agents ---&amp;gt; Output[Output]

    T3 --&amp;gt; A1
    T1 --&amp;gt; A2
    T2 --&amp;gt; A3

    style Tools fill:#189AB4,color:#fff
    style Agents fill:#8B0000,color:#fff
    style Input fill:#8B0000,color:#fff
    style Output fill:#8B0000,color:#fff
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;AI Agents with Memory&lt;/h2&gt; 
&lt;p&gt;Create AI agents with memory capabilities for maintaining context and information across tasks.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-mermaid&quot;&gt;flowchart TB
    subgraph Memory
        direction TB
        STM[Short Term]
        LTM[Long Term]
    end

    subgraph Store
        direction TB
        DB[(Vector DB)]
    end

    Input[Input] ---&amp;gt; Agents
    subgraph Agents
        direction LR
        A1[Agent 1]
        A2[Agent 2]
        A3[Agent 3]
    end
    Agents ---&amp;gt; Output[Output]

    Memory &amp;lt;--&amp;gt; Store
    Store &amp;lt;--&amp;gt; A1
    Store &amp;lt;--&amp;gt; A2
    Store &amp;lt;--&amp;gt; A3

    style Memory fill:#189AB4,color:#fff
    style Store fill:#2E8B57,color:#fff
    style Agents fill:#8B0000,color:#fff
    style Input fill:#8B0000,color:#fff
    style Output fill:#8B0000,color:#fff
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;AI Agents with Different Processes&lt;/h2&gt; 
&lt;h3&gt;Sequential Process&lt;/h3&gt; 
&lt;p&gt;The simplest form of task execution where tasks are performed one after another.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-mermaid&quot;&gt;graph LR
    Input[Input] --&amp;gt; A1
    subgraph Agents
        direction LR
        A1[Agent 1] --&amp;gt; A2[Agent 2] --&amp;gt; A3[Agent 3]
    end
    A3 --&amp;gt; Output[Output]

    classDef input fill:#8B0000,stroke:#7C90A0,color:#fff
    classDef process fill:#189AB4,stroke:#7C90A0,color:#fff
    classDef transparent fill:none,stroke:none

    class Input,Output input
    class A1,A2,A3 process
    class Agents transparent
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Hierarchical Process&lt;/h3&gt; 
&lt;p&gt;Uses a manager agent to coordinate task execution and agent assignments.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-mermaid&quot;&gt;graph TB
    Input[Input] --&amp;gt; Manager
    
    subgraph Agents
        Manager[Manager Agent]
        
        subgraph Workers
            direction LR
            W1[Worker 1]
            W2[Worker 2]
            W3[Worker 3]
        end
        
        Manager --&amp;gt; W1
        Manager --&amp;gt; W2
        Manager --&amp;gt; W3
    end
    
    W1 --&amp;gt; Manager
    W2 --&amp;gt; Manager
    W3 --&amp;gt; Manager
    Manager --&amp;gt; Output[Output]

    classDef input fill:#8B0000,stroke:#7C90A0,color:#fff
    classDef process fill:#189AB4,stroke:#7C90A0,color:#fff
    classDef transparent fill:none,stroke:none

    class Input,Output input
    class Manager,W1,W2,W3 process
    class Agents,Workers transparent
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Workflow Process&lt;/h3&gt; 
&lt;p&gt;Advanced process type supporting complex task relationships and conditional execution.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-mermaid&quot;&gt;graph LR
    Input[Input] --&amp;gt; Start
    
    subgraph Workflow
        direction LR
        Start[Start] --&amp;gt; C1{Condition}
        C1 --&amp;gt; |Yes| A1[Agent 1]
        C1 --&amp;gt; |No| A2[Agent 2]
        A1 --&amp;gt; Join
        A2 --&amp;gt; Join
        Join --&amp;gt; A3[Agent 3]
    end
    
    A3 --&amp;gt; Output[Output]

    classDef input fill:#8B0000,stroke:#7C90A0,color:#fff
    classDef process fill:#189AB4,stroke:#7C90A0,color:#fff
    classDef decision fill:#2E8B57,stroke:#7C90A0,color:#fff
    classDef transparent fill:none,stroke:none

    class Input,Output input
    class Start,A1,A2,A3,Join process
    class C1 decision
    class Workflow transparent
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Agentic Routing Workflow&lt;/h4&gt; 
&lt;p&gt;Create AI agents that can dynamically route tasks to specialized LLM instances.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-mermaid&quot;&gt;flowchart LR
    In[In] --&amp;gt; Router[LLM Call Router]
    Router --&amp;gt; LLM1[LLM Call 1]
    Router --&amp;gt; LLM2[LLM Call 2]
    Router --&amp;gt; LLM3[LLM Call 3]
    LLM1 --&amp;gt; Out[Out]
    LLM2 --&amp;gt; Out
    LLM3 --&amp;gt; Out
    
    style In fill:#8B0000,color:#fff
    style Router fill:#2E8B57,color:#fff
    style LLM1 fill:#2E8B57,color:#fff
    style LLM2 fill:#2E8B57,color:#fff
    style LLM3 fill:#2E8B57,color:#fff
    style Out fill:#8B0000,color:#fff
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Agentic Orchestrator Worker&lt;/h4&gt; 
&lt;p&gt;Create AI agents that orchestrate and distribute tasks among specialized workers.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-mermaid&quot;&gt;flowchart LR
    In[In] --&amp;gt; Router[LLM Call Router]
    Router --&amp;gt; LLM1[LLM Call 1]
    Router --&amp;gt; LLM2[LLM Call 2]
    Router --&amp;gt; LLM3[LLM Call 3]
    LLM1 --&amp;gt; Synthesizer[Synthesizer]
    LLM2 --&amp;gt; Synthesizer
    LLM3 --&amp;gt; Synthesizer
    Synthesizer --&amp;gt; Out[Out]
    
    style In fill:#8B0000,color:#fff
    style Router fill:#2E8B57,color:#fff
    style LLM1 fill:#2E8B57,color:#fff
    style LLM2 fill:#2E8B57,color:#fff
    style LLM3 fill:#2E8B57,color:#fff
    style Synthesizer fill:#2E8B57,color:#fff
    style Out fill:#8B0000,color:#fff
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Agentic Autonomous Workflow&lt;/h4&gt; 
&lt;p&gt;Create AI agents that can autonomously monitor, act, and adapt based on environment feedback.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-mermaid&quot;&gt;flowchart LR
    Human[Human] &amp;lt;--&amp;gt; LLM[LLM Call]
    LLM --&amp;gt;|ACTION| Environment[Environment]
    Environment --&amp;gt;|FEEDBACK| LLM
    LLM --&amp;gt; Stop[Stop]
    
    style Human fill:#8B0000,color:#fff
    style LLM fill:#2E8B57,color:#fff
    style Environment fill:#8B0000,color:#fff
    style Stop fill:#333,color:#fff
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Agentic Parallelization&lt;/h4&gt; 
&lt;p&gt;Create AI agents that can execute tasks in parallel for improved performance.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-mermaid&quot;&gt;flowchart LR
    In[In] --&amp;gt; LLM2[LLM Call 2]
    In --&amp;gt; LLM1[LLM Call 1]
    In --&amp;gt; LLM3[LLM Call 3]
    LLM1 --&amp;gt; Aggregator[Aggregator]
    LLM2 --&amp;gt; Aggregator
    LLM3 --&amp;gt; Aggregator
    Aggregator --&amp;gt; Out[Out]
    
    style In fill:#8B0000,color:#fff
    style LLM1 fill:#2E8B57,color:#fff
    style LLM2 fill:#2E8B57,color:#fff
    style LLM3 fill:#2E8B57,color:#fff
    style Aggregator fill:#fff,color:#000
    style Out fill:#8B0000,color:#fff
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Agentic Prompt Chaining&lt;/h4&gt; 
&lt;p&gt;Create AI agents with sequential prompt chaining for complex workflows.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-mermaid&quot;&gt;flowchart LR
    In[In] --&amp;gt; LLM1[LLM Call 1] --&amp;gt; Gate{Gate}
    Gate --&amp;gt;|Pass| LLM2[LLM Call 2] --&amp;gt;|Output 2| LLM3[LLM Call 3] --&amp;gt; Out[Out]
    Gate --&amp;gt;|Fail| Exit[Exit]
    
    style In fill:#8B0000,color:#fff
    style LLM1 fill:#2E8B57,color:#fff
    style LLM2 fill:#2E8B57,color:#fff
    style LLM3 fill:#2E8B57,color:#fff
    style Out fill:#8B0000,color:#fff
    style Exit fill:#8B0000,color:#fff
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Agentic Evaluator Optimizer&lt;/h4&gt; 
&lt;p&gt;Create AI agents that can generate and optimize solutions through iterative feedback.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-mermaid&quot;&gt;flowchart LR
    In[In] --&amp;gt; Generator[LLM Call Generator] 
    Generator --&amp;gt;|SOLUTION| Evaluator[LLM Call Evaluator] --&amp;gt;|ACCEPTED| Out[Out]
    Evaluator --&amp;gt;|REJECTED + FEEDBACK| Generator
    
    style In fill:#8B0000,color:#fff
    style Generator fill:#2E8B57,color:#fff
    style Evaluator fill:#2E8B57,color:#fff
    style Out fill:#8B0000,color:#fff
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Repetitive Agents&lt;/h4&gt; 
&lt;p&gt;Create AI agents that can efficiently handle repetitive tasks through automated loops.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-mermaid&quot;&gt;flowchart LR
    In[Input] --&amp;gt; LoopAgent[(&quot;Looping Agent&quot;)]
    LoopAgent --&amp;gt; Task[Task]
    Task --&amp;gt; |Next iteration| LoopAgent
    Task --&amp;gt; |Done| Out[Output]
    
    style In fill:#8B0000,color:#fff
    style LoopAgent fill:#2E8B57,color:#fff,shape:circle
    style Task fill:#2E8B57,color:#fff
    style Out fill:#8B0000,color:#fff
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Adding Models&lt;/h2&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;a href=&quot;https://docs.praison.ai/models&quot;&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;https://img.shields.io/badge/%F0%9F%93%9A_Models-Visit_docs.praison.ai-blue?style=for-the-badge&amp;amp;logo=bookstack&amp;amp;logoColor=white&quot; alt=&quot;Models&quot;&gt; &lt;/p&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;Ollama Integration&lt;/h2&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;export OPENAI_BASE_URL=http://localhost:11434/v1
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Groq Integration&lt;/h2&gt; 
&lt;p&gt;Replace xxxx with Groq API KEY:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;export OPENAI_API_KEY=xxxxxxxxxxx
export OPENAI_BASE_URL=https://api.groq.com/openai/v1
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;No Code Options&lt;/h2&gt; 
&lt;h2&gt;Agents Playbook&lt;/h2&gt; 
&lt;h3&gt;Simple Playbook Example&lt;/h3&gt; 
&lt;p&gt;Create &lt;code&gt;agents.yaml&lt;/code&gt; file and add the code below:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-yaml&quot;&gt;framework: praisonai
topic: Artificial Intelligence
roles:
  screenwriter:
    backstory: &quot;Skilled in crafting scripts with engaging dialogue about {topic}.&quot;
    goal: Create scripts from concepts.
    role: Screenwriter
    tasks:
      scriptwriting_task:
        description: &quot;Develop scripts with compelling characters and dialogue about {topic}.&quot;
        expected_output: &quot;Complete script ready for production.&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;To run the playbook:&lt;/em&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;praisonai agents.yaml
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Use 100+ Models&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.praison.ai/models/&quot;&gt;https://docs.praison.ai/models/&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;a href=&quot;https://docs.praison.ai&quot;&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;https://img.shields.io/badge/%F0%9F%93%9A_Documentation-Visit_docs.praison.ai-blue?style=for-the-badge&amp;amp;logo=bookstack&amp;amp;logoColor=white&quot; alt=&quot;Documentation&quot;&gt; &lt;/p&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;Development:&lt;/h2&gt; 
&lt;p&gt;Below is used for development only.&lt;/p&gt; 
&lt;h3&gt;Using uv&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# Install uv if you haven&#39;t already
pip install uv

# Install from requirements
uv pip install -r pyproject.toml

# Install with extras
uv pip install -r pyproject.toml --extra code
uv pip install -r pyproject.toml --extra &quot;crewai,autogen&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Fork on GitHub: Use the &quot;Fork&quot; button on the repository page.&lt;/li&gt; 
 &lt;li&gt;Clone your fork: &lt;code&gt;git clone https://github.com/yourusername/praisonAI.git&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Create a branch: &lt;code&gt;git checkout -b new-feature&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Make changes and commit: &lt;code&gt;git commit -am &quot;Add some feature&quot;&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Push to your fork: &lt;code&gt;git push origin new-feature&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Submit a pull request via GitHub&#39;s web interface.&lt;/li&gt; 
 &lt;li&gt;Await feedback from project maintainers.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Other Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ðŸ”„ Use CrewAI or AutoGen Framework&lt;/li&gt; 
 &lt;li&gt;ðŸ’» Chat with ENTIRE Codebase&lt;/li&gt; 
 &lt;li&gt;ðŸŽ¨ Interactive UIs&lt;/li&gt; 
 &lt;li&gt;ðŸ“„ YAML-based Configuration&lt;/li&gt; 
 &lt;li&gt;ðŸ› ï¸ Custom Tool Integration&lt;/li&gt; 
 &lt;li&gt;ðŸ” Internet Search Capability (using Crawl4AI and Tavily)&lt;/li&gt; 
 &lt;li&gt;ðŸ–¼ï¸ Vision Language Model (VLM) Support&lt;/li&gt; 
 &lt;li&gt;ðŸŽ™ï¸ Real-time Voice Interaction&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://docs.praison.ai&quot;&gt;&lt;img src=&quot;https://api.star-history.com/svg?repos=MervinPraison/PraisonAI&amp;amp;type=Date&quot; alt=&quot;Star History Chart&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Video Tutorials&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Topic&lt;/th&gt; 
   &lt;th&gt;Video&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;AI Agents with Self Reflection&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=vLXobEN2Vc8&quot;&gt;&lt;img src=&quot;https://img.youtube.com/vi/vLXobEN2Vc8/0.jpg&quot; alt=&quot;Self Reflection&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Reasoning Data Generating Agent&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=fUT332Y2zA8&quot;&gt;&lt;img src=&quot;https://img.youtube.com/vi/fUT332Y2zA8/0.jpg&quot; alt=&quot;Reasoning Data&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;AI Agents with Reasoning&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=KNDVWGN3TpM&quot;&gt;&lt;img src=&quot;https://img.youtube.com/vi/KNDVWGN3TpM/0.jpg&quot; alt=&quot;Reasoning&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Multimodal AI Agents&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=hjAWmUT1qqY&quot;&gt;&lt;img src=&quot;https://img.youtube.com/vi/hjAWmUT1qqY/0.jpg&quot; alt=&quot;Multimodal&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;AI Agents Workflow&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=yWTH44QPl2A&quot;&gt;&lt;img src=&quot;https://img.youtube.com/vi/yWTH44QPl2A/0.jpg&quot; alt=&quot;Workflow&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Async AI Agents&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=VhVQfgo00LE&quot;&gt;&lt;img src=&quot;https://img.youtube.com/vi/VhVQfgo00LE/0.jpg&quot; alt=&quot;Async&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Mini AI Agents&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=OkvYp5aAGSg&quot;&gt;&lt;img src=&quot;https://img.youtube.com/vi/OkvYp5aAGSg/0.jpg&quot; alt=&quot;Mini&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;AI Agents with Memory&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=1hVfVxvPnnQ&quot;&gt;&lt;img src=&quot;https://img.youtube.com/vi/1hVfVxvPnnQ/0.jpg&quot; alt=&quot;Memory&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Repetitive Agents&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=dAYGxsjDOPg&quot;&gt;&lt;img src=&quot;https://img.youtube.com/vi/dAYGxsjDOPg/0.jpg&quot; alt=&quot;Repetitive&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Introduction&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=Fn1lQjC0GO0&quot;&gt;&lt;img src=&quot;https://img.youtube.com/vi/Fn1lQjC0GO0/0.jpg&quot; alt=&quot;Introduction&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Tools Overview&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=XaQRgRpV7jo&quot;&gt;&lt;img src=&quot;https://img.youtube.com/vi/XaQRgRpV7jo/0.jpg&quot; alt=&quot;Tools Overview&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Custom Tools&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=JSU2Rndh06c&quot;&gt;&lt;img src=&quot;https://img.youtube.com/vi/JSU2Rndh06c/0.jpg&quot; alt=&quot;Custom Tools&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Firecrawl Integration&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=UoqUDcLcOYo&quot;&gt;&lt;img src=&quot;https://img.youtube.com/vi/UoqUDcLcOYo/0.jpg&quot; alt=&quot;Firecrawl&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;User Interface&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=tg-ZjNl3OCg&quot;&gt;&lt;img src=&quot;https://img.youtube.com/vi/tg-ZjNl3OCg/0.jpg&quot; alt=&quot;UI&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Crawl4AI Integration&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=KAvuVUh0XU8&quot;&gt;&lt;img src=&quot;https://img.youtube.com/vi/KAvuVUh0XU8/0.jpg&quot; alt=&quot;Crawl4AI&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Chat Interface&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=sw3uDqn2h1Y&quot;&gt;&lt;img src=&quot;https://img.youtube.com/vi/sw3uDqn2h1Y/0.jpg&quot; alt=&quot;Chat&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Code Interface&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=_5jQayO-MQY&quot;&gt;&lt;img src=&quot;https://img.youtube.com/vi/_5jQayO-MQY/0.jpg&quot; alt=&quot;Code&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Mem0 Integration&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=KIGSgRxf1cY&quot;&gt;&lt;img src=&quot;https://img.youtube.com/vi/KIGSgRxf1cY/0.jpg&quot; alt=&quot;Mem0&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Training&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=aLawE8kwCrI&quot;&gt;&lt;img src=&quot;https://img.youtube.com/vi/aLawE8kwCrI/0.jpg&quot; alt=&quot;Training&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Realtime Voice Interface&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=frRHfevTCSw&quot;&gt;&lt;img src=&quot;https://img.youtube.com/vi/frRHfevTCSw/0.jpg&quot; alt=&quot;Realtime&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Call Interface&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=m1cwrUG2iAk&quot;&gt;&lt;img src=&quot;https://img.youtube.com/vi/m1cwrUG2iAk/0.jpg&quot; alt=&quot;Call&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Reasoning Extract Agents&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=2PPamsADjJA&quot;&gt;&lt;img src=&quot;https://img.youtube.com/vi/2PPamsADjJA/0.jpg&quot; alt=&quot;Reasoning Extract&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt;</description>
    </item>
    
    <item>
      <title>google-gemini/cookbook</title>
      <link>https://github.com/google-gemini/cookbook</link>
      <description>&lt;p&gt;Examples and guides for using the Gemini API&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Welcome to the Gemini API Cookbook&lt;/h1&gt; 
&lt;p&gt;This cookbook provides a structured learning path for using the Gemini API, focusing on hands-on tutorials and practical examples.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;For comprehensive API documentation, visit &lt;a href=&quot;https://ai.google.dev/gemini-api/docs&quot;&gt;ai.google.dev&lt;/a&gt;.&lt;/strong&gt; &lt;br&gt;&lt;br&gt;&lt;/p&gt; 
&lt;h2&gt;Important: Migration&lt;/h2&gt; 
&lt;p&gt;With Gemini 2 we are offering a &lt;a href=&quot;https://github.com/googleapis/python-genai&quot;&gt;new SDK&lt;/a&gt; (&lt;code&gt;&lt;a href=&quot;https://pypi.org/project/google-genai/&quot;&gt;google-genai&lt;/a&gt;&lt;/code&gt;, &lt;code&gt;v1.0&lt;/code&gt;). The updated SDK is fully compatible with all Gemini API models and features, including recent additions like the &lt;a href=&quot;https://aistudio.google.com/live&quot;&gt;live API&lt;/a&gt; (audio + video streaming), improved tool usage ( &lt;a href=&quot;https://ai.google.dev/gemini-api/docs/code-execution?lang=python&quot;&gt;code execution&lt;/a&gt;, &lt;a href=&quot;https://ai.google.dev/gemini-api/docs/function-calling/tutorial?lang=python&quot;&gt;function calling&lt;/a&gt; and integrated &lt;a href=&quot;https://ai.google.dev/gemini-api/docs/grounding?lang=python&quot;&gt;Google search grounding&lt;/a&gt;), and media generation (&lt;a href=&quot;https://ai.google.dev/gemini-api/docs/imagen&quot;&gt;Imagen&lt;/a&gt;). This SDK allows you to connect to the Gemini API through either &lt;a href=&quot;https://aistudio.google.com/prompts/new_chat?model=gemini-2.0-flash-exp&quot;&gt;Google AI Studio&lt;/a&gt; or &lt;a href=&quot;https://cloud.google.com/vertex-ai/generative-ai/docs/gemini-v2&quot;&gt;Vertex AI&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The &lt;code&gt;&lt;a href=&quot;https://pypi.org/project/google-generativeai&quot;&gt;google-generativeai&lt;/a&gt;&lt;/code&gt; package will continue to support the original Gemini models. It &lt;em&gt;can&lt;/em&gt; also be used with Gemini 2 models, just with a limited feature set. All new features will be developed in the new Google GenAI SDK.&lt;/p&gt; 
&lt;p&gt;See the &lt;a href=&quot;https://ai.google.dev/gemini-api/docs/migrate&quot;&gt;migration guide&lt;/a&gt; for details. &lt;br&gt;&lt;br&gt;&lt;/p&gt; 
&lt;h2&gt;Navigating the Cookbook&lt;/h2&gt; 
&lt;p&gt;This cookbook is organized into two main categories:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/google-gemini/cookbook/tree/main/quickstarts/&quot;&gt;Quick Starts&lt;/a&gt;:&lt;/strong&gt; Step-by-step guides covering both introductory topics (&quot;&lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Get_started.ipynb&quot;&gt;Get Started&lt;/a&gt;&quot;) and specific API features.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/google-gemini/cookbook/tree/main/examples/&quot;&gt;Examples&lt;/a&gt;:&lt;/strong&gt; Practical use cases demonstrating how to combine multiple features.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;We also showcase &lt;strong&gt;Demos&lt;/strong&gt; in separate repositories, illustrating end-to-end applications of the Gemini API. &lt;br&gt;&lt;br&gt;&lt;/p&gt; 
&lt;h2&gt;What&#39;s New?&lt;/h2&gt; 
&lt;p&gt;Here are the recent additions and updates to the Gemini API and the Cookbook:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Gemini 2.0 models:&lt;/strong&gt; Explore the capabilities of the latest Gemini 2.0 models! See the &lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Get_started.ipynb&quot;&gt;Get Started Guide&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Imagen&lt;/strong&gt;: Get started with our image generation model with this brand new &lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Get_started_imagen.ipynb&quot;&gt;Imagen guide&lt;/a&gt;!&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Recently Added Guides:&lt;/strong&gt;. 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/examples/Browser_as_a_tool.ipynb&quot;&gt;Browser as a tool&lt;/a&gt;: Use a web browser for live and internal (intranet) web interactions&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Code_Execution.ipynb&quot;&gt;Code execution&lt;/a&gt;: Generating and running Python code to solve complex tasks and even ouput graphs&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Get_started_thinking.ipynb&quot;&gt;Thinking model&lt;/a&gt;: Discover the thinking model capabilities.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; 
&lt;h2&gt;1. Quick Starts&lt;/h2&gt; 
&lt;p&gt;The &lt;a href=&quot;https://github.com/google-gemini/cookbook/tree/main/quickstarts/&quot;&gt;quickstarts section&lt;/a&gt; contains step-by-step tutorials to get you started with Gemini and learn about its specific features.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;To begin, you&#39;ll need:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;A Google account.&lt;/li&gt; 
 &lt;li&gt;An API key (create one in &lt;a href=&quot;https://aistudio.google.com/app/apikey&quot;&gt;Google AI Studio&lt;/a&gt;). &lt;br&gt;&lt;br&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;We recommend starting with the following:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Authentication.ipynb&quot;&gt;Authentication&lt;/a&gt;: Set up your API key for access.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Get_started.ipynb&quot;&gt;&lt;strong&gt;Get started&lt;/strong&gt;&lt;/a&gt;: Get started with Gemini models and the Gemini API, covering basic prompting and multimodal input. &lt;br&gt;&lt;br&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Then, explore the other quickstarts tutorials to learn about individual features:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Get_started_LiveAPI.ipynb&quot;&gt;Get started with Live API&lt;/a&gt;: Get started with the live API with this comprehensive overview of its capabilities&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Get_started_imagen.ipynb&quot;&gt;Get started with Imagen&lt;/a&gt; and &lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Image_out.ipynb&quot;&gt;Image-out&lt;/a&gt;: Get started with our image generation capabilities&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Search_Grounding.ipynb&quot;&gt;Grounding&lt;/a&gt;: use Google Search for grounded responses&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Code_Execution.ipynb&quot;&gt;Code execution&lt;/a&gt;: Generating and running Python code to solve complex tasks and even ouput graphs&lt;/li&gt; 
 &lt;li&gt;And &lt;a href=&quot;https://github.com/google-gemini/cookbook/tree/main/quickstarts/&quot;&gt;many more&lt;/a&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;2. Examples (Practical Use Cases)&lt;/h2&gt; 
&lt;p&gt;These examples demonstrate how to combine multiple Gemini API features or 3rd-party tools to build more complex applications.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/examples/Book_illustration.ipynb&quot;&gt;Illustrate a book&lt;/a&gt;: Use Gemini and Imagen to create illustration for an open-source book&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/examples/Animated_Story_Video_Generation_gemini.ipynb&quot;&gt;Animated Story Generation&lt;/a&gt;: Create animated videos by combining Gemini&#39;s story generation, Imagen, and audio synthesis&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/examples/LiveAPI_plotting_and_mapping.ipynb&quot;&gt;Plotting and mapping Live&lt;/a&gt;: Mix &lt;em&gt;Live API&lt;/em&gt; and &lt;em&gt;Code execution&lt;/em&gt; to solve complex tasks live&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/examples/Spatial_understanding_3d.ipynb&quot;&gt;3D Spatial understanding&lt;/a&gt;: Use Gemini &lt;em&gt;3D spatial&lt;/em&gt; abilities to understand 3D scenes&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/examples/gradio_audio.py&quot;&gt;Gradio and live API&lt;/a&gt;: Use gradio to deploy your own instance of the &lt;em&gt;Live API&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;And &lt;a href=&quot;https://github.com/google-gemini/cookbook/tree/main/examples/&quot;&gt;many many more&lt;/a&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;3. Demos (End-to-End Applications)&lt;/h2&gt; 
&lt;p&gt;These fully functional, end-to-end applications showcase the power of Gemini in real-world scenarios.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/google-gemini/gemini-api-quickstart&quot;&gt;Gemini API quickstart&lt;/a&gt;: Python Flask App running with the Google AI Gemini API, designed to get you started building with Gemini&#39;s multi-modal capabilities&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/google-gemini/multimodal-live-api-web-console&quot;&gt;Multimodal Live API Web Console&lt;/a&gt;: React-based starter app for using the Multimodal Live API over a websocket&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/google-gemini/starter-applets&quot;&gt;Google AI Studio Starter Applets&lt;/a&gt;: A collection of small apps that demonstrate how Gemini can be used to create interactive experiences &lt;br&gt;&lt;br&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Official SDKs&lt;/h2&gt; 
&lt;p&gt;The Gemini API is a REST API. You can call it directly using tools like &lt;code&gt;curl&lt;/code&gt; (see &lt;a href=&quot;https://github.com/google-gemini/cookbook/tree/main/quickstarts/rest/&quot;&gt;REST examples&lt;/a&gt; or the great &lt;a href=&quot;https://www.postman.com/ai-on-postman/google-gemini-apis/overview&quot;&gt;Postman workspace&lt;/a&gt;), or use one of our official SDKs:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/googleapis/python-genai&quot;&gt;Python&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/google/generative-ai-go&quot;&gt;Go&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/google/generative-ai-js&quot;&gt;Node.js&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/google/generative-ai-dart&quot;&gt;Dart (Flutter)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/google/generative-ai-android&quot;&gt;Android&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/google/generative-ai-swift&quot;&gt;Swift&lt;/a&gt; &lt;br&gt;&lt;br&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Get Help&lt;/h2&gt; 
&lt;p&gt;Ask a question on the &lt;a href=&quot;https://discuss.ai.google.dev/&quot;&gt;Google AI Developer Forum&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;The Gemini API on Google Cloud Vertex AI&lt;/h2&gt; 
&lt;p&gt;For enterprise developers, the Gemini API is also available on Google Cloud Vertex AI. See &lt;a href=&quot;https://github.com/GoogleCloudPlatform/generative-ai&quot;&gt;this repo&lt;/a&gt; for examples.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Contributions are welcome! See &lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/CONTRIBUTING.md&quot;&gt;CONTRIBUTING.md&lt;/a&gt; for details.&lt;/p&gt; 
&lt;p&gt;Thank you for developing with the Gemini API! We&#39;re excited to see what you create.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>google-research/google-research</title>
      <link>https://github.com/google-research/google-research</link>
      <description>&lt;p&gt;Google Research&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Google Research&lt;/h1&gt; 
&lt;p&gt;This repository contains code released by &lt;a href=&quot;https://research.google&quot;&gt;Google Research&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;All datasets in this repository are released under the CC BY 4.0 International license, which can be found here: &lt;a href=&quot;https://creativecommons.org/licenses/by/4.0/legalcode&quot;&gt;https://creativecommons.org/licenses/by/4.0/legalcode&lt;/a&gt;. All source files in this repository are released under the Apache 2.0 license, the text of which can be found in the LICENSE file.&lt;/p&gt; 
&lt;hr&gt; 
&lt;p&gt;Because the repo is large, we recommend you download only the subdirectory of interest:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Use GitHub editor to open the project. To open the editor change the url from github.com to github.dev in the address bar.&lt;/li&gt; 
 &lt;li&gt;In the left navigation panel, right-click on the folder of interest and select download.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If you&#39;d like to submit a pull request, you&#39;ll need to clone the repository; we recommend making a shallow clone (without history).&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;git clone git@github.com:google-research/google-research.git --depth=1
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr&gt; 
&lt;p&gt;&lt;em&gt;Disclaimer: This is not an official Google product.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;Updated in 2023.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>facebookresearch/sam2</title>
      <link>https://github.com/facebookresearch/sam2</link>
      <description>&lt;p&gt;The repository provides code for running inference with the Meta Segment Anything Model 2 (SAM 2), links for downloading the trained model checkpoints, and example notebooks that show how to use the model.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;SAM 2: Segment Anything in Images and Videos&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://ai.meta.com/research/&quot;&gt;AI at Meta, FAIR&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://nikhilaravi.com/&quot;&gt;Nikhila Ravi&lt;/a&gt;, &lt;a href=&quot;https://gabeur.github.io/&quot;&gt;Valentin Gabeur&lt;/a&gt;, &lt;a href=&quot;https://scholar.google.com/citations?user=E8DVVYQAAAAJ&amp;amp;hl=en&quot;&gt;Yuan-Ting Hu&lt;/a&gt;, &lt;a href=&quot;https://ronghanghu.com/&quot;&gt;Ronghang Hu&lt;/a&gt;, &lt;a href=&quot;https://scholar.google.com/citations?user=4LWx24UAAAAJ&amp;amp;hl=en&quot;&gt;Chaitanya Ryali&lt;/a&gt;, &lt;a href=&quot;https://scholar.google.com/citations?user=VeTSl0wAAAAJ&amp;amp;hl=en&quot;&gt;Tengyu Ma&lt;/a&gt;, &lt;a href=&quot;https://hkhedr.com/&quot;&gt;Haitham Khedr&lt;/a&gt;, &lt;a href=&quot;https://scholar.google.de/citations?user=Tpt57v0AAAAJ&amp;amp;hl=en&quot;&gt;Roman RÃ¤dle&lt;/a&gt;, &lt;a href=&quot;https://scholar.google.com/citations?hl=fr&amp;amp;user=n-SnMhoAAAAJ&quot;&gt;Chloe Rolland&lt;/a&gt;, &lt;a href=&quot;https://scholar.google.com/citations?user=c8IpF9gAAAAJ&amp;amp;hl=en&quot;&gt;Laura Gustafson&lt;/a&gt;, &lt;a href=&quot;https://ericmintun.github.io/&quot;&gt;Eric Mintun&lt;/a&gt;, &lt;a href=&quot;https://junting.github.io/&quot;&gt;Junting Pan&lt;/a&gt;, &lt;a href=&quot;https://scholar.google.co.in/citations?user=m34oaWEAAAAJ&amp;amp;hl=en&quot;&gt;Kalyan Vasudev Alwala&lt;/a&gt;, &lt;a href=&quot;https://www.nicolascarion.com/&quot;&gt;Nicolas Carion&lt;/a&gt;, &lt;a href=&quot;https://chaoyuan.org/&quot;&gt;Chao-Yuan Wu&lt;/a&gt;, &lt;a href=&quot;https://www.rossgirshick.info/&quot;&gt;Ross Girshick&lt;/a&gt;, &lt;a href=&quot;https://pdollar.github.io/&quot;&gt;Piotr DollÃ¡r&lt;/a&gt;, &lt;a href=&quot;https://feichtenhofer.github.io/&quot;&gt;Christoph Feichtenhofer&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[&lt;a href=&quot;https://ai.meta.com/research/publications/sam-2-segment-anything-in-images-and-videos/&quot;&gt;&lt;code&gt;Paper&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&quot;https://ai.meta.com/sam2&quot;&gt;&lt;code&gt;Project&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&quot;https://sam2.metademolab.com/&quot;&gt;&lt;code&gt;Demo&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&quot;https://ai.meta.com/datasets/segment-anything-video&quot;&gt;&lt;code&gt;Dataset&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&quot;https://ai.meta.com/blog/segment-anything-2&quot;&gt;&lt;code&gt;Blog&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/sam2/main/#citing-sam-2&quot;&gt;&lt;code&gt;BibTeX&lt;/code&gt;&lt;/a&gt;]&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/facebookresearch/sam2/main/assets/model_diagram.png?raw=true&quot; alt=&quot;SAM 2 architecture&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Segment Anything Model 2 (SAM 2)&lt;/strong&gt; is a foundation model towards solving promptable visual segmentation in images and videos. We extend SAM to video by considering images as a video with a single frame. The model design is a simple transformer architecture with streaming memory for real-time video processing. We build a model-in-the-loop data engine, which improves model and data via user interaction, to collect &lt;a href=&quot;https://ai.meta.com/datasets/segment-anything-video&quot;&gt;&lt;strong&gt;our SA-V dataset&lt;/strong&gt;&lt;/a&gt;, the largest video segmentation dataset to date. SAM 2 trained on our data provides strong performance across a wide range of tasks and visual domains.&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/facebookresearch/sam2/main/assets/sa_v_dataset.jpg?raw=true&quot; alt=&quot;SA-V dataset&quot;&gt;&lt;/p&gt; 
&lt;h2&gt;Latest updates&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;12/11/2024 -- full model compilation for a major VOS speedup and a new &lt;code&gt;SAM2VideoPredictor&lt;/code&gt; to better handle multi-object tracking&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;We now support &lt;code&gt;torch.compile&lt;/code&gt; of the entire SAM 2 model on videos, which can be turned on by setting &lt;code&gt;vos_optimized=True&lt;/code&gt; in &lt;code&gt;build_sam2_video_predictor&lt;/code&gt;, leading to a major speedup for VOS inference.&lt;/li&gt; 
 &lt;li&gt;We update the implementation of &lt;code&gt;SAM2VideoPredictor&lt;/code&gt; to support independent per-object inference, allowing us to relax the assumption of prompting for multi-object tracking and adding new objects after tracking starts.&lt;/li&gt; 
 &lt;li&gt;See &lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/sam2/main/RELEASE_NOTES.md&quot;&gt;&lt;code&gt;RELEASE_NOTES.md&lt;/code&gt;&lt;/a&gt; for full details.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;09/30/2024 -- SAM 2.1 Developer Suite (new checkpoints, training code, web demo) is released&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;A new suite of improved model checkpoints (denoted as &lt;strong&gt;SAM 2.1&lt;/strong&gt;) are released. See &lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/sam2/main/#model-description&quot;&gt;Model Description&lt;/a&gt; for details. 
  &lt;ul&gt; 
   &lt;li&gt;To use the new SAM 2.1 checkpoints, you need the latest model code from this repo. If you have installed an earlier version of this repo, please first uninstall the previous version via &lt;code&gt;pip uninstall SAM-2&lt;/code&gt;, pull the latest code from this repo (with &lt;code&gt;git pull&lt;/code&gt;), and then reinstall the repo following &lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/sam2/main/#installation&quot;&gt;Installation&lt;/a&gt; below.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;The training (and fine-tuning) code has been released. See &lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/sam2/main/training/README.md&quot;&gt;&lt;code&gt;training/README.md&lt;/code&gt;&lt;/a&gt; on how to get started.&lt;/li&gt; 
 &lt;li&gt;The frontend + backend code for the SAM 2 web demo has been released. See &lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/sam2/main/demo/README.md&quot;&gt;&lt;code&gt;demo/README.md&lt;/code&gt;&lt;/a&gt; for details.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;SAM 2 needs to be installed first before use. The code requires &lt;code&gt;python&amp;gt;=3.10&lt;/code&gt;, as well as &lt;code&gt;torch&amp;gt;=2.5.1&lt;/code&gt; and &lt;code&gt;torchvision&amp;gt;=0.20.1&lt;/code&gt;. Please follow the instructions &lt;a href=&quot;https://pytorch.org/get-started/locally/&quot;&gt;here&lt;/a&gt; to install both PyTorch and TorchVision dependencies. You can install SAM 2 on a GPU machine using:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;git clone https://github.com/facebookresearch/sam2.git &amp;amp;&amp;amp; cd sam2

pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you are installing on Windows, it&#39;s strongly recommended to use &lt;a href=&quot;https://learn.microsoft.com/en-us/windows/wsl/install&quot;&gt;Windows Subsystem for Linux (WSL)&lt;/a&gt; with Ubuntu.&lt;/p&gt; 
&lt;p&gt;To use the SAM 2 predictor and run the example notebooks, &lt;code&gt;jupyter&lt;/code&gt; and &lt;code&gt;matplotlib&lt;/code&gt; are required and can be installed by:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install -e &quot;.[notebooks]&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;It&#39;s recommended to create a new Python environment via &lt;a href=&quot;https://www.anaconda.com/&quot;&gt;Anaconda&lt;/a&gt; for this installation and install PyTorch 2.5.1 (or higher) via &lt;code&gt;pip&lt;/code&gt; following &lt;a href=&quot;https://pytorch.org/&quot;&gt;https://pytorch.org/&lt;/a&gt;. If you have a PyTorch version lower than 2.5.1 in your current environment, the installation command above will try to upgrade it to the latest PyTorch version using &lt;code&gt;pip&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;The step above requires compiling a custom CUDA kernel with the &lt;code&gt;nvcc&lt;/code&gt; compiler. If it isn&#39;t already available on your machine, please install the &lt;a href=&quot;https://developer.nvidia.com/cuda-toolkit-archive&quot;&gt;CUDA toolkits&lt;/a&gt; with a version that matches your PyTorch CUDA version.&lt;/li&gt; 
 &lt;li&gt;If you see a message like &lt;code&gt;Failed to build the SAM 2 CUDA extension&lt;/code&gt; during installation, you can ignore it and still use SAM 2 (some post-processing functionality may be limited, but it doesn&#39;t affect the results in most cases).&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Please see &lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/sam2/main/INSTALL.md&quot;&gt;&lt;code&gt;INSTALL.md&lt;/code&gt;&lt;/a&gt; for FAQs on potential issues and solutions.&lt;/p&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;h3&gt;Download Checkpoints&lt;/h3&gt; 
&lt;p&gt;First, we need to download a model checkpoint. All the model checkpoints can be downloaded by running:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;cd checkpoints &amp;amp;&amp;amp; \
./download_ckpts.sh &amp;amp;&amp;amp; \
cd ..
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or individually from:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_tiny.pt&quot;&gt;sam2.1_hiera_tiny.pt&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_small.pt&quot;&gt;sam2.1_hiera_small.pt&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_base_plus.pt&quot;&gt;sam2.1_hiera_base_plus.pt&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_large.pt&quot;&gt;sam2.1_hiera_large.pt&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;(note that these are the improved checkpoints denoted as SAM 2.1; see &lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/sam2/main/#model-description&quot;&gt;Model Description&lt;/a&gt; for details.)&lt;/p&gt; 
&lt;p&gt;Then SAM 2 can be used in a few lines as follows for image and video prediction.&lt;/p&gt; 
&lt;h3&gt;Image prediction&lt;/h3&gt; 
&lt;p&gt;SAM 2 has all the capabilities of &lt;a href=&quot;https://github.com/facebookresearch/segment-anything&quot;&gt;SAM&lt;/a&gt; on static images, and we provide image prediction APIs that closely resemble SAM for image use cases. The &lt;code&gt;SAM2ImagePredictor&lt;/code&gt; class has an easy interface for image prompting.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import torch
from sam2.build_sam import build_sam2
from sam2.sam2_image_predictor import SAM2ImagePredictor

checkpoint = &quot;./checkpoints/sam2.1_hiera_large.pt&quot;
model_cfg = &quot;configs/sam2.1/sam2.1_hiera_l.yaml&quot;
predictor = SAM2ImagePredictor(build_sam2(model_cfg, checkpoint))

with torch.inference_mode(), torch.autocast(&quot;cuda&quot;, dtype=torch.bfloat16):
    predictor.set_image(&amp;lt;your_image&amp;gt;)
    masks, _, _ = predictor.predict(&amp;lt;input_prompts&amp;gt;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Please refer to the examples in &lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/sam2/main/notebooks/image_predictor_example.ipynb&quot;&gt;image_predictor_example.ipynb&lt;/a&gt; (also in Colab &lt;a href=&quot;https://colab.research.google.com/github/facebookresearch/sam2/blob/main/notebooks/image_predictor_example.ipynb&quot;&gt;here&lt;/a&gt;) for static image use cases.&lt;/p&gt; 
&lt;p&gt;SAM 2 also supports automatic mask generation on images just like SAM. Please see &lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/sam2/main/notebooks/automatic_mask_generator_example.ipynb&quot;&gt;automatic_mask_generator_example.ipynb&lt;/a&gt; (also in Colab &lt;a href=&quot;https://colab.research.google.com/github/facebookresearch/sam2/blob/main/notebooks/automatic_mask_generator_example.ipynb&quot;&gt;here&lt;/a&gt;) for automatic mask generation in images.&lt;/p&gt; 
&lt;h3&gt;Video prediction&lt;/h3&gt; 
&lt;p&gt;For promptable segmentation and tracking in videos, we provide a video predictor with APIs for example to add prompts and propagate masklets throughout a video. SAM 2 supports video inference on multiple objects and uses an inference state to keep track of the interactions in each video.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import torch
from sam2.build_sam import build_sam2_video_predictor

checkpoint = &quot;./checkpoints/sam2.1_hiera_large.pt&quot;
model_cfg = &quot;configs/sam2.1/sam2.1_hiera_l.yaml&quot;
predictor = build_sam2_video_predictor(model_cfg, checkpoint)

with torch.inference_mode(), torch.autocast(&quot;cuda&quot;, dtype=torch.bfloat16):
    state = predictor.init_state(&amp;lt;your_video&amp;gt;)

    # add new prompts and instantly get the output on the same frame
    frame_idx, object_ids, masks = predictor.add_new_points_or_box(state, &amp;lt;your_prompts&amp;gt;):

    # propagate the prompts to get masklets throughout the video
    for frame_idx, object_ids, masks in predictor.propagate_in_video(state):
        ...
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Please refer to the examples in &lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/sam2/main/notebooks/video_predictor_example.ipynb&quot;&gt;video_predictor_example.ipynb&lt;/a&gt; (also in Colab &lt;a href=&quot;https://colab.research.google.com/github/facebookresearch/sam2/blob/main/notebooks/video_predictor_example.ipynb&quot;&gt;here&lt;/a&gt;) for details on how to add click or box prompts, make refinements, and track multiple objects in videos.&lt;/p&gt; 
&lt;h2&gt;Load from ðŸ¤— Hugging Face&lt;/h2&gt; 
&lt;p&gt;Alternatively, models can also be loaded from &lt;a href=&quot;https://huggingface.co/models?search=facebook/sam2&quot;&gt;Hugging Face&lt;/a&gt; (requires &lt;code&gt;pip install huggingface_hub&lt;/code&gt;).&lt;/p&gt; 
&lt;p&gt;For image prediction:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import torch
from sam2.sam2_image_predictor import SAM2ImagePredictor

predictor = SAM2ImagePredictor.from_pretrained(&quot;facebook/sam2-hiera-large&quot;)

with torch.inference_mode(), torch.autocast(&quot;cuda&quot;, dtype=torch.bfloat16):
    predictor.set_image(&amp;lt;your_image&amp;gt;)
    masks, _, _ = predictor.predict(&amp;lt;input_prompts&amp;gt;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For video prediction:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import torch
from sam2.sam2_video_predictor import SAM2VideoPredictor

predictor = SAM2VideoPredictor.from_pretrained(&quot;facebook/sam2-hiera-large&quot;)

with torch.inference_mode(), torch.autocast(&quot;cuda&quot;, dtype=torch.bfloat16):
    state = predictor.init_state(&amp;lt;your_video&amp;gt;)

    # add new prompts and instantly get the output on the same frame
    frame_idx, object_ids, masks = predictor.add_new_points_or_box(state, &amp;lt;your_prompts&amp;gt;):

    # propagate the prompts to get masklets throughout the video
    for frame_idx, object_ids, masks in predictor.propagate_in_video(state):
        ...
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Model Description&lt;/h2&gt; 
&lt;h3&gt;SAM 2.1 checkpoints&lt;/h3&gt; 
&lt;p&gt;The table below shows the improved SAM 2.1 checkpoints released on September 29, 2024.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;strong&gt;Model&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;strong&gt;Size (M)&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;strong&gt;Speed (FPS)&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;strong&gt;SA-V test (J&amp;amp;F)&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;strong&gt;MOSE val (J&amp;amp;F)&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;strong&gt;LVOS v2 (J&amp;amp;F)&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;sam2.1_hiera_tiny &lt;br&gt; (&lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/sam2/main/sam2/configs/sam2.1/sam2.1_hiera_t.yaml&quot;&gt;config&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_tiny.pt&quot;&gt;checkpoint&lt;/a&gt;)&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;38.9&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;91.2&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;76.5&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;71.8&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;77.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;sam2.1_hiera_small &lt;br&gt; (&lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/sam2/main/sam2/configs/sam2.1/sam2.1_hiera_s.yaml&quot;&gt;config&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_small.pt&quot;&gt;checkpoint&lt;/a&gt;)&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;46&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;84.8&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;76.6&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;73.5&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;78.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;sam2.1_hiera_base_plus &lt;br&gt; (&lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/sam2/main/sam2/configs/sam2.1/sam2.1_hiera_b+.yaml&quot;&gt;config&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_base_plus.pt&quot;&gt;checkpoint&lt;/a&gt;)&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;80.8&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;64.1&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;78.2&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;73.7&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;78.2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;sam2.1_hiera_large &lt;br&gt; (&lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/sam2/main/sam2/configs/sam2.1/sam2.1_hiera_l.yaml&quot;&gt;config&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_large.pt&quot;&gt;checkpoint&lt;/a&gt;)&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;224.4&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;39.5&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;79.5&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;74.6&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;80.6&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;SAM 2 checkpoints&lt;/h3&gt; 
&lt;p&gt;The previous SAM 2 checkpoints released on July 29, 2024 can be found as follows:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;strong&gt;Model&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;strong&gt;Size (M)&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;strong&gt;Speed (FPS)&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;strong&gt;SA-V test (J&amp;amp;F)&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;strong&gt;MOSE val (J&amp;amp;F)&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;strong&gt;LVOS v2 (J&amp;amp;F)&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;sam2_hiera_tiny &lt;br&gt; (&lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/sam2/main/sam2/configs/sam2/sam2_hiera_t.yaml&quot;&gt;config&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_tiny.pt&quot;&gt;checkpoint&lt;/a&gt;)&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;38.9&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;91.5&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;75.0&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;70.9&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;75.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;sam2_hiera_small &lt;br&gt; (&lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/sam2/main/sam2/configs/sam2/sam2_hiera_s.yaml&quot;&gt;config&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_small.pt&quot;&gt;checkpoint&lt;/a&gt;)&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;46&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;85.6&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;74.9&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;71.5&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;76.4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;sam2_hiera_base_plus &lt;br&gt; (&lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/sam2/main/sam2/configs/sam2/sam2_hiera_b+.yaml&quot;&gt;config&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_base_plus.pt&quot;&gt;checkpoint&lt;/a&gt;)&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;80.8&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;64.8&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;74.7&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;72.8&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;75.8&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;sam2_hiera_large &lt;br&gt; (&lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/sam2/main/sam2/configs/sam2/sam2_hiera_l.yaml&quot;&gt;config&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt&quot;&gt;checkpoint&lt;/a&gt;)&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;224.4&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;39.7&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;76.0&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;74.6&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;79.8&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Speed measured on an A100 with &lt;code&gt;torch 2.5.1, cuda 12.4&lt;/code&gt;. See &lt;code&gt;benchmark.py&lt;/code&gt; for an example on benchmarking (compiling all the model components). Compiling only the image encoder can be more flexible and also provide (a smaller) speed-up (set &lt;code&gt;compile_image_encoder: True&lt;/code&gt; in the config).&lt;/p&gt; 
&lt;h2&gt;Segment Anything Video Dataset&lt;/h2&gt; 
&lt;p&gt;See &lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/sam2/main/sav_dataset/README.md&quot;&gt;sav_dataset/README.md&lt;/a&gt; for details.&lt;/p&gt; 
&lt;h2&gt;Training SAM 2&lt;/h2&gt; 
&lt;p&gt;You can train or fine-tune SAM 2 on custom datasets of images, videos, or both. Please check the training &lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/sam2/main/training/README.md&quot;&gt;README&lt;/a&gt; on how to get started.&lt;/p&gt; 
&lt;h2&gt;Web demo for SAM 2&lt;/h2&gt; 
&lt;p&gt;We have released the frontend + backend code for the SAM 2 web demo (a locally deployable version similar to &lt;a href=&quot;https://sam2.metademolab.com/demo&quot;&gt;https://sam2.metademolab.com/demo&lt;/a&gt;). Please see the web demo &lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/sam2/main/demo/README.md&quot;&gt;README&lt;/a&gt; for details.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;The SAM 2 model checkpoints, SAM 2 demo code (front-end and back-end), and SAM 2 training code are licensed under &lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/sam2/main/LICENSE&quot;&gt;Apache 2.0&lt;/a&gt;, however the &lt;a href=&quot;https://github.com/rsms/inter?tab=OFL-1.1-1-ov-file&quot;&gt;Inter Font&lt;/a&gt; and &lt;a href=&quot;https://github.com/googlefonts/noto-emoji&quot;&gt;Noto Color Emoji&lt;/a&gt; used in the SAM 2 demo code are made available under the &lt;a href=&quot;https://openfontlicense.org/open-font-license-official-text/&quot;&gt;SIL Open Font License, version 1.1&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;See &lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/sam2/main/CONTRIBUTING.md&quot;&gt;contributing&lt;/a&gt; and the &lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/sam2/main/CODE_OF_CONDUCT.md&quot;&gt;code of conduct&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributors&lt;/h2&gt; 
&lt;p&gt;The SAM 2 project was made possible with the help of many contributors (alphabetical):&lt;/p&gt; 
&lt;p&gt;Karen Bergan, Daniel Bolya, Alex Bosenberg, Kai Brown, Vispi Cassod, Christopher Chedeau, Ida Cheng, Luc Dahlin, Shoubhik Debnath, Rene Martinez Doehner, Grant Gardner, Sahir Gomez, Rishi Godugu, Baishan Guo, Caleb Ho, Andrew Huang, Somya Jain, Bob Kamma, Amanda Kallet, Jake Kinney, Alexander Kirillov, Shiva Koduvayur, Devansh Kukreja, Robert Kuo, Aohan Lin, Parth Malani, Jitendra Malik, Mallika Malhotra, Miguel Martin, Alexander Miller, Sasha Mitts, William Ngan, George Orlin, Joelle Pineau, Kate Saenko, Rodrick Shepard, Azita Shokrpour, David Soofian, Jonathan Torres, Jenny Truong, Sagar Vaze, Meng Wang, Claudette Ward, Pengchuan Zhang.&lt;/p&gt; 
&lt;p&gt;Third-party code: we use a GPU-based connected component algorithm adapted from &lt;a href=&quot;https://github.com/zsef123/Connected_components_PyTorch&quot;&gt;&lt;code&gt;cc_torch&lt;/code&gt;&lt;/a&gt; (with its license in &lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/sam2/main/LICENSE_cctorch&quot;&gt;&lt;code&gt;LICENSE_cctorch&lt;/code&gt;&lt;/a&gt;) as an optional post-processing step for the mask predictions.&lt;/p&gt; 
&lt;h2&gt;Citing SAM 2&lt;/h2&gt; 
&lt;p&gt;If you use SAM 2 or the SA-V dataset in your research, please use the following BibTeX entry.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bibtex&quot;&gt;@article{ravi2024sam2,
  title={SAM 2: Segment Anything in Images and Videos},
  author={Ravi, Nikhila and Gabeur, Valentin and Hu, Yuan-Ting and Hu, Ronghang and Ryali, Chaitanya and Ma, Tengyu and Khedr, Haitham and R{\&quot;a}dle, Roman and Rolland, Chloe and Gustafson, Laura and Mintun, Eric and Pan, Junting and Alwala, Kalyan Vasudev and Carion, Nicolas and Wu, Chao-Yuan and Girshick, Ross and Doll{\&#39;a}r, Piotr and Feichtenhofer, Christoph},
  journal={arXiv preprint arXiv:2408.00714},
  url={https://arxiv.org/abs/2408.00714},
  year={2024}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
  </channel>
</rss>
