<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>GitHub Perl Daily Trending</title>
    <description>Daily Trending of Perl in GitHub</description>
    <pubDate>Sun, 16 Mar 2025 01:36:15 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>tseemann/snippy</title>
      <link>https://github.com/tseemann/snippy</link>
      <description>&lt;p&gt;✂️ ⚡ Rapid haploid variant calling and core genome alignment&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/tseemann/snippy/actions/workflows/main.yml&quot;&gt;&lt;img src=&quot;https://github.com/tseemann/snippy/actions/workflows/main.yml/badge.svg?sanitize=true&quot; alt=&quot;CI&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://www.gnu.org/licenses/old-licenses/gpl-2.0.en.html&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/License-GPL%20v2-blue.svg?sanitize=true&quot; alt=&quot;License: GPL v2&quot;&gt;&lt;/a&gt; &lt;img src=&quot;https://img.shields.io/badge/Language-Perl_5-steelblue.svg?sanitize=true&quot; alt=&quot;Don&#39;t judge me&quot;&gt;&lt;/p&gt; 
&lt;h1&gt;Snippy&lt;/h1&gt; 
&lt;p&gt;Rapid haploid variant calling and core genome alignment&lt;/p&gt; 
&lt;h2&gt;Author&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://twitter.com/torstenseemann&quot;&gt;Torsten Seemann&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Synopsis&lt;/h2&gt; 
&lt;p&gt;Snippy finds SNPs between a haploid reference genome and your NGS sequence reads. It will find both substitutions (snps) and insertions/deletions (indels). It will use as many CPUs as you can give it on a single computer (tested to 64 cores). It is designed with speed in mind, and produces a consistent set of output files in a single folder. It can then take a set of Snippy results using the same reference and generate a core SNP alignment (and ultimately a phylogenomic tree).&lt;/p&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;% snippy --cpus 16 --outdir mysnps --ref Listeria.gbk --R1 FDA_R1.fastq.gz --R2 FDA_R2.fastq.gz
&amp;lt;cut&amp;gt;
Walltime used: 3 min, 42 sec
Results folder: mysnps
Done.

% ls mysnps
snps.vcf snps.bed snps.gff snps.csv snps.tab snps.html 
snps.bam snps.txt reference/ ...

% head -5 mysnps/snps.tab
CHROM  POS     TYPE    REF   ALT    EVIDENCE        FTYPE STRAND NT_POS AA_POS LOCUS_TAG GENE PRODUCT EFFECT
chr      5958  snp     A     G      G:44 A:0        CDS   +      41/600 13/200 ECO_0001  dnaA replication protein DnaA missense_variant c.548A&amp;gt;C p.Lys183Thr
chr     35524  snp     G     T      T:73 G:1 C:1    tRNA  -   
chr     45722  ins     ATT   ATTT   ATTT:43 ATT:1   CDS   -                    ECO_0045  gyrA DNA gyrase
chr    100541  del     CAAA  CAA    CAA:38 CAAA:1   CDS   +                    ECO_0179      hypothetical protein
plas      619  complex GATC  AATA   GATC:28 AATA:0  
plas     3221  mnp     GA    CT     CT:39 CT:0      CDS   +                    ECO_p012  rep  hypothetical protein

% snippy-core --prefix core mysnps1 mysnps2 mysnps3 mysnps4 
Loaded 4 SNP tables.
Found 2814 core SNPs from 96615 SNPs.

% ls core.*
core.aln core.tab core.tab core.txt core.vcf
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;Installation&lt;/h1&gt; 
&lt;h2&gt;Conda&lt;/h2&gt; 
&lt;p&gt;Install &lt;a href=&quot;https://bioconda.github.io/user/install.html&quot;&gt;Bioconda&lt;/a&gt; then:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;conda install -c conda-forge -c bioconda -c defaults snippy
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Homebrew&lt;/h2&gt; 
&lt;p&gt;Install &lt;a href=&quot;http://brew.sh/&quot;&gt;Homebrew&lt;/a&gt; (MacOS) or &lt;a href=&quot;http://linuxbrew.sh/&quot;&gt;LinuxBrew&lt;/a&gt; (Linux) then:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;brew install brewsci/bio/snippy
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Source&lt;/h2&gt; 
&lt;p&gt;This will install the latest version direct from Github. You&#39;ll need to add Snippy&#39;s &lt;code&gt;bin&lt;/code&gt; directory to your &lt;code&gt;$PATH&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;cd $HOME
git clone https://github.com/tseemann/snippy.git
$HOME/snippy/bin/snippy --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;Check installation&lt;/h1&gt; 
&lt;p&gt;Ensure you have the desired version:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;snippy --version
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Check that all dependencies are installed and working:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;snippy --check
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;Calling SNPs&lt;/h1&gt; 
&lt;h2&gt;Input Requirements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;a reference genome in FASTA or GENBANK format (can be in multiple contigs)&lt;/li&gt; 
 &lt;li&gt;sequence read file(s) in FASTQ or FASTA format (can be .gz compressed) format&lt;/li&gt; 
 &lt;li&gt;a folder to put the results in&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Output Files&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Extension&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;.tab&lt;/td&gt; 
   &lt;td&gt;A simple &lt;a href=&quot;http://en.wikipedia.org/wiki/Tab-separated_values&quot;&gt;tab-separated&lt;/a&gt; summary of all the variants&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;.csv&lt;/td&gt; 
   &lt;td&gt;A &lt;a href=&quot;http://en.wikipedia.org/wiki/Comma-separated_values&quot;&gt;comma-separated&lt;/a&gt; version of the .tab file&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;.html&lt;/td&gt; 
   &lt;td&gt;A &lt;a href=&quot;http://en.wikipedia.org/wiki/HTML&quot;&gt;HTML&lt;/a&gt; version of the .tab file&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;.vcf&lt;/td&gt; 
   &lt;td&gt;The final annotated variants in &lt;a href=&quot;http://en.wikipedia.org/wiki/Variant_Call_Format&quot;&gt;VCF&lt;/a&gt; format&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;.bed&lt;/td&gt; 
   &lt;td&gt;The variants in &lt;a href=&quot;http://genome.ucsc.edu/FAQ/FAQformat.html#format1&quot;&gt;BED&lt;/a&gt; format&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;.gff&lt;/td&gt; 
   &lt;td&gt;The variants in &lt;a href=&quot;http://www.sequenceontology.org/gff3.shtml&quot;&gt;GFF3&lt;/a&gt; format&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;.bam&lt;/td&gt; 
   &lt;td&gt;The alignments in &lt;a href=&quot;http://en.wikipedia.org/wiki/SAMtools&quot;&gt;BAM&lt;/a&gt; format. Includes unmapped, multimapping reads. Excludes duplicates.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;.bam.bai&lt;/td&gt; 
   &lt;td&gt;Index for the .bam file&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;.log&lt;/td&gt; 
   &lt;td&gt;A log file with the commands run and their outputs&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;.aligned.fa&lt;/td&gt; 
   &lt;td&gt;A version of the reference but with &lt;code&gt;-&lt;/code&gt; at position with &lt;code&gt;depth=0&lt;/code&gt; and &lt;code&gt;N&lt;/code&gt; for &lt;code&gt;0 &amp;lt; depth &amp;lt; --mincov&lt;/code&gt; (&lt;strong&gt;does not have variants&lt;/strong&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;.consensus.fa&lt;/td&gt; 
   &lt;td&gt;A version of the reference genome with &lt;em&gt;all&lt;/em&gt; variants instantiated&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;.consensus.subs.fa&lt;/td&gt; 
   &lt;td&gt;A version of the reference genome with &lt;em&gt;only substitution&lt;/em&gt; variants instantiated&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;.raw.vcf&lt;/td&gt; 
   &lt;td&gt;The unfiltered variant calls from Freebayes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;.filt.vcf&lt;/td&gt; 
   &lt;td&gt;The filtered variant calls from Freebayes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;.vcf.gz&lt;/td&gt; 
   &lt;td&gt;Compressed .vcf file via &lt;a href=&quot;http://blastedbio.blogspot.com.au/2011/11/bgzf-blocked-bigger-better-gzip.html&quot;&gt;BGZIP&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;.vcf.gz.csi&lt;/td&gt; 
   &lt;td&gt;Index for the .vcf.gz via &lt;code&gt;bcftools index&lt;/code&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;span&gt;⚠&lt;/span&gt; &lt;span&gt;❌&lt;/span&gt; Snippy 4.x does &lt;strong&gt;NOT&lt;/strong&gt; produce the following files that Snippy 3.x did&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Extension&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;.vcf.gz.tbi&lt;/td&gt; 
   &lt;td&gt;Index for the .vcf.gz via &lt;a href=&quot;http://bioinformatics.oxfordjournals.org/content/27/5/718.full&quot;&gt;TABIX&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;.depth.gz&lt;/td&gt; 
   &lt;td&gt;Output of &lt;code&gt;samtools depth -aa&lt;/code&gt; for the &lt;code&gt;.bam&lt;/code&gt; file&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;.depth.gz.tbi&lt;/td&gt; 
   &lt;td&gt;Index for the &lt;code&gt;.depth.gz&lt;/code&gt; file&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Columns in the TAB/CSV/HTML formats&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Name&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;CHROM&lt;/td&gt; 
   &lt;td&gt;The sequence the variant was found in eg. the name after the &lt;code&gt;&amp;gt;&lt;/code&gt; in the FASTA reference&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;POS&lt;/td&gt; 
   &lt;td&gt;Position in the sequence, counting from 1&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;TYPE&lt;/td&gt; 
   &lt;td&gt;The variant type: snp msp ins del complex&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;REF&lt;/td&gt; 
   &lt;td&gt;The nucleotide(s) in the reference&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ALT&lt;/td&gt; 
   &lt;td&gt;The alternate nucleotide(s) supported by the reads&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;EVIDENCE&lt;/td&gt; 
   &lt;td&gt;Frequency counts for REF and ALT&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;If you supply a Genbank file as the &lt;code&gt;--reference&lt;/code&gt; rather than a FASTA file, Snippy will fill in these extra columns by using the genome annotation to tell you which feature was affected by the variant:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Name&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;FTYPE&lt;/td&gt; 
   &lt;td&gt;Class of feature affected: CDS tRNA rRNA ...&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;STRAND&lt;/td&gt; 
   &lt;td&gt;Strand the feature was on: + - .&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;NT_POS&lt;/td&gt; 
   &lt;td&gt;Nucleotide position of the variant withinthe feature / Length in nt&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;AA_POS&lt;/td&gt; 
   &lt;td&gt;Residue position / Length in aa (only if FTYPE is CDS)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;LOCUS_TAG&lt;/td&gt; 
   &lt;td&gt;The &lt;code&gt;/locus_tag&lt;/code&gt; of the feature (if it existed)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;GENE&lt;/td&gt; 
   &lt;td&gt;The &lt;code&gt;/gene&lt;/code&gt; tag of the feature (if it existed)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;PRODUCT&lt;/td&gt; 
   &lt;td&gt;The &lt;code&gt;/product&lt;/code&gt; tag of the feature (if it existed)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;EFFECT&lt;/td&gt; 
   &lt;td&gt;The &lt;code&gt;snpEff&lt;/code&gt; annotated consequence of this variant (ANN tag in .vcf)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Columns in TXT format&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Name&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ID&lt;/td&gt; 
   &lt;td&gt;Reference + Sample&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;LENGTH&lt;/td&gt; 
   &lt;td&gt;Length of the reference&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ALIGNED&lt;/td&gt; 
   &lt;td&gt;Number of sites aligned to&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;UNALIGNED&lt;/td&gt; 
   &lt;td&gt;Number of sites unaligned&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;VARIANT&lt;/td&gt; 
   &lt;td&gt;Number of sites different from the reference&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;HET&lt;/td&gt; 
   &lt;td&gt;Number of sites heterozygous or poor quality genotype represented with an n (&lt;code&gt;--minqual&lt;/code&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MASKED&lt;/td&gt; 
   &lt;td&gt;Number of sites masked in reference represented with an X (&lt;code&gt;--mask&lt;/code&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;LOWCOV&lt;/td&gt; 
   &lt;td&gt;Number of sites low coverage in this sample represented with an N (&lt;code&gt;--mincov&lt;/code&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Variant Types&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Name&lt;/th&gt; 
   &lt;th&gt;Example&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;snp&lt;/td&gt; 
   &lt;td&gt;Single Nucleotide Polymorphism&lt;/td&gt; 
   &lt;td&gt;A =&amp;gt; T&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;mnp&lt;/td&gt; 
   &lt;td&gt;Multiple Nuclotide Polymorphism&lt;/td&gt; 
   &lt;td&gt;GC =&amp;gt; AT&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ins&lt;/td&gt; 
   &lt;td&gt;Insertion&lt;/td&gt; 
   &lt;td&gt;ATT =&amp;gt; AGTT&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;del&lt;/td&gt; 
   &lt;td&gt;Deletion&lt;/td&gt; 
   &lt;td&gt;ACGG =&amp;gt; ACG&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;complex&lt;/td&gt; 
   &lt;td&gt;Combination of snp/mnp&lt;/td&gt; 
   &lt;td&gt;ATTC =&amp;gt; GTTA&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;The variant caller&lt;/h2&gt; 
&lt;p&gt;The variant calling is done by &lt;a href=&quot;https://github.com/ekg/freebayes&quot;&gt;Freebayes&lt;/a&gt;. The key parameters under user control are:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--mincov&lt;/code&gt; - the minimum number of reads covering a site to be considered (default=10)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--minfrac&lt;/code&gt; - the minimum proportion of those reads which must differ from the reference&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--minqual&lt;/code&gt; - the minimum VCF variant call &quot;quality&quot; (default=100)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Looking at variants in detail with &lt;code&gt;snippy-vcf_report&lt;/code&gt;&lt;/h2&gt; 
&lt;p&gt;If you run Snippy with the &lt;code&gt;--report&lt;/code&gt; option it will automatically run &lt;code&gt;snippy-vcf_report&lt;/code&gt; and generate a &lt;code&gt;snps.report.txt&lt;/code&gt; which has a section like this for each SNP in &lt;code&gt;snps.vcf&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
&amp;gt;LBB_contig000001:10332 snp A=&amp;gt;T DP=7 Q=66.3052 [7]

         10301     10311     10321     10331     10341     10351     10361
tcttctccgagaagggaatataatttaaaaaaattcttaaataattcccttccctcccgttataaaaattcttcgcttat
........................................T.......................................
,,,,,,  ,,,,,,,,,,,,,,,,,,,,,t,,,,,,,,,,t,,t,,,,,,,,,,,,,,,,g,,,,,,,g,,,,,,,,,t,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, .......T..................A............A.......
.........................A........A.....T...........    .........C..............
.....A.....................C..C........CT.................TA.............
,a,,,,,a,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,t,t,,,g,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,ga,,,,,,,c,,,,,,,t,,,,,,,,,,g,,,,,,t,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
                            ............T.C..............G...............G......
                                                    ,,,,,,,g,,,,,,,,g,,,,,,,,,,,
                                                           g,,,,,,,,,,,,,,,,,,,,
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you wish to generate this report &lt;em&gt;after&lt;/em&gt; you have run Snippy, you can run it directly:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;cd snippydir
snippy-vcf_report --cpus 8 --auto &amp;gt; snps.report.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you want a HTML version for viewing in a web browser, use the &lt;code&gt;--html&lt;/code&gt; option:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;cd snippydir
snippy-vcf_report --html --cpus 16 --auto &amp;gt; snps.report.html
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;It works by running &lt;code&gt;samtools tview&lt;/code&gt; for each variant, which can be very slow if you have 1000s of variants. Using &lt;code&gt;--cpus&lt;/code&gt; as high as possible is recommended.&lt;/p&gt; 
&lt;h2&gt;Options&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;--rgid&lt;/code&gt; will set the Read Group (&lt;code&gt;RG&lt;/code&gt;) ID (&lt;code&gt;ID&lt;/code&gt;) and Sample (&lt;code&gt;SM&lt;/code&gt;) in the BAM and VCF file. If not supplied, it will will use the &lt;code&gt;--outdir&lt;/code&gt; folder name for both &lt;code&gt;ID&lt;/code&gt; and &lt;code&gt;SM&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;--mapqual&lt;/code&gt; is the minimum mapping quality to accept in variant calling. BWA MEM using &lt;code&gt;60&lt;/code&gt; to mean a read is &quot;uniquely mapped&quot;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;--basequal&lt;/code&gt; is minimum quality a nucleotide needs to be used in variant calling. We use &lt;code&gt;13&lt;/code&gt; which corresponds to error probability of ~5%. It is a traditional SAMtools value.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;--maxsoft&lt;/code&gt; is how many bases of an alignment to allow to be soft-clipped before discarding the alignment. This is to encourage global over local alignment, and is passed to the &lt;code&gt;samclip&lt;/code&gt; tool.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;--mincov&lt;/code&gt; and &lt;code&gt;--minfrac&lt;/code&gt; are used to apply hard thresholds to the variant calling beyond the existing statistical measure.. The optimal values depend on your sequencing depth and contamination rate. Values of 10 and 0.9 are commonly used.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;--targets&lt;/code&gt; takes a BED file and only calls variants in those regions. Not normally needed unless you are only interested in variants in specific locii (eg. AMR genes) but are still performing WGS rather than amplicon sequencing.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;--contigs&lt;/code&gt; allows you to call SNPs from contigs rather than reads. It shreds the contigs into synthetic reads, as to put the calls on even footing with other read samples in a multi-sample analysis.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Core SNP phylogeny&lt;/h1&gt; 
&lt;p&gt;If you call SNPs for multiple isolates from the same reference, you can produce an alignment of &quot;core SNPs&quot; which can be used to build a high-resolution phylogeny (ignoring possible recombination). A &quot;core site&quot; is a genomic position that is present in &lt;em&gt;all&lt;/em&gt; the samples. A core site can have the same nucleotide in every sample (&quot;monomorphic&quot;) or some samples can be different (&quot;polymorphic&quot; or &quot;variant&quot;). If we ignore the complications of &quot;ins&quot;, &quot;del&quot; variant types, and just use variant sites, these are the &quot;core SNP genome&quot;.&lt;/p&gt; 
&lt;h2&gt;Input Requirements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;a set of Snippy folders which used the same &lt;code&gt;--ref&lt;/code&gt; sequence.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Using &lt;code&gt;snippy-multi&lt;/code&gt;&lt;/h3&gt; 
&lt;p&gt;To simplify running a set of isolate sequences (reads or contigs) against the same reference, you can use the &lt;code&gt;snippy-multi&lt;/code&gt; script. This script requires a &lt;em&gt;tab separated&lt;/em&gt; input file as follows, and can handle paired-end reads, single-end reads, and assembled contigs.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;# input.tab = ID R1 [R2]
Isolate1	/path/to/R1.fq.gz	/path/to/R2.fq.gz
Isolate1b	/path/to/R1.fastq.gz	/path/to/R2.fastq.gz
Isolate1c	/path/to/R1.fa		/path/to/R2.fa
# single end reads supported too
Isolate2	/path/to/SE.fq.gz
Isolate2b	/path/to/iontorrent.fastq
# or already assembled contigs if you don&#39;t have reads
Isolate3	/path/to/contigs.fa
Isolate3b	/path/to/reference.fna.gz
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then one would run this to generate the output script. The first parameter should be the &lt;code&gt;input.tab&lt;/code&gt; file. The remaining parameters should be any remaining shared &lt;code&gt;snippy&lt;/code&gt; parameters. The &lt;code&gt;ID&lt;/code&gt; will be used for each isolate&#39;s &lt;code&gt;--outdir&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;% snippy-multi input.tab --ref Reference.gbk --cpus 16 &amp;gt; runme.sh

% less runme.sh   # check the script makes sense

% sh ./runme.sh   # leave it running over lunch
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;It will also run &lt;code&gt;snippy-core&lt;/code&gt; at the end to generate the core genome SNP alignment files &lt;code&gt;core.*&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Output Files&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Extension&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;.aln&lt;/td&gt; 
   &lt;td&gt;A core SNP alignment in the &lt;code&gt;--aformat&lt;/code&gt; format (default FASTA)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;.full.aln&lt;/td&gt; 
   &lt;td&gt;A whole genome SNP alignment (includes invariant sites)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;.tab&lt;/td&gt; 
   &lt;td&gt;Tab-separated columnar list of &lt;strong&gt;core&lt;/strong&gt; SNP sites with alleles but NO annotations&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;.vcf&lt;/td&gt; 
   &lt;td&gt;Multi-sample VCF file with genotype &lt;code&gt;GT&lt;/code&gt; tags for all discovered alleles&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;.txt&lt;/td&gt; 
   &lt;td&gt;Tab-separated columnar list of alignment/core-size statistics&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;.ref.fa&lt;/td&gt; 
   &lt;td&gt;FASTA version/copy of the &lt;code&gt;--ref&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;.self_mask.bed&lt;/td&gt; 
   &lt;td&gt;BED file generated if &lt;code&gt;--mask auto&lt;/code&gt; is used.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Why is &lt;code&gt;core.full.aln&lt;/code&gt; an alphabet soup?&lt;/h2&gt; 
&lt;p&gt;The &lt;code&gt;core.full.aln&lt;/code&gt; file is a FASTA formatted mutliple sequence alignment file. It has one sequence for the reference, and one for each sample participating in the core genome calculation. Each sequence has the same length as the reference sequence.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Character&lt;/th&gt; 
   &lt;th&gt;Meaning&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ATGC&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Same as the reference&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;atgc&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Different from the reference&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Zero coverage in this sample &lt;strong&gt;or&lt;/strong&gt; a deletion relative to the reference&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;N&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Low coverage in this sample (based on &lt;code&gt;--mincov&lt;/code&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;X&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Masked region of reference (from &lt;code&gt;--mask&lt;/code&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;n&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Heterozygous or poor quality genotype (has &lt;code&gt;GT=0/1&lt;/code&gt; or &lt;code&gt;QUAL &amp;lt; --minqual&lt;/code&gt; in &lt;code&gt;snps.raw.vcf&lt;/code&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;You can remove all the &quot;weird&quot; characters and replace them with &lt;code&gt;N&lt;/code&gt; using the included &lt;code&gt;snippy-clean_full_aln&lt;/code&gt;. This is useful when you need to pass it to a tree-building or recombination-removal tool:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;% snippy-clean_full_aln core.full.aln &amp;gt; clean.full.aln
% run_gubbins.py -p gubbins clean.full.aln
% snp-sites -c gubbins.filtered_polymorphic_sites.fasta &amp;gt; clean.core.aln
% FastTree -gtr -nt clean.core.aln &amp;gt; clean.core.tree
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Options&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;If you want to mask certain regions of the genome, you can provide a BED file with the &lt;code&gt;--mask&lt;/code&gt; parameter. Any SNPs in those regions will be excluded. This is common for genomes like &lt;em&gt;M.tuberculosis&lt;/em&gt; where pesky repetitive PE/PPE/PGRS genes cause false positives, or masking phage regions. A &lt;code&gt;--mask&lt;/code&gt; bed file for &lt;em&gt;M.tb&lt;/em&gt; is provided with Snippy in the &lt;code&gt;etc/Mtb_NC_000962.3_mask.bed&lt;/code&gt; folder. It is derived from the XLSX file from &lt;a href=&quot;https://gph.niid.go.jp/tgs-tb/&quot;&gt;https://gph.niid.go.jp/tgs-tb/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;If you use the &lt;code&gt;snippy --cleanup&lt;/code&gt; option the reference files will be deleted. This means &lt;code&gt;snippy-core&lt;/code&gt; can not &quot;auto-find&quot; the reference. In this case you simply use &lt;code&gt;snippy-core --reference REF&lt;/code&gt; to provide the reference in FASTA format.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Advanced usage&lt;/h1&gt; 
&lt;h2&gt;Increasing speed when too many reads&lt;/h2&gt; 
&lt;p&gt;Sometimes you will have far more sequencing depth that you need to call SNPs. A common problem is a whole MiSeq flowcell for a single bacterial isolate, where 25 million reads results in genome depth as high as 2000x. This makes Snippy far slower than it needs to be, as most SNPs will be recovered with 50-100x depth. If you know you have 10 times as much data as you need, Snippy can randomly sub-sample your FASTQ data:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;# have 1000x depth, only need 100x so sample at 10%
snippy --subsample 0.1  ...
&amp;lt;snip&amp;gt;
Sub-sampling reads at rate 0.1
&amp;lt;snip&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Only calling SNPs in particular regions&lt;/h2&gt; 
&lt;p&gt;If you are looking for specific SNPs, say AMR releated ones in particular genes in your reference genome, you can save much time by only calling variants there. Just put the regions of interest into a BED file:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;snippy --targets sites.bed ...
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Finding SNPs between contigs&lt;/h2&gt; 
&lt;p&gt;Sometimes one of your samples is only available as contigs, without corresponding FASTQ reads. You can still use these contigs with Snippy to find variants against a reference. It does this by shredding the contigs into 250 bp single-end reads at &lt;code&gt;2 &amp;amp;times; --mincov&lt;/code&gt; uniform coverage.&lt;/p&gt; 
&lt;p&gt;To use this feature, instead of providing &lt;code&gt;--R1&lt;/code&gt; and &lt;code&gt;--R2&lt;/code&gt; you use the &lt;code&gt;--ctgs&lt;/code&gt; option with the contigs file:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;% ls
ref.gbk mutant.fasta

% snippy --outdir mut1 --ref ref.gbk --ctgs mut1.fasta
Shredding mut1.fasta into pseudo-reads.
Identified 257 variants.

% snippy --outdir mut2 --ref ref.gbk --ctgs mut2.fasta
Shredding mut2.fasta into pseudo-reads.
Identified 413 variants.

% snippy-core mut1 mut2 
Found 129 core SNPs from 541 variant sites.

% ls
core.aln core.full.aln ...
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This output folder is completely compatible with &lt;code&gt;snippy-core&lt;/code&gt; so you can mix FASTQ and contig based &lt;code&gt;snippy&lt;/code&gt; output folders to produce alignments.&lt;/p&gt; 
&lt;h2&gt;Correcting assembly errors&lt;/h2&gt; 
&lt;p&gt;The &lt;em&gt;de novo&lt;/em&gt; assembly process attempts to reconstruct the reads into the original DNA sequences they were derived from. These reconstructed sequences are called &lt;em&gt;contigs&lt;/em&gt; or &lt;em&gt;scaffolds&lt;/em&gt;. For various reasons, small errors can be introduced into the assembled contigs which are not supported by the original reads used in the assembly process.&lt;/p&gt; 
&lt;p&gt;A common strategy is to align the reads back to the contigs to check for discrepancies. These errors appear as variants (SNPs and indels). If we can &lt;em&gt;reverse&lt;/em&gt; these variants than we can &quot;correct&quot; the contigs to match the evidence provided by the original reads. Obviously this strategy can go wrong if one is not careful about &lt;em&gt;how&lt;/em&gt; the read alignment is performed and which variants are accepted.&lt;/p&gt; 
&lt;p&gt;Snippy is able to help with this contig correction process. In fact, it produces a &lt;code&gt;snps.consensus.fa&lt;/code&gt; FASTA file which is the &lt;code&gt;ref.fa&lt;/code&gt; input file provided but with the discovered variants in &lt;code&gt;snps.vcf&lt;/code&gt; applied!&lt;/p&gt; 
&lt;p&gt;However, Snippy is not perfect and sometimes finds questionable variants. Typically you would make a copy of &lt;code&gt;snps.vcf&lt;/code&gt; (let&#39;s call it &lt;code&gt;corrections.vcf&lt;/code&gt;) and remove those lines corresponding to variants we don&#39;t trust. For example, when correcting Roche 454 and PacBio SMRT contigs, we primarily expect to find homopolymer errors and hence expect to see &lt;code&gt;ins&lt;/code&gt; more than &lt;code&gt;snp&lt;/code&gt; type variants.&lt;/p&gt; 
&lt;p&gt;In this case you need to run the correcting process manually using these steps:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;% cd snippy-outdir
% cp snps.vcf corrections.vcf
% $EDITOR corrections.vcf
% bgzip -c corrections.vcf &amp;gt; corrections.vcf.gz
% tabix -p vcf corrections.vcf.gz
% vcf-consensus corrections.vcf.gz &amp;lt; ref.fa &amp;gt; corrected.fa
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You may wish to &lt;em&gt;iterate&lt;/em&gt; this process by using &lt;code&gt;corrected.fa&lt;/code&gt; as a new &lt;code&gt;--ref&lt;/code&gt; for a repeated run of Snippy. Sometimes correcting one error allows BWA to align things it couldn&#39;t before, and new errors are uncovered.&lt;/p&gt; 
&lt;p&gt;Snippy may not be the best way to correct assemblies - you should consider dedicated tools such as &lt;a href=&quot;http://www.broadinstitute.org/software/pilon/&quot;&gt;PILON&lt;/a&gt; or &lt;a href=&quot;http://icorn.sourceforge.net/&quot;&gt;iCorn2&lt;/a&gt;, or adjust the Quiver parameters (for Pacbio data).&lt;/p&gt; 
&lt;h2&gt;Unmapped Reads&lt;/h2&gt; 
&lt;p&gt;Sometimes you are interested in the reads which did &lt;em&gt;not&lt;/em&gt; align to the reference genome. These reads represent DNA that was novel to &lt;em&gt;your&lt;/em&gt; sample which is potentially interesting. A standard strategy is to &lt;em&gt;de novo&lt;/em&gt; assemble the unmapped reads to discover these novel DNA elements, which often comprise mobile genetic elements such as plasmids.&lt;/p&gt; 
&lt;p&gt;By default, Snippy does &lt;strong&gt;not&lt;/strong&gt; keep the unmapped reads, not even in the BAM file. If you wish to keep them, use the &lt;code&gt;--unmapped&lt;/code&gt; option and the unaligned reads will be saved to a compressed FASTQ file:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;% snippy --outdir out --unmapped ....

% ls out/
snps.unmapped.fastq.gz ....
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;Information&lt;/h1&gt; 
&lt;h2&gt;Etymology&lt;/h2&gt; 
&lt;p&gt;The name Snippy is a combination of &lt;a href=&quot;http://en.wikipedia.org/wiki/Single-nucleotide_polymorphism&quot;&gt;SNP&lt;/a&gt; (pronounced &quot;snip&quot;) , &lt;a href=&quot;http://www.thefreedictionary.com/snappy&quot;&gt;snappy&lt;/a&gt; (meaning &quot;quick&quot;) and &lt;a href=&quot;http://en.wikipedia.org/wiki/Skippy_the_Bush_Kangaroo&quot;&gt;Skippy the Bush Kangaroo&lt;/a&gt; (to represent its Australian origin)&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Snippy is free software, released under the &lt;a href=&quot;https://raw.githubusercontent.com/tseemann/snippy/master/LICENSE&quot;&gt;GPL (version 2)&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Issues&lt;/h2&gt; 
&lt;p&gt;Please submit suggestions and bug reports to the &lt;a href=&quot;https://github.com/tseemann/snippy/issues&quot;&gt;Issue Tracker&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;perl &amp;gt;= 5.18&lt;/li&gt; 
 &lt;li&gt;bioperl &amp;gt;= 1.7&lt;/li&gt; 
 &lt;li&gt;bwa mem &amp;gt;= 0.7.12&lt;/li&gt; 
 &lt;li&gt;minimap2 &amp;gt;= 2.0&lt;/li&gt; 
 &lt;li&gt;samtools &amp;gt;= 1.7&lt;/li&gt; 
 &lt;li&gt;bcftools &amp;gt;= 1.7&lt;/li&gt; 
 &lt;li&gt;bedtools &amp;gt;= 2.0&lt;/li&gt; 
 &lt;li&gt;GNU parallel &amp;gt;= 2013xxxx&lt;/li&gt; 
 &lt;li&gt;freebayes &amp;gt;= 1.1 (freebayes, freebayes-parallel, fasta_generate_regions.py)&lt;/li&gt; 
 &lt;li&gt;vcflib &amp;gt;= 1.0 (vcfstreamsort, vcfuniq, vcffirstheader)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://genome.sph.umich.edu/wiki/Vt&quot;&gt;vt&lt;/a&gt; &amp;gt;= 0.5&lt;/li&gt; 
 &lt;li&gt;snpEff &amp;gt;= 4.3&lt;/li&gt; 
 &lt;li&gt;samclip &amp;gt;= 0.2&lt;/li&gt; 
 &lt;li&gt;seqtk &amp;gt;= 1.2&lt;/li&gt; 
 &lt;li&gt;snp-sites &amp;gt;= 2.0&lt;/li&gt; 
 &lt;li&gt;any2fasta &amp;gt;= 0.4&lt;/li&gt; 
 &lt;li&gt;wgsim &amp;gt;= 1.8 (for testing only - &lt;code&gt;wgsim&lt;/code&gt; command)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Bundled binaries&lt;/h2&gt; 
&lt;p&gt;For Linux (compiled on Ubuntu 16.04 LTS) and macOS (compiled on High Sierra Brew) some of the binaries, JARs and scripts are included.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>znuny/Znuny</title>
      <link>https://github.com/znuny/Znuny</link>
      <description>&lt;p&gt;Znuny/Znuny LTS is a fork of the ((OTRS)) Community Edition, one of the most flexible web-based ticketing systems used for Customer Service, Help Desk, IT Service Management.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&quot;https://www.znuny.org&quot;&gt;&lt;img align=&quot;center&quot; src=&quot;https://raw.githubusercontent.com/znuny/Znuny/dev/var/httpd/htdocs/skins/Agent/default/img/logo.png&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;a href=&quot;https://download.znuny.org/releases/znuny-latest-7.1.tar.gz&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/v/tag/znuny/Znuny?filter=rel-7_1_*&amp;amp;label=latest%20release&amp;amp;color=ff9b00&quot;&gt;&lt;/a&gt; &lt;br&gt; &lt;a href=&quot;https://github.com/znuny/Znuny/actions&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/actions/workflow/status/znuny/Znuny/ci.yaml?label=CI&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/znuny/Znuny/actions&quot;&gt;&lt;img src=&quot;https://badge.proxy.znuny.com/Znuny/rel-7_1&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://translations.znuny.org/engage/znuny/&quot;&gt;&lt;img src=&quot;https://translations.znuny.org/widgets/znuny/-/znuny/svg-badge.svg?sanitize=true&quot; alt=&quot;Translation status&quot;&gt;&lt;/a&gt; &lt;br&gt; &lt;a href=&quot;https://github.com/znuny/Znuny/issues&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/issues-raw/znuny/Znuny?&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/znuny/Znuny/issues?q=is%3Aissue+is%3Aclosed&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/issues-closed-raw/znuny/Znuny?color=#44CC44&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/znuny/Znuny/pulls&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/issues-pr-raw/znuny/Znuny?&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/znuny/Znuny/pulls?q=is%3Apr+is%3Aclosed&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/issues-pr-closed-raw/znuny/Znuny?color=brightgreen&quot;&gt;&lt;/a&gt; &lt;br&gt; &lt;a href=&quot;https://github.com/znuny/Znuny&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/languages/count/znuny/Znuny?style=flat&amp;amp;label=languages&amp;amp;color=brightgreen&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/znuny/Znuny/graphs/contributors&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/contributors/znuny/Znuny?&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://discord.gg/XTud3WWZTs&quot;&gt;&lt;img src=&quot;https://img.shields.io/discord/836533233885773825?style=flat&quot;&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;hr&gt; 
&lt;h1&gt;Znuny&lt;/h1&gt; 
&lt;p&gt;Znuny is a continuation of the ((OTRS)) Community Edition (version 6.0.30) which was declared end of life (EOL) at the end of December 2020.&lt;/p&gt; 
&lt;p&gt;The primary goal for this project is to provide a maintained and stable version of the well known ticket system and improve it with new features.&lt;/p&gt; 
&lt;p&gt;The second goal is to reestablish a connection to the community.&lt;/p&gt; 
&lt;h1&gt;License&lt;/h1&gt; 
&lt;p&gt;The project is distributed under the GNU General Public License (GPL v3) - see the accompanying &lt;a href=&quot;https://raw.githubusercontent.com/znuny/Znuny/dev/COPYING&quot;&gt;COPYING&lt;/a&gt; file for general license information. If you need more details you can have a look &lt;a href=&quot;https://snyk.io/learn/what-is-gpl-license-gplv3-explained/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Documentation&lt;/h1&gt; 
&lt;p&gt;You can find documentation &lt;a href=&quot;https://doc.znuny.org/&quot;&gt;here&lt;/a&gt;. The source code of Znuny is publicly available on &lt;a href=&quot;https://github.com/znuny/znuny&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You want to get in touch?&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.znuny.org&quot;&gt;Project website&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://community.znuny.org&quot;&gt;Community forum&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://discord.gg/XTud3WWZTs&quot;&gt;Discord Server&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.znuny.com&quot;&gt;Commercial services&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Software requirements&lt;/h1&gt; 
&lt;p&gt;Operating system&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Linux (Debian or Red Hat preferred)&lt;/li&gt; 
 &lt;li&gt;Perl 5.16.0 or higher&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Web server&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Apache 2 + mod_perl2 or higher (recommended)&lt;/li&gt; 
 &lt;li&gt;Web server with CGI support (CGI is not recommended)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Databases&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;MySQL 8.0 or higher&lt;/li&gt; 
 &lt;li&gt;MariaDB 10.3 or higher&lt;/li&gt; 
 &lt;li&gt;PostgreSQL 12.0 or higher&lt;/li&gt; 
 &lt;li&gt;Oracle 19c or higher&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Browsers&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;These browsers are NOT supported: 
  &lt;ul&gt; 
   &lt;li&gt;Internet Explorer before version 11&lt;/li&gt; 
   &lt;li&gt;Firefox before version 31&lt;/li&gt; 
   &lt;li&gt;Safari before version 6&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Vendor&lt;/h1&gt; 
&lt;p&gt;This project is mainly funded by Znuny GmbH, Berlin. If you need professional support or consulting, feel free to contact us.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.znuny.com&quot;&gt;Znuny Website&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>holzschu/a-shell</title>
      <link>https://github.com/holzschu/a-shell</link>
      <description>&lt;p&gt;A terminal for iOS, with multiple windows&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;a-shell: A terminal for iOS, with multiple windows&lt;/h1&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;https://img.shields.io/badge/Platform-iOS%2014.0+-lightgrey.svg?sanitize=true&quot; alt=&quot;Platform: iOS&quot;&gt; &lt;a href=&quot;https://twitter.com/a_Shell_iOS&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Twitter-@a__Shell__iOS-blue.svg?style=flat&quot; alt=&quot;Twitter&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://discord.gg/cvYnZm69Gy&quot;&gt;&lt;img src=&quot;https://img.shields.io/discord/935519150305050644?color=5865f2&amp;amp;label=Discord&amp;amp;style=flat&quot; alt=&quot;Discord&quot;&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;The goal in this project is to provide a simple Unix-like terminal on iOS. It uses &lt;a href=&quot;https://github.com/holzschu/ios_system/&quot;&gt;ios_system&lt;/a&gt; for command interpretation, and includes all commands from the &lt;a href=&quot;https://github.com/holzschu/ios_system/&quot;&gt;ios_system&lt;/a&gt; ecosystem (nslookup, whois, python3, lua, pdflatex, lualatex...)&lt;/p&gt; 
&lt;p&gt;The project uses iPadOS 13 ability to create and manage multiple windows. Each window has its own context, appearance, command history and current directory. &lt;code&gt;newWindow&lt;/code&gt; opens a new window, &lt;code&gt;exit&lt;/code&gt; closes the current window.&lt;/p&gt; 
&lt;p&gt;For help, type &lt;code&gt;help&lt;/code&gt; in the command line. &lt;code&gt;help -l&lt;/code&gt; lists all the available commands. &lt;code&gt;help -l | grep command&lt;/code&gt; will tell you if your favorite command is already installed.&lt;/p&gt; 
&lt;p&gt;You can change the appearance of a-Shell using &lt;code&gt;config&lt;/code&gt;. It lets you change the font, the font size, the background color, the text color and the cursor color and shape. Each window can have its own appearance. &lt;code&gt;config -p&lt;/code&gt; will make the settings for the current window permanent, that is used for all future windows. With &lt;code&gt;config -t&lt;/code&gt; you can also configure the toolbar.&lt;/p&gt; 
&lt;p&gt;When opening a new window, a-Shell executes the file &lt;code&gt;.profile&lt;/code&gt; if it exists. You can use this mechanism to customize further, e.g. have custom environment variables or cleanup temporary files.&lt;/p&gt; 
&lt;p&gt;For more tips on how to use a-Shell, see &lt;a href=&quot;https://bianshen00009.gitbook.io/a-guide-to-a-shell/&quot;&gt;the document&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;AppStore&lt;/h2&gt; 
&lt;p&gt;a-Shell is now &lt;a href=&quot;https://holzschu.github.io/a-Shell_iOS/&quot;&gt;available on the AppStore&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;How to compile it?&lt;/h2&gt; 
&lt;p&gt;If you want to compile the project yourself, you will need the following steps:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;download the entire project and its sub-modules: &lt;code&gt;git submodule update --init --recursive&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;download all the xcFrameworks: &lt;code&gt;downloadFrameworks.sh&lt;/code&gt; 
  &lt;ul&gt; 
   &lt;li&gt;this will download the standard Apple frameworks (in &lt;code&gt;xcfs/.build/artefacts/xcfs&lt;/code&gt;, with checksum control).&lt;/li&gt; 
   &lt;li&gt;There are too many Python frameworks (more than 2000) for automatic download. You can either remove them from the &quot;Embed&quot; step in the project, or compile them: 
    &lt;ul&gt; 
     &lt;li&gt;You&#39;ll need the Xcode command line tools, if you don&#39;t already have them: &lt;code&gt;sudo xcode-select --install&lt;/code&gt;&lt;/li&gt; 
     &lt;li&gt;You also need the OpenSSL libraries (libssl and libcrypto), XQuartz (freetype), and Node.js (npm) for macOS (we provide the versions for iOS and simulator).&lt;/li&gt; 
     &lt;li&gt;change directory to &lt;code&gt;cpython&lt;/code&gt;: &lt;code&gt;cd cpython&lt;/code&gt;&lt;/li&gt; 
     &lt;li&gt;build Python 3.11 and all the associated libraries / frameworks: &lt;code&gt;sh ./downloadAndCompile.sh&lt;/code&gt; (this step takes several hours on a 2GHz i5 MBP, YMMV).&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;a-Shell now runs on the devices. a-Shell mini can run on the devices and the simulator.&lt;/p&gt; 
&lt;p&gt;Because Python 3.x uses functions that are only available on the iOS 14 SDK, I&#39;ve set the minimum iOS version to 14.0. It also reduces the size of the binaries, so &lt;code&gt;ios_system&lt;/code&gt; and the other frameworks have the same settings. If you need to run it on an iOS 13 device, you&#39;ll have to recompile most frameworks.&lt;/p&gt; 
&lt;h2&gt;Home directory&lt;/h2&gt; 
&lt;p&gt;In iOS, you cannot write in the &lt;code&gt;~&lt;/code&gt; directory, only in &lt;code&gt;~/Documents/&lt;/code&gt;, &lt;code&gt;~/Library/&lt;/code&gt; and &lt;code&gt;~/tmp&lt;/code&gt;. Most Unix programs assume the configuration files are in &lt;code&gt;$HOME&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;So a-Shell changes several environment variables so that they point to &lt;code&gt;~/Documents&lt;/code&gt;. Type &lt;code&gt;env&lt;/code&gt; to see them.&lt;/p&gt; 
&lt;p&gt;Most configuration files (Python packages, TeX files, Clang SDK...) are in &lt;code&gt;~/Library&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Sandbox and Bookmarks&lt;/h2&gt; 
&lt;p&gt;a-Shell uses iOS 13 ability to access directories in other Apps sandbox. Type &lt;code&gt;pickFolder&lt;/code&gt; to access a directory inside another App. Once you have selected a directory, you can do pretty much anything you want here, so be careful.&lt;/p&gt; 
&lt;p&gt;All the directories you access with &lt;code&gt;pickFolder&lt;/code&gt; are bookmarked, so you can return to them later without &lt;code&gt;pickFolder&lt;/code&gt;. You can also bookmark the current directory with &lt;code&gt;bookmark&lt;/code&gt;. &lt;code&gt;showmarks&lt;/code&gt; will list all the existing bookmarks, &lt;code&gt;jump mark&lt;/code&gt; and &lt;code&gt;cd ~mark&lt;/code&gt; will change the current directory to this specific bookmark, &lt;code&gt;renamemark&lt;/code&gt; will let you change the name of a specific bookmark and &lt;code&gt;deletemark&lt;/code&gt; will delete a bookmark.&lt;/p&gt; 
&lt;p&gt;A user-configurable option in Settings lets you use the commands &lt;code&gt;s&lt;/code&gt;, &lt;code&gt;g&lt;/code&gt;, &lt;code&gt;l&lt;/code&gt;, &lt;code&gt;r&lt;/code&gt; and &lt;code&gt;d&lt;/code&gt; instead or as well.&lt;/p&gt; 
&lt;p&gt;If you are lost, &lt;code&gt;cd&lt;/code&gt; will always bring you back to &lt;code&gt;~/Documents/&lt;/code&gt;. &lt;code&gt;cd -&lt;/code&gt; will change to the previous directory.&lt;/p&gt; 
&lt;h2&gt;Shortcuts&lt;/h2&gt; 
&lt;p&gt;a-Shell is compatible with Apple Shortcuts, giving users full control of the Shell. You can write complex Shortcuts to download, process and release files using a-Shell commands. There are three shortcuts:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;Execute Command&lt;/code&gt;, which takes a list of commands and executes them in order. The input can also be a file or a text node, in which case the commands inside the node are executed.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Put File&lt;/code&gt; and &lt;code&gt;Get File&lt;/code&gt; are used to transfer files to and from a-Shell.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Shortcuts can be executed either &quot;In Extension&quot; or &quot;In App&quot;. &quot;In Extension&quot; means the shortcut runs in a lightweight version of the App, without no graphical user interface. It is good for light commands that do not require configuration files or system libraries (mkdir, nslookup, whois, touch, cat, echo...). &quot;In App&quot; opens the main application to execute the shortcut. It has access to all the commands, but will take longer. Once a shortcut has opened the App, you can return to the Shortcuts app by calling the command &lt;code&gt;open shortcuts://&lt;/code&gt;. The default behaviour is to try to run the commands &quot;in Extension&quot; as much as possible, based on the content of the commands. You can force a specific shortcut to run &quot;in App&quot; or &quot;in Extension&quot;, with the warning that it won&#39;t always work.&lt;/p&gt; 
&lt;p&gt;Both kind of shortcuts run by default in the same specific directory, &lt;code&gt;$SHORTCUTS&lt;/code&gt; or &lt;code&gt;~shortcuts&lt;/code&gt;. Of course, since you can run the commands &lt;code&gt;cd&lt;/code&gt; and &lt;code&gt;jump&lt;/code&gt; in a shortcut, you can pretty much go anywhere.&lt;/p&gt; 
&lt;h2&gt;Programming / add more commands:&lt;/h2&gt; 
&lt;p&gt;a-Shell has several programming languages installed: Python, Lua, JS, C, C++ and TeX.&lt;/p&gt; 
&lt;p&gt;For C and C++, you compile your programs with &lt;code&gt;clang program.c&lt;/code&gt; and it produces a webAssembly file. You can then execute it with &lt;code&gt;wasm a.out&lt;/code&gt;. You can also link multiple object files together, make a static library with &lt;code&gt;ar&lt;/code&gt;, etc. Once you are satisfied with your program, if you move it to a directory in the &lt;code&gt;$PATH&lt;/code&gt; (e.g. &lt;code&gt;~/Documents/bin&lt;/code&gt;) and rename it &lt;code&gt;program.wasm&lt;/code&gt;, it will be executed if you type &lt;code&gt;program&lt;/code&gt; on the command line.&lt;/p&gt; 
&lt;p&gt;You can also cross-compile programs on your main computer using our specific &lt;a href=&quot;https://github.com/holzschu/wasi-sdk&quot;&gt;WASI-sdk&lt;/a&gt;, and transfer the WebAssembly file to your iPad or iPhone.&lt;/p&gt; 
&lt;p&gt;Precompiled WebAssembly commands specific for a-Shell are available here: &lt;a href=&quot;https://github.com/holzschu/a-Shell-commands&quot;&gt;https://github.com/holzschu/a-Shell-commands&lt;/a&gt; These include &lt;code&gt;zip&lt;/code&gt;, &lt;code&gt;unzip&lt;/code&gt;, &lt;code&gt;xz&lt;/code&gt;, &lt;code&gt;ffmpeg&lt;/code&gt;... You install them on your iPad by downloading them and placing them in the &lt;code&gt;$PATH&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;We have the limitations of WebAssembly: no sockets, no forks, no interactive user input (piping input from other commands with &lt;code&gt;command | wasm program.wasm&lt;/code&gt; works fine).&lt;/p&gt; 
&lt;p&gt;For Python, you can install more packages with &lt;code&gt;pip install packagename&lt;/code&gt;, but only if they are pure Python. The C compiler is not yet able to produce dynamic libraries that could be used by Python.&lt;/p&gt; 
&lt;p&gt;TeX files are not installed by default. Type any TeX command and the system will prompt you to download them. Same with LuaTeX files.&lt;/p&gt; 
&lt;h2&gt;VoiceOver&lt;/h2&gt; 
&lt;p&gt;If you enable VoiceOver in Settings, a-Shell will work with VoiceOver: reading commands as you type them, reading the result, letting you read the screen with your finger...&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>adrienverge/openfortivpn</title>
      <link>https://github.com/adrienverge/openfortivpn</link>
      <description>&lt;p&gt;Client for PPP+TLS VPN tunnel services&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;openfortivpn&lt;/h1&gt; 
&lt;p&gt;openfortivpn is a client for PPP+TLS VPN tunnel services. It spawns a pppd process and operates the communication between the gateway and this process.&lt;/p&gt; 
&lt;p&gt;It is compatible with Fortinet VPNs.&lt;/p&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;man openfortivpn
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Simply connect to a VPN:&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;openfortivpn vpn-gateway:8443 --username=foo
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Connect to a VPN using an authentication realm:&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;openfortivpn vpn-gateway:8443 --username=foo --realm=bar
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Store password securely with a pinentry program:&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;openfortivpn vpn-gateway:8443 --username=foo --pinentry=pinentry-mac
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Connect with a user certificate and no password:&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;openfortivpn vpn-gateway:8443 --username= --password= --user-cert=cert.pem --user-key=key.pem
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Connect using SAML login:&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;openfortivpn vpn-gateway:8443 --saml-login
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Don&#39;t set IP routes and don&#39;t add VPN nameservers to &lt;code&gt;/etc/resolv.conf&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;openfortivpn vpn-gateway:8443 -u foo --no-routes --no-dns --pppd-no-peerdns
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Using a configuration file:&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;openfortivpn -c /etc/openfortivpn/my-config
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;With &lt;code&gt;/etc/openfortivpn/my-config&lt;/code&gt; containing:&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-ini&quot;&gt;host = vpn-gateway
port = 8443
username = foo
set-dns = 0
pppd-use-peerdns = 0
# X509 certificate sha256 sum, trust only this one!
trusted-cert = e46d4aff08ba6914e64daa85bc6112a422fa7ce16631bff0b592a28556f993db
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;For the full list of config options, see the &lt;code&gt;CONFIGURATION&lt;/code&gt; section of&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;man openfortivpn
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Smartcard&lt;/h2&gt; 
&lt;p&gt;Smartcard support needs &lt;code&gt;openssl pkcs engine&lt;/code&gt; and &lt;code&gt;opensc&lt;/code&gt; to be installed. The pkcs11-engine from libp11 needs to be compiled with p11-kit-devel installed. Check &lt;a href=&quot;https://github.com/adrienverge/openfortivpn/issues/464&quot;&gt;#464&lt;/a&gt; for a discussion of known issues in this area.&lt;/p&gt; 
&lt;p&gt;To make use of your smartcard put at least &lt;code&gt;pkcs11:&lt;/code&gt; to the user-cert config or commandline option. It takes the full or a partial PKCS#11 token URI.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-ini&quot;&gt;user-cert = pkcs11:
user-cert = pkcs11:token=someuser
user-cert = pkcs11:model=PKCS%2315%20emulated;manufacturer=piv_II;serial=012345678;token=someuser
username =
password =
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;In most cases &lt;code&gt;user-cert = pkcs11:&lt;/code&gt; will do it, but if needed you can get the token-URI with &lt;code&gt;p11tool --list-token-urls&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Multiple readers are currently not supported.&lt;/p&gt; 
&lt;p&gt;Smartcard support has been tested with Yubikey under Linux, but other PIV enabled smartcards may work too. On Mac OS X Mojave it is known that the pkcs engine-by-id is not found.&lt;/p&gt; 
&lt;h2&gt;Installing&lt;/h2&gt; 
&lt;h3&gt;Installing existing packages&lt;/h3&gt; 
&lt;p&gt;Some Linux distributions provide &lt;code&gt;openfortivpn&lt;/code&gt; packages:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://packages.fedoraproject.org/pkgs/openfortivpn&quot;&gt;Fedora / CentOS&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://software.opensuse.org/package/openfortivpn&quot;&gt;openSUSE / SLE&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://packages.gentoo.org/packages/net-vpn/openfortivpn&quot;&gt;Gentoo&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/NixOS/nixpkgs/tree/master/pkgs/tools/networking/openfortivpn&quot;&gt;NixOS&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://archlinux.org/packages/extra/x86_64/openfortivpn&quot;&gt;Arch Linux&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://packages.debian.org/stable/openfortivpn&quot;&gt;Debian&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://packages.ubuntu.com/search?keywords=openfortivpn&quot;&gt;Ubuntu&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://dev.getsol.us/source/openfortivpn/&quot;&gt;Solus&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://pkgs.alpinelinux.org/package/edge/testing/x86_64/openfortivpn&quot;&gt;Alpine Linux&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;On macOS both &lt;a href=&quot;https://formulae.brew.sh/formula/openfortivpn&quot;&gt;Homebrew&lt;/a&gt; and &lt;a href=&quot;https://ports.macports.org/port/openfortivpn&quot;&gt;MacPorts&lt;/a&gt; provide an &lt;code&gt;openfortivpn&lt;/code&gt; package. Either &lt;a href=&quot;https://brew.sh/&quot;&gt;install Homebrew&lt;/a&gt; then install openfortivpn:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;# Install &#39;Homebrew&#39;
/usr/bin/ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot;

# Install &#39;openfortivpn&#39;
brew install openfortivpn
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or &lt;a href=&quot;https://www.macports.org/install.php&quot;&gt;install MacPorts&lt;/a&gt; then install openfortivpn:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;# Install &#39;openfortivpn&#39;
sudo port install openfortivpn
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;A more complete overview can be obtained from &lt;a href=&quot;https://repology.org/project/openfortivpn/versions&quot;&gt;repology&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Building and installing from source&lt;/h3&gt; 
&lt;p&gt;For other distros, you&#39;ll need to build and install from source:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Install build dependencies.&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;RHEL/CentOS/Fedora: &lt;code&gt;gcc&lt;/code&gt; &lt;code&gt;automake&lt;/code&gt; &lt;code&gt;autoconf&lt;/code&gt; &lt;code&gt;openssl-devel&lt;/code&gt; &lt;code&gt;make&lt;/code&gt; &lt;code&gt;pkg-config&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Debian/Ubuntu: &lt;code&gt;gcc&lt;/code&gt; &lt;code&gt;automake&lt;/code&gt; &lt;code&gt;autoconf&lt;/code&gt; &lt;code&gt;libssl-dev&lt;/code&gt; &lt;code&gt;make&lt;/code&gt; &lt;code&gt;pkg-config&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Arch Linux: &lt;code&gt;gcc&lt;/code&gt; &lt;code&gt;automake&lt;/code&gt; &lt;code&gt;autoconf&lt;/code&gt; &lt;code&gt;openssl&lt;/code&gt; &lt;code&gt;pkg-config&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Gentoo Linux: &lt;code&gt;net-dialup/ppp&lt;/code&gt; &lt;code&gt;pkg-config&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;openSUSE: &lt;code&gt;gcc&lt;/code&gt; &lt;code&gt;automake&lt;/code&gt; &lt;code&gt;autoconf&lt;/code&gt; &lt;code&gt;libopenssl-devel&lt;/code&gt; &lt;code&gt;pkg-config&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;macOS (Homebrew): &lt;code&gt;automake&lt;/code&gt; &lt;code&gt;autoconf&lt;/code&gt; &lt;code&gt;openssl@1.1&lt;/code&gt; &lt;code&gt;pkg-config&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;FreeBSD: &lt;code&gt;automake&lt;/code&gt; &lt;code&gt;autoconf&lt;/code&gt; &lt;code&gt;libressl&lt;/code&gt; &lt;code&gt;pkgconf&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;p&gt;On Linux, if you manage your kernel yourself, ensure to compile those modules:&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;CONFIG_PPP=m
CONFIG_PPP_ASYNC=m
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;On macOS, install &#39;Homebrew&#39; to install the build dependencies:&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;# Install &#39;Homebrew&#39;
/usr/bin/ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot;

# Install Dependencies
brew install automake autoconf openssl@1.1 pkg-config

# You may need to make this openssl available to compilers and pkg-config
export LDFLAGS=&quot;-L/usr/local/opt/openssl/lib $LDFLAGS&quot;
export CPPFLAGS=&quot;-I/usr/local/opt/openssl/include $CPPFLAGS&quot;
export PKG_CONFIG_PATH=&quot;/usr/local/opt/openssl/lib/pkgconfig:$PKG_CONFIG_PATH&quot;
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Build and install.&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;./autogen.sh
./configure --prefix=/usr/local --sysconfdir=/etc
make
sudo make install
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If targeting platforms with pppd &amp;lt; 2.5.0 such as current version of macOS, we suggest you configure with option --enable-legacy-pppd:&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;./autogen.sh
./configure --prefix=/usr/local --sysconfdir=/etc --enable-legacy-pppd
make
sudo make install
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you need to specify the openssl location you can set the &lt;code&gt;$PKG_CONFIG_PATH&lt;/code&gt; environment variable. For fine-tuning check the available configure arguments with &lt;code&gt;./configure --help&lt;/code&gt; especially when you are cross compiling.&lt;/p&gt; &lt;p&gt;Finally, install runtime dependency &lt;code&gt;ppp&lt;/code&gt; or &lt;code&gt;pppd&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Running as root?&lt;/h2&gt; 
&lt;p&gt;openfortivpn needs elevated privileges at three steps during tunnel set up:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;when spawning a &lt;code&gt;/usr/sbin/pppd&lt;/code&gt; process;&lt;/li&gt; 
 &lt;li&gt;when setting IP routes through VPN (when the tunnel is up);&lt;/li&gt; 
 &lt;li&gt;when adding nameservers to &lt;code&gt;/etc/resolv.conf&lt;/code&gt; (when the tunnel is up).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For these reasons, you need to use &lt;code&gt;sudo openfortivpn&lt;/code&gt;. If you need it to be usable by non-sudoer users, you might consider adding an entry in &lt;code&gt;/etc/sudoers&lt;/code&gt; or a file under &lt;code&gt;/etc/sudoers.d&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;visudo -f /etc/sudoers.d/openfortivpn
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;Cmnd_Alias  OPENFORTIVPN = /usr/bin/openfortivpn

%adm       ALL = (ALL) OPENFORTIVPN
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Adapt the above example by changing the &lt;code&gt;openfortivpn&lt;/code&gt; path or choosing a group different from &lt;code&gt;adm&lt;/code&gt; - such as a dedicated &lt;code&gt;openfortivpn&lt;/code&gt; group.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;: Make sure only trusted users can run openfortivpn as root! As described in &lt;a href=&quot;https://github.com/adrienverge/openfortivpn/issues/54&quot;&gt;#54&lt;/a&gt;, a malicious user could use &lt;code&gt;--pppd-plugin&lt;/code&gt; and &lt;code&gt;--pppd-log&lt;/code&gt; options to divert the program&#39;s behaviour.&lt;/p&gt; 
&lt;h2&gt;SSO/SAML/2FA&lt;/h2&gt; 
&lt;p&gt;In some cases, the server may require the VPN client to load and interact with a web page containing JavaScript. Depending on the complexity of the web page, interpreting the web page might be beyond the reach of a command line program such as openfortivpn.&lt;/p&gt; 
&lt;p&gt;In such cases, you may use an external program spawning a full-fledged web browser such as &lt;a href=&quot;https://github.com/gm-vm/openfortivpn-webview&quot;&gt;openfortivpn-webview&lt;/a&gt; to authenticate and retrieve a session cookie. This cookie can be fed to openfortivpn using option &lt;code&gt;--cookie-on-stdin&lt;/code&gt;. Obviously, such a solution requires a graphic session.&lt;/p&gt; 
&lt;p&gt;When started using &lt;code&gt;--saml-login&lt;/code&gt; the program creates a web server that accepts SAML login requests. To login using SAML you just have to open &lt;code&gt;&amp;lt;your-vpn-domain&amp;gt;/remote/saml/start?redirect=1&lt;/code&gt; and follow the login steps. At the end of the login process the page will be redirected to &lt;code&gt;http://127.0.0.1:8020/?id=&amp;lt;session-id&amp;gt;&lt;/code&gt;&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Feel free to make pull requests!&lt;/p&gt; 
&lt;p&gt;C coding style should follow the &lt;a href=&quot;https://www.kernel.org/doc/html/latest/process/coding-style.html&quot;&gt;Linux kernel coding style&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>LMS-Community/slimserver</title>
      <link>https://github.com/LMS-Community/slimserver</link>
      <description>&lt;p&gt;Server for Squeezebox and compatible players. This server is also called Lyrion Music Server.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a name=&quot;readme-top&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/LMS-Community/slimserver/actions/workflows/00_smoketest.yaml&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/actions/workflow/status/LMS-Community/slimserver/00_smoketest.yaml?style=flat&quot; alt=&quot;Build passing&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://perl.org/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Written_in-perl-orange?logo=perl&quot; alt=&quot;Perl&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/LMS-Community/slimserver/graphs/contributors&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/contributors/LMS-Community/slimserver.svg?style=flat&quot; alt=&quot;Contributors&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/LMS-Community/slimserver/network/members&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/forks/LMS-Community/slimserver.svg?style=flat&quot; alt=&quot;Forks&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/LMS-Community/slimserver/stargazers&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/LMS-Community/slimserver.svg?style=flat&quot; alt=&quot;Stargazers&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/LMS-Community/slimserver/issues&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/issues/LMS-Community/slimserver.svg?style=flat&quot; alt=&quot;Issues&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/LMS-Community/slimserver/pulls&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/issues-pr/LMS-Community/slimserver.svg?style=flat&quot; alt=&quot;Pull requests&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://hub.docker.com/r/lmscommunity/lyrionmusicserver&quot;&gt;&lt;img src=&quot;https://img.shields.io/docker/pulls/lmscommunity/lyrionmusicserver?style=flat&quot; alt=&quot;Docker pulls&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;br&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;a href=&quot;https://github.com/LMS-Community/slimserver&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/LMS-Community/slimserver-platforms/public/9.0/win32/res/SqueezeCenter.ico&quot; alt=&quot;Logo&quot; width=&quot;128&quot; height=&quot;128&quot;&gt; &lt;/a&gt; 
 &lt;h3 align=&quot;center&quot;&gt;Lyrion Music Server&lt;/h3&gt; 
 &lt;p align=&quot;center&quot;&gt; Streaming audio server &lt;br&gt; &lt;a href=&quot;https://lyrion.org&quot;&gt;&lt;strong&gt;Explore the docs »&lt;/strong&gt;&lt;/a&gt; &lt;br&gt; &lt;br&gt; &lt;a href=&quot;https://github.com/LMS-Community/slimserver/issues/new?labels=bug&quot;&gt;Report Bug&lt;/a&gt; · &lt;a href=&quot;https://github.com/LMS-Community/slimserver/issues/new?labels=enhancement&quot;&gt;Request Feature&lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;About The Project&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://lyrion.org&quot;&gt;&lt;img src=&quot;https://lyrion.org/assets/screenshot.png&quot; alt=&quot;LMS Screen Shot&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Lyrion Music Server (aka. LMS, fka. Logitech Mediaserver, SlimServer, SqueezeCenter, SqueezeboxServer, SliMP3) is the server software that powers audio players from &lt;a href=&quot;https://www.logi.com&quot;&gt;Logitech&lt;/a&gt; (formerly known as SlimDevices), including &lt;a href=&quot;https://lms-community.github.io/players-and-controllers/hardware-comparison/&quot;&gt;Squeezebox 3rd Generation, Squeezebox Boom, Squeezebox Receiver, Transporter, Squeezebox2, Squeezebox and SLIMP3&lt;/a&gt;, and many software emulators like &lt;a href=&quot;https://sourceforge.net/projects/lmsclients/files/&quot;&gt;Squeezelite and SqueezePlay&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;With the help of many plugins, Lyrion Music Server can stream not only your local music collection, but content from many music services and internet radio stations to your players.&lt;/p&gt; 
&lt;p&gt;Lyrion Music Server is written in Perl. It runs on pretty much any platform that Perl runs on, including Linux, Mac OSX, Solaris and Windows.&lt;/p&gt; 
&lt;p&gt;Enjoy the music on your Squeezebox!&lt;/p&gt; 
&lt;p align=&quot;right&quot;&gt;(&lt;a href=&quot;https://raw.githubusercontent.com/LMS-Community/slimserver/public/9.1/#readme-top&quot;&gt;back to top&lt;/a&gt;)&lt;/p&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;LMS runs on almost anything! Whether you have a Windows, macOS or Linux system, whether this is a single board computer (such as a Raspberry Pi), desktop, laptop or even a NAS, LMS will most likely run on it. Go to the &lt;a href=&quot;https://lyrion.org/getting-started/&quot;&gt;Getting started documentation&lt;/a&gt; for installation instructions.&lt;/p&gt; 
&lt;p align=&quot;right&quot;&gt;(&lt;a href=&quot;https://raw.githubusercontent.com/LMS-Community/slimserver/public/9.1/#readme-top&quot;&gt;back to top&lt;/a&gt;)&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Contributions are what make the open source community such an amazing place to learn, inspire, and create. Any contributions you make are &lt;strong&gt;greatly appreciated&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;If you have a suggestion that would make this better, please fork the repo and create a pull request. You can also simply open an issue with the tag &quot;enhancement&quot;. Don&#39;t forget to give the project a star! Thanks again!&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Fork the Project&lt;/li&gt; 
 &lt;li&gt;Create your Feature Branch (&lt;code&gt;git checkout -b feature/AmazingFeature&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Commit your Changes (&lt;code&gt;git commit -m &#39;Add some AmazingFeature&#39;&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Push to the Branch (&lt;code&gt;git push origin feature/AmazingFeature&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Open a Pull Request&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p align=&quot;right&quot;&gt;(&lt;a href=&quot;https://raw.githubusercontent.com/LMS-Community/slimserver/public/9.1/#readme-top&quot;&gt;back to top&lt;/a&gt;)&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>linux-test-project/lcov</title>
      <link>https://github.com/linux-test-project/lcov</link>
      <description>&lt;p&gt;LCOV&lt;/p&gt;&lt;hr&gt;&lt;hr&gt; 
&lt;ul&gt; 
 &lt;li&gt;README file for the LTP GCOV extension (LCOV) -&lt;/li&gt; 
 &lt;li&gt;Last changes: 2024-12-25&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr&gt; 
&lt;h2&gt;Description&lt;/h2&gt; 
&lt;p&gt;LCOV is a tool to manipulate and display information about what parts of a program are actually executed (i.e. &quot;covered&quot;) while running a particular test case or set of testcases. LCOV consists of a set of Perl scripts which build on the text output of various coverage tools - e.g., gcov, llvm-cov, Coverage.py, Cobertura, Devel::Cover, Jacoco, etc. - to implement the following enhanced functionality:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;* HTML based output: coverage rates are indicated using bar
  graphs and specific colors in a hyperlinked coverage report, intended
  to enable the user to quickly diagnose and address coverage issues.

* Support for large projects: overview pages allow quick browsing of
  coverage data by providing a hierarchical directory structure
  view, a flat list of all source files in the project, or a three-level
  detail view: directory, file and source code view.

* Support for multiple languages - including C/C++, Perl, and Python.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;LCOV was initially designed to support Linux kernel coverage measurements, but works as well for coverage measurements on standard user space applications.&lt;/p&gt; 
&lt;p&gt;LCOV supports differential coverage, as well as date- and owner-binning. See: &lt;a href=&quot;https://arxiv.org/abs/2008.07947&quot;&gt;https://arxiv.org/abs/2008.07947&lt;/a&gt; or &lt;a href=&quot;https://ieeexplore.ieee.org/document/9438597&quot;&gt;https://ieeexplore.ieee.org/document/9438597&lt;/a&gt; for a detailed explanation of the concepts and several possible use models.&lt;/p&gt; 
&lt;p&gt;A video presentation of the basic ideas can be found at &lt;a href=&quot;http://doi.org/10.5281/zenodo.4653252&quot;&gt;http://doi.org/10.5281/zenodo.4653252&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;In addition, several other features and capabilities are available. See section 6, below, for a brief description - and also see the man pages and the test cases.&lt;/p&gt; 
&lt;h2&gt;Further README contents&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Included files&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Installing LCOV&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Dependencies&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;An example of how to access kernel coverage data&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;An example of how to access coverage data for a user space program&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;LCOV features&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Questions and Comments&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Filing a new issue&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Important files&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr&gt; 
&lt;p&gt;README - This README file CHANGES - List of changes between releases bin/lcov - Tool for capturing LCOV coverage data bin/genhtml - Tool for creating HTML output from LCOV data bin/gendesc - Tool for creating description files as used by genhtml bin/perl2lcov - Tool to translate Perl Devel::Cover data to lcov format bin/llvm2lcov - Tool to translate LLVM &#39;llvm-cov&#39; JSON data to LCOV format bin/py2lcov - Tool to translate Python Coverage.py to lcov format bin/xml2lcov - Tool to translate Cobertura-like XML coverage data to lcov format bin/geninfo - Internal tool (creates LCOV data files) bin/genpng - Internal tool (creates png overviews of source files) lcovrc - LCOV configuration file man - Directory containing man pages for included tools example - Directory containing an example to demonstrate LCOV tests - Directory containing lcov regression tests Makefile - Makefile providing &#39;install&#39; and &#39;uninstall&#39; targets&lt;/p&gt; 
&lt;ol start=&quot;2&quot;&gt; 
 &lt;li&gt;Installing LCOV&lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr&gt; 
&lt;p&gt;The LCOV package is available as either RPM or tarball from:&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/linux-test-project/lcov/releases&quot;&gt;https://github.com/linux-test-project/lcov/releases&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;To install the tarball, unpack it to a directory and run:&lt;/p&gt; 
&lt;p&gt;make install&lt;/p&gt; 
&lt;p&gt;Use Git for the most recent (but possibly unstable) version:&lt;/p&gt; 
&lt;p&gt;git clone &lt;a href=&quot;https://github.com/linux-test-project/lcov.git&quot;&gt;https://github.com/linux-test-project/lcov.git&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Change to the resulting lcov directory and type:&lt;/p&gt; 
&lt;p&gt;make install&lt;/p&gt; 
&lt;p&gt;The default install location is /usr/local. Note that you may need to have superuser permissions to write into system directories.&lt;/p&gt; 
&lt;p&gt;To install in a different location - for example, your home directory, run:&lt;/p&gt; 
&lt;p&gt;make PREFIX=$HOME/my_lcov install&lt;/p&gt; 
&lt;p&gt;your PREFIX should be an absolute path.&lt;/p&gt; 
&lt;p&gt;To run the LCOV regression test suite on your installation:&lt;/p&gt; 
&lt;p&gt;$ cp -r $LCOV_HOME/share/test path/to/myTestDir $ cd path/to/myTestDir $ make [COVERAGE=1]&lt;/p&gt; 
&lt;p&gt;If desired, you can collect coverage data for the LCOV module by setting the COVERAGE makefile variable. Note that the Devel::Cover package must be installed if COVERAGE is enabled or if you want to use the perl2lcov utility. To view the collected coverage information, point your browser to .../lcov_coverage/index.html after running the tests.&lt;/p&gt; 
&lt;p&gt;Note that the testcases are primarily intended to test LCOV functionality and not to be easily readable tutorial examples.&lt;/p&gt; 
&lt;ol start=&quot;3&quot;&gt; 
 &lt;li&gt;Dependencies:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr&gt; 
&lt;p&gt;The lcov module is implemented primarily in Perl - and requires both a moderately up-to-date Perl installation and multiple Perl packages.&lt;/p&gt; 
&lt;p&gt;These perl packages include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Capture::Tiny&lt;/li&gt; 
 &lt;li&gt;DateTime&lt;/li&gt; 
 &lt;li&gt;Devel::Cover&lt;/li&gt; 
 &lt;li&gt;Digest::MD5&lt;/li&gt; 
 &lt;li&gt;File::Spec&lt;/li&gt; 
 &lt;li&gt;at least one flavor of JSON module. In order of performance/preference: 
  &lt;ul&gt; 
   &lt;li&gt;JSON::XS&lt;/li&gt; 
   &lt;li&gt;Cpanel::JSON::XS&lt;/li&gt; 
   &lt;li&gt;JSON::PP&lt;/li&gt; 
   &lt;li&gt;JSON&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Memory::Process&lt;/li&gt; 
 &lt;li&gt;Module::Load::Conditional&lt;/li&gt; 
 &lt;li&gt;Scalar::Util&lt;/li&gt; 
 &lt;li&gt;Time::HiRes&lt;/li&gt; 
 &lt;li&gt;TimeDate&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If your system is missing any of these, then you may be able to install them via:&lt;/p&gt; 
&lt;p&gt;$ perl -MCPAN -e &#39;install(
 &lt;packagename&gt;
  )&#39;
 &lt;/packagename&gt;&lt;/p&gt; 
&lt;p&gt;You will very likely need superuser access to be able to install Perl modules.&lt;/p&gt; 
&lt;p&gt;Some of the applications provided with the lcov module are written in Python - and may require additional Python packages. In particular, &#39;xlsxwriter&#39; is required in order to generate any of the spreadsheet reports.&lt;/p&gt; 
&lt;p&gt;To measure Python code coverage, users will need Python packages:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Coverage.py&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;In addition, contributors will need:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;perltidy&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Your platform may support other mechanisms to install and/or update required packages.&lt;/p&gt; 
&lt;ol start=&quot;4&quot;&gt; 
 &lt;li&gt;An example of how to access Linux kernel coverage data&lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr&gt; 
&lt;p&gt;Requirements: Follow the Linux kernel coverage setup instructions at:&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://docs.kernel.org/dev-tools/gcov.html&quot;&gt;https://docs.kernel.org/dev-tools/gcov.html&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;As root, do the following:&lt;/p&gt; 
&lt;p&gt;a) Resetting counters&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt; lcov --zerocounters
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;b) Capturing the current coverage state to a file&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt; lcov --capture --output-file kernel.info
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;c) Getting HTML output&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt; genhtml kernel.info
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Point the web browser of your choice to the resulting index.html file.&lt;/p&gt; 
&lt;ol start=&quot;5&quot;&gt; 
 &lt;li&gt;An example of how to access coverage data for a user space program&lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr&gt; 
&lt;p&gt;a) Capture current coverage state to a file:&lt;/p&gt; 
&lt;p&gt;i) C/C++ code:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;  Compile your program using the &#39;--coverage&#39; GCC or LLVM
  option. During linking, make sure to specify &#39;--coverage&#39;:

    $ gcc -o myTest --coverage simple.c
      OR
    $ gcc -c file1.c file2.c ... --coverage
    $ gcc -o myOtherTest --coverage file1.o file2.o ....

  Alternately, LLVM users can use the &#39;profdata path&#39; (rather than the
  &#39;gcov path&#39;) to collect coverage data from their C/C++ code.  See
  https://github.com/linux-test-project/lcov/discussions/234 for more
  information.

   Run your testcase at least once:

    $ path/to/my/testcase/myTest

   Capture the current coverage state to a file:

     $ lcov --directory path/to/my/testcase --capture --output-file app.info

   (LLVM users using the &#39;profdata path&#39; will use a somewhat different
   command for this step - see the discussion referenced above.)

   If you want to collect Modified Condition / Decision Coverage (MD/DC)
   date, then:
     - you must use gcc/14.2 (or newer), or LLVM/18 (or newer)
     - your GCC compile- and link command line must include flag
       &#39;-fcondition-coverage&#39;.
     - LLVM users must use the &#39;profdata path&#39; for coverage data collection,
       and your compile command line must include
       &#39;-fprofile-inst-generate -fcoverage-mapping -fcoverage-mcdc&#39;.
       See the above referenced discussion for details.
     - your lcov and genhtml command line must include flag
       &#39;--mcdc-coverage&#39;
   See the &#39;--mcdc-coverage&#39; section in the genhtml and geninfo man pages.

   Note that runtime coverage data exists only after the application has
   been started and stopped at least once. Otherwise, no data will be found
   and lcov will abort with an error mentioning that there are no
   data/.gcda files.

   The coverage runtime emits data (the .gcda files) in an atexit
   callback.  If your application exits abnormally or crashes before
   the callback is executed, then no coverage data will be available.

   For further information on the gcc profiling mechanism, please
   consult the gcov man page.

  See &#39;man lcov&#39; for more information - especially if your build/test
  environment is not trivial.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;ii) Python code:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt; - install the Coverage.py module

 - execute your testcase to produce python coverage data:

     $ COVERAGE_FILE=./pycov.dat coverage run --append --branch \
         myPythonScript [my script args]

 - translate Python coverage data to LCOV format:

     $ py2lcov -o pycov.info [py2lcov_options] pycov.dat [x.dat]+

 See &#39;py2lcov --help&#39; and the Coverage.py documentation for more
 information.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;iii) Perl code:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt; - install the Devel::Cover module

 - execute your testcase to produce perl coverage data:

     $ perl -MDevel::Cover=-db,perlcov_db,-coverage,statement,branch,condition,subroutine,-silent,1 myPerlTest.pl [my script args]

 - translate Perl coverage data to LCOV format:

     $ perl2lcov --output perlcov.info perlcov_db [perl2lcov options]

 See &#39;perl2lcov --help&#39; and the Devel::Cover documentation for more
 information.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;iv) XML data (for example, generated by Cobertura):&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt; - translate XM coverage data to LCOV format:

     $ xml2lcov --output myData.info coverage.xml [xml2lcov options]

 See &#39;xml2lcov --help&#39; and the Cobertura documentation for more
 information.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;b) Generate an HTML coverage report:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Generate an HTML report, combining all of your LCOV data files:

  $ genhtml -o html_report app.info pycov.info perlcov.info

Point the web browser of your choice to the resulting file:
html_report/index.html.

See &#39;man genhtml&#39; for more details.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;c) Generate a differential coverage report:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;See the example in .../example (run &quot;make test_differential&quot;)
as well as the examples in .../tests/gendiffcov.
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start=&quot;6&quot;&gt; 
 &lt;li&gt;LCOV Features:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr&gt; 
&lt;p&gt;LCOV features and capabilities fall into 7 major categories:&lt;/p&gt; 
&lt;p&gt;a) Categorization&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt; This refers primarily to differential coverage categorization as
 well as date- and owner-binning.  See https://arxiv.org/abs/2008.07947
 or https://ieeexplore.ieee.org/document/9438597 for a detailed
 description of the concepts.

 Differential categorization and binning are orthogonal in the sense
 that you can generate differential report without binning as well
 as &#39;vanilla&#39; coverage reports with binning.  See the above papers
 and the genhtml man page for details.

 Related options:
    --baseline-file, --diff-file, --annotate-script, --select-script
    --date-bins, --date-labels --new-file-as-baseline,
    --elide-path-mismatch
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;b) Error handling&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt; A generic - but very simple - error handler has been added to the
 lcov tool suite.  The error handler is used to report exceptions,
 and provides a mechanism for the user to ignore the particular
 message if desired.  Note that ignoring certain errors can cause
 subsequent errors and/or can result in inconsistent or confusing
 coverage reports.
 See the genhtml/lcov/geninfo man pages for details.

 Note that some errors are unrecoverable - and cannot be suppressed or
 ignored.

 Related options:
    --ignore-error, --expect-message-count, --keep-going, --msg-log
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;c) Navigation and display:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt; Navigation aids such as hyperlinks to the first uncovered region,
 to the next uncovered region, etc. have been implemented.  Similarly,
 new tables, new columns, and new links between tables enable the
 user to identify the author of particular code (covered or not
 covered), as well as the time period when the code was written.

 Collectively, these features help the user to quickly identify the
 cause of code coverage issues, and to then decide what to do.

 An option to generate a &#39;hierarchical&#39; coverage report (which follows
 the source code directory structure) or &#39;flat&#39; (all files in top level
 of two-level report) as well as various other small features (tooltip
 popups, user-specified HTML header, footer, and table labels, etc.) are
 also available.

 See the genhtml man page for some details, as well as the
 &#39;gendiffcov/simple&#39; testcases for some examples.

  Related options:
      --baseline-title, --baseline-date, --current-date,
      --flat, --hierarchical,
      --show-owners, --show-noncode, --show-navigation, --show-proportion,
      --suppress-aliases
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;d) Data manipulation&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt; Filters are used to suppress or remove certain coverage artifacts -
 for example, branches generated by the compiler (e.g., for exception
 handling).  These artifacts can overwhelm the user code and obscure
 coverage features that are interesting to the user.

 Other options are used to focus on or to exclude certain sections
 of code, as well as to do regexp replacement of file names - possibly
 using case-insensitive comparison.
 (Path munging is useful primarily when the build structure does
 not exactly match the layout in your revision control system; this
 is common in large projects with reusable components.)

 During coverage data capture, the --build-directory option can be used
 to specify a search path, to find the .gcno (compile-time coverage data)
 file corresponding to a particular .gcda (runtime coverage data) file.
 Similarly, the --source-directory option can be used to specify a
 search path for source files.

 See the lcov/geninfo/genhtml man pages for a detailed description of
 the available filters and manipulation features.

 Related options:
    --include, --exclude, --erase-functions, --omit-lines,
    --substitute, --filter
    --build-directory --source-directory
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;e) Callbacks/customization&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt; The user can supply callbacks which are used to:

    i) interface with the revision control system
       Sample scripts:
         - Perforce:  see &#39;p4diff&#39;, &#39;p4annotate.pm&#39;, &#39;p4annotate&#39;
         - Git: see &#39;gitdiff&#39;, &#39;gitblame.pm&#39;, &#39;gitblame&#39;
    ii) verify that source code versions are compatible, and
        Sample scripts: see &#39;get_signature&#39;, &#39;P4version.pm&#39;, &#39;getp4version&#39;,
        &#39;gitversion&#39;, &#39;gitversion.pm&#39;, and &#39;batchGitVersion.pm&#39;
    iii) enforce a desired code coverage criteria
         Sample script: criteria.pm/criteria
    iv) find source files in more complicated environments - where
        simple substitutions become complicated or unweildy.
    v) select a subset of coverage data to display - e.g., to
       use in a code review which wants to concentrate on only
       the changes caused by a particular commit or range of commits,
       or to review changes in a particular release.
       Sample script:  select.pm
     vi) keep track of environment and other settings - to aid
        infrastructure debugging in more complicated use cases.

 The callback may be any desired script or executable - but there
 may be performance advantages if it is written as a Perl module.

 See the genhtml/lcov/geninfo man pages for details.

 Note that the various sample scripts are found in the source code
 &#39;scripts&#39; directory, but are installed in the
 $LCOV_HOME/share/lcov/support-scripts directory of the release.

 Related options:
   --annotate-script, --criteria-script, --version-script
   --resolve-script, --select-script, --context-script
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;f) Performance&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt; lcov/genhtml/geninfo have been refactored to parallelize computation
 across multiple cores, if requested.
 In general, this provides speedup that is nearly linear in the number
 of cores.
 There is also an option to throttle parallelism to not exceed peak
 memory consumption constraints, as well as options to enable simple
 profile data collection - so you can see where time is going and
 thus to hint at potential optimizations.  The &#39;spreadsheet.py&#39;
 script can be used to view generated profile data.

 There are several configuration file options which can be used to
 tweak certain parallelization parameters to optimize performance
 for your environment in cases that the default behaviour is suboptimal.
 See the lcovrc man page for more information.

 See the genhtml/lcov/geninfo man pages for details

 Related options: --parallel, --memory, --profile
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;g) Language/tool support&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt; Added &#39;llvm2lcov&#39;, &#39;py2lcov&#39;, &#39;perl2lcov&#39; and &#39;xml2lcov&#39; scripts.

   - llvm2lcov:

       translates JSON coverage data generated by &#39;llvm-cov export -format=text ...&#39;
       to lcov format.

       See &quot;llvm2lcov --help&quot; for brief instruction on how to use the
       translator.  Note that llvm2lcov uses a similar set of command line
       and configuration file options as lcov, genhtml, and geninfo.

   - py2lcov:

       translates python Coverage.py XML data to lcov format.

       See the Coverage.py documentation at https://coverage.readthedocs.io,
       as well as &quot;.../py2lcov --help&quot;

   - perl2lcov

      translates Perl Devel::Cover data to lcov format.

      See the Devel::Cover documentation at
        https://metacpan.org/pod/Devel::Cover
      to find out how to generate coverage data for Perl code.

      See &quot;perl2lcov --help&quot; for brief instructions on how to
      use the translator.
      Note that perl2lcov uses a similar set of command line and
      config file options as lcov, genhtml, and geninfo.

   - xml2lcov

      translates XML coverage data to lcov format.
      The XML data may come from Cobertura or similar tools.

      See &quot;xml2lcov --help&quot; for brief instructions on how to use
      the translator.
      See the Coburtura documentation for directions on how to
      generate XML data.

 Other languages can be integrated using a similar approach.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;In general, the new features and options are implemented uniformly in lcov, genhtml, and geninfo. Most of the features can be enabled/disabled using either command line options or by setting defaults in your &#39;lcovrc&#39; file. See the lcovrc man page for details.&lt;/p&gt; 
&lt;ol start=&quot;7&quot;&gt; 
 &lt;li&gt;Questions and comments&lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr&gt; 
&lt;p&gt;See the included man pages for more information on how to use the LCOV tools.&lt;/p&gt; 
&lt;p&gt;In case of further questions, feel free to open a new issue or discussion using the issue tracker on the LCOV code repository site at:&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/linux-test-project/lcov&quot;&gt;https://github.com/linux-test-project/lcov&lt;/a&gt;&lt;/p&gt; 
&lt;ol start=&quot;8&quot;&gt; 
 &lt;li&gt;Filing a new issue&lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr&gt; 
&lt;p&gt;Before filing a new issue - and if you are using an LCOV release (as opposed to using a clone of the github repo) - please verify whether the issue is still present in the LCOV master version. See section 2, above for directions on how to clone and install the most up-to-date LCOV version.&lt;/p&gt; 
&lt;p&gt;If possible, please include a testcase which illustrates the problem when you file an issue. Please describe your environment (platform, compiler, perl, and python versions, etc.). Please include a detailed description of the issue: what you were trying to do (your goal - not the mechanics of your procedure), what you did (the mechanics of your procedure), the result you wanted to see vs. what actually happened. Depending on the issue, your testcase may need to include source code and compile/link command lines, directions for how to run your example, the command lines used to capture and generate your lcov reports, etc. In other cases, the captured &#39;.info&#39; files may be sufficient to reproduce the issue. When in doubt: more is better than less.&lt;/p&gt; 
&lt;p&gt;If you cannot include a testcase - e.g., because you feel that it is senstitive or proprietary - then your detailed description is even more important. Note that, without an example, it may be difficult or impossible to diagnose or fix the problem.&lt;/p&gt; 
&lt;p&gt;Bear in mind that you are asking for help from volunteers. Your priority might not be their priority. Civility, consideration and politeness go a long way.&lt;/p&gt; 
&lt;p&gt;Please check back and to verify the fix and close the issue once it has been addressed. Again: remember that you are asking for help from volunteers. Make sure that you are doing your part.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Gaius-Augustus/BRAKER</title>
      <link>https://github.com/Gaius-Augustus/BRAKER</link>
      <description>&lt;p&gt;BRAKER is a pipeline for fully automated prediction of protein coding gene structures with GeneMark-ES/ET/EP/ETP and AUGUSTUS in novel eukaryotic genomes&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&quot;https://img.shields.io/docker/pulls/teambraker/braker3&quot; alt=&quot;Docker Pulls&quot;&gt;&lt;/p&gt; 
&lt;h1&gt;BRAKER User Guide&lt;/h1&gt; 
&lt;h1&gt;News&lt;/h1&gt; 
&lt;p&gt;Here is a recording of the first BGA23 workshop session on BRAKER. If learning by watching videos is easy for you, consider watching that: &lt;a href=&quot;https://www.youtube.com/watch?v=UXTkJ4mUkyg&quot;&gt;https://www.youtube.com/watch?v=UXTkJ4mUkyg&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;BRAKER3 is now in &lt;a href=&quot;https://usegalaxy.eu/&quot;&gt;https://usegalaxy.eu/&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Contacts for Repository&lt;/h1&gt; 
&lt;p&gt;TSEBRA &amp;amp; BRAKER3 related:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Lars Gabriel, University of Greifswald, Germany, &lt;a href=&quot;mailto:lars.gabriel@uni-greifswald.de&quot;&gt;lars.gabriel@uni-greifswald.de&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;BRAKER &amp;amp; AUGUSTUS related:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Katharina J. Hoff, University of Greifswald, Germany, &lt;a href=&quot;mailto:katharina.hoff@uni-greifswald.de&quot;&gt;katharina.hoff@uni-greifswald.de&lt;/a&gt;, +49 3834 420 4624&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;GeneMark related:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Mark Borodovsky, Georgia Tech, U.S.A., &lt;a href=&quot;mailto:borodovsky@gatech.edu&quot;&gt;borodovsky@gatech.edu&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Tomas Bruna, Joint Genome Institute, U.S.A., &lt;a href=&quot;mailto:bruna.tomas@gmail.com&quot;&gt;bruna.tomas@gmail.com&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Alexandre Lomsazde, Georgia Tech, U.S.A., &lt;a href=&quot;mailto:alexandre.lomsadze@bme.gatech.edu&quot;&gt;alexandre.lomsadze@bme.gatech.edu&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Core Authors of BRAKER&lt;/h1&gt; 
&lt;p&gt;&lt;b id=&quot;aff1&quot;&gt;[a]&lt;/b&gt; University of Greifswald, Institute for Mathematics and Computer Science, Walther-Rathenau-Str. 47, 17489 Greifswald, Germany&lt;/p&gt; 
&lt;p&gt;&lt;b id=&quot;aff2&quot;&gt;[b]&lt;/b&gt; University of Greifswald, Center for Functional Genomics of Microbes, Felix-Hausdorff-Str. 8, 17489 Greifswald, Germany&lt;/p&gt; 
&lt;p&gt;&lt;b id=&quot;aff3&quot;&gt;[c]&lt;/b&gt; Joint Georgia Tech and Emory University Wallace H Coulter Department of Biomedical Engineering, 30332 Atlanta, USA&lt;/p&gt; 
&lt;p&gt;&lt;b id=&quot;aff4&quot;&gt;[d]&lt;/b&gt; School of Computational Science and Engineering, 30332 Atlanta, USA&lt;/p&gt; 
&lt;p&gt;&lt;b id=&quot;aff5&quot;&gt;[e]&lt;/b&gt; Moscow Institute of Physics and Technology, Moscow Region 141701, Dolgoprudny, Russia&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/docs/figs/mario.png&quot; alt=&quot;braker2-team-2[fig10]&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/docs/figs/alex-katharina-tomas.png&quot; alt=&quot;braker2-team-1[fig11]&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/docs/figs/lars.jpg&quot; alt=&quot;braker2-team-3[fig12]&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/docs/figs/mark.png&quot; alt=&quot;braker2-team-4[fig13]&quot;&gt;&lt;/p&gt; 
&lt;p&gt;Figure 1: Current BRAKER authors, from left to right: Mario Stanke, Alexandre Lomsadze, Katharina J. Hoff, Tomas Bruna, Lars Gabriel, and Mark Borodovsky. We acknowledge that a larger community of scientists contributed to the BRAKER code (e.g. via pull requests).&lt;/p&gt; 
&lt;h1&gt;Funding&lt;/h1&gt; 
&lt;p&gt;The development of BRAKER1, BRAKER2, and BRAKER3 was supported by the National Institutes of Health (NIH) [GM128145 to M.B. and M.S.]. Development of BRAKER3 was partially funded by Project Data Competency granted to K.J.H. and M.S. by the government of Mecklenburg-Vorpommern, Germany.&lt;/p&gt; 
&lt;h1&gt;Related Software&lt;/h1&gt; 
&lt;p&gt;The Transcript Selector for BRAKER (TSEBRA) is available at &lt;a href=&quot;https://github.com/Gaius-Augustus/TSEBRA&quot;&gt;https://github.com/Gaius-Augustus/TSEBRA&lt;/a&gt; .&lt;/p&gt; 
&lt;p&gt;GeneMark-ETP, one of the gene finders at the core of BRAKER, is available at &lt;a href=&quot;https://github.com/gatech-genemark/GeneMark-ETP&quot;&gt;https://github.com/gatech-genemark/GeneMark-ETP&lt;/a&gt; .&lt;/p&gt; 
&lt;p&gt;AUGUSTUS, the second gene finder at the core of BRAKER, is available at &lt;a href=&quot;https://github.com/Gaius-Augustus/Augustus&quot;&gt;https://github.com/Gaius-Augustus/Augustus&lt;/a&gt; .&lt;/p&gt; 
&lt;p&gt;GALBA, a BRAKER pipeline spin-off for using Miniprot or GenomeThreader to generate training genes, is available at &lt;a href=&quot;https://github.com/Gaius-Augustus/GALBA&quot;&gt;https://github.com/Gaius-Augustus/GALBA&lt;/a&gt; .&lt;/p&gt; 
&lt;h1&gt;Contents&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#authors-of-braker&quot;&gt;Authors&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#funding&quot;&gt;Funding&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#what-is-braker&quot;&gt;What is BRAKER?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#keys-to-successful-gene-prediction&quot;&gt;Keys to successful gene prediction&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#overview-of-modes-for-running-braker&quot;&gt;Overview of modes for running BRAKER&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#container&quot;&gt;Container&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#installation&quot;&gt;Installation&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#supported-software-versions&quot;&gt;Supported software versions&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#braker&quot;&gt;BRAKER&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#perl-pipeline-dependencies&quot;&gt;Perl pipeline dependencies&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#braker-components&quot;&gt;BRAKER components&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#bioinformatics-software-dependencies&quot;&gt;Bioinformatics software dependencies&lt;/a&gt; 
      &lt;ul&gt; 
       &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#mandatory-tools&quot;&gt;Mandatory tools&lt;/a&gt;&lt;/li&gt; 
       &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#mandatory-tools-for-braker3&quot;&gt;Mandatory tools for BRAKER3&lt;/a&gt;&lt;/li&gt; 
       &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#optional-tools&quot;&gt;Optional tools&lt;/a&gt;&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#running-braker&quot;&gt;Running BRAKER&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#different-braker-pipeline-modes&quot;&gt;BRAKER pipeline modes&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#braker-with-rna-seq-data&quot;&gt;BRAKER with RNA-Seq data&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#braker-with-protein-data&quot;&gt;BRAKER with protein data&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#braker-with-rna-seq-and-protein-data&quot;&gt;BRAKER with RNA-Seq and protein data&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#braker-with-short-and-long-read-rna-seq-and-protein-data&quot;&gt;BRAKER with short and long read RNA-Seq and protein data&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#description-of-selected-braker-command-line-options&quot;&gt;Description of selected BRAKER command line options&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#--ab_initio&quot;&gt;--ab_initio&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#--augustus_args--some_argbla&quot;&gt;--augustus_args=--some_arg=bla&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#--threadsint&quot;&gt;--threads=INT&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#--fungus&quot;&gt;--fungus&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#--useexisting&quot;&gt;--useexisting&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#--crf&quot;&gt;--crf&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#--lambdaint&quot;&gt;--lambda=int&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#--utron&quot;&gt;--UTR=on&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#--addutron&quot;&gt;--addUTR=on&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#--stranded-&quot;&gt;--stranded=+,-,.,...&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#--makehub---emailyourmailde&quot;&gt;--makehub --email=your@mail.de&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#--busco-lineage&quot;&gt;--busco_lineage lineage&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#output-of-braker&quot;&gt;Output of BRAKER&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#example-data&quot;&gt;Example data&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#data-description&quot;&gt;Data description&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#testing-braker-with-rna-seq-data&quot;&gt;Testing BRAKER with RNA-Seq data&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#testing-braker-with-proteins&quot;&gt;Testing BRAKER with proteins&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#testing-braker-with-proteins-and-rna-seq&quot;&gt;Testing BRAKER with proteins and RNA-Seq&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#testing-braker-with-pre-trained-parameters&quot;&gt;Testing BRAKER with pre-trained parameters&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#testing-braker-with-genome-sequence&quot;&gt;Testing BRAKER with genome sequence&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#starting-braker-on-the-basis-of-previously-existing-braker-runs&quot;&gt;Starting BRAKER on the basis of previously existing BRAKER runs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#bug-reporting&quot;&gt;Bug reporting&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#reporting-bugs-on-github&quot;&gt;Reporting bugs on github&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#common-problems&quot;&gt;Common problems&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#citing-braker-and-software-called-by-braker&quot;&gt;Citing BRAKER and software called by BRAKER&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#license&quot;&gt;License&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;What is BRAKER?&lt;/h1&gt; 
&lt;p&gt;The rapidly growing number of sequenced genomes requires fully automated methods for accurate gene structure annotation. With this goal in mind, we have developed BRAKER1&lt;sup name=&quot;a1&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#f1&quot;&gt;R1&lt;/a&gt;&lt;/sup&gt;&lt;sup name=&quot;a0&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#f0&quot;&gt;R0&lt;/a&gt;&lt;/sup&gt;, a combination of GeneMark-ET &lt;sup name=&quot;a2&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#f2&quot;&gt;R2&lt;/a&gt;&lt;/sup&gt; and AUGUSTUS &lt;sup name=&quot;a3&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#f3&quot;&gt;R3, &lt;/a&gt;&lt;/sup&gt;&lt;sup name=&quot;a4&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#f4&quot;&gt;R4&lt;/a&gt;&lt;/sup&gt;, that uses genomic and RNA-Seq data to automatically generate full gene structure annotations in novel genome.&lt;/p&gt; 
&lt;p&gt;However, the quality of RNA-Seq data that is available for annotating a novel genome is variable, and in some cases, RNA-Seq data is not available, at all.&lt;/p&gt; 
&lt;p&gt;BRAKER2 is an extension of BRAKER1 which allows for &lt;strong&gt;fully automated training&lt;/strong&gt; of the gene prediction tools GeneMark-ES/ET/EP/ETP &lt;sup name=&quot;a14&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#f14&quot;&gt;R14, &lt;/a&gt;&lt;/sup&gt;&lt;sup name=&quot;a15&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#f15&quot;&gt;R15, &lt;/a&gt;&lt;sup name=&quot;a17&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#f17&quot;&gt;R17, &lt;/a&gt;&lt;/sup&gt;&lt;/sup&gt;&lt;sup name=&quot;g1&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#g1&quot;&gt;F1&lt;/a&gt;&lt;/sup&gt; and AUGUSTUS from RNA-Seq and/or protein homology information, and that integrates the extrinsic evidence from RNA-Seq and protein homology information into the &lt;strong&gt;prediction&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;In contrast to other available methods that rely on protein homology information, BRAKER2 reaches high gene prediction accuracy even in the absence of the annotation of very closely related species and in the absence of RNA-Seq data.&lt;/p&gt; 
&lt;p&gt;BRAKER3 is the latest pipeline in the BRAKER suite. It enables the usage of RNA-seq &lt;strong&gt;and&lt;/strong&gt; protein data in a fully automated pipeline to train and predict highly reliable genes with GeneMark-ETP and AUGUSTUS. The result of the pipeline is the combined gene set of both gene prediction tools, which only contains genes with very high support from extrinsic evidence.&lt;/p&gt; 
&lt;p&gt;In this user guide, we will refer to BRAKER1, BRAKER2, and BRAKER3 simply as &lt;strong&gt;BRAKER&lt;/strong&gt; because they are executed by the same script (&lt;code&gt;braker.pl&lt;/code&gt;).&lt;/p&gt; 
&lt;h1&gt;Keys to successful gene prediction&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Use a high quality genome assembly. If you have a huge number of very short scaffolds in your genome assembly, those short scaffolds will likely increase runtime dramatically but will not increase prediction accuracy.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Use simple scaffold names in the genome file (e.g. &lt;code&gt;&amp;gt;contig1&lt;/code&gt; will work better than &lt;code&gt;&amp;gt;contig1my&amp;nbsp;custom&amp;nbsp;species&amp;nbsp;namesome&amp;nbsp;putative&amp;nbsp;function&amp;nbsp;/more/information/&amp;nbsp;&amp;nbsp;and&amp;nbsp;lots&amp;nbsp;of&amp;nbsp;special&amp;nbsp;characters&amp;nbsp;%&amp;amp;!*(){}&lt;/code&gt;). Make the scaffold names in all your fasta files simple before running any alignment program.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;In order to predict genes accurately in a novel genome, the genome should be masked for repeats. This will avoid the prediction of false positive gene structures in repetitive and low complexitiy regions. Repeat masking is also essential for mapping RNA-Seq data to a genome with some tools (other RNA-Seq mappers, such as HISAT2, ignore masking information). In case of GeneMark-ES/ET/EP/ETP and AUGUSTUS, softmasking (i.e.&amp;nbsp;putting repeat regions into lower case letters and all other regions into upper case letters) leads to better results than hardmasking (i.e.&amp;nbsp;replacing letters in repetitive regions by the letter &lt;code&gt;N&lt;/code&gt; for unknown nucleotide).&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Many genomes have gene structures that will be predicted accurately with standard parameters of GeneMark-ES/ET/EP/ETP and AUGUSTUS within BRAKER. However, some genomes have clade-specific features, i.e.&amp;nbsp;special branch point model in fungi, or non-standard splice-site patterns. Please read the options section [options] in order to determine whether any of the custom options may improve gene prediction accuracy in the genome of your target species.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Always check gene prediction results before further usage! You can e.g.&amp;nbsp;use a genome browser for visual inspection of gene models in context with extrinsic evidence data. BRAKER supports the generation of track data hubs for the UCSC Genome Browser with MakeHub for this purpose.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Overview of modes for running BRAKER&lt;/h1&gt; 
&lt;p&gt;BRAKER mainly features semi-unsupervised, extrinsic evidence data (RNA-Seq and/or protein spliced alignment information) supported training of GeneMark-ES/ET/EP/ETP&lt;sup name=&quot;g1&quot;&gt;[F1]&lt;/sup&gt; and subsequent training of AUGUSTUS with integration of extrinsic evidence in the final gene prediction step. However, there are now a number of additional pipelines included in BRAKER. In the following, we give an overview of possible input files and pipelines:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Genome file, only. In this mode, GeneMark-ES is trained on the genome sequence, alone. Long genes predicted by GeneMark-ES are selected for training AUGUSTUS. Final predictions by AUGUSTUS are &lt;em&gt;ab initio&lt;/em&gt;. This approach will likely yield lower prediction accuracy than all other here described pipelines. (see Figure &lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#fig1&quot;&gt;2&lt;/a&gt;),&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/docs/figs/braker-es.png&quot; alt=&quot;braker2-main-a[fig1]&quot;&gt;&lt;/p&gt; 
&lt;p&gt;Figure 2: BRAKER pipeline A: training GeneMark-ES on genome data, only; &lt;em&gt;ab initio&lt;/em&gt; gene prediction withAUGUSTUS&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Genome and RNA-Seq file from the same species (see figure &lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#fig2&quot;&gt;3&lt;/a&gt;); this approach is suitable for short read RNA-Seq libraries with a good coverage of the transcriptome, &lt;strong&gt;important:&lt;/strong&gt; this approach requires that each intron is covered by many alignments, i.e.&amp;nbsp;it does not work with assembled transcriptome mappings. In principle, also alignments of long read RNA-Seq data may lead to sufficient data for running BRAKER, but only if each transcript that will go into training was sequenced and aligned to the genome multiple times. Please be aware that at the current point in time, BRAKER does not officially support the integration of long read RNA-Seq data, yet.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/docs/figs/braker1.png&quot; alt=&quot;braker2-main-b[fig2]&quot;&gt;&lt;/p&gt; 
&lt;p&gt;Figure 3: BRAKER pipeline B: training GeneMark-ET supported by RNA-Seq spliced alignment information, prediction with AUGUSTUS with that same spliced alignment information.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Genome file and database of proteins that may be of &lt;strong&gt;unknown&lt;/strong&gt; evolutionary distance to the target species (see Figure &lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#fig3&quot;&gt;4&lt;/a&gt;); this approach is particularly suitable if no RNA-Seq data is available. This method will work better with proteins from species that are rather close to the target species, but accuracy will drop only very little if the reference proteins are more distant from the target species. &lt;strong&gt;Important:&lt;/strong&gt; This approach requires a database of protein families, i.e.&amp;nbsp;many representatives of each protein family must be present in the database. BRAKER has been tested with OrthoDB &lt;sup name=&quot;a19&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#f19&quot;&gt;R19&lt;/a&gt;&lt;/sup&gt;, successfully. The ProtHint &lt;sup name=&quot;a18&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#f18&quot;&gt;R18&lt;/a&gt;&lt;/sup&gt; protein mapping pipeline for generating required hints for BRAKER is available for download at &lt;a href=&quot;https://github.com/gatech-genemark/ProtHint&quot;&gt;https://github.com/gatech-genemark/ProtHint&lt;/a&gt;, the software on how to prepare the OrthoDB input proteins is available at &lt;a href=&quot;https://github.com/tomasbruna/orthodb-clades&quot;&gt;https://github.com/tomasbruna/orthodb-clades&lt;/a&gt;. You may add proteins of a closely related species to the OrthoDB fasta file in order to incorporate additional evidence into gene prediction. We provide pre-partitioned OrthoDB v.11 clades for download at &lt;a href=&quot;https://bioinf.uni-greifswald.de/bioinf/partitioned_odb11/&quot;&gt;https://bioinf.uni-greifswald.de/bioinf/partitioned_odb11/&lt;/a&gt; , and OrthoDB v.12 clades at &lt;a href=&quot;https://bioinf.uni-greifswald.de/bioinf/partitioned_odb12/&quot;&gt;https://bioinf.uni-greifswald.de/bioinf/partitioned_odb12/&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/docs/figs/braker2_ep.png&quot; alt=&quot;braker2-main-c[fig3]&quot;&gt;&lt;/p&gt; 
&lt;p&gt;Figure 4: BRAKER pipeline C: training GeneMark-EP+ on protein spliced alignment, start and stop information, prediction with AUGUSTUS with that same information, in addition chained CDSpart hints. Proteins used here can be of any evolutionary distance to the target organism.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Genome file and RNA-Seq set(s) from the same species, and proteins that may be of &lt;strong&gt;unknown&lt;/strong&gt; evolutionary distance to the target species (see figure &lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#fig4&quot;&gt;5&lt;/a&gt;); &lt;strong&gt;important:&lt;/strong&gt; this approach requires a database of protein families, i.e.&amp;nbsp;many representatives of each protein family must be present in the database, e.g. OrthoDB is suitable. (You may add proteins of a closely related species to the OrthoDB fasta file in order to incorporate additional evidence into gene prediction.)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/docs/figs/braker3_etp.png&quot; alt=&quot;braker3-main-a[fig4]&quot;&gt;&lt;/p&gt; 
&lt;p&gt;Figure 5: BRAKER pipeline D: If necessary, download and alignment of RNA-Seq sets for the target species. Training of GeneMark-ETP supported by the RNA-Seq alignments and a large protein database (proteins can be of any evolutionary distance). Subsequently, AUGUSTUS training and prediction using the same extrinsic information together with the GeneMark-ETP results. The final prediction is the TSEBRA combination of the AUGUSTUS and GeneMark-ETP results.&lt;/p&gt; 
&lt;h1&gt;Container&lt;/h1&gt; 
&lt;p&gt;We are aware that the &quot;manual&quot; installation of BRAKER3 and all its dependencies is tedious and really challenging without root permissions. Therefore, we provide a Docker container that has been developed to be run with Singularity. All information on this container can be found at &lt;a href=&quot;https://hub.docker.com/r/teambraker/braker3&quot;&gt;https://hub.docker.com/r/teambraker/braker3&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;In short, build it as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;singularity build braker3.sif docker://teambraker/braker3:latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Execute with:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;singularity exec braker3.sif braker.pl
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Test with:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;singularity exec -B $PWD:$PWD braker3.sif cp /opt/BRAKER/example/singularity-tests/test1.sh .
singularity exec -B $PWD:$PWD braker3.sif cp /opt/BRAKER/example/singularity-tests/test2.sh .
singularity exec -B $PWD:$PWD braker3.sif cp /opt/BRAKER/example/singularity-tests/test3.sh .
export BRAKER_SIF=/your/path/to/braker3.sif # may need to modify
bash test1.sh
bash test2.sh
bash test3.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Few users want to run their analysis inside Docker (since root permissions are required). However, if that&#39;s your goal, you can run and test the container as follows&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;sudo docker run --user 1000:100 --rm -it teambraker/braker3:latest bash
bash /opt/BRAKER/example/docker-tests/test1.sh # BRAKER1
bash /opt/BRAKER/example/docker-tests/test2.sh # BRAKER2
bash /opt/BRAKER/example/docker-tests/test3.sh # BRAKER3
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;span&gt;⚠&lt;/span&gt; The container does not include Java/GUSHR/anything UTR related because we are currently not maintaining UTR prediction with BRAKER. It&#39;s buggy and unstable. Do not use it.&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;⚠&lt;/span&gt; Users have reported that you need to manually copy the AUGUSTUS_CONFIG_PATH contents to a writable location before running our containers from Nextflow. Afterwards, you need to specify the writable AUGUSTUS_CONFIG_PATH as command line argument to BRAKER in Nextflow.&lt;/p&gt; 
&lt;p&gt;Good luck ;-)&lt;/p&gt; 
&lt;h1&gt;Installation&lt;/h1&gt; 
&lt;p&gt;&lt;span&gt;⚠&lt;/span&gt; &lt;strong&gt;Warning:&lt;/strong&gt; If you previously used BRAKER1 and/or BRAKER2, please be aware that the usage changed in several aspects. Also, older GeneMark versions that linger in your &lt;code&gt;$PATH&lt;/code&gt; variable might lead to unforeseen interferences, causing program failures. Please move all older GeneMark versions out of your &lt;code&gt;$PATH&lt;/code&gt; (also e.g. the GeneMark in &lt;code&gt;ProtHint/dependencies&lt;/code&gt;).&lt;/p&gt; 
&lt;h2&gt;Supported software versions&lt;/h2&gt; 
&lt;p&gt;At the time of release, this BRAKER version was tested with:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;AUGUSTUS 3.5.0 &lt;sup name=&quot;g2&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#g2&quot;&gt;F2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;GeneMark-ETP (source see Dockerfile)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;BAMTOOLS 2.5.1&lt;sup name=&quot;a5&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#f5&quot;&gt;R5&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;SAMTOOLS 1.7-4-g93586ed&lt;sup name=&quot;a6&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#f6&quot;&gt;R6&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Spaln 2.3.3d &lt;sup name=&quot;a8&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#f8&quot;&gt;R8, &lt;/a&gt;&lt;/sup&gt;&lt;sup name=&quot;a9&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#f9&quot;&gt;R9, &lt;/a&gt;&lt;/sup&gt;&lt;sup name=&quot;a10&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#f10&quot;&gt;R10&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;NCBI BLAST+ 2.2.31+ &lt;sup name=&quot;a12&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#f12&quot;&gt;R12, &lt;/a&gt;&lt;/sup&gt;&lt;sup name=&quot;a13&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#f13&quot;&gt;R13&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;DIAMOND 0.9.24&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;cdbfasta 0.99&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;cdbyank 0.981&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;GUSHR 1.0.0&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;SRA Toolkit 3.00 &lt;sup name=&quot;a22&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#f14&quot;&gt;R14&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;HISAT2 2.2.1 &lt;sup name=&quot;a23&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#f15&quot;&gt;R15&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;BEDTOOLS 2.30 &lt;sup name=&quot;a24&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#f16&quot;&gt;R16&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;StringTie2 2.2.1 &lt;sup name=&quot;a25&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#f17&quot;&gt;R17&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;GFFRead 0.12.7 &lt;sup name=&quot;a26&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#f18&quot;&gt;R18&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;compleasm 0.2.5 &lt;sup name=&quot;a27&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#f27&quot;&gt;R27&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;BRAKER&lt;/h2&gt; 
&lt;h3&gt;Perl pipeline dependencies&lt;/h3&gt; 
&lt;p&gt;Running BRAKER requires a Linux-system with &lt;code&gt;bash&lt;/code&gt; and Perl. Furthermore, BRAKER requires the following CPAN-Perl modules to be installed:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;File::Spec::Functions&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;Hash::Merge&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;List::Util&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;MCE::Mutex&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;Module::Load::Conditional&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;Parallel::ForkManager&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;POSIX&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;Scalar::Util::Numeric&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;YAML&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;Math::Utils&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;File::HomeDir&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For GeneMark-ETP, used when protein and RNA-Seq is supplied:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;YAML::XS&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Data::Dumper&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Thread::Queue&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;threads&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;On Ubuntu, for example, install the modules with CPANminus&lt;sup name=&quot;g4&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#g4&quot;&gt;F4&lt;/a&gt;&lt;/sup&gt;: &lt;code&gt;sudo cpanm Module::Name&lt;/code&gt;, e.g. &lt;code&gt;sudo cpanm Hash::Merge&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;BRAKER also uses a Perl module &lt;code&gt;helpMod_braker.pm&lt;/code&gt; that is not available on CPAN. This module is part of the BRAKER release and does not require separate installation.&lt;/p&gt; 
&lt;p&gt;If you do not have root permissions on the Linux machine, try setting up an &lt;strong&gt;Anaconda&lt;/strong&gt; (&lt;a href=&quot;https://www.anaconda.com/distribution/&quot;&gt;https://www.anaconda.com/distribution/&lt;/a&gt;) environment as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;wget https://repo.anaconda.com/archive/Anaconda3-2018.12-Linux-x86_64.sh
bash bin/Anaconda3-2018.12-Linux-x86_64.sh # do not install VS (needs root privileges)
conda install -c anaconda perl
conda install -c anaconda biopython
conda install -c bioconda perl-app-cpanminus
conda install -c bioconda perl-file-spec
conda install -c bioconda perl-hash-merge
conda install -c bioconda perl-list-util
conda install -c bioconda perl-module-load-conditional
conda install -c bioconda perl-posix
conda install -c bioconda perl-file-homedir
conda install -c bioconda perl-parallel-forkmanager
conda install -c bioconda perl-scalar-util-numeric
conda install -c bioconda perl-yaml
conda install -c bioconda perl-class-data-inheritable
conda install -c bioconda perl-exception-class
conda install -c bioconda perl-test-pod
conda install -c bioconda perl-file-which # skip if you are not comparing to reference annotation
conda install -c bioconda perl-mce
conda install -c bioconda perl-threaded
conda install -c bioconda perl-list-util
conda install -c bioconda perl-math-utils
conda install -c bioconda cdbtools
conda install -c eumetsat perl-yaml-xs
conda install -c bioconda perl-data-dumper
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Subsequently install BRAKER and other software &quot;as usual&quot; while being in your conda environment. &lt;strong&gt;Note:&lt;/strong&gt; There is a bioconda braker package, and a bioconda augustus package. They work. But they are usually lagging behind the development code of both tools on github. We therefore recommend manual installation and usage of lastest sources.&lt;/p&gt; 
&lt;h3&gt;BRAKER components&lt;/h3&gt; 
&lt;p&gt;BRAKER is a collection of Perl and Python scripts and a Perl module. The main script that will be called in order to run BRAKER is &lt;code&gt;braker.pl&lt;/code&gt;. Additional Perl and Python components are:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;filterGenemark.pl&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;filterIntronsFindStrand.pl&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;helpMod_braker.pm&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;findGenesInIntrons.pl&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;downsample_traingenes.pl&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;ensure_n_training_genes.py&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;get_gc_content.py&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;get_etp_hints.py&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;All scripts (files ending with &lt;code&gt;*.pl&lt;/code&gt; and &lt;code&gt;*.py&lt;/code&gt;) that are part of BRAKER must be executable in order to run BRAKER. This should already be the case if you download BRAKER from GitHub. Executability may be overwritten if you e.g.&amp;nbsp;transfer BRAKER on a USB-stick to another computer. In order to check whether required files are executable, run the following command in the directory that contains BRAKER Perl scripts:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;ls -l *.pl *.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The output should be similar to this:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    -rwxr-xr-x 1 katharina katharina  18191 Mai  7 10:25 align2hints.pl
    -rwxr-xr-x 1 katharina katharina   6090 Feb 19 09:35 braker_cleanup.pl
    -rwxr-xr-x 1 katharina katharina 408782 Aug 17 18:24 braker.pl
    -rwxr-xr-x 1 katharina katharina   5024 Mai  7 10:25 downsample_traingenes.pl
    -rwxr-xr-x 1 katharina katharina   5024 Mai  7 10:23 ensure_n_training_genes.py
    -rwxr-xr-x 1 katharina katharina   4542 Apr  3  2019 filter_augustus_gff.pl
    -rwxr-xr-x 1 katharina katharina  30453 Mai  7 10:25 filterGenemark.pl
    -rwxr-xr-x 1 katharina katharina   5754 Mai  7 10:25 filterIntronsFindStrand.pl
    -rwxr-xr-x 1 katharina katharina   7765 Mai  7 10:25 findGenesInIntrons.pl
    -rwxr-xr-x 1 katharina katharina   1664 Feb 12  2019 gatech_pmp2hints.pl
    -rwxr-xr-x 1 katharina katharina   2250 Jan  9 13:55 log_reg_prothints.pl
    -rwxr-xr-x 1 katharina katharina   4679 Jan  9 13:55 merge_transcript_sets.pl
    -rwxr-xr-x 1 katharina katharina  41674 Mai  7 10:25 startAlign.pl
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;It is important that the &lt;code&gt;x&lt;/code&gt; in &lt;code&gt;-rwxr-xr-x&lt;/code&gt; is present for each script. If that is not the case, run&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;`chmod a+x *.pl *.py`
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;in order to change file attributes.&lt;/p&gt; 
&lt;p&gt;You may find it helpful to add the directory in which BRAKER perl scripts reside to your &lt;code&gt;$PATH&lt;/code&gt; environment variable. For a single bash session, enter:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    PATH=/your_path_to_braker/:$PATH
    export PATH
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To make this &lt;code&gt;$PATH&lt;/code&gt; modification available to all bash sessions, add the above lines to a startup script (e.g.&lt;code&gt;~/.bashrc&lt;/code&gt;).&lt;/p&gt; 
&lt;h2&gt;Bioinformatics software dependencies&lt;/h2&gt; 
&lt;p&gt;BRAKER calls upon various bioinformatics software tools that are not part of BRAKER. Some tools are obligatory, i.e.&amp;nbsp;BRAKER will not run at all if these tools are not present on your system. Other tools are optional. Please install all tools that are required for running BRAKER in the mode of your choice.&lt;/p&gt; 
&lt;h3&gt;Mandatory tools&lt;/h3&gt; 
&lt;h4&gt;GeneMark-ETP&lt;/h4&gt; 
&lt;p&gt;Download GeneMark-ETP&lt;sup name=&quot;g1&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#g1&quot;&gt;F1&lt;/a&gt;&lt;/sup&gt; from &lt;a href=&quot;http://github.com/gatech-genemark/GeneMark-ETP&quot;&gt;http://github.com/gatech-genemark/GeneMark-ETP&lt;/a&gt; or &lt;a href=&quot;https://topaz.gatech.edu/GeneMark/etp.for_braker.tar.gz&quot;&gt;https://topaz.gatech.edu/GeneMark/etp.for_braker.tar.gz&lt;/a&gt;. Unpack and install GeneMark-ETP as described in GeneMark-ETP’s &lt;code&gt;README&lt;/code&gt; file.&lt;/p&gt; 
&lt;p&gt;If already contained in your &lt;code&gt;$PATH&lt;/code&gt; variable, BRAKER will guess the location of &lt;code&gt;gmes_petap.pl&lt;/code&gt; or &lt;code&gt;gmetp.pl&lt;/code&gt; automatically. Otherwise, BRAKER can find GeneMark-ES/ET/EP/ETP executables either by locating them in an environment variable &lt;code&gt;GENEMARK_PATH&lt;/code&gt;, or by taking a command line argument (&lt;code&gt;--GENEMARK_PATH=/your_path_to_GeneMark_executables/&lt;/code&gt;).&lt;/p&gt; 
&lt;p&gt;In order to set the environment variable for your current Bash session, type:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;export GENEMARK_PATH=/your_path_to_GeneMark_executables/
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Add the above lines to a startup script (e.g.&amp;nbsp;&lt;code&gt;~/.bashrc&lt;/code&gt;) in order to make it available to all bash sessions.&lt;/p&gt; 
&lt;p&gt;You can check whether GeneMark-ES/ET/EP is installed properly by running the &lt;code&gt;check_install.bash&lt;/code&gt; and/or executing examples in &lt;code&gt;GeneMark-E-tests&lt;/code&gt; directory.&lt;/p&gt; 
&lt;p&gt;GeneMark-ETP is downward compatible, i.e. it covers the functionality of GeneMark-EP and GeneMark-ET in BRAKER, too.&lt;/p&gt; 
&lt;p&gt;Warning: installing GeneMark-ETP for BRAKER in conda environments has lead to multiple problems reported by users (Issues!). We can not offer support for conda installations. Please use the singularity image instead.&lt;/p&gt; 
&lt;h4&gt;AUGUSTUS&lt;/h4&gt; 
&lt;p&gt;Download AUGUSTUS from its master branch at &lt;a href=&quot;https://github.com/Gaius-Augustus/Augustus&quot;&gt;https://github.com/Gaius-Augustus/Augustus&lt;/a&gt;. Unpack AUGUSTUS and install AUGUSTUS according to AUGUSTUS &lt;code&gt;README.TXT&lt;/code&gt;. &lt;em&gt;&lt;strong&gt;Do not use outdated AUGUSTUS versions from other sources, e.g. Debian package or Bioconda package! BRAKER highly depends in particular on an up-to-date Augustus/scripts directory, and other sources are often lagging behind.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;You should compile AUGUSTUS on your own system in order to avoid problems with versions of libraries used by AUGUSTUS. Compilation instructions are provided in the AUGUSTUS &lt;code&gt;README.TXT&lt;/code&gt; file (&lt;code&gt;Augustus/README.txt&lt;/code&gt;).&lt;/p&gt; 
&lt;p&gt;AUGUSTUS consists of &lt;code&gt;augustus&lt;/code&gt;, the gene prediction tool, additional C++ tools located in &lt;code&gt;Augustus/auxprogs&lt;/code&gt; and Perl scripts located in &lt;code&gt;Augustus/scripts&lt;/code&gt;. Perl scripts must be executable (see instructions in section &lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#executability&quot;&gt;BRAKER components&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The C++ tool &lt;code&gt;bam2hints&lt;/code&gt; is an essential component of BRAKER when run with RNA-Seq. Sources are located in &lt;code&gt;Augustus/auxprogs/bam2hints&lt;/code&gt;. Make sure that you compile &lt;code&gt;bam2hints&lt;/code&gt; on your system (it should be automatically compiled when AUGUSTUS is compiled, but in case of problems with &lt;code&gt;bam2hints&lt;/code&gt;, please read troubleshooting instructions in &lt;code&gt;Augustus/auxprogs/bam2hints/README&lt;/code&gt;).&lt;/p&gt; 
&lt;p&gt;Since BRAKER is a pipeline that trains AUGUSTUS, i.e.&amp;nbsp;writes species specific parameter files, BRAKER needs writing access to the configuration directory of AUGUSTUS that contains such files (&lt;code&gt;Augustus/config/&lt;/code&gt;). If you install AUGUSTUS globally on your system, the &lt;code&gt;config&lt;/code&gt; folder will typically not be writable by all users. Either make the directory where &lt;code&gt;config&lt;/code&gt; resides recursively writable to users of AUGUSTUS, or copy the &lt;code&gt;config/&lt;/code&gt; folder (recursively) to a location where users have writing permission.&lt;/p&gt; 
&lt;p&gt;AUGUSTUS will locate the &lt;code&gt;config&lt;/code&gt; folder by looking for an environment variable &lt;code&gt;$AUGUSTUS_CONFIG_PATH&lt;/code&gt;. If the &lt;code&gt;$AUGUSTUS_CONFIG_PATH&lt;/code&gt; environment variable is not set, then BRAKER will look in the path &lt;code&gt;../config&lt;/code&gt; relative to the directory in which it finds an AUGUSTUS executable. Alternatively, you can supply the variable as a command line argument to BRAKER (&lt;code&gt;--AUGUSTUS_CONFIG_PATH=/your_path_to_AUGUSTUS/Augustus/config/&lt;/code&gt;). We recommend that you export the variable e.g.&amp;nbsp;for your current bash session:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    export AUGUSTUS_CONFIG_PATH=/your_path_to_AUGUSTUS/Augustus/config/
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;In order to make the variable available to all Bash sessions, add the above line to a startup script, e.g.&amp;nbsp;&lt;code&gt;~/.bashrc&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Please have a look at the &lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/Dockerfile&quot;&gt;Dockerfile&lt;/a&gt; in case you want to install AUGUSTUS as Debian package. A number of scripts needs to be patched, then.&lt;/p&gt; 
&lt;h5&gt;Important:&lt;/h5&gt; 
&lt;p&gt;BRAKER expects the entire &lt;code&gt;config&lt;/code&gt; directory of AUGUSTUS at &lt;code&gt;$AUGUSTUS_CONFIG_PATH&lt;/code&gt;, i.e.&amp;nbsp;the subfolders &lt;code&gt;species&lt;/code&gt; with its contents (at least &lt;code&gt;generic&lt;/code&gt;) and &lt;code&gt;extrinsic&lt;/code&gt;! Providing a writable but empty folder at &lt;code&gt;$AUGUSTUS_CONFIG_PATH&lt;/code&gt; will not work for BRAKER. If you need to separate augustus binary and &lt;code&gt;$AUGUSTUS_CONFIG_PATH&lt;/code&gt;, we recommend that you recursively copy the un-writable config contents to a writable location.&lt;/p&gt; 
&lt;p&gt;If you have a system-wide installation of AUGUSTUS at &lt;code&gt;/usr/bin/augustus&lt;/code&gt;, an unwritable copy of &lt;code&gt;config&lt;/code&gt; sits at &lt;code&gt;/usr/bin/augustus_config/&lt;/code&gt;. The folder &lt;code&gt;/home/yours/&lt;/code&gt; is writable to you. Copy with the following command (and additionally set the then required variables):&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;cp -r /usr/bin/Augustus/config/ /home/yours/
export AUGUSTUS_CONFIG_PATH=/home/yours/augustus_config
export AUGUSTUS_BIN_PATH=/usr/bin
export AUGUSTUS_SCRIPTS_PATH=/usr/bin/augustus_scripts
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;Modification of $PATH&lt;/h5&gt; 
&lt;p&gt;Adding directories of AUGUSTUS binaries and scripts to your &lt;code&gt;$PATH&lt;/code&gt; variable enables your system to locate these tools, automatically. It is not a requirement for running BRAKER to do this, because BRAKER will try to guess them from the location of another environment variable (&lt;code&gt;$AUGUSTUS_CONFIG_PATH&lt;/code&gt;), or both directories can be supplied as command line arguments to &lt;code&gt;braker.pl&lt;/code&gt;, but we recommend to add them to your &lt;code&gt;$PATH&lt;/code&gt; variable. For your current bash session, type:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    PATH=:/your_path_to_augustus/bin/:/your_path_to_augustus/scripts/:$PATH
    export PATH
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For all your BASH sessions, add the above lines to a startup script (e.g.&lt;code&gt;~/.bashrc&lt;/code&gt;).&lt;/p&gt; 
&lt;h4&gt;Python3&lt;/h4&gt; 
&lt;p&gt;On Ubuntu, Python3 is usually installed by default, &lt;code&gt;python3&lt;/code&gt; will be in your &lt;code&gt;$PATH&lt;/code&gt; variable, by default, and BRAKER will automatically locate it. However, you have the option to specify the &lt;code&gt;python3&lt;/code&gt; binary location in two other ways:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Export an environment variable &lt;code&gt;$PYTHON3_PATH&lt;/code&gt;, e.g.&amp;nbsp;in your &lt;code&gt;~/.bashrc&lt;/code&gt; file:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;export PYTHON3_PATH=/path/to/python3/
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Specify the command line option &lt;code&gt;--PYTHON3_PATH=/path/to/python3/&lt;/code&gt; to &lt;code&gt;braker.pl&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Bamtools&lt;/h4&gt; 
&lt;p&gt;Download BAMTOOLS (e.g.&amp;nbsp;&lt;code&gt;git&amp;nbsp;clone&amp;nbsp;https://github.com/pezmaster31/bamtools.git&lt;/code&gt;). Install BAMTOOLS by typing the following in your shell:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    cd your-bamtools-directory mkdir build cd build cmake .. make
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If already in your &lt;code&gt;$PATH&lt;/code&gt; variable, BRAKER will find bamtools, automatically. Otherwise, BRAKER can locate the bamtools binary either by using an environment variable &lt;code&gt;$BAMTOOLS_PATH&lt;/code&gt;, or by taking a command line argument (&lt;code&gt;--BAMTOOLS_PATH=/your_path_to_bamtools/bin/&lt;/code&gt;&lt;sup name=&quot;g6&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#g6&quot;&gt;F6&lt;/a&gt;&lt;/sup&gt;). In order to set the environment variable e.g.&amp;nbsp;for your current bash session, type:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    export BAMTOOLS_PATH=/your_path_to_bamtools/bin/
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Add the above line to a startup script (e.g.&amp;nbsp;&lt;code&gt;~/.bashrc&lt;/code&gt;) in order to set the environment variable for all bash sessions.&lt;/p&gt; 
&lt;h4&gt;NCBI BLAST+ or DIAMOND&lt;/h4&gt; 
&lt;p&gt;You can use either NCBI BLAST+ or DIAMOND for removal of redundant training genes. You do not need both tools. If DIAMOND is present, it will be preferred because it is much faster.&lt;/p&gt; 
&lt;p&gt;Obtain and unpack DIAMOND as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    wget http://github.com/bbuchfink/diamond/releases/download/v0.9.24/diamond-linux64.tar.gz
    tar xzf diamond-linux64.tar.gz
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If already in your &lt;code&gt;$PATH&lt;/code&gt; variable, BRAKER will find diamond, automatically. Otherwise, BRAKER can locate the diamond binary either by using an environment variable &lt;code&gt;$DIAMOND_PATH&lt;/code&gt;, or by taking a command line argument (&lt;code&gt;--DIAMOND_PATH=/your_path_to_diamond&lt;/code&gt;). In order to set the environment variable e.g. for your current bash session, type:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    export DIAMOND_PATH=/your_path_to_diamond/
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Add the above line to a startup script (e.g.&amp;nbsp;&lt;code&gt;~/.bashrc&lt;/code&gt;) in order to set the environment variable for all bash sessions.&lt;/p&gt; 
&lt;p&gt;If you decide for BLAST+, install NCBI BLAST+ with &lt;code&gt;sudo&amp;nbsp;apt-get&amp;nbsp;install&amp;nbsp;ncbi-blast+&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;If already in your &lt;code&gt;$PATH&lt;/code&gt; variable, BRAKER will find blastp, automatically. Otherwise, BRAKER can locate the blastp binary either by using an environment variable &lt;code&gt;$BLAST_PATH&lt;/code&gt;, or by taking a command line argument (&lt;code&gt;--BLAST_PATH=/your_path_to_blast/&lt;/code&gt;). In order to set the environment variable e.g.&amp;nbsp;for your current bash session, type:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    export BLAST_PATH=/your_path_to_blast/
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Add the above line to a startup script (e.g.&amp;nbsp;&lt;code&gt;~/.bashrc&lt;/code&gt;) in order to set the environment variable for all bash sessions.&lt;/p&gt; 
&lt;h3&gt;Mandatory tools for BRAKER3&lt;/h3&gt; 
&lt;p&gt;Following tools are required by GeneMark-ETP and it will try to locate them in your &lt;code&gt;$PATH&lt;/code&gt; variable. So make sure to add their location to your &lt;code&gt;$PATH&lt;/code&gt;, e.g.:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;export PATH=$PATH:/your/path/to/Tool
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For all tools below, add the above line to a startup script (e.g.&amp;nbsp;&lt;code&gt;~/.bashrc&lt;/code&gt;) in order to extend your &lt;code&gt;$PATH&lt;/code&gt; variable for all bash sessions.&lt;/p&gt; 
&lt;p&gt;These software tools are only mandatory if you run BRAKER with RNA-Seq &lt;strong&gt;and&lt;/strong&gt; protein data!&lt;/p&gt; 
&lt;h4&gt;StringTie2&lt;/h4&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/skovaka/stringtie2&quot;&gt;StringTie2&lt;/a&gt; is used by GeneMark-ETP to assemble aligned RNA-Seq alignments. A precompiled version of StringTie2 can be downloaded from &lt;a href=&quot;https://ccb.jhu.edu/software/stringtie/#install&quot;&gt;https://ccb.jhu.edu/software/stringtie/#install&lt;/a&gt;.&lt;/p&gt; 
&lt;h4&gt;BEDTools&lt;/h4&gt; 
&lt;p&gt;The software package &lt;a href=&quot;https://bedtools.readthedocs.io/en/latest/&quot;&gt;bedtools&lt;/a&gt; is required by GeneMark-ETP if you want to run BRAKER with both RNA-Seq and protein data. You can download bedtools from &lt;a href=&quot;https://github.com/arq5x/bedtools2/releases&quot;&gt;https://github.com/arq5x/bedtools2/releases&lt;/a&gt;. Here, you can either download a precompiled version &lt;code&gt;bedtools.static.binary&lt;/code&gt;, e.g.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;wget https://github.com/arq5x/bedtools2/releases/download/v2.30.0/bedtools.static.binary
mv bedtools.static.binary bedtools
chmod a+x
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or you can download &lt;code&gt;bedtools-2.30.0.tar.gz&lt;/code&gt; and compile it from source using &lt;code&gt;make&lt;/code&gt;, e.g.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;wget https://github.com/arq5x/bedtools2/releases/download/v2.30.0/bedtools-2.30.0.tar.gz
tar -zxvf bedtools-2.30.0.tar.gz
cd bedtools2
make
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;a href=&quot;https://bedtools.readthedocs.io/en/latest/content/installation.html&quot;&gt;https://bedtools.readthedocs.io/en/latest/content/installation.html&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h4&gt;GffRead&lt;/h4&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/gpertea/gffread&quot;&gt;GffRead&lt;/a&gt; is a utility software required by GeneMark-ETP. It can be downloaded from &lt;a href=&quot;https://github.com/gpertea/gffread/releases/download/v0.12.7/gffread-0.12.7.Linux_x86_64.tar.gz&quot;&gt;https://github.com/gpertea/gffread/releases/download/v0.12.7/gffread-0.12.7.Linux_x86_64.tar.gz&lt;/a&gt; and installed with &lt;code&gt;make&lt;/code&gt;, e.g.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;wget https://github.com/gpertea/gffread/releases/download/v0.12.7/gffread-0.12.7.Linux_x86_64.tar.gz
tar xzf gffread-0.12.7.Linux_x86_64.tar.gz
cd gffread-0.12.7.Linux_x86_64
make
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Optional tools&lt;/h3&gt; 
&lt;h4&gt;Samtools&lt;/h4&gt; 
&lt;p&gt;Samtools is not required for running BRAKER without GeneMark-ETP if all your files are formatted, correctly (i.e.&amp;nbsp;all sequences should have short and unique fasta names). If you are not sure whether all your files are fomatted correctly, it might be helpful to have Samtools installed because BRAKER can automatically fix certain format issues by using Samtools.&lt;/p&gt; 
&lt;p&gt;As a prerequisite for Samtools, download and install &lt;code&gt;htslib&lt;/code&gt; (e.g. &lt;code&gt;git&amp;nbsp;clone&amp;nbsp;https://github.com/samtools/htslib.git&lt;/code&gt;, follow the &lt;code&gt;htslib&lt;/code&gt; documentation for installation).&lt;/p&gt; 
&lt;p&gt;Download and install Samtools (e.g. &lt;code&gt;git&amp;nbsp;clone&amp;nbsp;git://github.com/samtools/samtools.git&lt;/code&gt;), subsequently follow Samtools documentation for installation).&lt;/p&gt; 
&lt;p&gt;If already in your &lt;code&gt;$PATH&lt;/code&gt; variable, BRAKER will find samtools, automatically. Otherwise, BRAKER can find Samtools either by taking a command line argument (&lt;code&gt;--SAMTOOLS_PATH=/your_path_to_samtools/&lt;/code&gt;), or by using an environment variable &lt;code&gt;$SAMTOOLS_PATH&lt;/code&gt;. For exporting the variable, e.g.&amp;nbsp;for your current bash session, type:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    export SAMTOOLS_PATH=/your_path_to_samtools/
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Add the above line to a startup script (e.g.&amp;nbsp;&lt;code&gt;~/.bashrc&lt;/code&gt;) in order to set the environment variable for all bash sessions.&lt;/p&gt; 
&lt;h4&gt;Biopython&lt;/h4&gt; 
&lt;p&gt;If Biopython is installed, BRAKER can generate FASTA-files with coding sequences and protein sequences predicted by AUGUSTUS and generate track data hubs for visualization of a BRAKER run with MakeHub &lt;sup name=&quot;a16&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#f16&quot;&gt;R16&lt;/a&gt;&lt;/sup&gt;. These are optional steps. The first can be disabled with the command-line flag &lt;code&gt;--skipGetAnnoFromFasta&lt;/code&gt;, the second can be activated by using the command-line options &lt;code&gt;--makehub --email=your@mail.de&lt;/code&gt;, Biopython is not required if neither of these optional steps shall be performed.&lt;/p&gt; 
&lt;p&gt;On Ubuntu, install Python3 package manager with:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;`sudo apt-get install python3-pip`
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then, install Biopython with:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;`sudo pip3 install biopython`
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;cdbfasta&lt;/h4&gt; 
&lt;p&gt;cdbfasta and cdbyank are required by BRAKER for correcting AUGUSTUS genes with in frame stop codons (spliced stop codons) using the AUGUSTUS script fix_in_frame_stop_codon_genes.py. This can be skipped with &lt;code&gt;--skip_fixing_broken_genes&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;On Ubuntu, install cdbfasta with:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    sudo apt-get install cdbfasta
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For other systems, you can for example obtain cdbfasta from &lt;a href=&quot;https://github.com/gpertea/cdbfasta&quot;&gt;https://github.com/gpertea/cdbfasta&lt;/a&gt;, e.g.:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    git clone https://github.com/gpertea/cdbfasta.git
    cd cdbfasta
    make all
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;On Ubuntu, cdbfasta and cdbyank will be in your &lt;code&gt;$PATH&lt;/code&gt; variable after installation, and BRAKER will automatically locate them. However, you have the option to specify the &lt;code&gt;cdbfasta&lt;/code&gt; and &lt;code&gt;cdbyank&lt;/code&gt; binary location in two other ways:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Export an environment variable &lt;code&gt;$CDBTOOLS_PATH&lt;/code&gt;, e.g.&amp;nbsp;in your &lt;code&gt;~/.bashrc&lt;/code&gt; file:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code&gt;    export CDBTOOLS_PATH=/path/to/cdbtools/
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start=&quot;2&quot;&gt; 
 &lt;li&gt;Specify the command line option &lt;code&gt;--CDBTOOLS_PATH=/path/to/cdbtools/&lt;/code&gt; to &lt;code&gt;braker.pl&lt;/code&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Spaln&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Support of stand-alone Spaln (ouside of ProtHint) within BRAKER is deprecated.&lt;/p&gt; 
&lt;p&gt;This tool is required if you run ProtHint or if you would like to run protein to genome alignments with BRAKER using Spaln outside of ProtHint. Using Spaln outside of ProtHint is a suitable approach only if an annotated species of short evolutionary distance to your target genome is available. We recommend running Spaln through ProtHint for BRAKER. ProtHint brings along a Spaln binary. If that does not work on your system, download Spaln from &lt;a href=&quot;https://github.com/ogotoh/spaln&quot;&gt;https://github.com/ogotoh/spaln&lt;/a&gt;. Unpack and install according to &lt;code&gt;spaln/doc/SpalnReadMe22.pdf&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;BRAKER will try to locate the Spaln executable by using an environment variable &lt;code&gt;$ALIGNMENT_TOOL_PATH&lt;/code&gt;. Alternatively, this can be supplied as command line argument (&lt;code&gt;--ALIGNMENT_TOOL_PATH=/your/path/to/spaln&lt;/code&gt;).&lt;/p&gt; 
&lt;h4&gt;GUSHR&lt;/h4&gt; 
&lt;p&gt;This tool is only required if you want either add UTRs (from RNA-Seq data) to predicted genes or if you want to train UTR parameters for AUGUSTUS and predict genes with UTRs. In any case, GUSHR requires the input of RNA-Seq data.&lt;/p&gt; 
&lt;p&gt;GUSHR is available for download at &lt;a href=&quot;https://github.com/Gaius-Augustus/GUSHR&quot;&gt;https://github.com/Gaius-Augustus/GUSHR&lt;/a&gt;. Obtain it by typing:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;git clone https://github.com/Gaius-Augustus/GUSHR.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;GUSHR executes a GeMoMa jar file &lt;sup name=&quot;a19&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#f19&quot;&gt;R19, &lt;/a&gt;&lt;/sup&gt; &lt;sup name=&quot;a20&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#f20&quot;&gt;R20, &lt;/a&gt;&lt;/sup&gt; &lt;sup name=&quot;a21&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#f21&quot;&gt;R21&lt;/a&gt;&lt;/sup&gt;, and this jar file requires Java 1.8. On Ubuntu, you can install Java 1.8 with the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;sudo apt-get install openjdk-8-jdk
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you have several java versions installed on your system, make sure that you enable 1.8 prior running BRAKER with java by running&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;sudo update-alternatives --config java
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;and selecting the correct version.&lt;/p&gt; 
&lt;h4&gt;Tools from UCSC&lt;/h4&gt; 
&lt;p&gt;If you switch &lt;code&gt;--UTR=on&lt;/code&gt;, bamToWig.py will require the following tools that can be downloaded from &lt;a href=&quot;http://hgdownload.soe.ucsc.edu/admin/exe&quot;&gt;http://hgdownload.soe.ucsc.edu/admin/exe&lt;/a&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;twoBitInfo&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;faToTwoBit&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;It is optional to install these tools into your $PATH. If you don&#39;t, and you switch &lt;code&gt;--UTR=on&lt;/code&gt;, bamToWig.py will automatically download them into the working directory.&lt;/p&gt; 
&lt;h4&gt;MakeHub&lt;/h4&gt; 
&lt;p&gt;If you wish to automaticaly generate a track data hub of your BRAKER run, the MakeHub software, available at &lt;a href=&quot;https://github.com/Gaius-Augustus/MakeHub&quot;&gt;https://github.com/Gaius-Augustus/MakeHub&lt;/a&gt; is required. Download the software (either by running &lt;code&gt;git clone https://github.com/Gaius-Augustus/MakeHub.git&lt;/code&gt;, or by picking a release from &lt;a href=&quot;https://github.com/Gaius-Augustus/MakeHub/releases&quot;&gt;https://github.com/Gaius-Augustus/MakeHub/releases&lt;/a&gt;. Extract the release package if you downloaded a release (e.g. &lt;code&gt;unzip MakeHub.zip&lt;/code&gt; or &lt;code&gt;tar -zxvf MakeHub.tar.gz&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;BRAKER will try to locate the make_hub.py script by using an environment variable &lt;code&gt;$MAKEHUB_PATH&lt;/code&gt;. Alternatively, this can be supplied as command line argument (&lt;code&gt;--MAKEHUB_PATH=/your/path/to/MakeHub/&lt;/code&gt;). BRAKER can also try to guess the location of MakeHub on your system.&lt;/p&gt; 
&lt;h4&gt;SRA Toolkit&lt;/h4&gt; 
&lt;p&gt;If you want BRAKER to download RNA-Seq libraries from NCBI&#39;s SRA, the &lt;a href=&quot;https://github.com/ncbi/sra-tools/wiki&quot;&gt;SRA Toolkit&lt;/a&gt; is required. You can get a precompiled version of the SRA Toolkit from &lt;a href=&quot;http://daehwankimlab.github.io/hisat2/download/#version-hisat2-221&quot;&gt;http://daehwankimlab.github.io/hisat2/download/#version-hisat2-221&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;BRAKER will try to find executable binaries from the SRA Toolkit (fastq-dump, prefetch) by using an environment variable &lt;code&gt;$SRATOOLS_PATH&lt;/code&gt;. Alternatively, this can be supplied as command line argument (&lt;code&gt;--SRATOOLS_PATH=/your/path/to/SRAToolkit/&lt;/code&gt;). BRAKER can also try to guess the location of the SRA Toolkit on your system if the executables are in your &lt;code&gt;$PATH&lt;/code&gt; variable.&lt;/p&gt; 
&lt;h4&gt;HISAT2&lt;/h4&gt; 
&lt;p&gt;If you want to use unaligned RNA-Seq reads, the &lt;a href=&quot;http://daehwankimlab.github.io/hisat2&quot;&gt;HISAT2&lt;/a&gt; software is required to map them to the genome. A precompiled version of HISAT2 can be downloaded from &lt;a href=&quot;http://daehwankimlab.github.io/hisat2/download/#version-hisat2-221&quot;&gt;http://daehwankimlab.github.io/hisat2/download/#version-hisat2-221&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;BRAKER will try to find executable HISAT2 binaries (hisat2, hisat2-build) by using an environment variable &lt;code&gt;$HISAT2_PATH&lt;/code&gt;. Alternatively, this can be supplied as command line argument (&lt;code&gt;--HISAT2_PATH=/your/path/to/HISAT2/&lt;/code&gt;). BRAKER can also try to guess the location of HISAT2 on your system if the executables are in your &lt;code&gt;$PATH&lt;/code&gt; variable.&lt;/p&gt; 
&lt;h4&gt;compleasm&lt;/h4&gt; 
&lt;p&gt;If you want to run TSEBRA within BRAKER in a BUSCO completeness maximizing mode, you need to install &lt;a href=&quot;https://github.com/huangnengCSU/compleasm&quot;&gt;compleasm&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;wget https://github.com/huangnengCSU/compleasm/releases/download/v0.2.4/compleasm-0.2.4_x64-linux.tar.bz2
tar -xvjf compleasm-0.2.4_x64-linux.tar.bz2 &amp;amp;&amp;amp; \
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Add the resulting folder compleasm_kit to your &lt;code&gt;$PATH&lt;/code&gt; variable, e.g.:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;export PATH=$PATH:/your/path/to/compleasm_kit
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Compleasm requires pandas, which can be installed with:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install pandas
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;System dependencies&lt;/h2&gt; 
&lt;p&gt;BRAKER (braker.pl) uses getconf to see how many threads can be run on your system. On Ubuntu, you can install it with:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;sudo apt-get install libc-bin
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;Running BRAKER&lt;/h1&gt; 
&lt;h2&gt;Different BRAKER pipeline modes&lt;/h2&gt; 
&lt;p&gt;In the following, we describe “typical” BRAKER calls for different input data types. In general, we recommend that you run BRAKER on genomic sequences that have been softmasked for Repeats. BRAKER should only be applied to genomes that have been softmasked for repeats!&lt;/p&gt; 
&lt;h3&gt;BRAKER with RNA-Seq data&lt;/h3&gt; 
&lt;p&gt;This approach is suitable for genomes of species for which RNA-Seq libraries with good transcriptome coverage are available and for which protein data is not at hand. The pipeline is illustrated in Figure &lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#fig1&quot;&gt;2&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;BRAKER has several ways to receive RNA-Seq data as input:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;You can provide ID(s) of RNA-Seq libraries from SRA (in case of multiple IDs, separate them by comma) as argument to &lt;code&gt;--rnaseq_sets_ids&lt;/code&gt;. The libraries belonging to the IDs are then downloaded automatically by BRAKER, e.g.:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;    braker.pl --species=yourSpecies --genome=genome.fasta \
       --rnaseq_sets_ids=SRA_ID1,SRA_ID2
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;You can use local &lt;code&gt;FASTQ&lt;/code&gt; file(s) of unaligned reads as input. In this case, you have to provide BRAKER with the ID(s) of the RNA-Seq set(s) as argument to &lt;code&gt;--rnaseq_sets_ids&lt;/code&gt; and the path(s) to the directories, where the &lt;code&gt;FASTQ&lt;/code&gt; files are located as argument to &lt;code&gt;--rnaseq_sets_dirs&lt;/code&gt;. For each ID &lt;code&gt;ID&lt;/code&gt;, BRAKER will search in these directories for one &lt;code&gt;FASTQ&lt;/code&gt; file named &lt;code&gt;ID.fastq&lt;/code&gt; if the reads are unpaired, or for two &lt;code&gt;FASTQ&lt;/code&gt; files named &lt;code&gt;ID_1.fastq&lt;/code&gt; and &lt;code&gt;ID_2.fastq&lt;/code&gt; if they are paired.&lt;/p&gt; &lt;p&gt;For example, if you have a paired library called &#39;SRA_ID1&#39; and an unpaired library named &#39;SRA_ID2&#39;, you have to have a directory &lt;code&gt;/path/to/local/fastq/files/&lt;/code&gt;, where the files &lt;code&gt;SRA_ID1_1.fastq&lt;/code&gt;, &lt;code&gt;SRA_ID1_2.fastq&lt;/code&gt;, and &lt;code&gt;SRA_ID2.fastq&lt;/code&gt; reside. Then, you could run BRAKER with following command:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;    braker.pl --species=yourSpecies --genome=genome.fasta \
       --rnaseq_sets_ids=SRA_ID1,SRA_ID2 \
       --rnaseq_sets_dirs=/path/to/local/fastq/files/
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;There are two ways of supplying BRAKER with RNA-Seq data as &lt;code&gt;bam&lt;/code&gt; file(s). First, you can do it in the same way as you would supply &lt;code&gt;FASTQ&lt;/code&gt; file(s): Provide the ID(s)/name(s) of your &lt;code&gt;bam&lt;/code&gt; file(s) as argument to &lt;code&gt;--rnaseq_sets_ids&lt;/code&gt; and specify directories where the &lt;code&gt;bam&lt;/code&gt; files reside with &lt;code&gt;--rnaseq_sets_dirs&lt;/code&gt;. BRAKER will automatically detect that these ID(s) are &lt;code&gt;bam&lt;/code&gt; and not &lt;code&gt;FASTQ&lt;/code&gt; file(s), e.g.:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;    braker.pl --species=yourSpecies --genome=genome.fasta \
       --rnaseq_sets_ids=BAM_ID1,BAM_ID2 \
       --rnaseq_sets_dirs=/path/to/local/bam/files/
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Second, you can specify the paths to your &lt;code&gt;bam&lt;/code&gt; file(s) directly, e.g. can either extract RNA-Seq spliced alignment information from &lt;code&gt;bam&lt;/code&gt; files, or it can use such extracted information, directly.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;    braker.pl --species=yourSpecies --genome=genome.fasta \
       --bam=file1.bam,file2.bam
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Please note that we generally assume that bam files were generated with HiSat2 because that is the aligner that would also be executed by BRAKER3 with fastq input. If you want for some reason to generate the bam files with STAR, use the option &lt;code&gt;--outSAMstrandField intronMotif&lt;/code&gt; of STAR to produce files that are compatible wiht StringTie in BRAKER3.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;In order to run BRAKER with RNA-Seq spliced alignment information that has already been extracted, run:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;    braker.pl --species=yourSpecies --genome=genome.fasta \
       --hints=hints1.gff,hints2.gff
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The format of such a hints file must be as follows (tabulator separated file):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;    chrName b2h intron  6591    8003    1   +   .   pri=4;src=E
    chrName b2h intron  6136    9084    11  +   .   mult=11;pri=4;src=E
    ...
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The source &lt;code&gt;b2h&lt;/code&gt; in the second column and the source tag &lt;code&gt;src=E&lt;/code&gt; in the last column are essential for BRAKER to determine whether a hint has been generated from RNA-Seq data.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;It is also possible to provide RNA-Seq sets in different ways for the same BRAKER run, any combination of above options is possible. It is not recommended to provide RNA-Seq data with &lt;code&gt;--hints&lt;/code&gt; if you run BRAKER in ETPmode (RNA-Seq &lt;em&gt;and&lt;/em&gt; protein data), because GeneMark-ETP won&#39;t use these hints!&lt;/p&gt; 
&lt;h3&gt;BRAKER with protein data&lt;/h3&gt; 
&lt;p&gt;This approach is suitable for genomes of species for which no RNA-Seq libraries are available. A large database of proteins (with possibly longer evolutionary distance to the target species) should be used in this case. This mode is illustrated in figure &lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#fig8&quot;&gt;9&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/docs/figs/braker2-full.png&quot; alt=&quot;braker2-main-a&quot;&gt;&lt;/p&gt; 
&lt;p&gt;Figure 9: BRAKER with proteins of any evolutionary distance. ProtHint protein mapping pipelines is used to generate protein hints. ProtHint automatically determines which alignments are from close relatives, and which are from rather distant relatives.&lt;/p&gt; 
&lt;p&gt;For running BRAKER in this mode, type:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;braker.pl --genome=genome.fa --prot_seq=proteins.fa
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;We recommend using OrthoDB as basis for &lt;code&gt;proteins.fa&lt;/code&gt;. The instructions on how to prepare the input OrthoDB proteins are documented here: &lt;a href=&quot;https://github.com/gatech-genemark/ProtHint#protein-database-preparation&quot;&gt;https://github.com/gatech-genemark/ProtHint#protein-database-preparation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You can of course add additional protein sequences to that file, or try with a completely different database. Any database will need several representatives for each protein, though.&lt;/p&gt; 
&lt;p&gt;Instead of having BRAKER run ProtHint, you can also start BRAKER with hints already produced by ProtHint, by providing ProtHint&#39;s &lt;code&gt;prothint_augustus.gff&lt;/code&gt; output:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;braker.pl --genome=genome.fa --hints=prothint_augustus.gff
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The format of &lt;code&gt;prothint_augustus.gff&lt;/code&gt; in this mode looks like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;2R ProtHint intron 11506230 11506648 4 + . src=M;mult=4;pri=4
2R ProtHint intron 9563406  9563473  1 + . grp=69004_0:001de1_702_g;src=C;pri=4;
2R ProtHint intron 8446312  8446371  1 + . grp=43151_0:001cae_473_g;src=C;pri=4;
2R ProtHint intron 8011796  8011865  2 - . src=P;mult=1;pri=4;al_score=0.12;
2R ProtHint start  234524   234526   1 + . src=P;mult=1;pri=4;al_score=0.08;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The prediction of all hints with &lt;code&gt;src=M&lt;/code&gt; will be enforced. Hints with &lt;code&gt;src=C&lt;/code&gt; are &#39;chained evidence&#39;, i.e. they will only be incorporated if all members of the group (grp=...) can be incorporated in a single transcript. All other hints have &lt;code&gt;src=P&lt;/code&gt; in the last column. Supported features in column 3 are &lt;code&gt;intron&lt;/code&gt;, &lt;code&gt;start&lt;/code&gt;, &lt;code&gt;stop&lt;/code&gt; and &lt;code&gt;CDSpart&lt;/code&gt;.&lt;/p&gt; 
&lt;h4&gt;Training and prediction of UTRs, integration of coverage information&lt;/h4&gt; 
&lt;p&gt;If RNA-Seq (and only RNA-Seq) data is provided to BRAKER as a bam-file, and if the genome is softmasked for repeats, BRAKER can automatically train UTR parameters for AUGUSTUS. After successful training of UTR parameters, BRAKER will automatically predict genes including coverage information form RNA-Seq data. Example call:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;braker.pl --species=yourSpecies --genome=genome.fasta \
   --bam=file.bam --UTR=on
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Warnings:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;This feature is experimental!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;--UTR=on is currently not compatible with bamToWig.py as released in AUGUSTUS 3.3.3; it requires the current development code version from the github repository (git clone &lt;a href=&quot;https://github.com/Gaius-Augustus/Augustus.git&quot;&gt;https://github.com/Gaius-Augustus/Augustus.git&lt;/a&gt;).&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;--UTR=on increases memory consumption of AUGUSTUS. Carefully monitor jobs if your machine was close to maxing RAM without --UTR=on! Reducing the number of cores will also reduce RAM consumption.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;UTR prediction sometimes improves coding sequence prediction accuracy, but not always. If you try this feature, carefully compare results with and without UTR parameters, afterwards (e.g. in UCSC Genome Browser).&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Stranded RNA-Seq alignments&lt;/h4&gt; 
&lt;p&gt;For running BRAKER without UTR parameters, it is not very important whether RNA-Seq data was generated by a &lt;em&gt;stranded&lt;/em&gt; protocol (because spliced alignments are ’artificially stranded’ by checking the splice site pattern). However, for UTR training and prediction, stranded libraries may provide information that is valuable for BRAKER.&lt;/p&gt; 
&lt;p&gt;After alignment of the stranded RNA-Seq libraries, separate the resulting bam file entries into two files: one for plus strand mappings, one for minus strand mappings. Call BRAKER as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;braker.pl --species=yourSpecies --genome=genome.fasta \
    --bam=plus.bam,minus.bam --stranded=+,- \
    --UTR=on
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You may additionally include bam files from unstranded libraries. Those files will not used for generating UTR training examples, but they will be included in the final gene prediction step as unstranded coverage information, example call:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;braker.pl --species=yourSpecies --genome=genome.fasta \
   --bam=plus.bam,minus.bam,unstranded.bam \
   --stranded=+,-,. --UTR=on
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Warning:&lt;/strong&gt; This feature is experimental and currently has low priority on our maintenance list!&lt;/p&gt; 
&lt;h3&gt;BRAKER with RNA-Seq &lt;strong&gt;and&lt;/strong&gt; protein data&lt;/h3&gt; 
&lt;p&gt;The native mode for running BRAKER with RNA-Seq and protein data. This will call GeneMark-ETP, which will use RNA-Seq and protein hints for training GeneMark-ETP. Subsequently, AUGUSTUS is trained on &#39;high-confindent&#39; genes (genes with very high extrinsic evidence support) from the GeneMark-ETP prediction and a set of genes is predicted by AUGUSTUS. In a last step, the predictions of AUGUSTUS and GeneMark-ETP are combined using TSEBRA.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Alignment of RNA-Seq reads&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;GeneMark-ETP utilizes Stringtie2 to assemble RNA-Seq data, which requires that the aligned reads (BAM files) contain the XS (strand) tag for spliced reads. Therefore, if you align your reads with HISAT2, you must enable the --dta option, or if you use STAR, you must use the --outSAMstrandField intronMotif option. TopHat alignments include this tag by default.&lt;/p&gt; 
&lt;p&gt;To call the pipeline in this mode, you have to provide it with a protein database using &lt;code&gt;--prot_seq&lt;/code&gt; (as described in &lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#braker-with-protein-data&quot;&gt;BRAKER with protein data&lt;/a&gt;), and RNA-Seq data either by their SRA ID so that they are downloaded by BRAKER, as unaligned reads in &lt;code&gt;FASTQ&lt;/code&gt; format, and/or as aligned reads in &lt;code&gt;bam&lt;/code&gt; format (as described in &lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#braker-with-rna-seq-data&quot;&gt;BRAKER with RNA-Seq data&lt;/a&gt;). You could also specify already processed extrinsic evidence using the &lt;code&gt;--hints&lt;/code&gt; option. However, this is not recommend for a normal BRAKER run in ETPmode, as these hints won&#39;t be used in the GeneMark-ETP step. Only use &lt;code&gt;--hints&lt;/code&gt; when you want to skip the GenMark-ETP step!&lt;/p&gt; 
&lt;p&gt;Examples of how you could run BRAKER in ETPmode:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    braker.pl --genome=genome.fa --prot_seq=orthodb.fa \
        --rnaseq_sets_ids=SRA_ID1,SRA_ID2 \
        --rnaseq_sets_dirs=/path/to/local/RNA-Seq/files/
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code&gt;    braker.pl --genome=genome.fa --prot_seq=orthodb.fa \
        --rnaseq_sets_ids=SRA_ID1,SRA_ID2,SRA_ID3
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code&gt;        braker.pl --genome=genome.fa --prot_seq=orthodb.fa \
            --bam=/path/to/SRA_ID1.bam,/path/to/SRA_ID2.bam
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;BRAKER with short and long read RNA-Seq and protein data&lt;/h3&gt; 
&lt;p&gt;A preliminary protocol for integration of assembled subreads from PacBio ccs sequencing in combination with short read Illumina RNA-Seq and protein database is described at &lt;a href=&quot;https://github.com/Gaius-Augustus/BRAKER/raw/master/docs/long_reads/long_read_protocol.md&quot;&gt;https://github.com/Gaius-Augustus/BRAKER/blob/master/docs/long_reads/long_read_protocol.md&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;BRAKER with long read RNA-Seq (only) and protein data&lt;/h3&gt; 
&lt;p&gt;We forked GeneMark-ETP and hard coded that StringTie will perform long read assembly in that particular version. If you want to use this &#39;fast-hack&#39; version for BRAKER, you have to prepare the BAM file with long read to genome spliced alignments outside of BRAKER, e.g.:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;T=48 # adapt to your number of threads
minimap2 -t${T} -ax splice:hq -uf genome.fa isoseq.fa &amp;gt; isoseq.sam     
samtools view -bS --threads ${T} isoseq.sam -o isoseq.bam
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Pull the adapted container:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;singularity build braker3_lr.sif docker://teambraker/braker3:isoseq
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Calling BRAKER3 with a BAM file of spliced-aligned IsoSeq Reads:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;singularity exec -B ${PWD}:${PWD} braker3_lr.sif braker.pl --genome=genome.fa --prot_seq=protein_db.fa –-bam=isoseq.bam --threads=${T} 
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt; Do NOT mix short read and long read data in this BRAKER/GeneMark-ETP variant!&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt; The accuracy of gene prediction here heavily depends on the depth of your isoseq data. We verified with PacBio HiFi reads from 2022 that given sufficient completeness of the assembled transcriptome you will reach similar results as with short reads. However, we also observed a drop in accuracy compared to short reads when using other long read data sets with higher error rates and less sequencing depth.&lt;/p&gt; 
&lt;h2&gt;Description of selected BRAKER command line options&lt;/h2&gt; 
&lt;p&gt;Please run &lt;code&gt;braker.pl&amp;nbsp;--help&lt;/code&gt; to obtain a full list of options.&lt;/p&gt; 
&lt;h3&gt;--ab_initio&lt;/h3&gt; 
&lt;p&gt;Compute AUGUSTUS &lt;em&gt;ab initio&lt;/em&gt; predictions in addition to AUGUSTUS predictions with hints (additional output files: &lt;code&gt;augustus.ab_initio.*&lt;/code&gt;. This may be useful for estimating the quality of training gene parameters when inspecting predictions in a Browser.&lt;/p&gt; 
&lt;h3&gt;--augustus_args=&quot;--some_arg=bla&quot;&lt;/h3&gt; 
&lt;p&gt;One or several command line arguments to be passed to AUGUSTUS, if several arguments are given, separate them by whitespace, i.e.&amp;nbsp;&lt;code&gt;&quot;--first_arg=sth&amp;nbsp;--second_arg=sth&quot;&lt;/code&gt;. This may be be useful if you know that gene prediction in your particular species benefits from a particular AUGUSTUS argument during the prediction step.&lt;/p&gt; 
&lt;h3&gt;--threads=INT&lt;/h3&gt; 
&lt;p&gt;Specifies the maximum number of threads that can be used during computation. BRAKER has to run some steps on a single thread, others can take advantage of multiple threads. If you use more than 8 threads, this will not speed up all parallelized steps, in particular, the time consuming &lt;code&gt;optimize_augustus.pl&lt;/code&gt; will not use more than 8 threads. However, if you don’t mind some threads being idle, using more than 8 threads will speed up other steps.&lt;/p&gt; 
&lt;h3&gt;--fungus&lt;/h3&gt; 
&lt;p&gt;GeneMark-ETP option: run algorithm with branch point model. Use this option if you genome is a fungus.&lt;/p&gt; 
&lt;h3&gt;--useexisting&lt;/h3&gt; 
&lt;p&gt;Use the present config and parameter files if they exist for &#39;species&#39;; will overwrite original parameters if BRAKER performs an AUGUSTUS training.&lt;/p&gt; 
&lt;h3&gt;--crf&lt;/h3&gt; 
&lt;p&gt;Execute CRF training for AUGUSTUS; resulting parameters are only kept for final predictions if they show higher accuracy than HMM parameters. This increases runtime!&lt;/p&gt; 
&lt;h3&gt;--lambda=int&lt;/h3&gt; 
&lt;p&gt;Change the parameter $\lambda$ of the Poisson distribution that is used for downsampling training genes according to their number of introns (only genes with up to 5 introns are downsampled). The default value is $\lambda=2$. You might want to set it to 0 for organisms that mainly have single-exon genes. (Generally, single-exon genes contribute less value to increasing AUGUSTUS parameters compared to genes with many exons.)&lt;/p&gt; 
&lt;h3&gt;--UTR=on&lt;/h3&gt; 
&lt;p&gt;Generate UTR training examples for AUGUSTUS from RNA-Seq coverage information, train AUGUSTUS UTR parameters and predict genes with AUGUSTUS and UTRs, including coverage information for RNA-Seq as evidence. &lt;em&gt;This is an experimental feature!&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;If you performed a BRAKER run without --UTR=on, you can add UTR parameter training and gene prediction with UTR parameters (and only RNA-Seq hints) with the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;braker.pl --genome=../genome.fa --addUTR=on \
    --bam=../RNAseq.bam --workingdir=$wd \
    --AUGUSTUS_hints_preds=augustus.hints.gtf \
    --threads=8 --skipAllTraining --species=somespecies
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Modify &lt;code&gt;augustus.hints.gtf&lt;/code&gt; to point to the AUGUSTUS predictions with hints from previous BRAKER run; modify flaning_DNA value to the flanking region from the log file of your previous BRAKER run; modify some_new_working_directory to the location where BRAKER should store results of the additional BRAKER run; modify somespecies to the species name used in your previous BRAKER run.&lt;/p&gt; 
&lt;h3&gt;--addUTR=on&lt;/h3&gt; 
&lt;p&gt;Add UTRs from RNA-Seq converage information to AUGUSTUS gene predictions using GUSHR. No training of UTR parameters and no gene prediction with UTR parameters is performed.&lt;/p&gt; 
&lt;p&gt;If you performed a BRAKER run without --addUTR=on, you can add UTRs results of a previous BRAKER run with the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;braker.pl --genome=../genome.fa --addUTR=on \
    --bam=../RNAseq.bam --workingdir=$wd \
    --AUGUSTUS_hints_preds=augustus.hints.gtf --threads=8 \
    --skipAllTraining --species=somespecies
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Modify &lt;code&gt;augustus.hints.gtf&lt;/code&gt; to point to the AUGUSTUS predictions with hints from previous BRAKER run; modify some_new_working_directory to the location where BRAKER should store results of the additional BRAKER run; this run will not modify AUGUSTUS parameters. We recommend that you specify the original species of the original run with &lt;code&gt;--species=somespecies&lt;/code&gt;. Otherwise, BRAKER will create an unneeded species parameters directory &lt;code&gt;Sp_*&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;--stranded=+,-,.,...&lt;/h3&gt; 
&lt;p&gt;If &lt;code&gt;--UTR=on&lt;/code&gt; is enabled, strand-separated bam-files can be provided with &lt;code&gt;--bam=plus.bam,minus.bam&lt;/code&gt;. In that case, &lt;code&gt;--stranded=...&lt;/code&gt; should hold the strands of the bam files (&lt;code&gt;+&lt;/code&gt; for plus strand, &lt;code&gt;-&lt;/code&gt; for minus strand, &lt;code&gt;.&lt;/code&gt; for unstranded). Note that unstranded data will be used in the gene prediction step, only, if the parameter &lt;code&gt;--stranded=...&lt;/code&gt; is set. &lt;em&gt;This is an experimental feature! GUSHR currently does not take advantage of stranded data.&lt;/em&gt;&lt;/p&gt; 
&lt;h3&gt;--makehub --email=&lt;a href=&quot;mailto:your@mail.de&quot;&gt;your@mail.de&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;If &lt;code&gt;--makehub&lt;/code&gt; and &lt;code&gt;--email=your@mail.de&lt;/code&gt; (with your valid e-mail adress) are provided, a track data hub for visualizing results with the UCSC Genome Browser will be generated using MakeHub (&lt;a href=&quot;https://github.com/Gaius-Augustus/MakeHub&quot;&gt;https://github.com/Gaius-Augustus/MakeHub&lt;/a&gt;).&lt;/p&gt; 
&lt;h3&gt;--gc_probability=DECIMAL&lt;/h3&gt; 
&lt;p&gt;By default, GeneMark-ES/ET/EP/ETP uses a probability of 0.001 for predicting the donor splice site pattern GC (instead of GT). It may make sense to increase this value for species where this donor splice site is more common. For example, in the species &lt;em&gt;Emiliania huxleyi&lt;/em&gt;, about 50% of donor splice sites have the pattern GC (&lt;a href=&quot;https://media.nature.com/original/nature-assets/nature/journal/v499/n7457/extref/nature12221-s2.pdf&quot;&gt;https://media.nature.com/original/nature-assets/nature/journal/v499/n7457/extref/nature12221-s2.pdf&lt;/a&gt;, page 5).&lt;/p&gt; 
&lt;h3&gt;--busco_lineage=lineage&lt;/h3&gt; 
&lt;p&gt;Use a species-specific lineage, e.g. arthropoda_odb10 for an arthropod. BRAKER does not support auto-typing of the lineage.&lt;/p&gt; 
&lt;p&gt;Specifying a BUSCO-lineage invokes two changes in BRAKER &lt;sup name=&quot;a28&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#f28&quot;&gt;R28&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;BRAKER will run compleasm with the specified lineage in genome mode and convert the detected BUSCO matches into hints for AUGUSTUS. This may increase the number of BUSCOs in the augustus.hints.gtf file slightly.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;BRAKER will invoke best_by_compleasm.py to check whether the braker.gtf file that is by default generated by TSEBRA has the lowest amount of missing BUSCOs compared to the augustus.hints.gtf and the genemark.gtf file. If not, the following decision schema is applied to re-run TSEBRA to minimize the missing BUSCOs in the final output of BRAKER (always braker.gtf). If an alternative and better gene set is created, the original braker.gtf gene set is moved to a directory called braker_original. Information on what happened during the best_by_compleasm.py run is written to the file best_by_compleasm.log.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/docs/figs/best_by_compleasm.jpg&quot; alt=&quot;best_by_busco[fig14]&quot;&gt;&lt;/p&gt; 
&lt;p&gt;Please note that using BUSCO to assess the quality of a gene set, in particular when comparing BRAKER to other pipelines, does not make sense once you specified a BUSCO lineage. We recommend that you use other measures to assess the quality of your gene set, e.g. by comparing it to a reference gene set or running OMArk.&lt;/p&gt; 
&lt;h1&gt;Output of BRAKER&lt;/h1&gt; 
&lt;p&gt;BRAKER produces several important output files in the working directory.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;braker.gtf: Final gene set of BRAKER. This file may contain different contents depending on how you called BRAKER&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;in ETPmode: Final gene set of BRAKER consisting of genes predicted by AUGUSTUS and GeneMark-ETP that were combined and filtered by TSEBRA.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;otherwise: Union of augustus.hints.gtf and reliable GeneMark-ES/ET/EP predictions (genes fully supported by external evidence). In &lt;code&gt;--esmode&lt;/code&gt;, this is the union of augustus.ab_initio.gtf and all GeneMark-ES genes. Thus, this set is generally more sensitive (more genes correctly predicted) and can be less specific (more false-positive predictions can be present). This output is not necessarily better than augustus.hints.gtf, and it is not recommended to use it if BRAKER was run in ESmode.&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;braker.codingseq: Final gene set with coding sequences in FASTA format&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;braker.aa: Final gene set with protein sequences in FASTA format&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;braker.gff3: Final gene set in gff3 format (only produced if the flag &lt;code&gt;--gff3&lt;/code&gt; was specified to BRAKER.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Augustus/*: Augustus gene set(s) in as gtf/conding/aa files&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;GeneMark-E*/genemark.gtf: Genes predicted by GeneMark-ES/ET/EP/EP+/ETP in GTF-format.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;hintsfile.gff: The extrinsic evidence data extracted from RNAseq.bam and/or protein data.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;braker_original/*: Genes predicted by BRAKER (TSEBRA merge) before compleasm was used to improve BUSCO completeness&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;bbc/*: output folder of best_by_compleasm.py script from TSEBRA that is used to improve BUSCO completeness in the final output of BRAKER&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Output files may be present with the following name endings and formats:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Coding sequences in FASTA-format are produced if the flag &lt;code&gt;--skipGetAnnoFromFasta&lt;/code&gt; was not set.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Protein sequence files in FASTA-format are produced if the flag &lt;code&gt;--skipGetAnnoFromFasta&lt;/code&gt; was not set.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For details about gtf format, see &lt;a href=&quot;http://www.sanger.ac.uk/Software/formats/GFF/&quot;&gt;http://www.sanger.ac.uk/Software/formats/GFF/&lt;/a&gt;. A GTF-format file contains one line per predicted exon. Example:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    HS04636 AUGUSTUS initial   966 1017 . + 0 transcript_id &quot;g1.1&quot;; gene_id &quot;g1&quot;;
    HS04636 AUGUSTUS internal 1818 1934 . + 2 transcript_id &quot;g1.1&quot;; gene_id &quot;g1&quot;;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The columns (fields) contain:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    seqname source feature start end score strand frame transcript ID and gene ID
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If the &lt;code&gt;--makehub&lt;/code&gt; option was used and MakeHub is available on your system, a hub directory beginning with the name &lt;code&gt;hub_&lt;/code&gt; will be created. Copy this directory to a publicly accessible web server. A file &lt;code&gt;hub.txt&lt;/code&gt; resides in the directory. Provide the link to that file to the UCSC Genome Browser for visualizing results.&lt;/p&gt; 
&lt;h1&gt;Example data&lt;/h1&gt; 
&lt;p&gt;An incomplete example data set is contained in the directory &lt;code&gt;BRAKER/example&lt;/code&gt;. In order to complete the data set, please download the RNA-Seq alignment file (134 MB) with &lt;code&gt;wget&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;cd BRAKER/example
wget http://topaz.gatech.edu/GeneMark/Braker/RNAseq.bam
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;In case you have trouble accessing that file, there&#39;s also a copy available from another server:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;cd BRAKER/example
wget http://bioinf.uni-greifswald.de/augustus/datasets/RNAseq.bam
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The example data set was not compiled in order to achieve optimal prediction accuracy, but in order to quickly test pipeline components. The small subset of the genome used in these test examples is not long enough for BRAKER training to work well.&lt;/p&gt; 
&lt;h2&gt;Data description&lt;/h2&gt; 
&lt;p&gt;Data corresponds to the last 1,000,000 nucleotides of &lt;em&gt;Arabidopsis thaliana&lt;/em&gt;&#39;s chromosome Chr5, split into 8 artificial contigs.&lt;/p&gt; 
&lt;p&gt;RNA-Seq alignments were obtained by &lt;a href=&quot;https://github.com/Gaius-Augustus/VARUS&quot;&gt;VARUS&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The protein sequences are a subset of &lt;a href=&quot;https://v100.orthodb.org/download/odb10_plants_fasta.tar.gz&quot;&gt;OrthoDB v10 plants proteins&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;List of files:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;genome.fa&lt;/code&gt; - genome file in fasta format&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;RNAseq.bam&lt;/code&gt; - RNA-Seq alignment file in bam format (this file is not a part of this repository, it must be downloaded separately from &lt;a href=&quot;http://topaz.gatech.edu/GeneMark/Braker/RNAseq.bam&quot;&gt;http://topaz.gatech.edu/GeneMark/Braker/RNAseq.bam&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;RNAseq.hints&lt;/code&gt; - RNA-Seq hints (can be used instead of RNAseq.bam as RNA-Seq input to BRAKER)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;proteins.fa&lt;/code&gt; - protein sequences in fasta format&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The below given commands assume that you configured all paths to tools by exporting bash variables or that you have the necessary tools in your $PATH.&lt;/p&gt; 
&lt;p&gt;The example data set also contains scripts &lt;code&gt;tests/test*.sh&lt;/code&gt; that will execute below listed commands for testing BRAKER with the example data set. You find example results of AUGUSTUS and GeneMark-ES/ET/EP/ETP in the folder &lt;code&gt;results/test*&lt;/code&gt;. Be aware that BRAKER contains several parts where random variables are used, i.e.&amp;nbsp;results that you obtain when running the tests may not be exactly identical. To compare your test results with the reference ones, you can use the &lt;a href=&quot;https://github.com/Gaius-Augustus/BRAKER/raw/master/scripts/compare_intervals_exact.pl&quot;&gt;compare_intervals_exact.pl&lt;/a&gt; script as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;# Compare CDS features
compare_intervals_exact.pl --f1 augustus.hints.gtf --f2 ../../results/test${N}/augustus.hints.gtf --verbose
# Compare transcripts
compare_intervals_exact.pl --f1 augustus.hints.gtf --f2 ../../results/test${N}/augustus.hints.gtf --trans --verbose
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Several tests use &lt;code&gt;--gm_max_intergenic 10000&lt;/code&gt; option to make the test runs faster. It is not recommended to use this option in real BRAKER runs, the speed increase achieved by adjusting this option is negligible on full-sized genomes.&lt;/p&gt; 
&lt;p&gt;We give runtime estimations derived from computing on &lt;em&gt;Intel(R) Xeon(R) CPU E5530 @ 2.40GHz&lt;/em&gt;.&lt;/p&gt; 
&lt;h2&gt;Testing BRAKER with RNA-Seq data&lt;/h2&gt; 
&lt;p&gt;The following command will run the pipeline according to Figure &lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#fig2&quot;&gt;3&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;braker.pl --genome genome.fa --bam RNAseq.bam --threads N --busco_lineage=lineage_odb10
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This test is implemented in &lt;code&gt;test1.sh&lt;/code&gt;, expected runtime is ~20 minutes.&lt;/p&gt; 
&lt;h2&gt;Testing BRAKER with proteins&lt;/h2&gt; 
&lt;p&gt;The following command will run the pipeline according to Figure &lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#fig3&quot;&gt;4&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;braker.pl --genome genome.fa --prot_seq proteins.fa --threads N --busco_lineage=lineage_odb10
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This test is implemented in &lt;code&gt;test2.sh&lt;/code&gt;, expected runtime is ~20 minutes.&lt;/p&gt; 
&lt;h2&gt;Testing BRAKER with proteins and RNA-Seq&lt;/h2&gt; 
&lt;p&gt;The following command will run a pipeline that first trains GeneMark-ETP with protein and RNA-Seq hints and subsequently trains AUGUSTUS on the basis of GeneMark-ETP predictions. AUGUSTUS predictions are also performed with hints from both sources, see Figure &lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#fig4&quot;&gt;5&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Run with local RNA-Seq file:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;braker.pl --genome genome.fa --prot_seq proteins.fa --bam ../RNAseq.bam --threads N --busco_lineage=lineage_odb10
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This test is implemented in &lt;code&gt;test3.sh&lt;/code&gt;, expected runtime is ~20 minutes.&lt;/p&gt; 
&lt;p&gt;Download RNA-Seq library from Sequence Read Archive (~1gb):&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;braker.pl --genome genome.fa --prot_seq proteins.fa --rnaseq_sets_ids ERR5767212 --threads N --busco_lineage=lineage_odb10
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This test is implemented in &lt;code&gt;test3_4.sh&lt;/code&gt;, expected runtime is ~35 minutes.&lt;/p&gt; 
&lt;h2&gt;Testing BRAKER with pre-trained parameters&lt;/h2&gt; 
&lt;p&gt;The training step of all pipelines can be skipped with the option &lt;code&gt;--skipAllTraining&lt;/code&gt;. This means, only AUGUSTUS predictions will be performed, using pre-trained, already existing parameters. For example, you can predict genes with the command:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    braker.pl --genome=genome.fa --bam RNAseq.bam --species=arabidopsis \
        --skipAllTraining --threads N
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This test is implemented in &lt;code&gt;test4.sh&lt;/code&gt;, expected runtime is ~1 minute.&lt;/p&gt; 
&lt;h2&gt;Testing BRAKER with genome sequence&lt;/h2&gt; 
&lt;p&gt;The following command will run the pipeline with no extrinsic evidence:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;braker.pl --genome=genome.fa --esmode --threads N
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This test is implemented in &lt;code&gt;test5.sh&lt;/code&gt;, expected runtime is ~20 minutes.&lt;/p&gt; 
&lt;h2&gt;Testing BRAKER with RNA-Seq data and --UTR=on&lt;/h2&gt; 
&lt;p&gt;The following command will run BRAKER with training UTR parameters from RNA-Seq coverage data:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;braker.pl --genome genome.fa --bam RNAseq.bam --UTR=on --threads N
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This test is implemented in &lt;code&gt;test6.sh&lt;/code&gt;, expected runtime is ~20 minutes.&lt;/p&gt; 
&lt;h2&gt;Testing BRAKER with RNA-Seq data and --addUTR=on&lt;/h2&gt; 
&lt;p&gt;The following command will add UTRs to augustus.hints.gtf from RNA-Seq coverage data:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;braker.pl --genome genome.fa --bam RNAseq.bam --addUTR=on --threads N
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This test is implemented in &lt;code&gt;test7.sh&lt;/code&gt;, expected runtime is ~20 minutes.&lt;/p&gt; 
&lt;h1&gt;Starting BRAKER on the basis of previously existing BRAKER runs&lt;/h1&gt; 
&lt;p&gt;There is currently no clean way to restart a failed BRAKER run (after solving some problem). However, it is possible to start a new BRAKER run based on results from a previous run -- given that the old run produced the required intermediate results. We will in the following refer to the old working directory with variable &lt;code&gt;${BRAKER_OLD}&lt;/code&gt;, and to the new BRAKER working directory with &lt;code&gt;${BRAKER_NEW}&lt;/code&gt;. The file &lt;code&gt;what-to-cite.txt&lt;/code&gt; will always only refer to the software that was actually called by a particular run. You might have to combine the contents of &lt;code&gt;${BRAKER_NEW}/what-to-cite.txt&lt;/code&gt; with &lt;code&gt;${BRAKER_OLD}/what-to-cite.txt&lt;/code&gt; for preparing a publication. The following figure illustrates at which points BRAKER run may be intercepted.&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/docs/figs/braker-intercept.png&quot; alt=&quot;braker-intercept[fig8]&quot;&gt;&lt;/p&gt; 
&lt;p&gt;Figure 10: Points for intercepting a BRAKER run and reusing intermediate results in a new BRAKER run.&lt;/p&gt; 
&lt;h2&gt;Option 1: starting BRAKER with existing hints file(s) before training&lt;/h2&gt; 
&lt;p&gt;This option is only possible for BRAKER in ETmode or EPmode and &lt;ins&gt;not&lt;/ins&gt; in ETPmode!&lt;/p&gt; 
&lt;p&gt;If you have access to an existing BRAKER output that contains hintsfiles that were generated from extrinsic data, such as RNA-Seq or protein sequences, you can recycle these hints files in a new BRAKER run. Also, hints from a separate ProtHint run can be directly used in BRAKER.&lt;/p&gt; 
&lt;p&gt;The hints can be given to BRAKER with &lt;code&gt;--hints ${BRAKER_OLD}/hintsfile.gff&lt;/code&gt; option. This is illustrated in the test files &lt;code&gt;test1_restart1.sh&lt;/code&gt;, &lt;code&gt;test2_restart1.sh&lt;/code&gt;, &lt;code&gt;test4_restart1.sh&lt;/code&gt;. The other modes (for which this test is missing) cannot be restarted in this way.&lt;/p&gt; 
&lt;h2&gt;Option 2: starting BRAKER after GeneMark-ES/ET/EP/ETP had finished, before training AUGUSTUS&lt;/h2&gt; 
&lt;p&gt;The GeneMark result can be given to BRAKER with &lt;code&gt;--geneMarkGtf ${BRAKER_OLD}/GeneMark*/genemark.gtf&lt;/code&gt; option if BRAKER is run in ETmode or EPmode. This is illustrated in the test files &lt;code&gt;test1_restart2.sh&lt;/code&gt;, &lt;code&gt;test2_restart2.sh&lt;/code&gt;, &lt;code&gt;test5_restart2.sh&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;In ETPmode, you can either provide BRAKER with the results of the GeneMarkETP step manually, with &lt;code&gt;--geneMarkGtf ${BRAKER_OLD}/GeneMark-ETP/proteins.fa/genemark.gtf&lt;/code&gt;, &lt;code&gt;--traingenes ${BRAKER_OLD}/GeneMark-ETP/training.gtf&lt;/code&gt;, and &lt;code&gt;--hints ${BRAKER_OLD}/hintsfile.gff&lt;/code&gt; (see &lt;code&gt;test3_restart1.sh&lt;/code&gt; for an example), or you can specify the previous GeneMark-ETP results with the option &lt;code&gt;--gmetp_results_dir ${BRAKER_OLD}/GeneMark-ETP/&lt;/code&gt; so that BRAKER can search for the files automatically (see &lt;code&gt;test3_restart2.sh&lt;/code&gt; for an example).&lt;/p&gt; 
&lt;h2&gt;Option 3: starting BRAKER after AUGUSTUS training&lt;/h2&gt; 
&lt;p&gt;The trained species parameters for AGUSTUS can be passed with &lt;code&gt;--skipAllTraining&lt;/code&gt; and &lt;code&gt;--species $speciesName&lt;/code&gt; options. This is illustrated in &lt;code&gt;test*_restart3.sh&lt;/code&gt; files. Note that in ETPmode you have to specify the GeneMark files as described in Option 2!&lt;/p&gt; 
&lt;h1&gt;Bug reporting&lt;/h1&gt; 
&lt;p&gt;Before reporting bugs, please check that you are using the most recent versions of GeneMark-ES/ET/EP/ETP, AUGUSTUS and BRAKER. Also, check the list of &lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#common-problems&quot;&gt;Common problems&lt;/a&gt;, and the Issue list on GitHub before reporting bugs. We do monitor open issues on GitHub. Sometimes, we are unable to help you, immediately, but we try hard to solve your problems.&lt;/p&gt; 
&lt;h2&gt;Reporting bugs on GitHub&lt;/h2&gt; 
&lt;p&gt;If you found a bug, please open an issue at &lt;a href=&quot;https://github.com/Gaius-Augustus/BRAKER/issues&quot;&gt;https://github.com/Gaius-Augustus/BRAKER/issues&lt;/a&gt; (or contact &lt;a href=&quot;mailto:katharina.hoff@uni-greifswald.de&quot;&gt;katharina.hoff@uni-greifswald.de&lt;/a&gt; or &lt;a href=&quot;mailto:bruna.tomas@gatech.edu&quot;&gt;bruna.tomas@gatech.edu&lt;/a&gt;).&lt;/p&gt; 
&lt;p&gt;Information worth mentioning in your bug report:&lt;/p&gt; 
&lt;p&gt;Check in &lt;code&gt;braker/yourSpecies/braker.log&lt;/code&gt; at which step &lt;code&gt;braker.pl&lt;/code&gt; crashed.&lt;/p&gt; 
&lt;p&gt;There are a number of other files that might be of interest, depending on where in the pipeline the problem occurred. Some of the following files will not be present if they did not contain any errors.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;braker/yourSpecies/errors/bam2hints.*.stderr&lt;/code&gt; - will give details on a bam2hints crash (step for converting bam file to intron gff file)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;braker/yourSpecies/hintsfile.gff&lt;/code&gt; - is this file empty? If yes, something went wrong during hints generation - does this file contain hints from source “b2h” and of type “intron”? If not: GeneMark-ET will not be able to execute properly. Conversely, GeneMark-EP+ will not be able to execute correctly if hints from the source &quot;ProtHint&quot; are missing.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;braker/yourSpecies/spaln/*err&lt;/code&gt; - errors reported by spaln&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;braker/yourSpecies/errors/GeneMark-{ET,EP,ETP}.stderr&lt;/code&gt; - errors reported by GeneMark-ET/EP+/ETP&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;braker/yourSpecies/errors/GeneMark-{ET,EP,ETP).stdout&lt;/code&gt; - may give clues about the point at which errors in GeneMark-ET/EP+/ETP occured&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;braker/yourSpecies/GeneMark-{ET,EP,ETP}/genemark.gtf&lt;/code&gt; - is this file empty? If yes, something went wrong during executing GeneMark-ET/EP+/ETP&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;braker/yourSpecies/GeneMark-{ET,EP}/genemark.f.good.gtf&lt;/code&gt; - is this file empty? If yes, something went wrong during filtering GeneMark-ET/EP+ genes for training AUGUSTUS&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;braker/yourSpecies/genbank.good.gb&lt;/code&gt; - try a “grep -c LOCUS genbank.good.gb” to determine the number of training genes for training AUGUSTUS, should not be low&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;braker/yourSpecies/errors/firstetraining.stderr&lt;/code&gt; - contains errors from first iteration of training AUGUSTUS&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;braker/yourSpecies/errors/secondetraining.stderr&lt;/code&gt; - contains errors from second iteration of training AUGUSTUS&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;braker/yourSpecies/errors/optimize_augustus.stderr&lt;/code&gt; - contains errors optimize_augustus.pl (additional training set for AUGUSTUS)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;braker/yourSpecies/errors/augustus*.stderr&lt;/code&gt; - contain AUGUSTUS execution errors&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;braker/yourSpecies/startAlign.stderr&lt;/code&gt; - if you provided a protein fasta file, something went wrong during protein alignment&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;braker/yourSpecies/startAlign.stdout&lt;/code&gt; - may give clues on at which point protein alignment went wrong&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Common problems&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;em&gt;BRAKER complains that the RNA-Seq file does not correspond to the provided genome file, but I am sure the files correspond to each other!&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Please check the headers of the genome FASTA file. If the headers are long and contain whitespaces, some RNA-Seq alignment tools will truncate sequence names in the BAM file. This leads to an error with BRAKER. Solution: shorten/simplify FASTA headers in the genome file before running the RNA-Seq alignment and BRAKER.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;em&gt;GeneMark fails!&lt;/em&gt;&lt;/p&gt; &lt;p&gt;(a) GeneMark by default only uses contigs longer than 50k for training. If you have a highly fragmented assembly, this might lead to &quot;no data&quot; for training. You can override the default minimal length by setting the BRAKER argument &lt;code&gt;--min_contig=10000&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;(b) see &quot;[something] failed to execute&quot; below.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;em&gt;[something] failed to execute!&lt;/em&gt;&lt;/p&gt; &lt;p&gt;When providing paths to software to BRAKER, please use absolute, non-abbreviated paths. For example, BRAKER might have problems with &lt;code&gt;--SAMTOOLS_PATH=./samtools/&lt;/code&gt; or &lt;code&gt;--SAMTOOLS_PATH=~/samtools/&lt;/code&gt;. Please use &lt;code&gt;SAMTOOLS_PATH=/full/absolute/path/to/samtools/&lt;/code&gt;, instead. This applies to all path specifications as command line options to &lt;code&gt;braker.pl&lt;/code&gt;. Relative paths and absolute paths will not pose problems if you export a bash variable, instead, or if you append the location of tools to your $PATH variable.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;em&gt;GeneMark-ETP in BRAKER dies with &#39;/scratch/11232323&#39;: No such file or directory.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;This appears to be related to sorting large files, and it&#39;s a system configuration depending problem. Solve it with &lt;code&gt;export TMPDIR=/tmp/&lt;/code&gt; before calling BRAKER via Singularity.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;em&gt;BRAKER cannot find the Augustus script XYZ...&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Update Augustus from github with &lt;code&gt;git clone https://github.com/Gaius-Augustus/Augustus.git&lt;/code&gt;. Do not use Augustus from other sources. BRAKER is highly dependent on an up-to-date Augustus. Augustus releases happen rather rarely, updates to the Augustus scripts folder occur rather frequently.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;em&gt;Does BRAKER depend on Python3?&lt;/em&gt;&lt;/p&gt; &lt;p&gt;It does. The python scripts employed by BRAKER are not compatible with Python2.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;em&gt;Why does BRAKER predict more genes than I expected?&lt;/em&gt;&lt;/p&gt; &lt;p&gt;If transposable elements (or similar) have not been masked appropriately, AUGUSTUS tends to predict those elements as protein coding genes. This can lead to a huge number genes. You can check whether this is the case for your project by BLASTing (or DIAMONDing) the predicted protein sequences against themselves (all vs. all) and counting how many of the proteins have a high number of high quality matches. You can use the output of this analysis to divide your gene set into two groups: the protein coding genes that you want to find and the repetitive elements that were additionally predicted.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;em&gt;I am running BRAKER in Anaconda and something fails...&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Update AUGUSTUS and BRAKER from github with &lt;code&gt;git clone https://github.com/Gaius-Augustus/Augustus.git&lt;/code&gt; and &lt;code&gt;git clone https://github.com/Gaius-Augustus/BRAKER.git&lt;/code&gt;. The Anaconda installation is great, but it relies on releases of AUGUSTUS and BRAKER - which are often lagging behind. Please use the current GitHub code, instead.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;em&gt;Why and where is the GenomeThreader support gone?&lt;/em&gt;&lt;/p&gt; &lt;p&gt;BRAKER is a joint project between teams from University of Greifswald and Georgia Tech. While the group of Mark Bordovsky from Georgia Tech contributes GeneMark expertise, the group of Mario Stanke from University of Greifswald contributes AUGUSTUS expertise. Using GenomeThreader to build training genes for AUGUSTUS in BRAKER circumvents execution of GeneMark. Thus, the GenomeThreader mode is strictly speaking not part of the BRAKER project. The previous functionality of BRAKER with GenomeThreader has been moved to GALBA at &lt;a href=&quot;https://github.com/Gaius-Augustus/GALBA&quot;&gt;https://github.com/Gaius-Augustus/GALBA&lt;/a&gt;. Note that GALBA has also undergone extension for using Miniprot instead of GenomeThreader.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;em&gt;My BRAKER gene set has too many BUSCO duplicates!&lt;/em&gt;&lt;/p&gt; &lt;p&gt;AUGUSTUS within BRAKER can predict alternative splicing isoforms. Also the merge of the AUGUSTUS and GeneMark gene set by TSEBRA within BRAKER may result in additional isoforms for a single gene. The BUSCO duplicates usually come from alternative splicing isoforms, i.e. they are expected.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;em&gt;Augustus and/or etraining within BRAKER complain that the file &lt;code&gt;aug_cmdln_parameters.json&lt;/code&gt; is missing. Even though I am using the latest Singularity container!&lt;/em&gt;&lt;/p&gt; &lt;p&gt;BRAKER copies the AUGUSTUS_CONFIG_PATH folder to a writable location. In older versions of Augustus, that file was indeed not existing. If the local writable copy of a folder already exists, BRAKER will not re-copy it. Simply delete the old folder. (It is often &lt;code&gt;~/.augustus&lt;/code&gt;, so you can simply do &lt;code&gt;rm -rf ~/.augustus&lt;/code&gt;; the folder might be residing in $PWD if your home directory was not writable).&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;em&gt;I sit behind a firewall, compleasm cannot download the BUSCO files, what can I do?&lt;/em&gt; See Issue &lt;a href=&quot;https://github.com/Gaius-Augustus/BRAKER/issues/785#issuecomment-2079787188&quot;&gt;https://github.com/Gaius-Augustus/BRAKER/issues/785#issuecomment-2079787188&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Citing BRAKER and software called by BRAKER&lt;/h1&gt; 
&lt;p&gt;Since BRAKER is a pipeline that calls several Bioinformatics tools, publication of results obtained by BRAKER requires that not only BRAKER is cited, but also the tools that are called by BRAKER. BRAKER will output a file &lt;code&gt;what-to-cite.txt&lt;/code&gt; in the BRAKER working directory, informing you about which exact sources apply to your run.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Always cite:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;Stanke, M., Diekhans, M., Baertsch, R. and Haussler, D. (2008). Using native and syntenically mapped cDNA alignments to improve de novo gene finding. Bioinformatics, doi: 10.1093/bioinformatics/btn013.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Stanke. M., Schöffmann, O., Morgenstern, B. and Waack, S. (2006). Gene prediction in eukaryotes with a generalized hidden Markov model that uses hints from external sources. BMC Bioinformatics 7, 62.&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;If you provided any kind of evidence for BRAKER, cite:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Gabriel, L., Bruna, T., Hoff, K. J., Borodovsky, M., Stanke, M. (2021) TSEBRA: transcript selector for BRAKER. BMC Bioinformatics 22, 1-12.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;If you provided both short read RNA-Seq evidence and a large database of proteins, cite:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;Gabriel, L., Bruna, T., Hoff, K. J., Ebel, M., Lomsadze, A., Borodovsky, M., Stanke, M. (2024). BRAKER3: Fully Automated Genome Annotation Using RNA-Seq and Protein Evidence with GeneMark-ETP, AUGUSTUS and TSEBRA. Genome Research, doi:10.1101/gr.278090.123&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Brůna, T., Lomsadze, A., &amp;amp; Borodovsky, M. (2024). GeneMark-ETP significantly improves the accuracy of automatic annotation of large eukaryotic genomes. Genome Research, doi:10.1101/gr.278373.123.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Kovaka, S., Zimin, A. V., Pertea, G. M., Razaghi, R., Salzberg, S. L., &amp;amp; Pertea, M. (2019). Transcriptome assembly from long-read RNA-seq alignments with StringTie2. Genome biology, 20(1):1-13.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Pertea, G., &amp;amp; Pertea, M. (2020). GFF utilities: GffRead and GffCompare. F1000Research, 9.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Quinlan, A. R. (2014). BEDTools: the Swiss‐army tool for genome feature analysis. Current protocols in bioinformatics, 47(1):11-12.&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;If the only source of evidence for BRAKER was a large database of protein sequences, cite:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Bruna, T., Hoff, K.J., Lomsadze, A., Stanke, M., &amp;amp; Borodovsky, M. (2021). BRAKER2: Automatic Eukaryotic Genome Annotation with GeneMark-EP+ and AUGUSTUS Supported by a Protein Database. NAR Genomics and Bioinformatics 3(1):lqaa108, doi: 10.1093/nargab/lqaa108.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;If the only source of evidence for BRAKER was RNA-Seq data, cite:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;Hoff, K.J., Lange, S., Lomsadze, A., Borodovsky, M. and Stanke, M. (2016). BRAKER1: unsupervised RNA-Seq-based genome annotation with GeneMark-ET and AUGUSTUS. Bioinformatics, 32(5):767-769.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Lomsadze, A., Paul D.B., and Mark B. (2014) Integration of Mapped Rna-Seq Reads into Automatic Training of Eukaryotic Gene Finding Algorithm. Nucleic Acids Research 42(15): e119--e119&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;If you called BRAKER3 with an IsoSeq BAM file, or if you envoked the &lt;code&gt;--busco_lineage&lt;/code&gt; option, cite:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Bruna, T., Gabriel, L., Hoff, K. J. (2024). Navigating Eukaryotic Genome Annotation Pipelines: A Route Map to BRAKER, Galba, and TSEBRA. arXiv, doi: 10.48550/arXiv.2403.19416 .&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;If you called BRAKER with the &lt;code&gt;--busco_lineage&lt;/code&gt; option, in addition, cite:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;Simão, F. A., Waterhouse, R. M., Ioannidis, P., Kriventseva, E. V., &amp;amp; Zdobnov, E. M. (2015). BUSCO: assessing genome assembly and annotation completeness with single-copy orthologs. Bioinformatics, 31(19), 3210-3212.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Li, H. (2023). Protein-to-genome alignment with miniprot. Bioinformatics, 39(1), btad014.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Huang, N., &amp;amp; Li, H. (2023). compleasm: a faster and more accurate reimplementation of BUSCO. Bioinformatics, 39(10), btad595.&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;If any kind of AUGUSTUS training was performed by BRAKER, check carefully whether you configured BRAKER to use NCBI BLAST or DIAMOND. One of them was used to filter out redundant training gene structures.&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;If you used NCBI BLAST, please cite:&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt; &lt;p&gt;Altschul, A.F., Gish, W., Miller, W., Myers, E.W. and Lipman, D.J. (1990). A basic local alignment search tool. J Mol Biol 215:403--410.&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;Camacho, C., Coulouris, G., Avagyan, V., Ma, N., Papadopoulos, J., Bealer, K., and Madden, T.L. (2009). Blast+: architecture and applications. BMC bioinformatics, 10(1):421.&lt;/p&gt; &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;If you used DIAMOND, please cite:&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Buchfink, B., Xie, C., Huson, D.H. (2015). Fast and sensitive protein alignment using DIAMOND. Nature Methods 12:59-60.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;If BRAKER was executed with a genome file and no extrinsic evidence, cite, then GeneMark-ES was used, cite:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;Lomsadze, A., Ter-Hovhannisyan, V., Chernoff, Y.O. and Borodovsky, M. (2005). Gene identification in novel eukaryotic genomes by self-training algorithm. Nucleic Acids Research, 33(20):6494--6506.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Ter-Hovhannisyan, V., Lomsadze, A., Chernoff, Y.O. and Borodovsky, M. (2008). Gene prediction in novel fungal genomes using an ab initio algorithm with unsupervised training. Genome research, pages gr--081612, 2008.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Hoff, K.J., Lomsadze, A., Borodovsky, M. and Stanke, M. (2019). Whole-Genome Annotation with BRAKER. Methods Mol Biol. 1962:65-95, doi: 10.1007/978-1-4939-9173-0_5.&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;If BRAKER was run with proteins as source of evidence, please cite all tools that are used by the ProtHint pipeline to generate hints:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;Bruna, T., Lomsadze, A., &amp;amp; Borodovsky, M. (2020). GeneMark-EP+: eukaryotic gene prediction with self-training in the space of genes and proteins. NAR Genomics and Bioinformatics, 2(2), lqaa026.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Buchfink, B., Xie, C., Huson, D.H. (2015). Fast and sensitive protein alignment using DIAMOND. Nature Methods 12:59-60.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Lomsadze, A., Ter-Hovhannisyan, V., Chernoff, Y.O. and Borodovsky, M. (2005). Gene identification in novel eukaryotic genomes by self-training algorithm. Nucleic Acids Research, 33(20):6494--6506.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Iwata, H., and Gotoh, O. (2012). Benchmarking spliced alignment programs including Spaln2, an extended version of Spaln that incorporates additional species-specific features. Nucleic acids research, 40(20), e161-e161.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Gotoh, O., Morita, M., Nelson, D. R. (2014). Assessment and refinement of eukaryotic gene structure prediction with gene-structure-aware multiple protein sequence alignment. BMC bioinformatics, 15(1), 189.&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;If BRAKER was executed with RNA-Seq alignments in bam-format, then SAMtools was used, cite:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;Li, H., Handsaker, B., Wysoker, A., Fennell, T., Ruan, J., Homer, N., Marth, G., Abecasis, G., Durbin, R.; 1000 Genome Project Data Processing Subgroup (2009). The Sequence Alignment/Map format and SAMtools. Bioinformatics, 25(16):2078-9.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Barnett, D.W., Garrison, E.K., Quinlan, A.R., Strömberg, M.P. and Marth G.T. (2011). BamTools: a C++ API and toolkit for analyzing and managing BAM files. Bioinformatics, 27(12):1691-2&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;If BRAKER downloaded RNA-Seq libraries from SRA using their IDs, cite SRA, SRA toolkit, and HISAT2:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;Leinonen, R., Sugawara, H., Shumway, M., &amp;amp; International Nucleotide Sequence Database Collaboration. (2010). The sequence read archive. Nucleic acids research, 39(suppl_1), D19-D21.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;SRA Toolkit Development Team (2020). SRA Toolkit. &lt;a href=&quot;https://trace.ncbi.nlm.nih.gov/Traces/sra/sra.cgi?view=software&quot;&gt;https://trace.ncbi.nlm.nih.gov/Traces/sra/sra.cgi?view=software&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Kim, D., Paggi, J. M., Park, C., Bennett, C., &amp;amp; Salzberg, S. L. (2019). Graph-based genome alignment and genotyping with HISAT2 and HISAT-genotype. Nature biotechnology, 37(8):907-915.&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;If BRAKER was executed using RNA-Seq data in FASTQ format, cite HISAT2:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Kim, D., Paggi, J. M., Park, C., Bennett, C., &amp;amp; Salzberg, S. L. (2019). Graph-based genome alignment and genotyping with HISAT2 and HISAT-genotype. Nature biotechnology, 37(8):907-915.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;If BRAKER called MakeHub for creating a track data hub for visualization of BRAKER results with the UCSC Genome Browser, cite:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Hoff, K. J. (2019). MakeHub: fully automated generation of UCSC genome browser assembly hubs. Genomics, Proteomics and Bioinformatics, 17(5), 546-549.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;If BRAKER called GUSHR for generating UTRs, cite:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;Keilwagen, J., Hartung, F., Grau, J. (2019) GeMoMa: Homology-based gene prediction utilizing intron position conservation and RNA-seq data. Methods Mol Biol. 1962:161-177, doi: 10.1007/978-1-4939-9173-0_9.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Keilwagen, J., Wenk, M., Erickson, J.L., Schattat, M.H., Grau, J., Hartung F. (2016) Using intron position conservation for homology-based gene prediction. Nucleic Acids Research, 44(9):e89.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Keilwagen, J., Hartung, F., Paulini, M., Twardziok, S.O., Grau, J. (2018) Combining RNA-seq data and homology-based gene prediction for plants, animals and fungi. BMC Bioinformatics, 19(1):189.&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;License&lt;/h1&gt; 
&lt;p&gt;All source code, i.e. &lt;code&gt;scripts/*.pl&lt;/code&gt; or &lt;code&gt;scripts/*.py&lt;/code&gt; are under the Artistic License (see &lt;a href=&quot;http://www.opensource.org/licenses/artistic-license.php&quot;&gt;http://www.opensource.org/licenses/artistic-license.php&lt;/a&gt;).&lt;/p&gt; 
&lt;h1&gt;Footnotes&lt;/h1&gt; 
&lt;p&gt;&lt;b id=&quot;g1&quot;&gt;[F1]&lt;/b&gt; EX = ES/ET/EP/ETP, all available for download under the name &lt;em&gt;GeneMark-ES/ET/EP&lt;/em&gt; &lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#g1&quot;&gt;↩&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;b id=&quot;g2&quot;&gt;[F2]&lt;/b&gt; Please use the latest version from the master branch of AUGUSTUS distributed by the original developers, it is available from github at &lt;a href=&quot;https://github.com/Gaius-Augustus/Augustus&quot;&gt;https://github.com/Gaius-Augustus/Augustus&lt;/a&gt;. Problems have been reported from users that tried to run BRAKER with AUGUSTUS releases maintained by third parties, i.e. Bioconda. &lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#g2&quot;&gt;↩&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;b id=&quot;g4&quot;&gt;[F4]&lt;/b&gt; install with &lt;code&gt;sudo apt-get install cpanminus&lt;/code&gt; &lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#g4&quot;&gt;↩&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;b id=&quot;g6&quot;&gt;[F6]&lt;/b&gt; The binary may e.g. reside in bamtools/build/src/toolkit &lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#g6&quot;&gt;↩&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;References&lt;/h1&gt; 
&lt;p&gt;&lt;b id=&quot;f0&quot;&gt;[R0]&lt;/b&gt; Bruna, Tomas, Hoff, Katharina J., Lomsadze, Alexandre, Stanke, Mario, and Borodovsky, Mark. 2021. “BRAKER2: automatic eukaryotic genome annotation with GeneMark-EP+ and AUGUSTUS supported by a protein database.&quot; &lt;em&gt;NAR Genomics and Bioinformatics&lt;/em&gt; 3(1):lqaa108.&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#a0&quot;&gt;↩&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;b id=&quot;f1&quot;&gt;[R1]&lt;/b&gt; Hoff, Katharina J, Simone Lange, Alexandre Lomsadze, Mark Borodovsky, and Mario Stanke. 2015. “BRAKER1: Unsupervised Rna-Seq-Based Genome Annotation with Genemark-et and Augustus.” &lt;em&gt;Bioinformatics&lt;/em&gt; 32 (5). Oxford University Press: 767--69.&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#a1&quot;&gt;↩&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;b id=&quot;f2&quot;&gt;[R2]&lt;/b&gt; Lomsadze, Alexandre, Paul D Burns, and Mark Borodovsky. 2014. “Integration of Mapped Rna-Seq Reads into Automatic Training of Eukaryotic Gene Finding Algorithm.” &lt;em&gt;Nucleic Acids Research&lt;/em&gt; 42 (15). Oxford University Press: e119--e119.&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#a2&quot;&gt;↩&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;b id=&quot;f3&quot;&gt;[R3]&lt;/b&gt; Stanke, Mario, Mark Diekhans, Robert Baertsch, and David Haussler. 2008. “Using Native and Syntenically Mapped cDNA Alignments to Improve de Novo Gene Finding.” &lt;em&gt;Bioinformatics&lt;/em&gt; 24 (5). Oxford University Press: 637--44.&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#a3&quot;&gt;↩&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;b id=&quot;f4&quot;&gt;[R4]&lt;/b&gt; Stanke, Mario, Oliver Schöffmann, Burkhard Morgenstern, and Stephan Waack. 2006. “Gene Prediction in Eukaryotes with a Generalized Hidden Markov Model That Uses Hints from External Sources.” &lt;em&gt;BMC Bioinformatics&lt;/em&gt; 7 (1). BioMed Central: 62.&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#a4&quot;&gt;↩&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;b id=&quot;f5&quot;&gt;[R5]&lt;/b&gt; Barnett, Derek W, Erik K Garrison, Aaron R Quinlan, Michael P Strömberg, and Gabor T Marth. 2011. “BamTools: A C++ Api and Toolkit for Analyzing and Managing Bam Files.” &lt;em&gt;Bioinformatics&lt;/em&gt; 27 (12). Oxford University Press: 1691--2.&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#a5&quot;&gt;↩&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;b id=&quot;f6&quot;&gt;[R6]&lt;/b&gt; Li, Heng, Handsaker, Bob, Alec Wysoker, Tim Fennell, Jue Ruan, Nils Homer, Gabor Marth, Goncalo Abecasis, and Richard Durbin. 2009. “The Sequence Alignment/Map Format and Samtools.” &lt;em&gt;Bioinformatics&lt;/em&gt; 25 (16). Oxford University Press: 2078--9.&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#a6&quot;&gt;↩&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;b id=&quot;f7&quot;&gt;[R7]&lt;/b&gt; Gremme, G. 2013. “Computational Gene Structure Prediction.” PhD thesis, Universität Hamburg.&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#a7&quot;&gt;↩&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;b id=&quot;f8&quot;&gt;[R8]&lt;/b&gt; Gotoh, Osamu. 2008a. “A Space-Efficient and Accurate Method for Mapping and Aligning cDNA Sequences onto Genomic Sequence.” &lt;em&gt;Nucleic Acids Research&lt;/em&gt; 36 (8). Oxford University Press: 2630--8.&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#a8&quot;&gt;↩&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;b id=&quot;f9&quot;&gt;[R9]&lt;/b&gt; Iwata, Hiroaki, and Osamu Gotoh. 2012. “Benchmarking Spliced Alignment Programs Including Spaln2, an Extended Version of Spaln That Incorporates Additional Species-Specific Features.” &lt;em&gt;Nucleic Acids Research&lt;/em&gt; 40 (20). Oxford University Press: e161--e161.&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#a9&quot;&gt;↩&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;b id=&quot;f10&quot;&gt;[R10]&lt;/b&gt; Osamu Gotoh. 2008b. “Direct Mapping and Alignment of Protein Sequences onto Genomic Sequence.” &lt;em&gt;Bioinformatics&lt;/em&gt; 24 (21). Oxford University Press: 2438--44.&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#a10&quot;&gt;↩&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;b id=&quot;f11&quot;&gt;[R11]&lt;/b&gt; Slater, Guy St C, and Ewan Birney. 2005. “Automated Generation of Heuristics for Biological Sequence Comparison.” &lt;em&gt;BMC Bioinformatics&lt;/em&gt; 6(1). BioMed Central: 31.&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#a11&quot;&gt;↩&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;b id=&quot;f12&quot;&gt;[R12]&lt;/b&gt; Altschul, S.F., W. Gish, W. Miller, E.W. Myers, and D.J. Lipman. 1990. “Basic Local Alignment Search Tool.” &lt;em&gt;Journal of Molecular Biology&lt;/em&gt; 215:403--10.&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#a12&quot;&gt;↩&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;b id=&quot;f13&quot;&gt;[R13]&lt;/b&gt; Camacho, Christiam, et al. 2009. “BLAST+: architecture and applications.“ &lt;em&gt;BMC Bioinformatics&lt;/em&gt; 1(1): 421.&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#a13&quot;&gt;↩&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;b id=&quot;f14&quot;&gt;[R14]&lt;/b&gt; Lomsadze, A., V. Ter-Hovhannisyan, Y.O. Chernoff, and M. Borodovsky. 2005. “Gene identification in novel eukaryotic genomes by self-training algorithm.” &lt;em&gt;Nucleic Acids Research&lt;/em&gt; 33 (20): 6494--6506. doi:&lt;a href=&quot;https://doi.org/10.1093/nar/gki937&quot;&gt;10.1093/nar/gki937&lt;/a&gt;.&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#a14&quot;&gt;↩&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;b id=&quot;f15&quot;&gt;[R15]&lt;/b&gt; Ter-Hovhannisyan, Vardges, Alexandre Lomsadze, Yury O Chernoff, and Mark Borodovsky. 2008. “Gene Prediction in Novel Fungal Genomes Using an Ab Initio Algorithm with Unsupervised Training.” &lt;em&gt;Genome Research&lt;/em&gt;. Cold Spring Harbor Lab, gr--081612.&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#a15&quot;&gt;↩&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;b id=&quot;f16&quot;&gt;[R16]&lt;/b&gt; Hoff, K.J. 2019. MakeHub: Fully automated generation of UCSC Genome Browser Assembly Hubs. &lt;em&gt;Genomics, Proteomics and Bioinformatics&lt;/em&gt;, in press, preprint on bioarXive, doi: &lt;a href=&quot;https://doi.org/10.1101/550145&quot;&gt;https://doi.org/10.1101/550145&lt;/a&gt;.&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#a16&quot;&gt;↩&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;b id=&quot;f17&quot;&gt;[R17]&lt;/b&gt; Bruna, T., Lomsadze, A., &amp;amp; Borodovsky, M. 2020. GeneMark-EP+: eukaryotic gene prediction with self-training in the space of genes and proteins. NAR Genomics and Bioinformatics, 2(2), lqaa026. doi: &lt;a href=&quot;https://doi.org/10.1093/nargab/lqaa026&quot;&gt;https://doi.org/10.1093/nargab/lqaa026&lt;/a&gt;.&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#a17&quot;&gt;↩&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;b id=&quot;f18&quot;&gt;[R18]&lt;/b&gt; Kriventseva, E. V., Kuznetsov, D., Tegenfeldt, F., Manni, M., Dias, R., Simão, F. A., and Zdobnov, E. M. 2019. OrthoDB v10: sampling the diversity of animal, plant, fungal, protist, bacterial and viral genomes for evolutionary and functional annotations of orthologs. Nucleic Acids Research, 47(D1), D807-D811.&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#a18&quot;&gt;↩&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;b id=&quot;f19&quot;&gt;[R19]&lt;/b&gt; Keilwagen, J., Hartung, F., Grau, J. (2019) GeMoMa: Homology-based gene prediction utilizing intron position conservation and RNA-seq data. Methods Mol Biol. 1962:161-177, doi: 10.1007/978-1-4939-9173-0_9.&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#a19&quot;&gt;↩&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;b id=&quot;f20&quot;&gt;[R20]&lt;/b&gt; Keilwagen, J., Wenk, M., Erickson, J.L., Schattat, M.H., Grau, J., Hartung F. (2016) Using intron position conservation for homology-based gene prediction. Nucleic Acids Research, 44(9):e89.&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#a20&quot;&gt;↩&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;b id=&quot;f21&quot;&gt;[R21]&lt;/b&gt; Keilwagen, J., Hartung, F., Paulini, M., Twardziok, S.O., Grau, J. (2018) Combining RNA-seq data and homology-based gene prediction for plants, animals and fungi. BMC Bioinformatics, 19(1):189.&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#a21&quot;&gt;↩&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;b id=&quot;f22&quot;&gt;[R22]&lt;/b&gt; SRA Toolkit Development Team (2020). SRA Toolkit. &lt;a href=&quot;https://trace.ncbi.nlm.nih.gov/Traces/sra/sra.cgi?view=software.%5B%E2%86%A9%5D(#a22)&quot;&gt;https://trace.ncbi.nlm.nih.gov/Traces/sra/sra.cgi?view=software.[↩](#a22)&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;b id=&quot;f23&quot;&gt;[R23]&lt;/b&gt; Kim, D., Paggi, J. M., Park, C., Bennett, C., &amp;amp; Salzberg, S. L. (2019). Graph-based genome alignment and genotyping with HISAT2 and HISAT-genotype. Nature biotechnology, 37(8):907-915.&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#a23&quot;&gt;↩&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;b id=&quot;f24&quot;&gt;[R24]&lt;/b&gt; Quinlan, A. R. (2014). BEDTools: the Swiss‐army tool for genome feature analysis. Current protocols in bioinformatics, 47(1):11-12.&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#a24&quot;&gt;↩&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;b id=&quot;f25&quot;&gt;[R25]&lt;/b&gt; Kovaka, S., Zimin, A. V., Pertea, G. M., Razaghi, R., Salzberg, S. L., &amp;amp; Pertea, M. (2019). Transcriptome assembly from long-read RNA-seq alignments with StringTie2. Genome biology, 20(1):1-13.&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#a25&quot;&gt;↩&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;b id=&quot;f26&quot;&gt;[R26]&lt;/b&gt; Pertea, G., &amp;amp; Pertea, M. (2020). GFF utilities: GffRead and GffCompare. F1000Research, 9.&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#a26&quot;&gt;↩&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;b id=&quot;f27&quot;&gt;[R27]&lt;/b&gt; Huang, N., &amp;amp; Li, H. (2023). compleasm: a faster and more accurate reimplementation of BUSCO. Bioinformatics, 39(10), btad595.&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#a27&quot;&gt;↩&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;b id=&quot;f28&quot;&gt;[R28]&lt;/b&gt; Bruna, T., Gabriel, L. &amp;amp; Hoff, K. J. (2024). Navigating Eukaryotic Genome Annotation Pipelines: A Route Map to BRAKER, Galba, and TSEBRA. arXiv, &lt;a href=&quot;https://doi.org/10.48550/arXiv.2403.19416&quot;&gt;https://doi.org/10.48550/arXiv.2403.19416&lt;/a&gt; .&lt;a href=&quot;https://raw.githubusercontent.com/Gaius-Augustus/BRAKER/master/#a28&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>DGFiP/Test-Compta-Demat</title>
      <link>https://github.com/DGFiP/Test-Compta-Demat</link>
      <description>&lt;p&gt;Ce logiciel permet de contrôler le respect des normes des fichiers d&#39;écritures comptables (FEC) conformément aux dispositions de l&#39;article A.47 A-1 du livre des procédures fiscales.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Test-Compta-Demat&lt;/h1&gt; 
&lt;p&gt;Copyright 2014 Direction Générale des Finances Publiques - France&lt;/p&gt; 
&lt;p&gt;Ce logiciel permet de contrôler le respect des normes des fichiers d&#39;écritures comptables (FEC) conformément aux dispositions de l&#39;article A.47 A-1 du livre des procédures fiscales.&lt;/p&gt; 
&lt;p&gt;Ce logiciel est régi par la licence CeCILL V2.1 soumise au droit français et respectant les principes de diffusion des logiciels libres. Vous pouvez utiliser, modifier et/ou redistribuer ce programme sous les conditions de la licence CeCILL V2.1 telle que diffusée par le CEA, le CNRS et l&#39;INRIA sur le site &quot;&lt;a href=&quot;http://www.cecill.info&quot;&gt;http://www.cecill.info&lt;/a&gt;&quot;.&lt;/p&gt; 
&lt;h1&gt;Récupération des sources du programme&lt;/h1&gt; 
&lt;p&gt;Pour les visualiser se rendre dans le dossier src/. Les sources utilisées sont disponibles pour adaptation sur des environnements différents. Pour cela, cloner le dépôt git (cf doc de git).&lt;/p&gt; 
&lt;h1&gt;Récupération des distributions Windows&lt;/h1&gt; 
&lt;p&gt;Le programme est disponible sous la forme d&#39;une distribution exécutable livrée dans une archive autoextractive.&lt;/p&gt; 
&lt;h2&gt;Version windows 32 bits&lt;/h2&gt; 
&lt;p&gt;Cliquer sur release pour voir les exécutables windows puis cliquer sur le bouton vert qui correspond à la version windows (x86.exe) pour le télécharger.&lt;/p&gt; 
&lt;h2&gt;Version spécifique pour windows 64 bits :&lt;/h2&gt; 
&lt;p&gt;Cliquer sur release pour voir les exécutables windows puis cliquer sur le bouton vert qui correspond à la version windows en 64 bits (x86_64.exe) pour le télécharger.&lt;/p&gt; 
&lt;h1&gt;Documentations&lt;/h1&gt; 
&lt;p&gt;voir le contenu du dossier doc : pour visualiser un pdf, il faut le télécharger (cliquer sur View Raw) puis ouvrir le fichier téléchargé.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>inverse-inc/packetfence</title>
      <link>https://github.com/inverse-inc/packetfence</link>
      <description>&lt;p&gt;PacketFence is a fully supported, trusted, Free and Open Source network access control (NAC) solution. Boasting an impressive feature set including a captive-portal for registration and remediation, centralized wired and wireless management, powerful BYOD management options, 802.1X support, layer-2 isolation of problematic devices; PacketFence c…&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;PacketFence&lt;/h1&gt; 
&lt;h2&gt;What is PacketFence?&lt;/h2&gt; 
&lt;p&gt;PacketFence is a fully supported, trusted, Free and Open Source network access control (NAC) system. Boasting an impressive feature set including a captive-portal for registration and remediation, centralized wired and wireless management, 802.1X support, layer-2 isolation of problematic devices, integration with IDS solutions and vulnerability scanners; PacketFence can be used to effectively secure networks - from small to very large heterogeneous networks.&lt;/p&gt; 
&lt;p&gt;You want to know who is on your network? You want to give different access to your network based on who is connecting? PacketFence is for you!&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;Follow the instructions provided in the &lt;a href=&quot;https://packetfence.org/support/index.html#/documentation&quot;&gt;Administration Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;More Information&lt;/h2&gt; 
&lt;p&gt;Noteworthy changes since the last release see the &lt;a href=&quot;https://github.com/inverse-inc/packetfence/raw/devel/NEWS.asciidoc&quot;&gt;NEWS file&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Upgrading? See the &lt;a href=&quot;https://github.com/inverse-inc/packetfence/raw/devel/docs/PacketFence_Upgrade_Guide.asciidoc&quot;&gt;Upgrade Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For more details and developer visible changes see the &lt;a href=&quot;https://github.com/inverse-inc/packetfence&quot;&gt;project&#39;s page on Github&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Support&lt;/h2&gt; 
&lt;p&gt;Join the &lt;a href=&quot;https://packetfence.org/support/index.html#/community&quot;&gt;community&lt;/a&gt; or request &lt;a href=&quot;https://packetfence.org/support/index.html#/commercial&quot;&gt;commercial support&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contribute&lt;/h2&gt; 
&lt;p&gt;PacketFence is a collaborative effort in order to create the best Open Source NAC solution. There are multiple ways you can contribute to the project.&lt;/p&gt; 
&lt;h3&gt;&lt;em&gt;You are a network vendor&lt;/em&gt;&lt;/h3&gt; 
&lt;p&gt;Provide Inverse with switches, access points, wireless controllers, etc. so we can support even more equipment.&lt;/p&gt; 
&lt;h3&gt;&lt;em&gt;You are a security software vendor&lt;/em&gt;&lt;/h3&gt; 
&lt;p&gt;Provide Inverse with licenses of your software so we can integrate your IDS, Netflow analyzer, IPS, Web filter, etc. directly into PacketFence and its captive portal technology.&lt;/p&gt; 
&lt;h3&gt;&lt;em&gt;You are a PacketFence user&lt;/em&gt;&lt;/h3&gt; 
&lt;p&gt;You can provide:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Documentation reviews, enhancements and translations&lt;/li&gt; 
 &lt;li&gt;Share your ideas and participate to the discussion in &lt;a href=&quot;https://packetfence.org/support/index.html#/community&quot; title=&quot;Community Mailing Lists&quot;&gt;mailing lists&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Provide Inverse with switches, access points, wireless controllers, etc. so we can support even more equipment&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;em&gt;You are a developer&lt;/em&gt;&lt;/h3&gt; 
&lt;p&gt;You can provide:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Documentation reviews, enhancements and translations&lt;/li&gt; 
 &lt;li&gt;Share your ideas and participate to the discussion in &lt;a href=&quot;https://packetfence.org/support/index.html#/community&quot; title=&quot;Community Mailing Lists&quot;&gt;mailing lists&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Patches for bugs or enhancements&lt;/li&gt; 
 &lt;li&gt;Write tests&lt;/li&gt; 
 &lt;li&gt;Handle tasks in our Roadmap&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;em&gt;You are a security researcher&lt;/em&gt;&lt;/h3&gt; 
&lt;p&gt;Push PacketFence into new areas by leveraging the extensibility built into PacketFence. A lot of the low-level plumbing is done for you so you can focus on demoing your ideas.&lt;/p&gt; 
&lt;p&gt;Get in touch with us on the developer &lt;a href=&quot;https://packetfence.org/support/index.html#/community&quot; title=&quot;Community Mailing Lists&quot;&gt;mailing list&lt;/a&gt; with your ideas!&lt;/p&gt; 
&lt;h2&gt;Source&lt;/h2&gt; 
&lt;p&gt;Feel free to fork our &lt;a href=&quot;https://github.com/inverse-inc/packetfence&quot;&gt;github repository&lt;/a&gt; if you are willing to contribute.&lt;/p&gt; 
&lt;p&gt;Most of the development happens in branches. Once ready for integration into &lt;a href=&quot;https://github.com/inverse-inc/packetfence/tree/devel&quot;&gt;devel&lt;/a&gt;, a pull request is opened and a code review takes place. See the list of &lt;a href=&quot;https://github.com/inverse-inc/packetfence/branches&quot;&gt;all branches in the works&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Translations&lt;/h2&gt; 
&lt;p&gt;PacketFence is available in various languages. The following list describes the official translations alongside their maintainers:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;English - Inverse inc.&lt;/li&gt; 
 &lt;li&gt;Brazilian Portuguese - Diego de Souza Lopes&lt;/li&gt; 
 &lt;li&gt;French - Inverse inc.&lt;/li&gt; 
 &lt;li&gt;Norwegian&lt;/li&gt; 
 &lt;li&gt;Polish - Maciej Uhlig&lt;/li&gt; 
 &lt;li&gt;Spanish (Spain) - Dominique Couot&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If you would like to translate the software in an other language, please consult the &lt;a href=&quot;https://packetfence.org/support/faq/article/how-to-translate-packetfence-in-another-language.html&quot;&gt;FAQ&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Licensed under the GNU General Public License v2.&lt;/p&gt; 
&lt;hr&gt; 
&lt;p&gt;&lt;a href=&quot;https://inverse.ca/&quot;&gt;Inverse inc.&lt;/a&gt; leads the development of the solution. Over the years, numerous people and organizations have contributed to the project and we would like to thank them all !&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>trinityrnaseq/trinityrnaseq</title>
      <link>https://github.com/trinityrnaseq/trinityrnaseq</link>
      <description>&lt;p&gt;Trinity RNA-Seq de novo transcriptome assembly&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;trinityrnaseq&lt;/h1&gt; 
&lt;p&gt;Trinity RNA-Seq de novo transcriptome assembly see the Trinity &lt;a href=&quot;https://github.com/trinityrnaseq/trinityrnaseq/wiki&quot;&gt;wiki&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We encourage you to contribute to Trinity! Please check out the &lt;a href=&quot;https://github.com/trinityrnaseq/trinityrnaseq/wiki/Contributing&quot;&gt;Contributing&lt;/a&gt; for the guidelines.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Ensembl/ensembl-vep</title>
      <link>https://github.com/Ensembl/ensembl-vep</link>
      <description>&lt;p&gt;The Ensembl Variant Effect Predictor predicts the functional effects of genomic variants&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ensembl-vep&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/Ensembl/ensembl-vep/raw/release/113/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/license/Ensembl/ensembl-vep.svg?sanitize=true&quot; alt=&quot;GitHub&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://coveralls.io/github/Ensembl/ensembl-vep?branch=release/113&quot;&gt;&lt;img src=&quot;https://coveralls.io/repos/github/Ensembl/ensembl-vep/badge.svg?branch=main&quot; alt=&quot;Coverage Status&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://hub.docker.com/r/ensemblorg/ensembl-vep&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/actions/workflow/status/Ensembl/ensembl-vep/docker.yml?label=docker%20build&quot; alt=&quot;Docker Build Status&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://hub.docker.com/r/ensemblorg/ensembl-vep&quot;&gt;&lt;img src=&quot;https://img.shields.io/docker/pulls/ensemblorg/ensembl-vep.svg?sanitize=true&quot; alt=&quot;Docker Hub Pulls&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;VEP&lt;/strong&gt; (Variant Effect Predictor) predicts the functional effects of genomic variants.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Haplosaurus&lt;/strong&gt; uses phased genotype data to predict whole-transcript haplotype sequences.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Variant Recoder&lt;/strong&gt; translates between different variant encodings.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h5&gt;Table of contents&lt;/h5&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Ensembl/ensembl-vep/release/113/#install&quot;&gt;Installation and requirements&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Ensembl/ensembl-vep/release/113/#vep&quot;&gt;VEP&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Ensembl/ensembl-vep/release/113/#vepusage&quot;&gt;Usage&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Ensembl/ensembl-vep/release/113/#haplo&quot;&gt;Haplosaurus&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Ensembl/ensembl-vep/release/113/#haplousage&quot;&gt;Usage&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Ensembl/ensembl-vep/release/113/#haplooutput&quot;&gt;Output&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Ensembl/ensembl-vep/release/113/#haploREST&quot;&gt;REST&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Ensembl/ensembl-vep/release/113/#haploflags&quot;&gt;Flags&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Ensembl/ensembl-vep/release/113/#recoder&quot;&gt;Variant Recoder&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Ensembl/ensembl-vep/release/113/#recoderusage&quot;&gt;Usage&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Ensembl/ensembl-vep/release/113/#recoderoutput&quot;&gt;Output&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr&gt; 
&lt;p&gt;&lt;a name=&quot;install&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Installation and requirements&lt;/h3&gt; 
&lt;p&gt;The VEP package requires:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;gcc&lt;/strong&gt;, &lt;strong&gt;g++&lt;/strong&gt; and &lt;strong&gt;make&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Perl&lt;/strong&gt; (&amp;gt;=5.10 recommended, tested on 5.10, 5.14, 5.18, 5.22, 5.26)&lt;/li&gt; 
 &lt;li&gt;Perl libraries &lt;a href=&quot;https://metacpan.org/pod/Archive::Zip&quot;&gt;Archive::Zip&lt;/a&gt; and &lt;a href=&quot;https://metacpan.org/pod/DBI&quot;&gt;DBI&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The remaining dependencies can be installed using the included &lt;a href=&quot;http://www.ensembl.org/info/docs/tools/vep/script/vep_download.html#installer&quot;&gt;INSTALL.pl&lt;/a&gt; script. Basic instructions:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;git clone https://github.com/Ensembl/ensembl-vep.git
cd ensembl-vep
perl INSTALL.pl
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The installer may also be used to check for updates to this and co-dependent packages, simply re-run INSTALL.pl.&lt;/p&gt; 
&lt;p&gt;See &lt;a href=&quot;http://www.ensembl.org/info/docs/tools/vep/script/vep_download.html&quot;&gt;documentation&lt;/a&gt; for full installation instructions.&lt;/p&gt; 
&lt;h4&gt;Additional CPAN modules&lt;/h4&gt; 
&lt;p&gt;The following modules are optional but most users will benefit from installing them. We recommend using &lt;a href=&quot;http://search.cpan.org/~miyagawa/Menlo-1.9003/script/cpanm-menlo&quot;&gt;cpanminus&lt;/a&gt; to install.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;http://search.cpan.org/~michielb/DBD-mysql/lib/DBD/mysql.pm&quot;&gt;DBD::mysql&lt;/a&gt; - required for database access (&lt;code&gt;--database&lt;/code&gt; or &lt;code&gt;--cache&lt;/code&gt; without &lt;code&gt;--offline&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;http://search.cpan.org/~benbooth/Set-IntervalTree/lib/Set/IntervalTree.pm&quot;&gt;Set::IntervalTree&lt;/a&gt; - required for Haplosaurus, also confers speed updates to VEP&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;http://search.cpan.org/dist/JSON/&quot;&gt;JSON&lt;/a&gt; - required for writing JSON output&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;http://search.cpan.org/~nwclark/PerlIO-gzip-0.19/gzip.pm&quot;&gt;PerlIO::gzip&lt;/a&gt; - faster compressed file parsing&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;http://search.cpan.org/~lds/Bio-BigFile-1.07/lib/Bio/DB/BigFile.pm&quot;&gt;Bio::DB::BigFile&lt;/a&gt; - required for reading custom annotation data from BigWig files&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Docker&lt;/h4&gt; 
&lt;p&gt;A docker image for VEP is available from &lt;a href=&quot;https://hub.docker.com/r/ensemblorg/ensembl-vep&quot;&gt;DockerHub&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;See &lt;a href=&quot;http://www.ensembl.org/info/docs/tools/vep/script/vep_download.html#docker&quot;&gt;documentation&lt;/a&gt; for the Docker installation instructions.&lt;/p&gt; 
&lt;hr&gt; 
&lt;p&gt;&lt;a name=&quot;vep&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;VEP&lt;/h2&gt; 
&lt;p&gt;&lt;a name=&quot;vepusage&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Usage&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;./vep -i input.vcf -o out.txt -offline
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;a href=&quot;http://www.ensembl.org/info/docs/tools/vep/script/vep_options.html#basic&quot;&gt;documentation&lt;/a&gt; for full command line instructions.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Please report any bugs or issues by &lt;a href=&quot;http://www.ensembl.org/info/about/contact/index.html&quot;&gt;contacting Ensembl&lt;/a&gt; or creating a &lt;a href=&quot;https://github.com/Ensembl/ensembl-vep/issues&quot;&gt;GitHub issue&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr&gt; 
&lt;p&gt;&lt;a name=&quot;haplo&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Haplosaurus&lt;/h2&gt; 
&lt;p&gt;&lt;code&gt;haplo&lt;/code&gt; is a local tool implementation of the same functionality that powers the &lt;a href=&quot;http://www.ensembl.org/Homo_sapiens/Transcript/Haplotypes?t=ENST00000304748&quot;&gt;Ensembl transcript haplotypes view&lt;/a&gt;. It takes phased genotypes from a VCF and constructs a pair of haplotype sequences for each overlapped transcript; these sequences are also translated into predicted protein haplotype sequences. Each variant haplotype sequence is aligned and compared to the reference, and an HGVS-like name is constructed representing its differences to the reference.&lt;/p&gt; 
&lt;p&gt;This approach offers an advantage over VEP&#39;s analysis, which treats each input variant independently. By considering the combined change contributed by all the variant alleles across a transcript, the compound effects the variants may have are correctly accounted for.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;haplo&lt;/code&gt; shares much of the same command line functionality with &lt;code&gt;vep&lt;/code&gt;, and can use VEP caches, Ensembl databases, GFF and GTF files as sources of transcript data; all &lt;code&gt;vep&lt;/code&gt; command line flags relating to this functionality work the same with &lt;code&gt;haplo&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a name=&quot;haplousage&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Usage&lt;/h3&gt; 
&lt;p&gt;Input data must be a &lt;a href=&quot;http://samtools.github.io/hts-specs/VCFv4.3.pdf&quot;&gt;VCF&lt;/a&gt; containing phased genotype data for at least one individual and file must be sorted by chromosome and genomic position; no other formats are currently supported.&lt;/p&gt; 
&lt;p&gt;When using a VEP cache as the source of transcript annotation, the first time you run &lt;code&gt;haplo&lt;/code&gt; with a particular cache it will spend some time scanning transcript locations in the cache.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;./haplo -i input.vcf -o out.txt -cache
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a name=&quot;haplooutput&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Output&lt;/h3&gt; 
&lt;p&gt;The default output format is a simple tab-delimited file reporting all observed non-reference haplotypes. It has the following fields:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Transcript stable ID&lt;/li&gt; 
 &lt;li&gt;CDS haplotype name&lt;/li&gt; 
 &lt;li&gt;Comma-separated list of &lt;a href=&quot;https://raw.githubusercontent.com/Ensembl/ensembl-vep/release/113/#haploflags&quot;&gt;flags&lt;/a&gt; for CDS haplotype&lt;/li&gt; 
 &lt;li&gt;Protein haplotype name&lt;/li&gt; 
 &lt;li&gt;Comma-separated list of &lt;a href=&quot;https://raw.githubusercontent.com/Ensembl/ensembl-vep/release/113/#haploflags&quot;&gt;flags&lt;/a&gt; for protein haplotype&lt;/li&gt; 
 &lt;li&gt;Comma-separated list of frequency data for protein haplotype&lt;/li&gt; 
 &lt;li&gt;Comma-separated list of contributing variants&lt;/li&gt; 
 &lt;li&gt;Comma-separated list of sample:count that exhibit this haplotype&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;The altered haplotype sequences can be obtained by switching to JSON output using &lt;code&gt;--json&lt;/code&gt; which will display them by default. Each transcript analysed is summarised as a JSON object written to one line of the output file.&lt;/p&gt; 
&lt;p&gt;The &lt;a href=&quot;https://raw.githubusercontent.com/Ensembl/ensembl-vep/release/113/#haploREST&quot;&gt;JSON output&lt;/a&gt; structure matches the format of the &lt;a href=&quot;https://rest.ensembl.org/documentation/info/transcript_haplotypes_get&quot;&gt;transcript haplotype REST endpoint&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You may exclude fields in the JSON from being exported with &lt;code&gt;--dont_export field1,field2&lt;/code&gt;. This may be used, for example, to exclude the full haplotype sequence and aligned sequences from the output with &lt;code&gt;--dont_export seq,aligned_sequences&lt;/code&gt;.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Note JSON output does not currently include side-loaded frequency data.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;a name=&quot;haploREST&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;REST service&lt;/h3&gt; 
&lt;p&gt;The &lt;a href=&quot;https://rest.ensembl.org/documentation/info/transcript_haplotypes_get&quot;&gt;transcript haplotype REST endpoint&lt;/a&gt;. returns arrays of protein_haplotypes and cds_haplotypes for a given transcript. The default haplotype record includes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;population_counts&lt;/strong&gt;: the number of times the haplotype is seen in each population&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;population_frequencies&lt;/strong&gt;: the frequency of the haplotype in each population&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;contributing_variants&lt;/strong&gt;: variants contributing to the haplotype&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;diffs&lt;/strong&gt;: differences between the reference and this haplotype&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;hex&lt;/strong&gt;: the md5 hex of this haplotype sequence&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;other_hexes&lt;/strong&gt;: the md5 hex of other related haplotype sequences ( CDSHaplotypes that translate to this ProteinHaplotype or ProteinHaplotype representing the translation of this CDSHaplotype)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;has_indel&lt;/strong&gt;: does the haplotype contain insertions or deletions&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;type&lt;/strong&gt;: the type of haplotype - cds, protein&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;name&lt;/strong&gt;: a human readable name for the haplotype (sequence id + REF or a change description)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;flags&lt;/strong&gt;: &lt;a href=&quot;https://raw.githubusercontent.com/Ensembl/ensembl-vep/release/113/#haploflags&quot;&gt;flags&lt;/a&gt; for the haplotype&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;frequency&lt;/strong&gt;: haplotype frequency in full sample set&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;count&lt;/strong&gt;: haplotype count in full sample set&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The REST service does not return raw sequences, sample-haplotype assignments and the aligned sequences used to generate differences by default.&lt;/p&gt; 
&lt;p&gt;&lt;a name=&quot;haploflags&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Flags&lt;/h3&gt; 
&lt;p&gt;Haplotypes may be flagged with one or more of the following:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;indel&lt;/strong&gt;: haplotype contains an insertion or deletion (indel) relative to the reference.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;frameshift:&lt;/strong&gt; haplotype contains at least one indel that disrupts the reading frame of the transcript.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;resolved_frameshift:&lt;/strong&gt; haplotype contains two or more indels whose combined effect restores the reading frame of the transcript.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;stop_changed:&lt;/strong&gt; indicates either a STOP codon is gained (protein truncating variant, PTV) or the existing reference STOP codon is lost.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;deleterious_sift_or_polyphen:&lt;/strong&gt; haplotype contains at least one single amino acid substitution event flagged as deleterious (SIFT) or probably damaging (PolyPhen2).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a name=&quot;bioperl-ext&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;bioperl-ext&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;haplo&lt;/code&gt; can make use of a fast compiled alignment algorithm from the &lt;a href=&quot;https://github.com/bioperl/bioperl-ext&quot;&gt;bioperl-ext&lt;/a&gt; package; this can speed up analysis, particularly in longer transcripts where insertions and/or deletions are introduced. The bioperl-ext package is no longer maintained and requires some tweaking to install. The following instructions install the package in &lt;code&gt;$HOME/perl5&lt;/code&gt;; edit &lt;code&gt;PREFIX=[path]&lt;/code&gt; to change this. You may also need to edit the &lt;code&gt;export&lt;/code&gt; command to point to the path created for the architecture on your machine.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;git clone https://github.com/bioperl/bioperl-ext.git
cd bioperl-ext/Bio/Ext/Align/
perl -pi -e&quot;s|(cd libs.+)CFLAGS=\\\&#39;|\$1CFLAGS=\\\&#39;-fPIC |&quot; Makefile.PL
perl Makefile.PL PREFIX=~/perl5
make
make install
cd -
export PERL5LIB=${PERL5LIB}:${HOME}/perl5/lib/x86_64-linux-gnu/perl/5.22.1/
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If successful the following should print &lt;code&gt;OK&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;perl -MBio::Tools::dpAlign -e&quot;print qq{OK\n}&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr&gt; 
&lt;p&gt;&lt;a name=&quot;recoder&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Variant Recoder&lt;/h2&gt; 
&lt;p&gt;&lt;code&gt;variant_recoder&lt;/code&gt; is a tool for translating between different variant encodings. It accepts as input any format supported by VEP (VCF, variant ID, HGVS), with extensions to allow for parsing of potentially ambiguous HGVS notations. For each input variant, &lt;code&gt;variant_recoder&lt;/code&gt; reports all possible encodings including variant IDs from &lt;a href=&quot;http://www.ensembl.org/info/genome/variation/species/sources_documentation.html&quot;&gt;all sources imported into the Ensembl database&lt;/a&gt; and HGVS (genomic, transcript and protein), reported on Ensembl, RefSeq and LRG sequences.&lt;/p&gt; 
&lt;p&gt;&lt;a name=&quot;recoderusage&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Usage&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;variant_recoder&lt;/code&gt; depends on database access for identifier lookup, and cannot be used in offline mode as per VEP. The output format is JSON and the &lt;a href=&quot;http://search.cpan.org/dist/JSON/&quot;&gt;JSON perl module&lt;/a&gt; is required.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;./variant_recoder --id [input_data_string]
./variant_recoder -i [input_file] --species [species]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a name=&quot;recoderoutput&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Output&lt;/h3&gt; 
&lt;p&gt;Output is a JSON array of objects, one per input variant, with the following keys:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;input&lt;/strong&gt;: input string&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;id&lt;/strong&gt;: variant identifiers&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;hgvsg&lt;/strong&gt;: HGVS genomic nomenclature&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;hgvsc&lt;/strong&gt;: HGVS transcript nomenclature&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;hgvsp&lt;/strong&gt;: HGVS protein nomenclature&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;spdi&lt;/strong&gt;: Genomic SPDI notation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;vcf_string&lt;/strong&gt;: VCF format (optional)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;var_synonyms&lt;/strong&gt;: Variation synonyms (optional)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;mane_select&lt;/strong&gt;: MANE Select transcripts (optional)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;warnings&lt;/strong&gt;: Warnings generated e.g. for invalid HGVS&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Use &lt;code&gt;--pretty&lt;/code&gt; to pre-format and indent JSON output.&lt;/p&gt; 
&lt;p&gt;Example output:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;./variant_recoder --id &quot;AGT:p.Met259Thr&quot; --pretty
[
   {
     &quot;warnings&quot; : [
         &quot;Possible invalid use of gene or protein identifier &#39;AGT&#39; as HGVS reference; AGT:p.Met259Thr may resolve to multiple genomic locations&quot;
      ],
     &quot;C&quot; : {
        &quot;input&quot; : &quot;AGT:p.Met259Thr&quot;,
        &quot;id&quot; : [
           &quot;rs699&quot;,
           &quot;CM920010&quot;,
           &quot;COSV64184214&quot;
        ],
        &quot;hgvsg&quot; : [
           &quot;NC_000001.11:g.230710048A&amp;gt;G&quot;
        ],
        &quot;hgvsc&quot; : [
           &quot;ENST00000366667.6:c.776T&amp;gt;C&quot;,
           &quot;ENST00000679684.1:c.776T&amp;gt;C&quot;,
           &quot;ENST00000679738.1:c.776T&amp;gt;C&quot;,
           &quot;ENST00000679802.1:c.776T&amp;gt;C&quot;,
           &quot;ENST00000679854.1:n.1287T&amp;gt;C&quot;,
           &quot;ENST00000679957.1:c.776T&amp;gt;C&quot;,
           &quot;ENST00000680041.1:c.776T&amp;gt;C&quot;,
           &quot;ENST00000680783.1:c.776T&amp;gt;C&quot;,
           &quot;ENST00000681269.1:c.776T&amp;gt;C&quot;,
           &quot;ENST00000681347.1:n.1287T&amp;gt;C&quot;,
           &quot;ENST00000681514.1:c.776T&amp;gt;C&quot;,
           &quot;ENST00000681772.1:c.776T&amp;gt;C&quot;,
           &quot;NM_001382817.3:c.776T&amp;gt;C&quot;,
           &quot;NM_001384479.1:c.776T&amp;gt;C&quot;
        ],
        &quot;hgvsp&quot; : [
           &quot;ENSP00000355627.5:p.Met259Thr&quot;,
           &quot;ENSP00000505981.1:p.Met259Thr&quot;,
           &quot;ENSP00000505063.1:p.Met259Thr&quot;,
           &quot;ENSP00000505184.1:p.Met259Thr&quot;,
           &quot;ENSP00000506646.1:p.Met259Thr&quot;,
           &quot;ENSP00000504866.1:p.Met259Thr&quot;,
           &quot;ENSP00000506329.1:p.Met259Thr&quot;,
           &quot;ENSP00000505985.1:p.Met259Thr&quot;,
           &quot;ENSP00000505963.1:p.Met259Thr&quot;,
           &quot;ENSP00000505829.1:p.Met259Thr&quot;,
           &quot;NP_001369746.2:p.Met259Thr&quot;,
           &quot;NP_001371408.1:p.Met259Thr&quot;
        ],
        &quot;spdi&quot; : [
           &quot;NC_000001.11:230710047:A:G&quot;
        ]
     }
   }
]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a name=&quot;recoderopts&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Options&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;variant_recoder&lt;/code&gt; shares many of the same command line flags as VEP. Others are unique to &lt;code&gt;variant_recoder&lt;/code&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;-id|--input_data [input_string]&lt;/code&gt;: a single variant as a string.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-i|--input_file [input_file]&lt;/code&gt;: input file containing one or more variants, one per line. Mixed formats disallowed.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--species&lt;/code&gt;: species to use (default: homo_sapiens).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--grch37&lt;/code&gt;: use GRCh37 assembly instead of GRCh38.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--genomes&lt;/code&gt;: set database parameters for &lt;a href=&quot;http://ensemblgenomes.org/&quot;&gt;Ensembl Genomes&lt;/a&gt; species.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--pretty&lt;/code&gt;: write pre-formatted indented JSON.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--fields [field1,field2]&lt;/code&gt;: limit output fields. Comma-separated list, one or more of: &lt;code&gt;id&lt;/code&gt;, &lt;code&gt;hgvsg&lt;/code&gt;, &lt;code&gt;hgvsc&lt;/code&gt;, &lt;code&gt;hgvsp&lt;/code&gt;, &lt;code&gt;spdi&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--vcf_string&lt;/code&gt; : report VCF&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--var_synonyms&lt;/code&gt; : report variation synonyms&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--mane_select&lt;/code&gt; : report MANE Select transcripts in HGVS format&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--host [db_host]&lt;/code&gt;: change database host from default &lt;code&gt;ensembldb.ensembl.org&lt;/code&gt; (UK); geographic mirrors are &lt;code&gt;useastdb.ensembl.org&lt;/code&gt; (US East Coast) and &lt;code&gt;asiadb.ensembl.org&lt;/code&gt; (Asia). &lt;code&gt;--user&lt;/code&gt;, &lt;code&gt;--port&lt;/code&gt; and &lt;code&gt;--pass&lt;/code&gt; may also be set.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--pick&lt;/code&gt;, &lt;code&gt;--per_gene&lt;/code&gt;, &lt;code&gt;--pick_allele&lt;/code&gt;, &lt;code&gt;--pick_allele_gene&lt;/code&gt;, &lt;code&gt;--pick_order&lt;/code&gt;: set and customise transcript selection process, see &lt;a href=&quot;http://www.ensembl.org/info/docs/tools/vep/script/vep_other.html#pick&quot;&gt;VEP documentation&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>glpi-project/glpi-agent</title>
      <link>https://github.com/glpi-project/glpi-agent</link>
      <description>&lt;p&gt;GLPI Agent&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;img src=&quot;https://raw.githubusercontent.com/glpi-project/glpi-agent/develop/share/html/logo.png&quot; alt=&quot;GLPI Agent&quot; width=&quot;32&quot; height=&quot;32&quot;&gt; GLPI Agent&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/glpi-project/glpi-agent/actions/workflows/glpi-agent-ci.yml&quot;&gt;&lt;img src=&quot;https://github.com/glpi-project/glpi-agent/actions/workflows/glpi-agent-ci.yml/badge.svg?sanitize=true&quot; alt=&quot;GLPI Agent CI&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/glpi-project/glpi-agent/actions/workflows/glpi-agent-packaging.yml&quot;&gt;&lt;img src=&quot;https://github.com/glpi-project/glpi-agent/actions/workflows/glpi-agent-packaging.yml/badge.svg?sanitize=true&quot; alt=&quot;GLPI Agent Packaging&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/glpi-project/glpi-agent/develop/#download&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/downloads/glpi-project/glpi-agent/total.svg?sanitize=true&quot; alt=&quot;Github All Releases&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://twitter.com/GLPI_PROJECT&quot;&gt;&lt;img src=&quot;https://img.shields.io/twitter/follow/GLPI_PROJECT.svg?style=social&amp;amp;label=Follow&quot; alt=&quot;Twitter Follow&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Summary&lt;/h2&gt; 
&lt;p&gt;The GLPI Agent is a generic management agent. It can perform a certain number of tasks, according to its own execution plan, or on behalf of a GLPI server acting as a control point.&lt;/p&gt; 
&lt;h2&gt;Description&lt;/h2&gt; 
&lt;p&gt;This agent is based on a fork of &lt;a href=&quot;https://github.com/fusioninventory/fusioninventory-agent&quot;&gt;FusionInventory agent&lt;/a&gt; and so works mainly like FusionInventory agent. It introduces new features and a new protocol to communicate directly with a GLPI server and its native inventory feature. Anyway it also keeps the compatibility with &lt;a href=&quot;https://github.com/fusioninventory/fusioninventory-for-glpi&quot;&gt;FusionInventory for GLPI plugin&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Download&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Release: See &lt;a href=&quot;https://github.com/glpi-project/glpi-agent/releases&quot;&gt;our github releases&lt;/a&gt; for official win32, MacOSX &amp;amp; linux packages.&lt;/li&gt; 
 &lt;li&gt;Development builds: 
  &lt;ul&gt; 
   &lt;li&gt;nightly builds for last &#39;develop&#39; branch commits: &lt;a href=&quot;http://nightly.glpi-project.org/glpi-agent&quot;&gt;GLPI-Agent nightly builds&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;with a github account, you can also access artifacts for any other branches supporting &lt;a href=&quot;https://github.com/glpi-project/glpi-agent/actions/workflows/glpi-agent-packaging.yml?query=is%3Asuccess+event%3Apush+-branch%3Adevelop&quot;&gt;&quot;GLPI Agent Packaging&quot; workflow&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;The GLPI Agent has its &lt;a href=&quot;https://github.com/glpi-project/doc-agent&quot;&gt;dedicated documentation project&lt;/a&gt; where any contribution will also be appreciated.&lt;/p&gt; 
&lt;p&gt;The documentation itself is &lt;a href=&quot;https://glpi-agent.readthedocs.io/&quot;&gt;readable online&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://glpi-agent.readthedocs.io/en/latest/?badge=latest&quot;&gt;&lt;img src=&quot;https://readthedocs.org/projects/glpi-agent/badge/?version=latest&quot; alt=&quot;Documentation Status&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Dependencies&lt;/h2&gt; 
&lt;h3&gt;Core&lt;/h3&gt; 
&lt;p&gt;Minimum perl version: 5.8&lt;/p&gt; 
&lt;p&gt;Mandatory Perl modules:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;File::Which&lt;/li&gt; 
 &lt;li&gt;LWP::UserAgent&lt;/li&gt; 
 &lt;li&gt;Net::IP&lt;/li&gt; 
 &lt;li&gt;Text::Template&lt;/li&gt; 
 &lt;li&gt;UNIVERSAL::require&lt;/li&gt; 
 &lt;li&gt;XML::LibXML&lt;/li&gt; 
 &lt;li&gt;Cpanel::JSON::XS&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Optional Perl modules:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Compress::Zlib, for message compression&lt;/li&gt; 
 &lt;li&gt;HTTP::Daemon, for web interface&lt;/li&gt; 
 &lt;li&gt;IO::Socket::SSL, for HTTPS support&lt;/li&gt; 
 &lt;li&gt;LWP::Protocol::https, for HTTPS support&lt;/li&gt; 
 &lt;li&gt;Proc::Daemon, for daemon mode (Unix only)&lt;/li&gt; 
 &lt;li&gt;Proc::PID::File, for daemon mode (Unix only)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Inventory task&lt;/h3&gt; 
&lt;p&gt;Optional Perl modules:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Net::CUPS, for printers detection&lt;/li&gt; 
 &lt;li&gt;Parse::EDID, for EDID data parsing&lt;/li&gt; 
 &lt;li&gt;DateTime, for reliable timezone name extraction&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Optional programs:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;dmidecode, for DMI data retrieval&lt;/li&gt; 
 &lt;li&gt;lspci, for PCI bus scanning&lt;/li&gt; 
 &lt;li&gt;hdparm, for additional disk drive info retrieval&lt;/li&gt; 
 &lt;li&gt;monitor-get-edid-using-vbe, monitor-get-edid or get-edid, for EDID data access&lt;/li&gt; 
 &lt;li&gt;ssh-keyscan, for host SSH public key retrieval&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Network discovery tasks&lt;/h3&gt; 
&lt;p&gt;Mandatory Perl modules:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Thread::Queue&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Optional Perl modules:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Net::NBName, for NetBios method support&lt;/li&gt; 
 &lt;li&gt;Net::SNMP, for SNMP method support&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Optional programs:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;arp, for arp table lookup method support&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Network inventory tasks&lt;/h3&gt; 
&lt;p&gt;Mandatory Perl modules:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Net::SNMP&lt;/li&gt; 
 &lt;li&gt;Thread::Queue&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Optional Perl modules:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Crypt::DES, for SNMPv3 support&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Wake on LAN task&lt;/h3&gt; 
&lt;p&gt;Optional Perl modules:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Net::Write::Layer2, for ethernet method support&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Deploy task&lt;/h3&gt; 
&lt;p&gt;Mandatory Perl modules:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Digest::SHA&lt;/li&gt; 
 &lt;li&gt;File::Copy::Recursive&lt;/li&gt; 
 &lt;li&gt;Cpanel::JSON::XS&lt;/li&gt; 
 &lt;li&gt;URI::Escape&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Mandatory Perl modules for P2P Support:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Net::Ping&lt;/li&gt; 
 &lt;li&gt;Parallel::ForkManager&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;MSI Packaging&lt;/h3&gt; 
&lt;p&gt;Tools:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/glpi-project/dmidecode&quot;&gt;dmidecode&lt;/a&gt; modified to be built with mingw32&lt;/li&gt; 
 &lt;li&gt;hdparm&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.7-zip.org/&quot;&gt;7zip&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Mandatory Perl modules:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Perl::Dist::Strawberry&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;MacOSX Packaging&lt;/h3&gt; 
&lt;p&gt;Tools:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/glpi-project/dmidecode/tree/macosx&quot;&gt;dmidecode&lt;/a&gt; modified to be built on macosx&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/munki/munki-pkg&quot;&gt;munkipkg&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Xcode&lt;/li&gt; 
 &lt;li&gt;productbuild&lt;/li&gt; 
 &lt;li&gt;hdiutil&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Public databases&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Pci.ids&lt;/li&gt; 
 &lt;li&gt;Usb.ids&lt;/li&gt; 
 &lt;li&gt;SysObject.ids: &lt;a href=&quot;https://github.com/glpi-project/sysobject.ids&quot;&gt;sysobject.ids&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Related contribs&lt;/h2&gt; 
&lt;p&gt;See &lt;a href=&quot;https://raw.githubusercontent.com/glpi-project/glpi-agent/develop/CONTRIB.md&quot;&gt;CONTRIB&lt;/a&gt; to find references to GLPI Agent related scritps/files&lt;/p&gt; 
&lt;h2&gt;Contacts&lt;/h2&gt; 
&lt;p&gt;Project websites:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;main site: &lt;a href=&quot;https://glpi-project.org/&quot;&gt;https://glpi-project.org/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;forum: &lt;a href=&quot;https://forum.glpi-project.org/&quot;&gt;https://forum.glpi-project.org/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;github: &lt;a href=&quot;http://github.com/glpi-project/glpi-agent&quot;&gt;http://github.com/glpi-project/glpi-agent&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Project Telegram channel:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://t.me/glpien&quot;&gt;https://t.me/glpien&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Please report any issues on project &lt;a href=&quot;https://github.com/glpi-project/glpi-agent/issues&quot;&gt;github issue tracker&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Active authors&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Guillaume Bougard &lt;a href=&quot;mailto:gbougard@teclib.com&quot;&gt;gbougard@teclib.com&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Copyright 2006-2010 &lt;a href=&quot;https://www.ocsinventory-ng.org/&quot;&gt;OCS Inventory contributors&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Copyright 2010-2019 &lt;a href=&quot;https://fusioninventory.org&quot;&gt;FusionInventory Team&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Copyright 2011-2021 &lt;a href=&quot;https://www.teclib-edition.com/&quot;&gt;Teclib Editions&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.gnu.org/licenses/old-licenses/gpl-2.0.en.html&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/License-GPL%20v2-blue.svg?sanitize=true&quot; alt=&quot;License: GPL v2&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;This software is licensed under the terms of GPLv2+, see LICENSE file for details.&lt;/p&gt; 
&lt;h2&gt;Additional pieces of software&lt;/h2&gt; 
&lt;p&gt;The glpi-injector script is based on fusioninventory-injector script:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;author: Pascal Danek&lt;/li&gt; 
 &lt;li&gt;copyright: 2005 Pascal Danek&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;GLPI::Agent::Task::Inventory::Vmsystem contains code from imvirt:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;url: &lt;a href=&quot;http://micky.ibh.net/~liske/imvirt.html&quot;&gt;http://micky.ibh.net/~liske/imvirt.html&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;author: Thomas Liske &lt;a href=&quot;mailto:liske@ibh.de&quot;&gt;liske@ibh.de&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;copyright: 2008 IBH IT-Service GmbH &lt;a href=&quot;http://www.ibh.de/&quot;&gt;http://www.ibh.de/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;License: GPLv2+&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>rjust/defects4j</title>
      <link>https://github.com/rjust/defects4j</link>
      <description>&lt;p&gt;A Database of Real Faults and an Experimental Infrastructure to Enable Controlled Experiments in Software Engineering Research&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Defects4J -- version 3.0.1 &lt;a href=&quot;https://github.com/rjust/defects4j/actions/workflows/ci.yml&quot;&gt;&lt;img src=&quot;https://github.com/rjust/defects4j/actions/workflows/ci.yml/badge.svg?sanitize=true&quot; alt=&quot;.github/workflows/ci.yml&quot;&gt;&lt;/a&gt;&lt;/h1&gt; 
&lt;p&gt;Defects4J is a collection of reproducible bugs and a supporting infrastructure with the goal of advancing software engineering research.&lt;/p&gt; 
&lt;h1&gt;Contents of Defects4J&lt;/h1&gt; 
&lt;h2&gt;The projects&lt;/h2&gt; 
&lt;p&gt;Defects4J contains 854 bugs (plus 10 deprecated bugs) from the following open-source projects:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Identifier&lt;/th&gt; 
   &lt;th&gt;Project name&lt;/th&gt; 
   &lt;th align=&quot;right&quot;&gt;Number of active bugs&lt;/th&gt; 
   &lt;th&gt;Active bug ids&lt;/th&gt; 
   &lt;th&gt;Deprecated bug ids (*)&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Chart&lt;/td&gt; 
   &lt;td&gt;jfreechart&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;26&lt;/td&gt; 
   &lt;td&gt;1-26&lt;/td&gt; 
   &lt;td&gt;None&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Cli&lt;/td&gt; 
   &lt;td&gt;commons-cli&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;39&lt;/td&gt; 
   &lt;td&gt;1-5,7-40&lt;/td&gt; 
   &lt;td&gt;6&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Closure&lt;/td&gt; 
   &lt;td&gt;closure-compiler&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;174&lt;/td&gt; 
   &lt;td&gt;1-62,64-92,94-176&lt;/td&gt; 
   &lt;td&gt;63,93&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Codec&lt;/td&gt; 
   &lt;td&gt;commons-codec&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;18&lt;/td&gt; 
   &lt;td&gt;1-18&lt;/td&gt; 
   &lt;td&gt;None&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Collections&lt;/td&gt; 
   &lt;td&gt;commons-collections&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;28&lt;/td&gt; 
   &lt;td&gt;1-28&lt;/td&gt; 
   &lt;td&gt;None&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Compress&lt;/td&gt; 
   &lt;td&gt;commons-compress&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;47&lt;/td&gt; 
   &lt;td&gt;1-47&lt;/td&gt; 
   &lt;td&gt;None&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Csv&lt;/td&gt; 
   &lt;td&gt;commons-csv&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;16&lt;/td&gt; 
   &lt;td&gt;1-16&lt;/td&gt; 
   &lt;td&gt;None&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Gson&lt;/td&gt; 
   &lt;td&gt;gson&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;18&lt;/td&gt; 
   &lt;td&gt;1-18&lt;/td&gt; 
   &lt;td&gt;None&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;JacksonCore&lt;/td&gt; 
   &lt;td&gt;jackson-core&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;26&lt;/td&gt; 
   &lt;td&gt;1-26&lt;/td&gt; 
   &lt;td&gt;None&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;JacksonDatabind&lt;/td&gt; 
   &lt;td&gt;jackson-databind&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;110&lt;/td&gt; 
   &lt;td&gt;1-64,66-88,90-112&lt;/td&gt; 
   &lt;td&gt;65,89&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;JacksonXml&lt;/td&gt; 
   &lt;td&gt;jackson-dataformat-xml&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;6&lt;/td&gt; 
   &lt;td&gt;1-6&lt;/td&gt; 
   &lt;td&gt;None&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Jsoup&lt;/td&gt; 
   &lt;td&gt;jsoup&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;93&lt;/td&gt; 
   &lt;td&gt;1-93&lt;/td&gt; 
   &lt;td&gt;None&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;JxPath&lt;/td&gt; 
   &lt;td&gt;commons-jxpath&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;22&lt;/td&gt; 
   &lt;td&gt;1-22&lt;/td&gt; 
   &lt;td&gt;None&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Lang&lt;/td&gt; 
   &lt;td&gt;commons-lang&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;61&lt;/td&gt; 
   &lt;td&gt;1,3-17,19-24,26-47,49-65&lt;/td&gt; 
   &lt;td&gt;2,18,25,48&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Math&lt;/td&gt; 
   &lt;td&gt;commons-math&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;106&lt;/td&gt; 
   &lt;td&gt;1-106&lt;/td&gt; 
   &lt;td&gt;None&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Mockito&lt;/td&gt; 
   &lt;td&gt;mockito&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;38&lt;/td&gt; 
   &lt;td&gt;1-38&lt;/td&gt; 
   &lt;td&gt;None&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Time&lt;/td&gt; 
   &lt;td&gt;joda-time&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;26&lt;/td&gt; 
   &lt;td&gt;1-20,22-27&lt;/td&gt; 
   &lt;td&gt;21&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;* Due to behavioral changes introduced in newer Java versions, some bugs are no longer reproducible. Hence, Defects4J distinguishes between active and deprecated bugs:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Active bugs can be accessed through &lt;code&gt;active-bugs.csv&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Deprecated bugs are removed from &lt;code&gt;active-bugs.csv&lt;/code&gt;, but their metadata is retained in the project directory.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Deprecated bugs can be accessed through &lt;code&gt;deprecated-bugs.csv&lt;/code&gt;, which also details when and why a bug was deprecated.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We do not re-enumerate active bugs because publications using Defects4J artifacts usually refer to bugs by their specific bug id.&lt;/p&gt; 
&lt;h2&gt;The bugs&lt;/h2&gt; 
&lt;p&gt;Each bug has the following properties:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Issue filed in the corresponding issue tracker, and issue tracker identifier mentioned in the fixing commit message.&lt;/li&gt; 
 &lt;li&gt;Fixed in a single commit.&lt;/li&gt; 
 &lt;li&gt;Minimized: the Defects4J maintainers manually pruned out irrelevant changes in the commit (e.g., refactorings or feature additions).&lt;/li&gt; 
 &lt;li&gt;Fixed by modifying the source code (as opposed to configuration files, documentation, or test files).&lt;/li&gt; 
 &lt;li&gt;A triggering test exists that failed before the fix and passes after the fix -- the test failure is not random or dependent on test execution order.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The (b)uggy and (f)ixed program revisions are labelled with &lt;code&gt;&amp;lt;id&amp;gt;b&lt;/code&gt; and &lt;code&gt;&amp;lt;id&amp;gt;f&lt;/code&gt;, respectively (&lt;code&gt;&amp;lt;id&amp;gt;&lt;/code&gt; is an integer).&lt;/p&gt; 
&lt;h1&gt;Reproducibility&lt;/h1&gt; 
&lt;h4&gt;Java version&lt;/h4&gt; 
&lt;p&gt;All bugs have been reproduced and triggering tests verified, using Java 11 (see the &lt;a href=&quot;https://raw.githubusercontent.com/rjust/defects4j/master/.github/workflows/ci.yml&quot;&gt;CI configuration&lt;/a&gt; for specifics). Using a different version of Java might result in unexpected failing tests and/or non-reproducible bugs.&lt;/p&gt; 
&lt;h4&gt;Timezone&lt;/h4&gt; 
&lt;p&gt;Defects4J generates and executes tests in the timezone &lt;code&gt;America/Los_Angeles&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;If you are using the bugs &lt;em&gt;outside&lt;/em&gt; of the Defects4J framework, set the &lt;code&gt;TZ&lt;/code&gt; environment variable to &lt;code&gt;America/Los_Angeles&lt;/code&gt; and export it. Not setting this option results in unexpected failing tests!&lt;/p&gt; 
&lt;h4&gt;Broken and Flaky Tests&lt;/h4&gt; 
&lt;p&gt;Defects4J excludes broken tests (tests that reliably fail on the fixed and the buggy version) and flaky tests (tests that intermittently fail on the fixed or buggy version).&lt;/p&gt; 
&lt;p&gt;If you are using the bugs &lt;em&gt;outside&lt;/em&gt; of the Defects4J framework, make sure to verify expected test behavior for your environment.&lt;/p&gt; 
&lt;h4&gt;Metadata&lt;/h4&gt; 
&lt;p&gt;We recommend using Defects4J through the provided command-line interface. All reproducibility tests within Defects4J rely on this interface. If you are using the bugs &lt;em&gt;outside&lt;/em&gt; of the Defects4J framework, make sure to use the Defects4J &lt;code&gt;export&lt;/code&gt; command to obtain relevant metadata such as source/test directories, classpath entries, and sets of tests.&lt;/p&gt; 
&lt;h1&gt;Setting up Defects4J&lt;/h1&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Java 11&lt;/li&gt; 
 &lt;li&gt;Git &amp;gt;= 1.9&lt;/li&gt; 
 &lt;li&gt;Subversion (svn) &amp;gt;= 1.8&lt;/li&gt; 
 &lt;li&gt;Perl &amp;gt;= 5.0.12&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;cpanm&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Defects4J version 2.x required Java 1.8.&lt;/p&gt; 
&lt;p&gt;Defects4J version 1.x and 0.x required Java 1.7.&lt;/p&gt; 
&lt;h4&gt;Perl dependencies&lt;/h4&gt; 
&lt;p&gt;All required Perl modules are listed in &lt;a href=&quot;https://github.com/rjust/defects4j/raw/master/cpanfile&quot;&gt;cpanfile&lt;/a&gt;. On many Unix platforms, these required Perl modules are installed by default. The setup instructions immediately below install them if necessary. If you do not have &lt;code&gt;cpanm&lt;/code&gt; installed, use cpan or a cpan wrapper to install the perl modules listed in &lt;code&gt;cpanfile&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Steps to set up Defects4J&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Clone Defects4J:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;git clone https://github.com/rjust/defects4j&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Initialize Defects4J (download the project repositories and external libraries, which are not included in the git repository for size purposes and to avoid redundancies):&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;cd defects4j&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;cpanm --installdeps .&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;./init.sh&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Add Defects4J&#39;s executables to your PATH:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;export PATH=$PATH:&quot;path2defects4j&quot;/framework/bin&lt;/code&gt; (&quot;path2defects4j&quot; points to the directory to which you cloned Defects4J; it looks like &quot;/user/yourComputerUserName/desktop/defects4j&quot;.)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Check installation:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;defects4j info -p Lang&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;On some platforms such as Windows, you might need to use &lt;code&gt;perl &quot;fullpath&quot;\defects4j&lt;/code&gt; where these instructions say to use &lt;code&gt;defects4j&lt;/code&gt;.&lt;/p&gt; 
&lt;h1&gt;Using Defects4J&lt;/h1&gt; 
&lt;h4&gt;Example commands&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Get information for a specific project (commons lang):&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;defects4j info -p Lang&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Get information for a specific bug (commons lang, bug 1):&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;defects4j info -p Lang -b 1&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Checkout a buggy source code version (commons lang, bug 1, buggy version):&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;defects4j checkout -p Lang -v 1b -w /tmp/lang_1_buggy&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Change to the working directory, compile sources and tests, and run tests:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;cd /tmp/lang_1_buggy&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;defects4j compile&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;defects4j test&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Some Defects4J commands take the project id as a command-line argument (possibly along with other arguments). Examples include &lt;code&gt;info&lt;/code&gt;, &lt;code&gt;checkout&lt;/code&gt;, and &lt;code&gt;query&lt;/code&gt;. Note that &lt;code&gt;info&lt;/code&gt; and &lt;code&gt;query&lt;/code&gt; report information that is derived from the Defects4J metadata and do not require access to project files that are in the project&#39;s VCS.&lt;/p&gt; &lt;p&gt;Other commands require a working directory, either set explicitly (&lt;code&gt;-w&lt;/code&gt; command-line argument) or implicitly (executed from within a working directory). Examples include any command that requires access to files under version control, including source code and build files. This includes all commands that build or test the code (&lt;code&gt;compile&lt;/code&gt;, &lt;code&gt;test&lt;/code&gt;, &lt;code&gt;coverage&lt;/code&gt;, &lt;code&gt;mutation&lt;/code&gt;) and commands that return version-specific information (&lt;code&gt;export&lt;/code&gt;).&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;The scripts in &lt;a href=&quot;https://raw.githubusercontent.com/rjust/defects4j/master/framework/test/&quot;&gt;&lt;code&gt;framework/test/&lt;/code&gt;&lt;/a&gt; are examples of how to use Defects4J, which you might find useful as inspiration when you are writing your own scripts that use Defects4J.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Command-line interface: defects4j command&lt;/h2&gt; 
&lt;p&gt;Use &lt;a href=&quot;http://defects4j.org/html_doc/defects4j.html&quot;&gt;&lt;code&gt;framework/bin/defects4j&lt;/code&gt;&lt;/a&gt; to execute any of the following commands:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Command&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://defects4j.org/html_doc/d4j/d4j-info.html&quot;&gt;info&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;View configuration of a specific project or summary of a specific bug&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://defects4j.org/html_doc/d4j/d4j-env.html&quot;&gt;env&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Print the environment of defects4j executions&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://defects4j.org/html_doc/d4j/d4j-checkout.html&quot;&gt;checkout&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Checkout a buggy or a fixed project version&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://defects4j.org/html_doc/d4j/d4j-compile.html&quot;&gt;compile&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Compile sources and developer-written tests of a buggy or a fixed project version&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://defects4j.org/html_doc/d4j/d4j-test.html&quot;&gt;test&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Run a single test method or a test suite on a buggy or a fixed project version&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://defects4j.org/html_doc/d4j/d4j-mutation.html&quot;&gt;mutation&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Run mutation analysis on a buggy or a fixed project version&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://defects4j.org/html_doc/d4j/d4j-coverage.html&quot;&gt;coverage&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Run code coverage analysis on a buggy or a fixed project version&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://defects4j.org/html_doc/d4j/d4j-monitor.test.html&quot;&gt;monitor.test&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Monitor the class loader during the execution of a single test or a test suite&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://defects4j.org/html_doc/d4j/d4j-bids.html&quot;&gt;bids&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Print the list of active or deprecated bug IDs for a specific project&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://defects4j.org/html_doc/d4j/d4j-pids.html&quot;&gt;pids&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Print a list of available project IDs&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://defects4j.org/html_doc/d4j/d4j-export.html&quot;&gt;export&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Export version-specific properties such as classpaths, directories, or lists of tests&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://defects4j.org/html_doc/d4j/d4j-query.html&quot;&gt;query&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Query the metadata to generate a CSV file of requested information for a specific project&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Export version-specific properties&lt;/h2&gt; 
&lt;p&gt;Use &lt;code&gt;defects4j export -p &amp;lt;property_name&amp;gt; [-o output_file]&lt;/code&gt; in the working directory to export a version-specific property:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Property&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;classes.modified&lt;/td&gt; 
   &lt;td&gt;Classes modified by the bug fix&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;classes.relevant&lt;/td&gt; 
   &lt;td&gt;Classes loaded by the JVM when executing all triggering tests&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;cp.compile&lt;/td&gt; 
   &lt;td&gt;Classpath to compile and run the project&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;cp.test&lt;/td&gt; 
   &lt;td&gt;Classpath to compile and run the developer-written tests&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;dir.src.classes&lt;/td&gt; 
   &lt;td&gt;Source directory of classes (relative to working directory)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;dir.bin.classes&lt;/td&gt; 
   &lt;td&gt;Target directory of classes (relative to working directory)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;dir.src.tests&lt;/td&gt; 
   &lt;td&gt;Source directory of tests (relative to working directory)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;dir.bin.tests&lt;/td&gt; 
   &lt;td&gt;Target directory of test classes (relative to working directory)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;tests.all&lt;/td&gt; 
   &lt;td&gt;List of all developer-written test classes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;tests.relevant&lt;/td&gt; 
   &lt;td&gt;List of relevant tests classes (a test class is relevant if, when executed, the JVM loads at least one of the modified classes)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;tests.trigger&lt;/td&gt; 
   &lt;td&gt;List of test methods that trigger (expose) the bug&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Export project-specific metadata&lt;/h2&gt; 
&lt;p&gt;Use &lt;code&gt;defects4j query -p &amp;lt;pid&amp;gt; -q &amp;lt;field_list&amp;gt; [-o &amp;lt;output_file&amp;gt;] [-D|-A]&lt;/code&gt; to generate a CSV file containing a set of requested metadata for each bug in a specific project.&lt;/p&gt; 
&lt;p&gt;By default, &lt;code&gt;defects4j query&lt;/code&gt; returns a list of active bug IDs for a project. To request specific metadata, the &lt;code&gt;-q&lt;/code&gt; flag should be provided with a comma-separated list of variables from the list below. For example, &lt;code&gt;defects4j query -p Chart -q &quot;report.id,report.url&quot;&lt;/code&gt; will provide the a list of all active bug IDs, along with the bug report ID and bug report URL for each.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Property&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;bug.id&lt;/td&gt; 
   &lt;td&gt;Assigned bug IDs (included in all results)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;project.id&lt;/td&gt; 
   &lt;td&gt;Assigned project ID&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;project.name&lt;/td&gt; 
   &lt;td&gt;Original project name&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;project.build.file&lt;/td&gt; 
   &lt;td&gt;Location of the Defects4J build file for the project&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;project.vcs&lt;/td&gt; 
   &lt;td&gt;Version control system used by the project&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;project.repository&lt;/td&gt; 
   &lt;td&gt;Location of the project repository&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;project.bugs.csv&lt;/td&gt; 
   &lt;td&gt;Location of the CSV containing information on that bug&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;revision.id.buggy&lt;/td&gt; 
   &lt;td&gt;Commit hashes for the buggy version of each bug&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;revision.id.fixed&lt;/td&gt; 
   &lt;td&gt;Commit hashes for the fixed version of each bug&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;revision.date.buggy&lt;/td&gt; 
   &lt;td&gt;Date of the buggy commit for each bug&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;revision.date.fixed&lt;/td&gt; 
   &lt;td&gt;Date of the fixed commit for each bug&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;report.id&lt;/td&gt; 
   &lt;td&gt;Bug report ID from the version tracker for each bug&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;report.url&lt;/td&gt; 
   &lt;td&gt;Bug report URL from the version tracker for each bug&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;classes.modified&lt;/td&gt; 
   &lt;td&gt;Classes modified by the bug fix&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;classes.relevant.src&lt;/td&gt; 
   &lt;td&gt;Source classes loaded by the JVM when executing all triggering tests&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;classes.relevant.test&lt;/td&gt; 
   &lt;td&gt;Test classes loaded by the JVM when executing all triggering tests&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;tests.relevant&lt;/td&gt; 
   &lt;td&gt;List of relevant tests classes (a test class is relevant if, when executed, the JVM loads at least one of the modified classes)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;tests.trigger&lt;/td&gt; 
   &lt;td&gt;List of test methods that trigger (expose) the bug, separated by semicolons (&lt;code&gt;;&lt;/code&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;tests.trigger.cause&lt;/td&gt; 
   &lt;td&gt;List of test methods that trigger (expose) the bug, along with the exception thrown. Each list element has the form &quot;methodName --&amp;gt; exceptionClass[: message]&quot;, and list elements are separated by semicolons (&lt;code&gt;;&lt;/code&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;deprecated.version&lt;/td&gt; 
   &lt;td&gt;(for deprecated bugs only) Version of Defects4J where a bug was deprecated&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;deprecated.reason&lt;/td&gt; 
   &lt;td&gt;(for deprecated bugs only) Reason for deprecation&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;By default, &lt;code&gt;defects4j query&lt;/code&gt; returns information on active bugs. The &lt;code&gt;[-D]&lt;/code&gt; flag returns information only on deprecated bugs, while the &lt;code&gt;[-A]&lt;/code&gt; flag returns information for all active and deprecated bugs.&lt;/p&gt; 
&lt;p&gt;To determine the methods that are changed between the buggy and fixed version of the code:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Add this line to your user-level git attributes file: &lt;code&gt;*.java diff=java&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run &lt;code&gt;git diff --no-index&lt;/code&gt;, for example &lt;code&gt;git diff --no-index /tmp/lang_1_buggy /tmp/lang_1_fixed&lt;/code&gt;. In the output, every line starting with &quot;@&quot; gives the method name of a changed method.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Test execution framework&lt;/h2&gt; 
&lt;p&gt;The test execution framework for generated test suites (&lt;code&gt;framework/bin&lt;/code&gt;) provides the following scripts:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Script&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://defects4j.org/html_doc/defects4j.html&quot;&gt;defects4j&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Main script, described above&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://defects4j.org/html_doc/gen_tests.html&quot;&gt;gen_tests&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Generate test suites using EvoSuite or Randoop&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://defects4j.org/html_doc/run_bug_detection.html&quot;&gt;run_bug_detection&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Determine the real fault detection rate&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://defects4j.org/html_doc/run_mutation.html&quot;&gt;run_mutation&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Determine the mutation score&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://defects4j.org/html_doc/run_coverage.html&quot;&gt;run_coverage&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Determine code coverage ratios (statement and branch coverage)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;Mining and contributing additional bugs to Defects4J&lt;/h1&gt; 
&lt;p&gt;We welcome your contributions to Defects4J! The bug-mining &lt;a href=&quot;https://raw.githubusercontent.com/rjust/defects4j/master/framework/bug-mining/README.md&quot;&gt;README&lt;/a&gt; details the bug-mining process.&lt;/p&gt; 
&lt;h1&gt;Additional resources&lt;/h1&gt; 
&lt;h2&gt;Scripts built on Defects4J&lt;/h2&gt; 
&lt;h4&gt;Fault localization (FL)&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://bitbucket.org/rjust/fault-localization-data&quot;&gt;Scripts and annotations for evaluating FL techniques&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Automated program repair (APR)&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/LASER-UMASS/AutomatedRepairApplicabilityData&quot;&gt;Scripts and annotations for evaluating APR techniques&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Spirals-Team/defects4j-repair&quot;&gt;Patches generated with the Nopol, jGenProg, and jKali APR systems&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;http://program-repair.org/defects4j-dissection/&quot;&gt;Repair actions and patterns for Defects4J v1.2.0&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Publications&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&quot;Defects4J: A Database of Existing Faults to Enable Controlled Testing Studies for Java Programs&quot; René Just, Darioush Jalali, and Michael D. Ernst, ISSTA 2014 &lt;a href=&quot;https://people.cs.umass.edu/~rjust/publ/defects4j_issta_2014.pdf&quot;&gt;[download]&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&quot;Are Mutants a Valid Substitute for Real Faults in Software Testing?&quot; René Just, Darioush Jalali, Laura Inozemtseva, Michael D. Ernst, Reid Holmes, and Gordon Fraser, FSE 2014 &lt;a href=&quot;https://people.cs.umass.edu/~rjust/publ/mutants_real_faults_fse_2014.pdf&quot;&gt;[download]&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&quot;Challenges in Using Search-Based Test Generation to Identify Real Faults in Mockito&quot; Gregory Gay, SSBSE 2016 &lt;a href=&quot;https://greg4cr.github.io/pdf/16mockito.pdf&quot;&gt;[download]&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&quot;Detecting Real Faults in the Gson Library Through Search-Based Unit Test Generation&quot; Gregory Gay, SSBSE 2018 &lt;a href=&quot;https://greg4cr.github.io/pdf/18gson.pdf&quot;&gt;[download]&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&quot;Defects4J as a Challenge Case for the Search-Based Software Engineering Community&quot; Gregory Gay and René Just, SSBSE 2020 &lt;a href=&quot;https://greg4cr.github.io/pdf/20d4j.pdf&quot;&gt;[download]&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href=&quot;https://scholar.google.com/scholar?q=defects4j&quot;&gt;More publications&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Implementation details&lt;/h2&gt; 
&lt;p&gt;Documentation for any script or module is available as &lt;a href=&quot;http://defects4j.org/html_doc/index.html&quot;&gt;HTML documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The directory structure of Defects4J is as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;defects4j
   |
   |--- project_repos:     The version control repositories of the provided projects (populated during initialization).
   |
   |--- developer:         Resources for Defects4J contributors.
   |
   |--- framework:         Libraries and executables of the core, test execution,
       |                   and bug-mining frameworks.
       |
       |--- bin:           Command line interface to Defects4J.
       |
       |--- bug-mining:    Bug-mining framework.
       |
       |--- core:          The modules of the core framework.
       |
       |--- doc:           Scripts and templates for the html documentation.
       |
       |--- lib:           Libraries used in the core framework.
       |
       |--- projects:      Project-specific resource files.
       |
       |--- test:          Scripts to test the framework.
       |
       |--- util:          Util scripts used by Defects4J.
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Versioning information&lt;/h2&gt; 
&lt;p&gt;Defects4J uses a semantic versioning scheme (&lt;code&gt;major&lt;/code&gt;.&lt;code&gt;minor&lt;/code&gt;.&lt;code&gt;patch&lt;/code&gt;):&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Change&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;code&gt;major&lt;/code&gt;&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;code&gt;minor&lt;/code&gt;&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;code&gt;patch&lt;/code&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Addition/Deletion of bugs&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;X&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;New/upgraded internal or external tools&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;X&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Fixes and documentation changes&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;X&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;MIT License, see &lt;a href=&quot;https://github.com/rjust/defects4j/raw/master/license.txt&quot;&gt;&lt;code&gt;license.txt&lt;/code&gt;&lt;/a&gt; for more information.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>AlDanial/cloc</title>
      <link>https://github.com/AlDanial/cloc</link>
      <description>&lt;p&gt;cloc counts blank lines, comment lines, and physical lines of source code in many programming languages.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a name=&quot;___top&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;cloc&lt;/h1&gt; 
&lt;p&gt;&lt;em&gt;Count Lines of Code&lt;/em&gt;&lt;/p&gt; 
&lt;hr&gt; 
&lt;p&gt;cloc counts blank lines, comment lines, and physical lines of source code in many programming languages.&lt;/p&gt; 
&lt;p&gt;Latest release: v2.04 (Jan. 31, 2025)&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/AlDanial/cloc&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/version-2.04-blue.svg?sanitize=true&quot; alt=&quot;Version&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/AlDanial/cloc/graphs/contributors&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/contributors/AlDanial/cloc.svg?sanitize=true&quot; alt=&quot;Contributors&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://doi.org/10.5281/zenodo.42029482&quot;&gt;&lt;img src=&quot;https://zenodo.org/badge/doi/10.5281/zenodo.42029482.svg?sanitize=true&quot; alt=&quot;DOI&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/AlDanial/cloc/network/members&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/forks/AlDanial/cloc.svg?sanitize=true&quot; alt=&quot;Forks&quot;&gt;&lt;/a&gt; &lt;a href=&quot;&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/downloads/AlDanial/cloc/total.svg?sanitize=true&quot; alt=&quot;Downloads&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;cloc moved to GitHub in September 2015 after being hosted at &lt;a href=&quot;http://cloc.sourceforge.net/&quot;&gt;http://cloc.sourceforge.net/&lt;/a&gt; since August 2006.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#quick-start-&quot;&gt;Quick Start&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#overview-&quot;&gt;Overview&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/AlDanial/cloc/releases/latest&quot;&gt;Download&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#install-via-package-manager&quot;&gt;Install via package manager&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#stable-release&quot;&gt;Stable release&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#development-version&quot;&gt;Development version&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#license-&quot;&gt;License&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#why-use-cloc-&quot;&gt;Why Use cloc?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#other-counters-&quot;&gt;Other Counters&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#building-a-windows-executable-&quot;&gt;Building a Windows Executable&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#basic-use-&quot;&gt;Basic Use&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#options-&quot;&gt;Options&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#recognized-languages-&quot;&gt;Recognized Languages&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#how-it-works-&quot;&gt;How it Works&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#advanced-use-&quot;&gt;Advanced Use&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#remove-comments-from-source-code-&quot;&gt;Remove Comments from Source Code&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#work-with-compressed-archives-&quot;&gt;Work with Compressed Archives&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#differences-&quot;&gt;Differences&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#create-custom-language-definitions-&quot;&gt;Create Custom Language Definitions&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#combine-reports-&quot;&gt;Combine Reports&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#sql-&quot;&gt;SQL&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#custom-column-output-&quot;&gt;Custom Column Output&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#wrapping-cloc-in-other-scripts-&quot;&gt;Wrapping cloc in other scripts&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#git-and-UTF8-pathnames-&quot;&gt;git and UTF8 pathnames&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#third-generation-language-scale-factors-&quot;&gt;Third Generation Language Scale Factors&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#optionstxt-configuration-file-&quot;&gt;options.txt configuration file&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#java-programmatic-interface-&quot;&gt;Java Programmatic Interface&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#complex-regular-subexpression-recursion-limit-&quot;&gt;Complex regular subexpression recursion limit &lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#limitations-&quot;&gt;Limitations&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#requesting-support-for-additional-languages-&quot;&gt;Requesting Support for Additional Languages&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#reporting-problems-&quot;&gt;Reporting Problems&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#citation-&quot;&gt;Citation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#acknowledgments-&quot;&gt;Acknowledgments&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#copyright-&quot;&gt;Copyright&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a name=&quot;Quick_Start&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/%7B%7B%7B1&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#___top&quot; title=&quot;click to go to top of document&quot;&gt;Quick Start ▲&lt;/a&gt;&lt;/h1&gt; 
&lt;p&gt;Step 1: Download cloc (several methods, see below) or run cloc&#39;s &lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#run-via-docker&quot;&gt;docker image&lt;/a&gt;. The Windows executable has no requirements. The source version of cloc requires a Perl interpreter, and the Docker version of cloc requires a Docker installation.&lt;/p&gt; 
&lt;p&gt;Step 2: Open a terminal (&lt;code&gt;cmd.exe&lt;/code&gt; on Windows).&lt;/p&gt; 
&lt;p&gt;Step 3: Invoke cloc to count your source files, directories, archives, or git commits. The executable name differs depending on whether you use the development source version (&lt;code&gt;cloc&lt;/code&gt;), source for a released version (&lt;code&gt;cloc-2.04.pl&lt;/code&gt;) or a Windows executable (&lt;code&gt;cloc-2.04.exe&lt;/code&gt;).&lt;/p&gt; 
&lt;p&gt;On this page, &lt;code&gt;cloc&lt;/code&gt; is the generic term used to refer to any of these.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/user/IncludeSecurity&quot;&gt;Include Security&lt;/a&gt; has a &lt;a href=&quot;https://www.youtube.com/watch?v=eRLTkDMsCqs&quot;&gt;YouTube video&lt;/a&gt; showing the steps in action.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;a file&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;
prompt&amp;gt; cloc hello.c
       1 text file.
       1 unique file.
       0 files ignored.

https://github.com/AlDanial/cloc v 1.65  T=0.04 s (28.3 files/s, 340.0 lines/s)
-------------------------------------------------------------------------------
Language                     files          blank        comment           code
-------------------------------------------------------------------------------
C                                1              0              7              5
-------------------------------------------------------------------------------
&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;a directory&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;
prompt&amp;gt; cloc gcc-5.2.0/gcc/c
      16 text files.
      15 unique files.
       3 files ignored.

https://github.com/AlDanial/cloc v 1.65  T=0.23 s (57.1 files/s, 188914.0 lines/s)
-------------------------------------------------------------------------------
Language                     files          blank        comment           code
-------------------------------------------------------------------------------
C                               10           4680           6621          30812
C/C++ Header                     3             99            286            496
-------------------------------------------------------------------------------
SUM:                            13           4779           6907          31308
-------------------------------------------------------------------------------
&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;an archive&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;We&#39;ll pull cloc&#39;s source zip file from GitHub, then count the contents:&lt;/p&gt; 
&lt;pre&gt;
prompt&amp;gt; wget https://github.com/AlDanial/cloc/archive/master.zip

prompt&amp;gt; cloc master.zip
https://github.com/AlDanial/cloc v 1.65  T=0.07 s (26.8 files/s, 141370.3 lines/s)
-------------------------------------------------------------------------------
Language                     files          blank        comment           code
-------------------------------------------------------------------------------
Perl                             2            725           1103           8713
-------------------------------------------------------------------------------
SUM:                             2            725           1103           8713
-------------------------------------------------------------------------------
&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;a git repository, using a specific commit&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;This example uses code from &lt;a href=&quot;https://pypi.python.org/pypi/pudb&quot;&gt;PuDB&lt;/a&gt;, a fantastic Python debugger.&lt;/p&gt; 
&lt;pre&gt;
prompt&amp;gt; git clone https://github.com/inducer/pudb.git

prompt&amp;gt; cd pudb

prompt&amp;gt; cloc 6be804e07a5db
      48 text files.
      41 unique files.                              
       8 files ignored.

github.com/AlDanial/cloc v 1.99  T=0.04 s (1054.9 files/s, 189646.8 lines/s)
-------------------------------------------------------------------------------
Language                     files          blank        comment           code
-------------------------------------------------------------------------------
Python                          28           1519            728           4659
reStructuredText                 6            102             20            203
YAML                             2              9              2             75
Bourne Shell                     3              6              0             17
Text                             1              0              0             11
make                             1              4              6             10
-------------------------------------------------------------------------------
SUM:                            41           1640            756           4975
-------------------------------------------------------------------------------
&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;each subdirectory of a particular directory&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Say you have a directory with three different git-managed projects, Project0, Project1, and Project2. You can use your shell&#39;s looping capability to count the code in each. This example uses bash (scroll down for cmd.exe example):&lt;/p&gt; 
&lt;pre&gt;
prompt&amp;gt; for d in ./*/ ; do (cd &quot;$d&quot; &amp;amp;&amp;amp; echo &quot;$d&quot; &amp;amp;&amp;amp; cloc --vcs git); done
./Project0/
7 text files.
       7 unique files.
       1 file ignored.

github.com/AlDanial/cloc v 1.71  T=0.02 s (390.2 files/s, 25687.6 lines/s)
-------------------------------------------------------------------------------
Language                     files          blank        comment           code
-------------------------------------------------------------------------------
D                                4             61             32            251
Markdown                         1              9              0             38
make                             1              0              0              4
-------------------------------------------------------------------------------
SUM:                             6             70             32            293
-------------------------------------------------------------------------------
./Project1/
       7 text files.
       7 unique files.
       0 files ignored.

github.com/AlDanial/cloc v 1.71  T=0.02 s (293.0 files/s, 52107.1 lines/s)
-------------------------------------------------------------------------------
Language                     files          blank        comment           code
-------------------------------------------------------------------------------
Go                               7            165            282            798
-------------------------------------------------------------------------------
SUM:                             7            165            282            798
-------------------------------------------------------------------------------
./Project2/
      49 text files.
      47 unique files.
      13 files ignored.

github.com/AlDanial/cloc v 1.71  T=0.10 s (399.5 files/s, 70409.4 lines/s)
-------------------------------------------------------------------------------
Language                     files          blank        comment           code
-------------------------------------------------------------------------------
Python                          33           1226           1026           3017
C                                4            327            337            888
Markdown                         1             11              0             28
YAML                             1              0              2             12
-------------------------------------------------------------------------------
SUM:                            39           1564           1365           3945
-------------------------------------------------------------------------------
&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;each subdirectory of a particular directory (Windows/cmd.exe)&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;
for /D %I in (.\*) do cd %I &amp;amp;&amp;amp; cloc --vcs git &amp;amp;&amp;amp; cd ..
&lt;/pre&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/1%7D%7D%7D&quot;&gt;&lt;/a&gt; &lt;a name=&quot;Overview&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/%7B%7B%7B1&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#___top&quot; title=&quot;click to go to top of document&quot;&gt;Overview ▲&lt;/a&gt;&lt;/h1&gt; 
&lt;p&gt;cloc counts blank lines, comment lines, and physical lines of source code in &lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#Languages&quot;&gt;many programming languages&lt;/a&gt;. Given two versions of a code base, cloc can compute differences in blank, comment, and source lines. It is written entirely in Perl with no dependencies outside the standard distribution of Perl v5.6 and higher (code from some external modules is &lt;a href=&quot;https://github.com/AlDanial/cloc#regexp_common&quot;&gt;embedded within cloc&lt;/a&gt;) and so is quite portable. cloc is known to run on many flavors of Linux, FreeBSD, NetBSD, OpenBSD, macOS, AIX, HP-UX, Solaris, IRIX, z/OS, and Windows. (To run the Perl source version of cloc on Windows one needs &lt;a href=&quot;http://www.activestate.com/activeperl&quot;&gt;ActiveState Perl&lt;/a&gt; 5.6.1 or higher, &lt;a href=&quot;http://strawberryperl.com/&quot;&gt;Strawberry Perl&lt;/a&gt;, Windows Subsystem for Linux, &lt;a href=&quot;http://www.cygwin.com/&quot;&gt;Cygwin&lt;/a&gt;, &lt;a href=&quot;http://mobaxterm.mobatek.net/&quot;&gt;MobaXTerm&lt;/a&gt; with the Perl plug-in installed, or a mingw environment and terminal such as provided by &lt;a href=&quot;https://gitforwindows.org/&quot;&gt;Git for Windows&lt;/a&gt;. Alternatively one can use the Windows binary of cloc generated with &lt;a href=&quot;http://search.cpan.org/~rschupp/PAR-Packer-1.019/lib/pp.pm&quot;&gt;PAR::Packer&lt;/a&gt; to run on Windows computers that have neither Perl nor Cygwin.)&lt;/p&gt; 
&lt;p&gt;In addition to counting code in individual text files, directories, and git repositories, cloc can also count code in archive files such as &lt;code&gt;.tar&lt;/code&gt; (including compressed versions), &lt;code&gt;.zip&lt;/code&gt;, Python wheel &lt;code&gt;.whl&lt;/code&gt;, Jupyter notebook &lt;code&gt;.ipynb&lt;/code&gt;, source RPMs &lt;code&gt;.rpm&lt;/code&gt; or &lt;code&gt;.src&lt;/code&gt; (requires &lt;code&gt;rpm2cpio&lt;/code&gt;), and Debian &lt;code&gt;.deb&lt;/code&gt; files (requires &lt;code&gt;dpkg-deb&lt;/code&gt;).&lt;/p&gt; 
&lt;p&gt;cloc contains code from David Wheeler&#39;s &lt;a href=&quot;http://www.dwheeler.com/sloccount/&quot;&gt;SLOCCount&lt;/a&gt;, Damian Conway and Abigail&#39;s Perl module &lt;a href=&quot;http://search.cpan.org/%7Eabigail/Regexp-Common-2.120/lib/Regexp/Common.pm&quot;&gt;Regexp::Common&lt;/a&gt;, Sean M. Burke&#39;s Perl module &lt;a href=&quot;http://search.cpan.org/%7Esburke/Win32-Autoglob-1.01/Autoglob.pm&quot;&gt;Win32::Autoglob&lt;/a&gt;, and Tye McQueen&#39;s Perl module &lt;a href=&quot;http://search.cpan.org/%7Etyemq/Algorithm-Diff-1.1902/lib/Algorithm/Diff.pm&quot;&gt;Algorithm::Diff&lt;/a&gt;. Language scale factors were derived from Mayes Consulting, LLC web site &lt;a href=&quot;http://softwareestimator.com/IndustryData2.htm&quot;&gt;http://softwareestimator.com/IndustryData2.htm&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;New releases nominally appear every six months. &lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/1%7D%7D%7D&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a name=&quot;Docker&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/%7B%7B%7B1&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Run via docker&lt;/h2&gt; 
&lt;p&gt;These docker commands count lines of code in and below the current directory:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;docker run --rm -v $PWD:/tmp aldanial/cloc .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Run via docker on git-bash&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;docker run --rm -v &quot;/$(pwd -W)&quot;:/tmp aldanial/cloc .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Install via package manager&lt;/h2&gt; 
&lt;p&gt;Depending your operating system, one of these installation methods may work for you (all but the last two entries for Windows require a Perl interpreter):&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;npm install -g cloc              # https://www.npmjs.com/package/cloc
sudo apt install cloc            # Debian, Ubuntu
sudo yum install cloc            # Red Hat, Fedora
sudo dnf install cloc            # Fedora 22 or later
sudo pacman -S cloc              # Arch
yay -S cloc-git                  # Arch AUR (latest git version)
sudo emerge -av dev-util/cloc    # Gentoo https://packages.gentoo.org/packages/dev-util/cloc
sudo apk add cloc                # Alpine Linux
doas pkg_add cloc                # OpenBSD
sudo pkg install cloc            # FreeBSD
sudo port install cloc           # macOS with MacPorts
brew install cloc                # macOS with Homebrew
winget install AlDanial.Cloc     # Windows with winget (might not work, ref https://github.com/AlDanial/cloc/issues/849)
choco install cloc               # Windows with Chocolatey
scoop install cloc               # Windows with Scoop
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: I don&#39;t control any of these packages. If you encounter a bug in cloc using one of the above packages, try with cloc pulled from the latest stable release here on GitHub (link follows below) before submitting a problem report. &lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/1%7D%7D%7D&quot;&gt;&lt;/a&gt; &lt;a name=&quot;Stable&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/%7B%7B%7B1&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Stable release&lt;/h2&gt; 
&lt;p&gt;Download the latest released cloc source file, for example &lt;code&gt;cloc-2.04.pl&lt;/code&gt;, or the Windows executable &lt;code&gt;cloc-2.04.exe&lt;/code&gt; from &lt;a href=&quot;https://github.com/AlDanial/cloc/releases/latest&quot;&gt;https://github.com/AlDanial/cloc/releases/latest&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a name=&quot;Dev&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Development version&lt;/h2&gt; 
&lt;p&gt;Download the cloc source code at &lt;a href=&quot;https://github.com/AlDanial/cloc/raw/master/cloc&quot;&gt;https://github.com/AlDanial/cloc/raw/master/cloc&lt;/a&gt; and save it as the file &lt;code&gt;cloc&lt;/code&gt; (or &lt;code&gt;cloc.pl&lt;/code&gt;, or whatever executable name you wish). The next step depends on the operating system you&#39;re using.&lt;/p&gt; 
&lt;h3&gt;On Unix-like systems, including macOS&lt;/h3&gt; 
&lt;p&gt;In a terminal, go to the download directory and make the cloc file executable, then give it a test run. For example&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;» cd ~/Downloads
» chmod +x cloc
» ./cloc --version
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For future use, move the file to a more convenient directory in your &lt;code&gt;PATH&lt;/code&gt; such as &lt;code&gt;/usr/local/bin&lt;/code&gt; or &lt;code&gt;~/bin&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;On Windows&lt;/h3&gt; 
&lt;p&gt;You&#39;ll need a Perl interpreter such as &lt;a href=&quot;http://strawberryperl.com/&quot;&gt;Strawberry Perl&lt;/a&gt; installed to run the source version of cloc. After downloading the cloc source file, open a command prompt or PowerShell window, navigate to the download directory (&lt;code&gt;C:\TEMP&lt;/code&gt; in the example below), then test cloc with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-dos&quot;&gt;cd C:\TEMP&amp;gt;
C:TEMP\&amp;gt; perl cloc --version
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/1%7D%7D%7D&quot;&gt;&lt;/a&gt; &lt;a name=&quot;License&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/%7B%7B%7B1&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#___top&quot; title=&quot;click to go to top of document&quot;&gt;License ▲&lt;/a&gt;&lt;/h1&gt; 
&lt;p&gt;cloc is licensed under the &lt;a href=&quot;http://www.gnu.org/licenses/gpl-2.0.html&quot;&gt;GNU General Public License, v 2&lt;/a&gt;, excluding portions which are copied from other sources. Code copied from the Regexp::Common, Win32::Autoglob, and Algorithm::Diff Perl modules is subject to the &lt;a href=&quot;https://opensource.org/license/artistic-2-0&quot;&gt;Artistic License&lt;/a&gt;. &lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/1%7D%7D%7D&quot;&gt;&lt;/a&gt; &lt;a name=&quot;why_use&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/%7B%7B%7B1&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#___top&quot; title=&quot;click to go to top of document&quot;&gt;Why Use cloc? ▲&lt;/a&gt;&lt;/h1&gt; 
&lt;p&gt;cloc has many features that make it easy to use, thorough, extensible, and portable:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Exists as a single, self-contained file that requires minimal installation effort---just download the file and run it.&lt;/li&gt; 
 &lt;li&gt;Can read language comment definitions from a file and thus potentially work with computer languages that do not yet exist.&lt;/li&gt; 
 &lt;li&gt;Allows results from multiple runs to be summed together by language and by project.&lt;/li&gt; 
 &lt;li&gt;Can produce results in a variety of formats: plain text, Markdown, SQL, JSON, XML, YAML, comma separated values.&lt;/li&gt; 
 &lt;li&gt;Can count code within compressed archives (tar balls, Zip files, Java .ear files).&lt;/li&gt; 
 &lt;li&gt;Has numerous troubleshooting options.&lt;/li&gt; 
 &lt;li&gt;Handles file and directory names with spaces and other unusual characters.&lt;/li&gt; 
 &lt;li&gt;Has no dependencies outside the standard Perl distribution.&lt;/li&gt; 
 &lt;li&gt;Runs on Linux, FreeBSD, NetBSD, OpenBSD, macOS, AIX, HP-UX, Solaris, IRIX, and z/OS systems that have Perl 5.6 or higher. The source version runs on Windows with either ActiveState Perl, Strawberry Perl, Cygwin, or MobaXTerm+Perl plugin. Alternatively on Windows one can run the Windows binary which has no dependencies. &lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/1%7D%7D%7D&quot;&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;a name=&quot;Other_Counters&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/%7B%7B%7B1&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#___top&quot; title=&quot;click to go to top of document&quot;&gt;Other Counters ▲&lt;/a&gt;&lt;/h1&gt; 
&lt;p&gt;If cloc does not suit your needs here are other freely available counters to consider:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/cgag/loc/&quot;&gt;loc&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/JoaoDanielRufino/gcloc&quot;&gt;gcloc&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/hhatto/gocloc/&quot;&gt;gocloc&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/blackducksoftware/ohcount/&quot;&gt;Ohcount&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/boyter/scc/&quot;&gt;scc&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://code.google.com/archive/p/sclc/&quot;&gt;sclc&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;http://www.dwheeler.com/sloccount/&quot;&gt;SLOCCount&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;http://www.sonarsource.org/&quot;&gt;Sonar&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Aaronepower/tokei/&quot;&gt;tokei&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;http://csse.usc.edu/ucc_new/wordpress/&quot;&gt;Unified Code Count&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Other references:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;QSM&#39;s &lt;a href=&quot;http://www.qsm.com/CodeCounters.html&quot;&gt;directory&lt;/a&gt; of code counting tools.&lt;/li&gt; 
 &lt;li&gt;The &lt;a href=&quot;http://en.wikipedia.org/wiki/Source_lines_of_code&quot;&gt;Wikipedia entry&lt;/a&gt; for source code line counts. &lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/1%7D%7D%7D&quot;&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;&lt;a name=&quot;regexp_common&quot;&gt;Regexp::Common, Digest::MD5, Win32::Autoglob, Algorithm::Diff&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/%7B%7B%7B1&quot;&gt;&lt;/a&gt;&lt;/h1&gt; 
&lt;p&gt;Although cloc does not need Perl modules outside those found in the standard distribution, cloc does rely on a few external modules. Code from three of these external modules--Regexp::Common, Win32::Autoglob, and Algorithm::Diff--is embedded within cloc. A fourth module, Digest::MD5, is used only if it is available. If cloc finds Regexp::Common or Algorithm::Diff installed locally it will use those installation. If it doesn&#39;t, cloc will install the parts of Regexp::Common and/or Algorithm:Diff it needs to temporary directories that are created at the start of a cloc run then removed when the run is complete. The necessary code from Regexp::Common v2.120 and Algorithm::Diff v1.1902 are embedded within the cloc source code (see subroutines &lt;code&gt;Install_Regexp_Common()&lt;/code&gt; and &lt;code&gt;Install_Algorithm_Diff()&lt;/code&gt; ). Only three lines are needed from Win32::Autoglob and these are included directly in cloc.&lt;/p&gt; 
&lt;p&gt;Additionally, cloc will use Digest::MD5 to validate uniqueness among equally-sized input files if Digest::MD5 is installed locally.&lt;/p&gt; 
&lt;p&gt;A parallel processing option, &lt;tt&gt;--processes=&lt;i&gt;N&lt;/i&gt;&lt;/tt&gt;, was introduced with cloc version 1.76 to enable faster runs on multi-core machines. However, to use it, one must have the module Parallel::ForkManager installed. This module does not work reliably on Windows so parallel processing will only work on Unix-like operating systems.&lt;/p&gt; 
&lt;p&gt;The Windows binary is built on a computer that has both Regexp::Common and Digest::MD5 installed locally. &lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/1%7D%7D%7D&quot;&gt;&lt;/a&gt; &lt;a name=&quot;building_exe&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/%7B%7B%7B1&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#___top&quot; title=&quot;click to go to top of document&quot;&gt;Building a Windows Executable ▲&lt;/a&gt;&lt;/h1&gt; 
&lt;h4&gt;Create your own executable&lt;/h4&gt; 
&lt;p&gt;The most robust option for creating a Windows executable of cloc is to use &lt;a href=&quot;http://www.activestate.com/perl-dev-kit&quot;&gt;ActiveState&#39;s Perl Development Kit&lt;/a&gt;. It includes a utility, &lt;code&gt;perlapp&lt;/code&gt;, which can build stand-alone Windows, Mac, and Linux binaries of Perl source code.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;http://www.indigostar.com/perl2exe/&quot;&gt;perl2exe&lt;/a&gt; will also do the trick. If you do have &lt;code&gt;perl2exe&lt;/code&gt;, modify lines 84-87 in the cloc source code for a minor code modification that is necessary to make a cloc Windows executable.&lt;/p&gt; 
&lt;p&gt;Otherwise, to build a Windows executable with &lt;code&gt;pp&lt;/code&gt; from &lt;code&gt;PAR::Packer&lt;/code&gt;, first install a Windows-based Perl distribution (for example Strawberry Perl or ActivePerl) following their instructions. Next, open a command prompt, aka a DOS window and install the PAR::Packer module. Finally, invoke the newly installed &lt;code&gt;pp&lt;/code&gt; command with the cloc source code to create an &lt;code&gt;.exe&lt;/code&gt; file:&lt;/p&gt; 
&lt;pre&gt;
C:&amp;gt; cpan -i Digest::MD5
C:&amp;gt; cpan -i Regexp::Common
C:&amp;gt; cpan -i Algorithm::Diff
C:&amp;gt; cpan -i PAR::Packer
C:&amp;gt; cpan -i Win32::LongPath
C:&amp;gt; pp -M Win32::LongPath -M Encode::Unicode -M Digest::MD5 -c -x -o cloc-2.04.exe cloc-2.04.pl
&lt;/pre&gt; 
&lt;p&gt;A variation on the instructions above is if you installed the portable version of Strawberry Perl, you will need to run &lt;code&gt;portableshell.bat&lt;/code&gt; first to properly set up your environment.&lt;/p&gt; 
&lt;p&gt;The Windows executable in the Releases section, &lt;tt&gt;cloc-2.04.exe&lt;/tt&gt;, was built on a 64 bit Windows 10 computer using &lt;a href=&quot;http://strawberryperl.com/&quot;&gt;Strawberry Perl&lt;/a&gt; 5.30.2 and &lt;a href=&quot;http://search.cpan.org/~rschupp/PAR-Packer-1.050/lib/pp.pm&quot;&gt;PAR::Packer&lt;/a&gt; to build the &lt;code&gt;.exe&lt;/code&gt;.&lt;/p&gt; 
&lt;h4&gt;Is the Windows executable safe to run? Does it have malware?&lt;/h4&gt; 
&lt;p&gt;Ideally, no one would need the Windows executable because they have a Perl interpreter installed on their machines and can run the cloc source file. On centrally-managed corporate Windows machines, however, this this may be difficult or impossible.&lt;/p&gt; 
&lt;p&gt;The Windows executable distributed with cloc is provided as a best-effort of a virus and malware-free &lt;code&gt;.exe&lt;/code&gt;. You are encouraged to run your own virus scanners against the executable and also check sites such &lt;a href=&quot;https://www.virustotal.com/&quot;&gt;https://www.virustotal.com/&lt;/a&gt; . The entries for recent versions are:&lt;/p&gt; 
&lt;p&gt;cloc-2.04.exe: &lt;a href=&quot;https://www.virustotal.com/gui/file/89cda0038bf4e13c6c13ebc1e60bec4dfad362e69ac8a5b8e2d5ebe3020359e1&quot;&gt;https://www.virustotal.com/gui/file/89cda0038bf4e13c6c13ebc1e60bec4dfad362e69ac8a5b8e2d5ebe3020359e1&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;cloc-2.02-winget.exe: (includes &lt;a href=&quot;https://github.com/AlDanial/cloc/pull/850&quot;&gt;PR 850&lt;/a&gt; to allow &lt;a href=&quot;https://github.com/AlDanial/cloc/issues/849&quot;&gt;running from a symlink on Windows&lt;/a&gt;) &lt;a href=&quot;https://www.virustotal.com/gui/file/be033061e091fea48a5bc9e8964cee0416ddd5b34bd5226a1c9aa4b30bdba66a?nocache=1&quot;&gt;https://www.virustotal.com/gui/file/be033061e091fea48a5bc9e8964cee0416ddd5b34bd5226a1c9aa4b30bdba66a?nocache=1&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;cloc-2.02.exe: &lt;a href=&quot;https://www.virustotal.com/gui/file/369ed76125f7399cd582d169adf39a2e08ae5066031fea0cc8b2836ea50e7ce2?nocache=1&quot;&gt;https://www.virustotal.com/gui/file/369ed76125f7399cd582d169adf39a2e08ae5066031fea0cc8b2836ea50e7ce2?nocache=1&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;cloc-2.00.exe: &lt;a href=&quot;https://www.virustotal.com/gui/file/7a234ef0cb495de1b5776acf88c5554e2bab1fb02725a5fb85756a6db3121c1f&quot;&gt;https://www.virustotal.com/gui/file/7a234ef0cb495de1b5776acf88c5554e2bab1fb02725a5fb85756a6db3121c1f&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;cloc-1.98.exe: &lt;a href=&quot;https://www.virustotal.com/gui/file/88615d193ec8c06f7ceec3cc1d661088af997798d87ddff331d9e9f9128a6782?nocache=1&quot;&gt;https://www.virustotal.com/gui/file/88615d193ec8c06f7ceec3cc1d661088af997798d87ddff331d9e9f9128a6782?nocache=1&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;cloc-1.96.1.exe: &lt;a href=&quot;https://www.virustotal.com/gui/file/00b1c9dbbfb920dabd374418e1b86d2c24b8cd2b8705aeb956dee910d0d75d45?nocache=1&quot;&gt;https://www.virustotal.com/gui/file/00b1c9dbbfb920dabd374418e1b86d2c24b8cd2b8705aeb956dee910d0d75d45?nocache=1&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;cloc-1.96.exe: &lt;a href=&quot;https://www.virustotal.com/gui/file/54bf5f46fbaba7949c4eb2d4837b03c774c0ba587448a5bad9b8efc0222b1583?nocache=1&quot;&gt;https://www.virustotal.com/gui/file/54bf5f46fbaba7949c4eb2d4837b03c774c0ba587448a5bad9b8efc0222b1583?nocache=1&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;cloc-1.94.exe: &lt;a href=&quot;https://www.virustotal.com/gui/file/b48a6002fb75fa66ec5d0c05a5c4d51f2ad22b5b025b7eb4e3945d18419c0952?nocache=1&quot;&gt;https://www.virustotal.com/gui/file/b48a6002fb75fa66ec5d0c05a5c4d51f2ad22b5b025b7eb4e3945d18419c0952?nocache=1&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;cloc-1.92.exe: &lt;a href=&quot;https://www.virustotal.com/gui/file/2668fcf8609c431e8934fe9e1866bc620c58d198c4eb262f1d3ef31ef4a690f7&quot;&gt;https://www.virustotal.com/gui/file/2668fcf8609c431e8934fe9e1866bc620c58d198c4eb262f1d3ef31ef4a690f7&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;cloc-1.90.exe: &lt;a href=&quot;https://www.virustotal.com/gui/file/d655caae55486f9bac39f7e3c7b7553bcfcfe2b88914c79bfc328055f22b8a37/detection&quot;&gt;https://www.virustotal.com/gui/file/d655caae55486f9bac39f7e3c7b7553bcfcfe2b88914c79bfc328055f22b8a37/detection&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;cloc-1.88.exe: &lt;a href=&quot;https://www.virustotal.com/gui/file/97d5d2631d1cccdbfd99267ab8a4cf5968816bbe52c0f9324e72e768857f642d/detection&quot;&gt;https://www.virustotal.com/gui/file/97d5d2631d1cccdbfd99267ab8a4cf5968816bbe52c0f9324e72e768857f642d/detection&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;cloc-1.86.exe: &lt;a href=&quot;https://www.virustotal.com/gui/file/1b2e189df1834411b34534db446330d1c379b4bc008af3042ee9ade818c6a1c8/detection&quot;&gt;https://www.virustotal.com/gui/file/1b2e189df1834411b34534db446330d1c379b4bc008af3042ee9ade818c6a1c8/detection&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;cloc-1.84.exe: &lt;a href=&quot;https://www.virustotal.com/gui/file/e73d490c1e4ae2f50ee174005614029b4fa2610dcb76988714839d7be68479af/detection&quot;&gt;https://www.virustotal.com/gui/file/e73d490c1e4ae2f50ee174005614029b4fa2610dcb76988714839d7be68479af/detection&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;cloc-1.82.exe: &lt;a href=&quot;https://www.virustotal.com/#/file/2e5fb443fdefd776d7b6b136a25e5ee2048991e735042897dbd0bf92efb16563/detection&quot;&gt;https://www.virustotal.com/#/file/2e5fb443fdefd776d7b6b136a25e5ee2048991e735042897dbd0bf92efb16563/detection&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;cloc-1.80.exe: &lt;a href=&quot;https://www.virustotal.com/#/file/9e547b01c946aa818ffad43b9ebaf05d3da08ed6ca876ef2b6847be3bf1cf8be/detection&quot;&gt;https://www.virustotal.com/#/file/9e547b01c946aa818ffad43b9ebaf05d3da08ed6ca876ef2b6847be3bf1cf8be/detection&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;cloc-1.78.exe: &lt;a href=&quot;https://www.virustotal.com/#/file/256ade3df82fa92febf2553853ed1106d96c604794606e86efd00d55664dd44f/detection&quot;&gt;https://www.virustotal.com/#/file/256ade3df82fa92febf2553853ed1106d96c604794606e86efd00d55664dd44f/detection&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;cloc-1.76.exe: &lt;a href=&quot;https://www.virustotal.com/#/url/c1b9b9fe909f91429f95d41e9a9928ab7c58b21351b3acd4249def2a61acd39d/detection&quot;&gt;https://www.virustotal.com/#/url/c1b9b9fe909f91429f95d41e9a9928ab7c58b21351b3acd4249def2a61acd39d/detection&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;cloc-1.74_x86.exe: &lt;a href=&quot;https://www.virustotal.com/#/file/b73dece71f6d3199d90d55db53a588e1393c8dbf84231a7e1be2ce3c5a0ec75b/detection&quot;&gt;https://www.virustotal.com/#/file/b73dece71f6d3199d90d55db53a588e1393c8dbf84231a7e1be2ce3c5a0ec75b/detection&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;cloc 1.72 exe: &lt;a href=&quot;https://www.virustotal.com/en/url/8fd2af5cd972f648d7a2d7917bc202492012484c3a6f0b48c8fd60a8d395c98c/analysis/&quot;&gt;https://www.virustotal.com/en/url/8fd2af5cd972f648d7a2d7917bc202492012484c3a6f0b48c8fd60a8d395c98c/analysis/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;cloc 1.70 exe: &lt;a href=&quot;https://www.virustotal.com/en/url/63edef209099a93aa0be1a220dc7c4c7ed045064d801e6d5daa84ee624fc0b4a/analysis/&quot;&gt;https://www.virustotal.com/en/url/63edef209099a93aa0be1a220dc7c4c7ed045064d801e6d5daa84ee624fc0b4a/analysis/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;cloc 1.68 exe: &lt;a href=&quot;https://www.virustotal.com/en/file/c484fc58615fc3b0d5569b9063ec1532980281c3155e4a19099b11ef1c24443b/analysis/&quot;&gt;https://www.virustotal.com/en/file/c484fc58615fc3b0d5569b9063ec1532980281c3155e4a19099b11ef1c24443b/analysis/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;cloc 1.66 exe: &lt;a href=&quot;https://www.virustotal.com/en/file/54d6662e59b04be793dd10fa5e5edf7747cf0c0cc32f71eb67a3cf8e7a171d81/analysis/1453601367/&quot;&gt;https://www.virustotal.com/en/file/54d6662e59b04be793dd10fa5e5edf7747cf0c0cc32f71eb67a3cf8e7a171d81/analysis/1453601367/&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;Why is the Windows executable so large?&lt;/h4&gt; 
&lt;p&gt;Windows executables of cloc versions 1.60 and earlier, created with perl2exe as noted above, are about 1.6 MB, while versions 1.62 and 1.54, created with &lt;code&gt;PAR::Packer&lt;/code&gt;, are 11 MB. Version 1.66, built with a newer version of &lt;code&gt;PAR::Packer&lt;/code&gt;, is about 5.5 MB. Why are the &lt;code&gt;PAR::Packer&lt;/code&gt;, executables so much larger than those built with perl2exe? My theory is that perl2exe uses smarter tree pruning logic than &lt;code&gt;PAR::Packer&lt;/code&gt;, but that&#39;s pure speculation.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/1%7D%7D%7D&quot;&gt;&lt;/a&gt; &lt;a name=&quot;Basic_Use&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/%7B%7B%7B1&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#___top&quot; title=&quot;click to go to top of document&quot;&gt;Basic Use ▲&lt;/a&gt;&lt;/h1&gt; 
&lt;p&gt;cloc is a command line program that takes file, directory, and/or archive names as inputs. Here&#39;s an example of running cloc against the Perl v5.22.0 source distribution:&lt;/p&gt; 
&lt;pre&gt;
prompt&amp;gt; cloc perl-5.22.0.tar.gz
    5605 text files.
    5386 unique files.
    2176 files ignored.

https://github.com/AlDanial/cloc v 1.65  T=25.49 s (134.7 files/s, 51980.3 lines/s)
-----------------------------------------------------------------------------------
Language                         files          blank        comment           code
-----------------------------------------------------------------------------------
Perl                              2892         136396         184362         536445
C                                  130          24676          33684         155648
C/C++ Header                       148           9766          16569         147858
Bourne Shell                       112           4044           6796          42668
Pascal                               8            458           1603           8592
XML                                 33            142              0           2410
YAML                                49             20             15           2078
C++                                 10            313            277           2033
make                                 4            426            488           1986
Prolog                              12            438              2           1146
JSON                                14              1              0           1037
yacc                                 1             85             76            998
Windows Message File                 1            102             11            489
DOS Batch                           14             92             41            389
Windows Resource File                3             10              0             85
D                                    1              5              7              8
Lisp                                 2              0              3              4
-----------------------------------------------------------------------------------
SUM:                              3434         176974         243934         903874
-----------------------------------------------------------------------------------

&lt;/pre&gt; 
&lt;p&gt;To run cloc on Windows computers, open up a command (aka DOS) window and invoke cloc.exe from the command line there. Alternatively, try ClocViewer, the GUI wrapper around cloc found at &lt;a href=&quot;https://github.com/Roemer/ClocViewer&quot;&gt;https://github.com/Roemer/ClocViewer&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;See also &lt;a href=&quot;https://github.com/jmensch1/codeflower&quot;&gt;https://github.com/jmensch1/codeflower&lt;/a&gt; for a graphical rendering of cloc results. &lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/1%7D%7D%7D&quot;&gt;&lt;/a&gt; &lt;a name=&quot;Options&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/%7B%7B%7B1&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#___top&quot; title=&quot;click to go to top of document&quot;&gt;Options ▲&lt;/a&gt;&lt;/h1&gt; 
&lt;pre&gt;
prompt&amp;gt; cloc --help

Usage: cloc [options] &amp;lt;file(s)/dir(s)/git hash(es)&amp;gt; | &amp;lt;set 1&amp;gt; &amp;lt;set 2&amp;gt; | &amp;lt;report files&amp;gt;

 Count, or compute differences of, physical lines of source code in the
 given files (may be archives such as compressed tarballs or zip files,
 or git commit hashes or branch names) and/or recursively below the
 given directories.

 Input Options
   --extract-with=&amp;lt;cmd&amp;gt;      This option is only needed if cloc is unable
                             to figure out how to extract the contents of
                             the input file(s) by itself.
                             Use &amp;lt;cmd&amp;gt; to extract binary archive files (e.g.:
                             .tar.gz, .zip, .Z).  Use the literal &#39;&amp;gt;FILE&amp;lt;&#39; as
                             a stand-in for the actual file(s) to be
                             extracted.  For example, to count lines of code
                             in the input files
                                gcc-4.2.tar.gz  perl-5.8.8.tar.gz
                             on Unix use
                               --extract-with=&#39;gzip -dc &amp;gt;FILE&amp;lt; | tar xf -&#39;
                             or, if you have GNU tar,
                               --extract-with=&#39;tar zxf &amp;gt;FILE&amp;lt;&#39;
                             and on Windows use, for example:
                               --extract-with=&quot;\&quot;c:\Program Files\WinZip\WinZip32.exe\&quot; -e -o &amp;gt;FILE&amp;lt; .&quot;
                             (if WinZip is installed there).
   --list-file=&amp;lt;file&amp;gt;        Take the list of file and/or directory names to
                             process from &amp;lt;file&amp;gt;, which has one file/directory
                             name per line.  Only exact matches are counted;
                             relative path names will be resolved starting from
                             the directory where cloc is invoked.  Set &amp;lt;file&amp;gt;
                             to - to read file names from a STDIN pipe.
                             See also --exclude-list-file.
   --diff-list-file=&amp;lt;file&amp;gt;   Take the pairs of file names to be diff&#39;ed from
                             &amp;lt;file&amp;gt;, whose format matches the output of
                             --diff-alignment.  (Run with that option to
                             see a sample.)  The language identifier at the
                             end of each line is ignored.  This enables --diff
                             mode and bypasses file pair alignment logic.
   --vcs=&amp;lt;VCS&amp;gt;               Invoke a system call to &amp;lt;VCS&amp;gt; to obtain a list of
                             files to work on.  If &amp;lt;VCS&amp;gt; is &#39;git&#39;, then will
                             invoke &#39;git ls-files&#39; to get a file list and
                             &#39;git submodule status&#39; to get a list of submodules
                             whose contents will be ignored.  See also --git
                             which accepts git commit hashes and branch names.
                             If &amp;lt;VCS&amp;gt; is &#39;svn&#39; then will invoke &#39;svn list -R&#39;.
                             The primary benefit is that cloc will then skip
                             files explicitly excluded by the versioning tool
                             in question, ie, those in .gitignore or have the
                             svn:ignore property.
                             Alternatively &amp;lt;VCS&amp;gt; may be any system command
                             that generates a list of files.
                             Note:  cloc must be in a directory which can read
                             the files as they are returned by &amp;lt;VCS&amp;gt;.  cloc will
                             not download files from remote repositories.
                             &#39;svn list -R&#39; may refer to a remote repository
                             to obtain file names (and therefore may require
                             authentication to the remote repository), but
                             the files themselves must be local.
                             Setting &amp;lt;VCS&amp;gt; to &#39;auto&#39; selects between &#39;git&#39;
                             and &#39;svn&#39; (or neither) depending on the presence
                             of a .git or .svn subdirectory below the directory
                             where cloc is invoked.
   --unicode                 Check binary files to see if they contain Unicode
                             expanded ASCII text.  This causes performance to
                             drop noticeably.

 Processing Options
   --autoconf                Count .in files (as processed by GNU autoconf) of
                             recognized languages.  See also --no-autogen.
   --by-file                 Report results for every source file encountered.
   --by-file-by-lang         Report results for every source file encountered
                             in addition to reporting by language.
   --config &amp;lt;file&amp;gt;           Read command line switches from &amp;lt;file&amp;gt; instead of
                             the default location of /home/al/.config/cloc/options.txt.
                             The file should contain one switch, along with
                             arguments (if any), per line.  Blank lines and lines
                             beginning with &#39;#&#39; are skipped.  Options given on
                             the command line take priority over entries read from
                             the file.
   --count-and-diff &amp;lt;set1&amp;gt; &amp;lt;set2&amp;gt;
                             First perform direct code counts of source file(s)
                             of &amp;lt;set1&amp;gt; and &amp;lt;set2&amp;gt; separately, then perform a diff
                             of these.  Inputs may be pairs of files, directories,
                             or archives.  If --out or --report-file is given,
                             three output files will be created, one for each
                             of the two counts and one for the diff.  See also
                             --diff, --diff-alignment, --diff-timeout,
                             --ignore-case, --ignore-whitespace.
   --diff &amp;lt;set1&amp;gt; &amp;lt;set2&amp;gt;      Compute differences in code and comments between
                             source file(s) of &amp;lt;set1&amp;gt; and &amp;lt;set2&amp;gt;.  The inputs
                             may be any mix of files, directories, archives,
                             or git commit hashes.  Use --diff-alignment to
                             generate a list showing which file pairs where
                             compared.  When comparing git branches, only files
                             which have changed in either commit are compared.
                             See also --git, --count-and-diff, --diff-alignment,
                             --diff-list-file, --diff-timeout, --ignore-case,
                             --ignore-whitespace.
   --diff-timeout &amp;lt;N&amp;gt;        Ignore files which take more than &amp;lt;N&amp;gt; seconds
                             to process.  Default is 10 seconds.  Setting &amp;lt;N&amp;gt;
                             to 0 allows unlimited time.  (Large files with many
                             repeated lines can cause Algorithm::Diff::sdiff()
                             to take hours.) See also --timeout.
   --docstring-as-code       cloc considers docstrings to be comments, but this is
                             not always correct as docstrings represent regular
                             strings when they appear on the right hand side of an
                             assignment or as function arguments.  This switch
                             forces docstrings to be counted as code.
   --follow-links            [Unix only] Follow symbolic links to directories
                             (sym links to files are always followed).
                             See also --stat.
   --force-lang=&amp;lt;lang&amp;gt;[,&amp;lt;ext&amp;gt;]
                             Process all files that have a &amp;lt;ext&amp;gt; extension
                             with the counter for language &amp;lt;lang&amp;gt;.  For
                             example, to count all .f files with the
                             Fortran 90 counter (which expects files to
                             end with .f90) instead of the default Fortran 77
                             counter, use
                               --force-lang=&quot;Fortran 90,f&quot;
                             If &amp;lt;ext&amp;gt; is omitted, every file will be counted
                             with the &amp;lt;lang&amp;gt; counter.  This option can be
                             specified multiple times (but that is only
                             useful when &amp;lt;ext&amp;gt; is given each time).
                             See also --script-lang, --lang-no-ext.
   --force-lang-def=&amp;lt;file&amp;gt;   Load language processing filters from &amp;lt;file&amp;gt;,
                             then use these filters instead of the built-in
                             filters.  Note:  languages which map to the same
                             file extension (for example:
                             MATLAB/Mathematica/Objective-C/MUMPS/Mercury;
                             Pascal/PHP; Lisp/OpenCL; Lisp/Julia; Perl/Prolog)
                             will be ignored as these require additional
                             processing that is not expressed in language
                             definition files.  Use --read-lang-def to define
                             new language filters without replacing built-in
                             filters (see also --write-lang-def,
                             --write-lang-def-incl-dup).
   --git                     Forces the inputs to be interpreted as git targets
                             (commit hashes, branch names, et cetera) if these
                             are not first identified as file or directory
                             names.  This option overrides the --vcs=git logic
                             if this is given; in other words, --git gets its
                             list of files to work on directly from git using
                             the hash or branch name rather than from
                             &#39;git ls-files&#39;.  This option can be used with
                             --diff to perform line count diffs between git
                             commits, or between a git commit and a file,
                             directory, or archive.  Use -v/--verbose to see
                             the git system commands cloc issues.
   --git-diff-rel            Same as --git --diff, or just --diff if the inputs
                             are recognized as git targets.  Only files which
                             have changed in either commit are compared.
   --git-diff-all            Git diff strategy #2:  compare all files in the
                             repository between the two commits.
   --ignore-whitespace       Ignore horizontal white space when comparing files
                             with --diff.  See also --ignore-case.
   --ignore-case             Ignore changes in case within file contents;
                             consider upper- and lowercase letters equivalent
                             when comparing files with --diff.  See also
                             --ignore-whitespace.
   --ignore-case-ext         Ignore case of file name extensions.  This will
                             cause problems counting some languages
                             (specifically, .c and .C are associated with C and
                             C++; this switch would count .C files as C rather
                             than C++ on *nix operating systems).  File name
                             case insensitivity is always true on Windows.
   --lang-no-ext=&amp;lt;lang&amp;gt;      Count files without extensions using the &amp;lt;lang&amp;gt;
                             counter.  This option overrides internal logic
                             for files without extensions (where such files
                             are checked against known scripting languages
                             by examining the first line for #!).  See also
                             --force-lang, --script-lang.
   --max-file-size=&amp;lt;MB&amp;gt;      Skip files larger than &amp;lt;MB&amp;gt; megabytes when
                             traversing directories.  By default, &amp;lt;MB&amp;gt;=100.
                             cloc&#39;s memory requirement is roughly twenty times
                             larger than the largest file so running with
                             files larger than 100 MB on a computer with less
                             than 2 GB of memory will cause problems.
                             Note:  this check does not apply to files
                             explicitly passed as command line arguments.
   --no-autogen[=list]       Ignore files generated by code-production systems
                             such as GNU autoconf.  To see a list of these files
                             (then exit), run with --no-autogen list
                             See also --autoconf.
   --original-dir            [Only effective in combination with
                             --strip-comments]  Write the stripped files
                             to the same directory as the original files.
   --read-binary-files       Process binary files in addition to text files.
                             This is usually a bad idea and should only be
                             attempted with text files that have embedded
                             binary data.
   --read-lang-def=&amp;lt;file&amp;gt;    Load new language processing filters from &amp;lt;file&amp;gt;
                             and merge them with those already known to cloc.
                             If &amp;lt;file&amp;gt; defines a language cloc already knows
                             about, cloc&#39;s definition will take precedence.
                             Use --force-lang-def to over-ride cloc&#39;s
                             definitions (see also --write-lang-def,
                             --write-lang-def-incl-dup).
   --script-lang=&amp;lt;lang&amp;gt;,&amp;lt;s&amp;gt;  Process all files that invoke &amp;lt;s&amp;gt; as a #!
                             scripting language with the counter for language
                             &amp;lt;lang&amp;gt;.  For example, files that begin with
                                #!/usr/local/bin/perl5.8.8
                             will be counted with the Perl counter by using
                                --script-lang=Perl,perl5.8.8
                             The language name is case insensitive but the
                             name of the script language executable, &amp;lt;s&amp;gt;,
                             must have the right case.  This option can be
                             specified multiple times.  See also --force-lang,
                             --lang-no-ext.
   --sdir=&amp;lt;dir&amp;gt;              Use &amp;lt;dir&amp;gt; as the scratch directory instead of
                             letting File::Temp chose the location.  Files
                             written to this location are not removed at
                             the end of the run (as they are with File::Temp).
   --skip-uniqueness         Skip the file uniqueness check.  This will give
                             a performance boost at the expense of counting
                             files with identical contents multiple times
                             (if such duplicates exist).
   --stat                    Some file systems (AFS, CD-ROM, FAT, HPFS, SMB)
                             do not have directory &#39;nlink&#39; counts that match
                             the number of its subdirectories.  Consequently
                             cloc may undercount or completely skip the
                             contents of such file systems.  This switch forces
                             File::Find to stat directories to obtain the
                             correct count.  File search speed will decrease.
                             See also --follow-links.
   --stdin-name=&amp;lt;file&amp;gt;       Give a file name to use to determine the language
                             for standard input.  (Use - as the input name to
                             receive source code via STDIN.)
   --strip-comments=&amp;lt;ext&amp;gt;    For each file processed, write to the current
                             directory a version of the file which has blank
                             and commented lines removed (in-line comments
                             persist).  The name of each stripped file is the
                             original file name with .&amp;lt;ext&amp;gt; appended to it.
                             It is written to the current directory unless
                             --original-dir is on.
   --strip-str-comments      Replace comment markers embedded in strings with
                             &#39;xx&#39;.  This attempts to work around a limitation
                             in Regexp::Common::Comment where comment markers
                             embedded in strings are seen as actual comment
                             markers and not strings, often resulting in a
                             &#39;Complex regular subexpression recursion limit&#39;
                             warning and incorrect counts.  There are two
                             disadvantages to using this switch:  1/code count
                             performance drops, and 2/code generated with
                             --strip-comments will contain different strings
                             where ever embedded comments are found.
   --sum-reports             Input arguments are report files previously
                             created with the --report-file option in plain
                             format (eg. not JSON, YAML, XML, or SQL).
                             Makes a cumulative set of results containing the
                             sum of data from the individual report files.
   --timeout &amp;lt;N&amp;gt;             Ignore files which take more than &amp;lt;N&amp;gt; seconds
                             to process at any of the language&#39;s filter stages.
                             The default maximum number of seconds spent on a
                             filter stage is the number of lines in the file
                             divided by one thousand.  Setting &amp;lt;N&amp;gt; to 0 allows
                             unlimited time.  See also --diff-timeout.
   --processes=NUM           [Available only on systems with a recent version
                             of the Parallel::ForkManager module.  Not
                             available on Windows.] Sets the maximum number of
                             cores that cloc uses.  The default value of 0
                             disables multiprocessing.
   --unix                    Override the operating system autodetection
                             logic and run in UNIX mode.  See also
                             --windows, --show-os.
   --use-sloccount           If SLOCCount is installed, use its compiled
                             executables c_count, java_count, pascal_count,
                             php_count, and xml_count instead of cloc&#39;s
                             counters.  SLOCCount&#39;s compiled counters are
                             substantially faster than cloc&#39;s and may give
                             a performance improvement when counting projects
                             with large files.  However, these cloc-specific
                             features will not be available: --diff,
                             --count-and-diff, --strip-comments, --unicode.
   --windows                 Override the operating system autodetection
                             logic and run in Microsoft Windows mode.
                             See also --unix, --show-os.

 Filter Options
   --include-content=&amp;lt;regex&amp;gt; Only count files containing text that matches the
                             given regular expression.
   --exclude-content=&amp;lt;regex&amp;gt; Exclude files containing text that matches the given
                             regular expression.
   --exclude-dir=&amp;lt;D1&amp;gt;[,D2,]  Exclude the given comma separated directories
                             D1, D2, D3, et cetera, from being scanned.  For
                             example  --exclude-dir=.cache,test  will skip
                             all files and subdirectories that have /.cache/
                             or /test/ as their parent directory.
                             Directories named .bzr, .cvs, .hg, .git, .svn,
                             and .snapshot are always excluded.
                             This option only works with individual directory
                             names so including file path separators is not
                             allowed.  Use --fullpath and --not-match-d=&amp;lt;regex&amp;gt;
                             to supply a regex matching multiple subdirectories.
   --exclude-ext=&amp;lt;ext1&amp;gt;[,&amp;lt;ext2&amp;gt;[...]]
                             Do not count files having the given file name
                             extensions.
   --exclude-lang=&amp;lt;L1&amp;gt;[,L2[...]]
                             Exclude the given comma separated languages
                             L1, L2, L3, et cetera, from being counted.
   --exclude-list-file=&amp;lt;file&amp;gt;  Ignore files and/or directories whose names
                             appear in &amp;lt;file&amp;gt;.  &amp;lt;file&amp;gt; should have one file
                             name per line.  Only exact matches are ignored;
                             relative path names will be resolved starting from
                             the directory where cloc is invoked.
                             See also --list-file.
   --fullpath                Modifies the behavior of --match-f, --not-match-f,
                             and --not-match-d to include the file&#39;s path
                             in the regex, not just the file&#39;s basename.
                             (This does not expand each file to include its
                             absolute path, instead it uses as much of
                             the path as is passed in to cloc.)
                             Note:  --match-d always looks at the full
                             path and therefore is unaffected by --fullpath.
   --include-ext=&amp;lt;ext1&amp;gt;[,ext2[...]]
                             Count only languages having the given comma
                             separated file extensions.  Use --show-ext to
                             see the recognized extensions.
   --include-lang=&amp;lt;L1&amp;gt;[,L2[...]]
                             Count only the given comma separated languages
                             L1, L2, L3, et cetera.  Use --show-lang to see
                             the list of recognized languages.
   --match-d=&amp;lt;regex&amp;gt;         Only count files in directories matching the Perl
                             regex.  For example
                               --match-d=&#39;/(src|include)/&#39;
                             only counts files in directories containing
                             /src/ or /include/.  Unlike --not-match-d,
                             --match-f, and --not-match-f, --match-d always
                             compares the fully qualified path against the
                             regex.
   --not-match-d=&amp;lt;regex&amp;gt;     Count all files except those in directories
                             matching the Perl regex.  Only the trailing
                             directory name is compared, for example, when
                             counting in /usr/local/lib, only &#39;lib&#39; is
                             compared to the regex.
                             Add --fullpath to compare parent directories to
                             the regex.
                             Do not include file path separators at the
                             beginning or end of the regex.
   --match-f=&amp;lt;regex&amp;gt;         Only count files whose basenames match the Perl
                             regex.  For example
                               --match-f=&#39;^[Ww]idget&#39;
                             only counts files that start with Widget or widget.
                             Add --fullpath to include parent directories
                             in the regex instead of just the basename.
   --not-match-f=&amp;lt;regex&amp;gt;     Count all files except those whose basenames
                             match the Perl regex.  Add --fullpath to include
                             parent directories in the regex instead of just
                             the basename.
   --skip-archive=&amp;lt;regex&amp;gt;    Ignore files that end with the given Perl regular
                             expression.  For example, if given
                               --skip-archive=&#39;(zip|tar(.(gz|Z|bz2|xz|7z))?)&#39;
                             the code will skip files that end with .zip,
                             .tar, .tar.gz, .tar.Z, .tar.bz2, .tar.xz, and
                             .tar.7z.
   --skip-win-hidden         On Windows, ignore hidden files.

 Debug Options
   --categorized=&amp;lt;file&amp;gt;      Save file sizes in bytes, identified languages
                             and names of categorized files to &amp;lt;file&amp;gt;.
   --counted=&amp;lt;file&amp;gt;          Save names of processed source files to &amp;lt;file&amp;gt;.
   --diff-alignment=&amp;lt;file&amp;gt;   Write to &amp;lt;file&amp;gt; a list of files and file pairs
                             showing which files were added, removed, and/or
                             compared during a run with --diff.  This switch
                             forces the --diff mode on.
   --explain=&amp;lt;lang&amp;gt;          Print the filters used to remove comments for
                             language &amp;lt;lang&amp;gt; and exit.  In some cases the
                             filters refer to Perl subroutines rather than
                             regular expressions.  An examination of the
                             source code may be needed for further explanation.
   --help                    Print this usage information and exit.
   --found=&amp;lt;file&amp;gt;            Save names of every file found to &amp;lt;file&amp;gt;.
   --ignored=&amp;lt;file&amp;gt;          Save names of ignored files and the reason they
                             were ignored to &amp;lt;file&amp;gt;.
   --print-filter-stages     Print processed source code before and after
                             each filter is applied.
   --show-ext[=&amp;lt;ext&amp;gt;]        Print information about all known (or just the
                             given) file extensions and exit.
   --show-lang[=&amp;lt;lang&amp;gt;]      Print information about all known (or just the
                             given) languages and exit.
   --show-os                 Print the value of the operating system mode
                             and exit.  See also --unix, --windows.
   -v[=&amp;lt;n&amp;gt;]                  Verbose switch (optional numeric value).
   -verbose[=&amp;lt;n&amp;gt;]            Long form of -v.
   --version                 Print the version of this program and exit.
   --write-lang-def=&amp;lt;file&amp;gt;   Writes to &amp;lt;file&amp;gt; the language processing filters
                             then exits.  Useful as a first step to creating
                             custom language definitions. Note: languages which
                             map to the same file extension will be excluded.
                             (See also --force-lang-def, --read-lang-def).
   --write-lang-def-incl-dup=&amp;lt;file&amp;gt;
                             Same as --write-lang-def, but includes duplicated
                             extensions.  This generates a problematic language
                             definition file because cloc will refuse to use
                             it until duplicates are removed.

 Output Options
   --3                       Print third-generation language output.
                             (This option can cause report summation to fail
                             if some reports were produced with this option
                             while others were produced without it.)
   --by-percent  X           Instead of comment and blank line counts, show
                             these values as percentages based on the value
                             of X in the denominator:
                                X = &#39;c&#39;   -&amp;gt; # lines of code
                                X = &#39;cm&#39;  -&amp;gt; # lines of code + comments
                                X = &#39;cb&#39;  -&amp;gt; # lines of code + blanks
                                X = &#39;cmb&#39; -&amp;gt; # lines of code + comments + blanks
                             For example, if using method &#39;c&#39; and your code
                             has twice as many lines of comments as lines
                             of code, the value in the comment column will
                             be 200%.  The code column remains a line count.
   --csv                     Write the results as comma separated values.
   --csv-delimiter=&amp;lt;C&amp;gt;       Use the character &amp;lt;C&amp;gt; as the delimiter for comma
                             separated files instead of ,.  This switch forces --csv to be on.
   --file-encoding=&amp;lt;E&amp;gt;       Write output files using the &amp;lt;E&amp;gt; encoding instead of
                             the default ASCII (&amp;lt;E&amp;gt; = &#39;UTF-7&#39;).  Examples: &#39;UTF-16&#39;,
                             &#39;euc-kr&#39;, &#39;iso-8859-16&#39;.  Known encodings can be
                             printed with
                               perl -MEncode -e &#39;print join(&quot;\n&quot;, Encode-&amp;gt;encodings(&quot;:all&quot;)), &quot;\n&quot;&#39;
   --hide-rate               Do not show line and file processing rates in the
                             output header. This makes output deterministic.
   --json                    Write the results as JavaScript Object Notation
                             (JSON) formatted output.
   --md                      Write the results as Markdown-formatted text.
   --out=&amp;lt;file&amp;gt;              Synonym for --report-file=&amp;lt;file&amp;gt;.
   --progress-rate=&amp;lt;n&amp;gt;       Show progress update after every &amp;lt;n&amp;gt; files are
                             processed (default &amp;lt;n&amp;gt;=100).  Set &amp;lt;n&amp;gt; to 0 to
                             suppress progress output (useful when redirecting
                             output to STDOUT).
   --quiet                   Suppress all information messages except for
                             the final report.
   --report-file=&amp;lt;file&amp;gt;      Write the results to &amp;lt;file&amp;gt; instead of STDOUT.
   --sql=&amp;lt;file&amp;gt;              Write results as SQL create and insert statements
                             which can be read by a database program such as
                             SQLite.  If &amp;lt;file&amp;gt; is -, output is sent to STDOUT.
   --sql-append              Append SQL insert statements to the file specified
                             by --sql and do not generate table creation
                             statements.  Only valid with the --sql option.
   --sql-project=&amp;lt;name&amp;gt;      Use &amp;lt;name&amp;gt; as the project identifier for the
                             current run.  Only valid with the --sql option.
   --sql-style=&amp;lt;style&amp;gt;       Write SQL statements in the given style instead
                             of the default SQLite format.  Styles include
                             &#39;Oracle&#39; and &#39;Named_Columns&#39;.
   --sum-one                 For plain text reports, show the SUM: output line
                             even if only one input file is processed.
   --xml                     Write the results in XML.
   --xsl=&amp;lt;file&amp;gt;              Reference &amp;lt;file&amp;gt; as an XSL stylesheet within
                             the XML output.  If &amp;lt;file&amp;gt; is 1 (numeric one),
                             writes a default stylesheet, cloc.xsl (or
                             cloc-diff.xsl if --diff is also given).
                             This switch forces --xml on.
   --yaml                    Write the results in YAML.
&lt;/pre&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/1%7D%7D%7D&quot;&gt;&lt;/a&gt; &lt;a name=&quot;Languages&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/%7B%7B%7B1&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#___top&quot; title=&quot;click to go to top of document&quot;&gt;Recognized Languages ▲&lt;/a&gt;&lt;/h1&gt; 
&lt;pre&gt;
prompt&amp;gt; cloc --show-lang

ABAP                       (abap)
ActionScript               (as)
Ada                        (ada, adb, ads, pad)
ADSO/IDSM                  (adso)
Agda                       (agda, lagda)
AMPLE                      (ample, dofile, startup)
AnsProlog                  (lp)
Ant                        (build.xml, build.xml)
ANTLR Grammar              (g, g4)
Apex Class                 (cls)
Apex Trigger               (trigger)
APL                        (apl, apla, aplc, aplf, apli, apln, aplo, dyalog, dyapp, mipage)
AppleScript                (applescript)
Arduino Sketch             (ino)
ArkTs                      (ets)
Arturo                     (art)
AsciiDoc                   (adoc, asciidoc)
ASP                        (asa, ashx, asp, axd)
ASP.NET                    (asax, ascx, asmx, aspx, master, sitemap, webinfo)
AspectJ                    (aj)
Assembly                   (a51, asm, nasm, S, s)
Astro                      (astro)
Asymptote                  (asy)
AutoHotkey                 (ahk, ahkl)
awk                        (auk, awk, gawk, mawk, nawk)
Bazel                      (BUILD)
BizTalk Orchestration      (odx)
BizTalk Pipeline           (btp)
Blade                      (blade, blade.php)
Bourne Again Shell         (bash)
Bourne Shell               (sh)
BrightScript               (brs)
builder                    (xml.builder)
C                          (c, cats, ec, idc, pgc)
C Shell                    (csh, tcsh)
C#                         (cs)
C# Designer                (designer.cs)
C++                        (C, c++, c++m, cc, ccm, CPP, cpp, cppm, cxx, cxxm, h++, inl, ipp, ixx, pcc, tcc, tpp)
C/C++ Header               (H, h, hh, hpp, hxx)
Cairo                      (cairo)
Cake Build Script          (cake)
Carbon                     (carbon)
CCS                        (ccs)
Chapel                     (chpl)
Circom                     (circom)
Clean                      (dcl, icl)
Clojure                    (boot, cl2, clj, cljs.hl, cljscm, cljx, hic, riemann.config)
ClojureC                   (cljc)
ClojureScript              (cljs)
CMake                      (cmake, cmake.in, CMakeLists.txt)
COBOL                      (CBL, cbl, ccp, COB, cob, cobol, cpy)
CoCoA 5                    (c5, cocoa5, cocoa5server, cpkg5)
CoffeeScript               (_coffee, cakefile, cjsx, coffee, iced)
ColdFusion                 (cfm, cfml)
ColdFusion CFScript        (cfc)
Constraint Grammar         (cg3, rlx)
Containerfile              (Containerfile)
Coq                        (v)
Crystal                    (cr)
CSON                       (cson)
CSS                        (css)
CSV                        (csv)
Cucumber                   (feature)
CUDA                       (cu, cuh)
Cython                     (pxd, pxi, pyx)
D                          (d)
Dafny                      (dfy)
DAL                        (da)
Dart                       (dart)
Delphi Form                (dfm)
DenizenScript              (dsc)
Derw                       (derw)
dhall                      (dhall)
DIET                       (dt)
diff                       (diff, patch)
DITA                       (dita)
Dockerfile                 (Dockerfile, dockerfile)
DOORS Extension Language   (dxl)
DOS Batch                  (BAT, bat, BTM, btm, CMD, cmd)
Drools                     (drl)
DTD                        (dtd)
dtrace                     (d)
ECPP                       (ecpp)
EEx                        (eex)
EJS                        (ejs)
Elixir                     (ex, exs)
Elm                        (elm)
Embedded Crystal           (ecr)
ERB                        (ERB, erb)
Erlang                     (app.src, emakefile, erl, hrl, rebar.config, rebar.config.lock, rebar.lock, xrl, yrl)
Expect                     (exp)
F#                         (fsi, fs, fs)
F# Script                  (fsx)
Fennel                     (fnl)
Finite State Language      (fsl, jssm)
Fish Shell                 (fish)
Flatbuffers                (fbs)
Focus                      (focexec)
Forth                      (4th, e4, f83, fb, forth, fpm, fr, frt, ft, fth, rx, fs, f, for)
Fortran 77                 (F, F77, f77, FOR, FTN, ftn, pfo, f, for)
Fortran 90                 (F90, f90)
Fortran 95                 (F95, f95)
Freemarker Template        (ftl)
Futhark                    (fut)
FXML                       (fxml)
GDScript                   (gd)
Gencat NLS                 (msg)
Glade                      (glade, ui)
Gleam                      (gleam)
Glimmer JavaScript         (gjs)
Glimmer TypeScript         (gts)
GLSL                       (comp, fp, frag, frg, fsh, fshader, geo, geom, glsl, glslv, gshader, tesc, tese, vert, vrx, vsh, vshader)
Go                         (go, ʕ◔ϖ◔ʔ)
Godot Resource             (tres)
Godot Scene                (tscn)
Godot Shaders              (gdshader)
Gradle                     (gradle, gradle.kts)
Grails                     (gsp)
GraphQL                    (gql, graphql, graphqls)
Groovy                     (gant, groovy, grt, gtpl, gvy, jenkinsfile)
Haml                       (haml, haml.deface)
Handlebars                 (handlebars, hbs)
Harbour                    (hb)
Hare                       (ha)
Haskell                    (hs, hsc, lhs)
Haxe                       (hx, hxsl)
HCL                        (hcl, nomad, tf, tfvars)
HLSL                       (cg, cginc, fxh, hlsl, hlsli, shader)
HolyC                      (HC)
Hoon                       (hoon)
HTML                       (htm, html, html.hl, xht)
HTML EEx                   (heex)
IDL                        (dlm, idl, pro)
Idris                      (idr)
Igor Pro                   (ipf)
Imba                       (imba)
INI                        (buildozer.spec, editorconfig, ini, lektorproject, prefs)
InstallShield              (ism)
IPL                        (ipl)
Jai                        (jai)
Janet                      (janet)
Java                       (java)
JavaScript                 (_js, bones, cjs, es6, jake, jakefile, js, jsb, jscad, jsfl, jsm, jss, mjs, njs, pac, sjs, ssjs, xsjs, xsjslib)
JavaServer Faces           (jsf)
JCL                        (jcl)
Jinja Template             (j2, jinja, jinja2)
JSON                       (arcconfig, avsc, composer.lock, geojson, gltf, har, htmlhintrc, json, json-tmlanguage, jsonl, mcmeta, mcmod.info, tern-config, tern-project, tfstate, tfstate.backup, topojson, watchmanconfig, webapp, webmanifest, yyp)
JSON5                      (json5)
JSP                        (jsp, jspf)
JSX                        (jsx)
Julia                      (jl)
Juniper Junos              (junos)
Jupyter Notebook           (ipynb)
Kermit                     (ksc)
Korn Shell                 (ksh)
Kotlin                     (kt, ktm, kts)
kvlang                     (kv)
Lean                       (hlean, lean)
Lem                        (lem)
LESS                       (less)
lex                        (l, lex)
LFE                        (lfe)
Linker Script              (ld)
liquid                     (liquid)
Lisp                       (asd, el, lisp, lsp, cl, jl)
Literate Idris             (lidr)
LiveLink OScript           (oscript)
LLVM IR                    (ll)
Logos                      (x, xm)
Logtalk                    (lgt, logtalk)
Lua                        (lua, nse, p8, pd_lua, rbxs, wlua)
Luau                       (luau)
m4                         (ac, m4)
make                       (am, Gnumakefile, gnumakefile, Makefile, makefile, mk)
Mako                       (mako, mao)
Markdown                   (contents.lr, markdown, md, mdown, mdwn, mdx, mkd, mkdn, mkdown, ronn, workbook)
Mathematica                (cdf, ma, mathematica, mt, nbp, wl, wlt, m)
MATLAB                     (m)
Maven                      (pom, pom.xml)
Meson                      (meson.build)
Metal                      (metal)
Modelica                   (mo)
Modula3                    (i3, ig, m3, mg)
Mojo                       (mojo, 🔥)
Mojom                      (mojom)
MoonBit                    (mbt, mbti, mbtx, mbty)
MSBuild script             (btproj, csproj, msbuild, vcproj, wdproj, wixproj)
MUMPS                      (mps, m)
Mustache                   (mustache)
MXML                       (mxml)
NAnt script                (build)
NASTRAN DMAP               (dmap)
Nemerle                    (n)
NetLogo                    (nlogo, nls)
Nickel                     (ncl)
Nim                        (nim, nim.cfg, nimble, nimrod, nims)
Nix                        (nix)
Nunjucks                   (njk)
Objective-C                (m)
Objective-C++              (mm)
OCaml                      (eliom, eliomi, ml, ml4, mli, mll, mly)
Odin                       (odin)
OpenCL                     (cl)
OpenSCAD                   (scad)
Oracle Forms               (fmt)
Oracle PL/SQL              (bod, fnc, prc, spc, trg)
Oracle Reports             (rex)
P4                         (p4)
Pascal                     (dpr, lpr, pas, pascal)
Pascal/Pawn                (p)
Pascal/Puppet              (pp)
Patran Command Language    (pcl, ses)
Pawn                       (pawn, pwn)
PEG                        (peg)
peg.js                     (pegjs)
peggy                      (peggy)
Perl                       (ack, al, cpanfile, makefile.pl, perl, ph, plh, plx, pm, psgi, rexfile, pl, p6)
Pest                       (pest)
PHP                        (aw, ctp, phakefile, php, php3, php4, php5, php_cs, php_cs.dist, phps, phpt, phtml)
PHP/Pascal/Fortran/Pawn    (inc)
Pig Latin                  (pig)
PL/I                       (pl1)
PL/M                       (lit, plm)
PlantUML                   (iuml, plantuml, pu, puml, wsd)
PO File                    (po)
Pony                       (pony)
PowerBuilder               (pbt, sra, srf, srm, srs, sru, srw)
PowerShell                 (ps1, psd1, psm1)
Prisma Schema              (prisma)
Processing                 (pde)
ProGuard                   (pro)
Prolog                     (P, prolog, yap, pl, p6, pro)
Properties                 (properties)
Protocol Buffers           (proto)
PRQL                       (prql)
Pug                        (jade, pug)
PureScript                 (purs)
Python                     (buck, build.bazel, gclient, gyp, gypi, lmi, py, py3, pyde, pyi, pyp, pyt, pyw, sconscript, sconstruct, snakefile, tac, workspace, wscript, wsgi, xpy)
QML                        (qbs, qml)
Qt                         (ui)
Qt Linguist                (ts)
Qt Project                 (pro)
R                          (expr-dist, R, r, rd, rprofile, rsx)
Racket                     (rkt, rktd, rktl, scrbl)
Raku                       (pm6, raku, rakumod)
Raku/Prolog                (P6, p6)
RAML                       (raml)
RapydScript                (pyj)
Razor                      (cshtml, razor)
ReasonML                   (re, rei)
ReScript                   (res, resi)
reStructuredText           (rest, rest.txt, rst, rst.txt)
Rexx                       (pprx, rexx)
Ring                       (rform, rh, ring)
Rmd                        (Rmd)
RobotFramework             (robot)
Ruby                       (appraisals, berksfile, brewfile, builder, buildfile, capfile, dangerfile, deliverfile, eye, fastfile, gemfile, gemfile.lock, gemspec, god, guardfile, irbrc, jarfile, jbuilder, mavenfile, mspec, podfile, podspec, pryrc, puppetfile, rabl, rake, rb, rbuild, rbw, rbx, ru, snapfile, thor, thorfile, vagrantfile, watchr)
Ruby HTML                  (rhtml)
Rust                       (rs, rs.in)
SaltStack                  (sls)
SAS                        (sas)
Sass                       (sass)
Scala                      (kojo, sbt, scala)
Scheme                     (sc, sch, scm, sld, sps, ss, sls)
SCSS                       (scss)
sed                        (sed)
SKILL                      (il)
SKILL++                    (ils)
Slice                      (ice)
Slim                       (slim)
Slint                      (slint)
Smalltalk                  (st, cs)
Smarty                     (smarty, tpl)
Snakemake                  (rules, smk)
Softbridge Basic           (SBL, sbl)
Solidity                   (sol)
SparForte                  (sp)
Specman e                  (e)
SQL                        (cql, mysql, psql, SQL, sql, tab, udf, viw)
SQL Data                   (data.sql)
SQL Stored Procedure       (spc.sql, spoc.sql, sproc.sql, udf.sql)
Squirrel                   (nut)
Standard ML                (fun, sig, sml)
Starlark                   (bazel, bzl)
Stata                      (ado, DO, do, doh, ihlp, mata, matah, sthlp)
Stylus                     (styl)
SugarSS                    (sss)
Svelte                     (svelte)
SVG                        (SVG, svg)
Swift                      (swift)
SWIG                       (i)
TableGen                   (td)
Tcl/Tk                     (itk, tcl, tk)
TEAL                       (teal)
Teamcenter met             (met)
Teamcenter mth             (mth)
Templ                      (templ)
TeX                        (aux, bbx, bib, bst, cbx, dtx, ins, lbx, ltx, mkii, mkiv, mkvi, sty, tex, cls)
Text                       (text, txt)
Thrift                     (thrift)
TITAN Project File Information (tpd)
Titanium Style Sheet       (tss)
TLA+                       (tla)
TNSDL                      (cii, cin, in1, in2, in3, in4, inf, interface, rou, sdl, sdt, spd, ssc, sst)
TOML                       (toml)
tspeg                      (jspeg, tspeg)
TTCN                       (ttcn, ttcn2, ttcn3, ttcnpp)
Twig                       (twig)
TypeScript                 (mts, tsx, ts)
Typst                      (typ)
Umka                       (um)
Unity-Prefab               (mat, prefab)
Vala                       (vala)
Vala Header                (vapi)
VB for Applications        (VBA, vba)
Velocity Template Language (vm)
Verilog-SystemVerilog      (sv, svh, v)
VHDL                       (VHD, vhd, VHDL, vhdl, vhf, vhi, vho, vhs, vht, vhw)
vim script                 (vim)
Visual Basic               (BAS, bas, ctl, dsr, frm, FRX, frx, VBHTML, vbhtml, vbp, vbw, cls)
Visual Basic .NET          (VB, vb, vbproj)
Visual Basic Script        (VBS, vbs)
Visual Fox Pro             (SCA, sca)
Visual Studio Solution     (sln)
Visualforce Component      (component)
Visualforce Page           (page)
Vuejs Component            (vue)
Vyper                      (vy)
Web Services Description   (wsdl)
WebAssembly                (wast, wat)
WGSL                       (wgsl)
Windows Message File       (mc)
Windows Module Definition  (def)
Windows Resource File      (rc, rc2)
WiX include                (wxi)
WiX source                 (wxs)
WiX string localization    (wxl)
WXML                       (wxml)
WXSS                       (wxss)
X++                        (xpo)
XAML                       (xaml)
xBase                      (prg, prw)
xBase Header               (ch)
XHTML                      (xhtml)
XMI                        (XMI, xmi)
XML                        (adml, admx, ant, app.config, axml, builds, ccproj, ccxml, classpath, clixml, cproject, cscfg, csdef, csl, ct, depproj, ditamap, ditaval, dll.config, dotsettings, filters, fsproj, gmx, grxml, iml, ivy, jelly, jsproj, kml, launch, mdpolicy, mjml, natvis, ndproj, nproj, nuget.config, nuspec, odd, osm, packages.config, pkgproj, plist, proj, project, props, ps1xml, psc1, pt, rdf, resx, rss, scxml, settings.stylecop, sfproj, shproj, srdf, storyboard, sttheme, sublime-snippet, targets, tmcommand, tml, tmlanguage, tmpreferences, tmsnippet, tmtheme, urdf, ux, vcxproj, vsixmanifest, vssettings, vstemplate, vxml, web.config, web.debug.config, web.release.config, wsf, x3d, xacro, xib, xlf, xliff, XML, xml, xml.dist, xproj, xspec, xul, zcml)
XQuery                     (xq, xql, xqm, xquery, xqy)
XSD                        (XSD, xsd)
XSLT                       (XSL, xsl, XSLT, xslt)
Xtend                      (xtend)
yacc                       (y, yacc)
YAML                       (clang-format, clang-tidy, gemrc, glide.lock, mir, reek, rviz, sublime-syntax, syntax, yaml, yaml-tmlanguage, yml, yml.mysql)
Yang                       (yang)
Zig                        (zig)
zsh                        (zsh)
&lt;/pre&gt; 
&lt;p&gt;The above list can be customized by reading language definitions from a file with the &lt;code&gt;--read-lang-def&lt;/code&gt; or &lt;code&gt;--force-lang-def&lt;/code&gt; options.&lt;/p&gt; 
&lt;p&gt;These file extensions map to multiple languages:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;cj&lt;/code&gt; files could be Clojure or Cangjie&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;cl&lt;/code&gt; files could be Lisp or OpenCL&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;cls&lt;/code&gt; files could be Visual Basic, TeX or Apex Class&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;cs&lt;/code&gt; files could be C# or Smalltalk&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;d&lt;/code&gt; files could be D or dtrace&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;f&lt;/code&gt; files could be Fortran 77 or Forth&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;fnc&lt;/code&gt; files could be Oracle PL or SQL&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;for&lt;/code&gt; files could be Fortran 77 or Forth&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;fs&lt;/code&gt; files could be F# or Forth&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;inc&lt;/code&gt; files could be PHP or Pascal&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;itk&lt;/code&gt; files could be Tcl or Tk&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;jl&lt;/code&gt; files could be Lisp or Julia&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;lit&lt;/code&gt; files could be PL or M&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;m&lt;/code&gt; files could be MATLAB, Mathematica, Objective-C, MUMPS or Mercury&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;p6&lt;/code&gt; files could be Perl or Prolog&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;pl&lt;/code&gt; files could be Perl or Prolog&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;PL&lt;/code&gt; files could be Perl or Prolog&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;pp&lt;/code&gt; files could be Pascal or Puppet&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;pro&lt;/code&gt; files could be IDL, Qt Project, Prolog or ProGuard&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ts&lt;/code&gt; files could be TypeScript or Qt Linguist&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ui&lt;/code&gt; files could be Qt or Glade&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;v&lt;/code&gt; files could be Verilog-SystemVerilog or Coq&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;cloc has subroutines that attempt to identify the correct language based on the file&#39;s contents for these special cases. Language identification accuracy is a function of how much code the file contains; .m files with just one or two lines for example, seldom have enough information to correctly distinguish between MATLAB, Mercury, MUMPS, or Objective-C.&lt;/p&gt; 
&lt;p&gt;Languages with file extension collisions are difficult to customize with &lt;code&gt;--read-lang-def&lt;/code&gt; or &lt;code&gt;--force-lang-def&lt;/code&gt; as they have no mechanism to identify languages with common extensions. In this situation one must modify the cloc source code. &lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/1%7D%7D%7D&quot;&gt;&lt;/a&gt; &lt;a name=&quot;How_it_works&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/%7B%7B%7B1&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#___top&quot; title=&quot;click to go to top of document&quot;&gt;How It Works ▲&lt;/a&gt;&lt;/h1&gt; 
&lt;p&gt;cloc&#39;s method of operation resembles SLOCCount&#39;s: First, create a list of files to consider. Next, attempt to determine whether or not found files contain recognized computer language source code. Finally, for files identified as source files, invoke language-specific routines to count the number of source lines.&lt;/p&gt; 
&lt;p&gt;A more detailed description:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;If the input file is an archive (such as a .tar.gz or .zip file), create a temporary directory and expand the archive there using a system call to an appropriate underlying utility (tar, bzip2, unzip, etc) then add this temporary directory as one of the inputs. (This works more reliably on Unix than on Windows.)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Use File::Find to recursively descend the input directories and make a list of candidate file names. Ignore binary and zero-sized files.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Make sure the files in the candidate list have unique contents (first by comparing file sizes, then, for similarly sized files, compare MD5 hashes of the file contents with Digest::MD5). For each set of identical files, remove all but the first copy, as determined by a lexical sort, of identical files from the set. The removed files are not included in the report. (The &lt;code&gt;--skip-uniqueness&lt;/code&gt; switch disables the uniqueness tests and forces all copies of files to be included in the report.) See also the &lt;code&gt;--ignored=&lt;/code&gt; switch to see which files were ignored and why.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Scan the candidate file list for file extensions which cloc associates with programming languages (see the &lt;code&gt;--show-lang&lt;/code&gt; and &lt;code&gt;--show-ext&lt;/code&gt; options). Files which match are classified as containing source code for that language. Each file without an extension is opened and its first line read to see if it is a Unix shell script (anything that begins with #!). If it is shell script, the file is classified by that scripting language (if the language is recognized). If the file does not have a recognized extension or is not a recognized scripting language, the file is ignored.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;All remaining files in the candidate list should now be source files for known programming languages. For each of these files:&lt;/p&gt; 
  &lt;ol&gt; 
   &lt;li&gt;Read the entire file into memory.&lt;/li&gt; 
   &lt;li&gt;Count the number of lines (= L&lt;sub&gt;original&lt;/sub&gt;).&lt;/li&gt; 
   &lt;li&gt;Remove blank lines, then count again (= L&lt;sub&gt;non_blank&lt;/sub&gt;).&lt;/li&gt; 
   &lt;li&gt;Loop over the comment filters defined for this language. (For example, C++ has two filters: (1) remove lines that start with optional whitespace followed by // and (2) remove text between /* and */) Apply each filter to the code to remove comments. Count the left over lines (= L&lt;sub&gt;code&lt;/sub&gt;).&lt;/li&gt; 
   &lt;li&gt;Save the counts for this language: 
    &lt;ul&gt; 
     &lt;li&gt;blank lines = L&lt;sub&gt;original&lt;/sub&gt; - L&lt;sub&gt;non_blank&lt;/sub&gt;&lt;/li&gt; 
     &lt;li&gt;comment lines = L&lt;sub&gt;non_blank&lt;/sub&gt; - L&lt;sub&gt;code&lt;/sub&gt;&lt;/li&gt; 
     &lt;li&gt;code lines = L&lt;sub&gt;code&lt;/sub&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;The options modify the algorithm slightly. The &lt;code&gt;--read-lang-def&lt;/code&gt; option for example allows the user to read definitions of comment filters, known file extensions, and known scripting languages from a file. The code for this option is processed between Steps 2 and 3. &lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/1%7D%7D%7D&quot;&gt;&lt;/a&gt; &lt;a name=&quot;Advanced_Use&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/%7B%7B%7B1&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#___top&quot; title=&quot;click to go to top of document&quot;&gt;Advanced Use ▲&lt;/a&gt;&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/1%7D%7D%7D&quot;&gt;&lt;/a&gt; &lt;a name=&quot;strip_comments&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/%7B%7B%7B1&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#___top&quot; title=&quot;click to go to top of document&quot;&gt;Remove Comments from Source Code ▲&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;How can you tell if cloc correctly identifies comments? One way to convince yourself cloc is doing the right thing is to use its &lt;code&gt;--strip-comments&lt;/code&gt; option to remove comments and blank lines from files, then compare the stripped-down files to originals.&lt;/p&gt; 
&lt;p&gt;Let&#39;s try this out with the SQLite amalgamation, a C file containing all code needed to build the SQLite library along with a header file:&lt;/p&gt; 
&lt;pre&gt;
prompt&amp;gt; tar zxf sqlite-amalgamation-3.5.6.tar.gz
prompt&amp;gt; cd sqlite-3.5.6/
prompt&amp;gt; cloc --strip-comments=nc sqlite.c
       1 text file.
       1 unique file.
Wrote sqlite3.c.nc
       0 files ignored.

http://cloc.sourceforge.net v 1.03  T=1.0 s (1.0 files/s, 82895.0 lines/s)
-------------------------------------------------------------------------------
Language          files     blank   comment      code    scale   3rd gen. equiv
-------------------------------------------------------------------------------
C                     1      5167     26827     50901 x   0.77 =       39193.77
-------------------------------------------------------------------------------
&lt;/pre&gt; 
&lt;p&gt;The extension argument given to --strip-comments is arbitrary; here nc was used as an abbreviation for &quot;no comments&quot;.&lt;/p&gt; 
&lt;p&gt;cloc removed over 31,000 lines from the file:&lt;/p&gt; 
&lt;pre&gt;
prompt&amp;gt; wc -l sqlite3.c sqlite3.c.nc
  82895 sqlite3.c
  50901 sqlite3.c.nc
 133796 total
prompt&amp;gt; echo &quot;82895 - 50901&quot; | bc
31994
&lt;/pre&gt; 
&lt;p&gt;We can now compare the original file, sqlite3.c and the one stripped of comments, sqlite3.c.nc with tools like diff or vimdiff and see what exactly cloc considered comments and blank lines. A rigorous proof that the stripped-down file contains the same C code as the original is to compile these files and compare checksums of the resulting object files.&lt;/p&gt; 
&lt;p&gt;First, the original source file:&lt;/p&gt; 
&lt;pre&gt;
prompt&amp;gt; gcc -c sqlite3.c
prompt&amp;gt; md5sum sqlite3.o
cce5f1a2ea27c7e44b2e1047e2588b49  sqlite3.o
&lt;/pre&gt; 
&lt;p&gt;Next, the version without comments:&lt;/p&gt; 
&lt;pre&gt;
prompt&amp;gt; mv sqlite3.c.nc sqlite3.c
prompt&amp;gt; gcc -c sqlite3.c
prompt&amp;gt; md5sum sqlite3.o
cce5f1a2ea27c7e44b2e1047e2588b49  sqlite3.o
&lt;/pre&gt; 
&lt;p&gt;cloc removed over 31,000 lines of comments and blanks but did not modify the source code in any significant way since the resulting object file matches the original. &lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/1%7D%7D%7D&quot;&gt;&lt;/a&gt; &lt;a name=&quot;compressed_arch&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/%7B%7B%7B1&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#___top&quot; title=&quot;click to go to top of document&quot;&gt;Work with Compressed Archives ▲&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;Versions of cloc before v1.07 required an &lt;code&gt;--extract-with=CMD&lt;/code&gt; option to tell cloc how to expand an archive file. Beginning with v1.07 this is extraction is attempted automatically. At the moment the automatic extraction method works reasonably well on Unix-type OS&#39;s for the following file types: &lt;code&gt;.tar.gz&lt;/code&gt;, &lt;code&gt;.tar.bz2&lt;/code&gt;, &lt;code&gt;.tar.xz&lt;/code&gt;, &lt;code&gt;.tgz&lt;/code&gt;, &lt;code&gt;.zip&lt;/code&gt;, &lt;code&gt;.ear&lt;/code&gt;, &lt;code&gt;.deb&lt;/code&gt;. Some of these extensions work on Windows if one has WinZip installed in the default location (&lt;code&gt;C:\Program Files\WinZip\WinZip32.exe&lt;/code&gt;). Additionally, with newer versions of WinZip, the [http://www.winzip.com/downcl.htm](command line add-on) is needed for correct operation; in this case one would invoke cloc with something like &lt;br&gt;&lt;/p&gt; 
&lt;pre&gt;
 --extract-with=&quot;\&quot;c:\Program Files\WinZip\wzunzip\&quot; -e -o &amp;gt;FILE&amp;lt; .&quot;
 
&lt;/pre&gt; 
&lt;p&gt;Ref. &lt;a href=&quot;http://sourceforge.net/projects/cloc/forums/forum/600963/topic/4021070?message=8938196&quot;&gt;http://sourceforge.net/projects/cloc/forums/forum/600963/topic/4021070?message=8938196&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;In situations where the automatic extraction fails, one can try the &lt;code&gt;--extract-with=CMD&lt;/code&gt; option to count lines of code within tar files, Zip files, or other compressed archives for which one has an extraction tool. cloc takes the user-provided extraction command and expands the archive to a temporary directory (created with File::Temp), counts the lines of code in the temporary directory, then removes that directory. While not especially helpful when dealing with a single compressed archive (after all, if you&#39;re going to type the extraction command anyway why not just manually expand the archive?) this option is handy for working with several archives at once.&lt;/p&gt; 
&lt;p&gt;For example, say you have the following source tarballs on a Unix machine&lt;br&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;perl-5.8.5.tar.gz
Python-2.4.2.tar.gz
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;and you want to count all the code within them. The command would be&lt;/p&gt; 
&lt;pre&gt;
cloc --extract-with=&#39;gzip -dc &amp;gt;FILE&amp;lt; | tar xf -&#39; perl-5.8.5.tar.gz Python-2.4.2.tar.gz
&lt;/pre&gt; 
&lt;p&gt;If that Unix machine has GNU tar (which can uncompress and extract in one step) the command can be shortened to&lt;/p&gt; 
&lt;pre&gt;
cloc --extract-with=&#39;tar zxf &amp;gt;FILE&amp;lt;&#39; perl-5.8.5.tar.gz Python-2.4.2.tar.gz
&lt;/pre&gt; 
&lt;p&gt;On a Windows computer with WinZip installed in &lt;code&gt;c:\Program Files\WinZip&lt;/code&gt; the command would look like&lt;/p&gt; 
&lt;pre&gt;
cloc.exe --extract-with=&quot;\&quot;c:\Program Files\WinZip\WinZip32.exe\&quot; -e -o &amp;gt;FILE&amp;lt; .&quot; perl-5.8.5.tar.gz Python-2.4.2.tar.gz
&lt;/pre&gt; 
&lt;p&gt;Java &lt;code&gt;.ear&lt;/code&gt; files are Zip files that contain additional Zip files. cloc can handle nested compressed archives without difficulty--provided all such files are compressed and archived in the same way. Examples of counting a Java &lt;code&gt;.ear&lt;/code&gt; file in Unix and Windows:&lt;/p&gt; 
&lt;pre&gt;
&lt;i&gt;Unix&amp;gt;&lt;/i&gt; cloc --extract-with=&quot;unzip -d . &amp;gt;FILE&amp;lt; &quot; Project.ear
&lt;i&gt;DOS&amp;gt;&lt;/i&gt; cloc.exe --extract-with=&quot;\&quot;c:\Program Files\WinZip\WinZip32.exe\&quot; -e -o &amp;gt;FILE&amp;lt; .&quot; Project.ear
&lt;/pre&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/1%7D%7D%7D&quot;&gt;&lt;/a&gt; &lt;a name=&quot;diff&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/%7B%7B%7B1&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#___top&quot; title=&quot;click to go to top of document&quot;&gt;Differences ▲&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;The &lt;code&gt;--diff&lt;/code&gt; switch allows one to measure the relative change in source code and comments between two versions of a file, directory, or archive. Differences reveal much more than absolute code counts of two file versions. For example, say a source file has 100 lines and its developer delivers a newer version with 102 lines. Did the developer add two comment lines, or delete seventeen source lines and add fourteen source lines and five comment lines, or did the developer do a complete rewrite, discarding all 100 original lines and adding 102 lines of all new source? The diff option tells how many lines of source were added, removed, modified or stayed the same, and how many lines of comments were added, removed, modified or stayed the same.&lt;/p&gt; 
&lt;p&gt;Differences in blank lines are handled much more coarsely because these are stripped by cloc early on. Unless a file pair is identical, cloc will report only differences in absolute counts of blank lines. In other words, one can expect to see only entries for &#39;added&#39; if the second file has more blanks than the first, and &#39;removed&#39; if the situation is reversed. The entry for &#39;same&#39; will be non-zero only when the two files are identical.&lt;/p&gt; 
&lt;p&gt;In addition to file pairs, one can give cloc pairs of directories, or pairs of file archives, or a file archive and a directory. cloc will try to align file pairs within the directories or archives and compare diffs for each pair. For example, to see what changed between GCC 4.4.0 and 4.5.0 one could do&lt;/p&gt; 
&lt;pre&gt;
cloc --diff gcc-4.4.0.tar.bz2  gcc-4.5.0.tar.bz2
&lt;/pre&gt; 
&lt;p&gt;Be prepared to wait a while for the results though; the &lt;code&gt;--diff&lt;/code&gt; option runs much more slowly than an absolute code count.&lt;/p&gt; 
&lt;p&gt;To see how cloc aligns files between the two archives, use the &lt;code&gt;--diff-alignment&lt;/code&gt; option&lt;/p&gt; 
&lt;pre&gt;
cloc --diff-alignment=align.txt gcc-4.4.0.tar.bz2  gcc-4.5.0.tar.bz2
&lt;/pre&gt; 
&lt;p&gt;to produce the file &lt;code&gt;align.txt&lt;/code&gt; which shows the file pairs as well as files added and deleted. The symbols &lt;code&gt;==&lt;/code&gt; and &lt;code&gt;!=&lt;/code&gt; before each file pair indicate if the files are identical (&lt;code&gt;==&lt;/code&gt;) or if they have different content (&lt;code&gt;!=&lt;/code&gt;).&lt;/p&gt; 
&lt;p&gt;Here&#39;s sample output showing the difference between the Python 2.6.6 and 2.7 releases:&lt;/p&gt; 
&lt;pre&gt;&lt;i&gt;prompt&amp;gt;&lt;/i&gt; cloc --diff Python-2.7.9.tgz Python-2.7.10.tar.xz
    4315 text files.
    4313 text files.s
    2173 files ignored.

4 errors:
Diff error, exceeded timeout:  /tmp/8ToGAnB9Y1/Python-2.7.9/Mac/Modules/qt/_Qtmodule.c
Diff error, exceeded timeout:  /tmp/M6ldvsGaoq/Python-2.7.10/Mac/Modules/qt/_Qtmodule.c
Diff error (quoted comments?):  /tmp/8ToGAnB9Y1/Python-2.7.9/Mac/Modules/qd/qdsupport.py
Diff error (quoted comments?):  /tmp/M6ldvsGaoq/Python-2.7.10/Mac/Modules/qd/qdsupport.py

https://github.com/AlDanial/cloc v 1.65  T=298.59 s (0.0 files/s, 0.0 lines/s)
-----------------------------------------------------------------------------
Language                   files          blank        comment           code
-----------------------------------------------------------------------------
Visual Basic
 same                          2              0              1             12
 modified                      0              0              0              0
 added                         0              0              0              0
 removed                       0              0              0              0
make
 same                         11              0            340           2952
 modified                      1              0              0              1
 added                         0              0              0              0
 removed                       0              0              0              0
diff
 same                          1              0             87            105
 modified                      0              0              0              0
 added                         0              0              0              0
 removed                       0              0              0              0
CSS
 same                          0              0             19            327
 modified                      1              0              0              1
 added                         0              0              0              0
 removed                       0              0              0              0
Objective-C
 same                          7              0             61            635
 modified                      0              0              0              0
 added                         0              0              0              0
 removed                       0              0              0              0
NAnt script
 same                          2              0              0             30
 modified                      0              0              0              0
 added                         0              0              0              0
 removed                       0              0              0              0
XML
 same                          3              0              2             72
 modified                      1              0              0              1
 added                         0              0              0              1
 removed                       0              1              0              0
Windows Resource File
 same                          3              0             56            206
 modified                      1              0              0              1
 added                         0              0              0              0
 removed                       0              0              0              0
Expect
 same                          6              0            161            565
 modified                      0              0              0              0
 added                         0              0              0              0
 removed                       0              0              0              0
HTML
 same                         14              0             11           2344
 modified                      0              0              0              0
 added                         0              0              0              0
 removed                       0              0              0              0
vim script
 same                          1              0              7            106
 modified                      0              0              0              0
 added                         0              0              0              0
 removed                       0              0              0              0
C++
 same                          2              0             18            128
 modified                      0              0              0              0
 added                         0              0              0              0
 removed                       0              0              0              0
Windows Module Definition
 same                          7              0            187           2080
 modified                      2              0              0              0
 added                         0              0              0              1
 removed                       0              1              0              2
Prolog
 same                          1              0              0             24
 modified                      0              0              0              0
 added                         0              0              0              0
 removed                       0              0              0              0
Javascript
 same                          3              0             49            229
 modified                      0              0              0              0
 added                         0              0              0              0
 removed                       0              0              0              0
Assembly
 same                         51              0           6794          12298
 modified                      0              0              0              0
 added                         0              0              0              0
 removed                       0              0              0              0
Bourne Shell
 same                         41              0           7698          45024
 modified                      1              0              0              3
 added                         0             13              2             64
 removed                       0              0              0              0
DOS Batch
 same                         29              0            107            494
 modified                      1              0              0              9
 added                         0              1              0              3
 removed                       0              0              0              0
MSBuild script
 same                         77              0              3          38910
 modified                      0              0              0              0
 added                         0              0              0              0
 removed                       0              0              0              0
Python
 same                       1947              0         109012         430335
 modified                    192              0             94            950
 added                         2            323            283           2532
 removed                       2             55             58            646
m4
 same                         18              0            191          15352
 modified                      1              0              0              2
 added                         1             31              0            205
 removed                       0              0              0              0
C
 same                        505              0          37439         347837
 modified                     45              0             13            218
 added                         0             90             33            795
 removed                       0              9              2            148
C/C++ Header
 same                        255              0          10361          66635
 modified                      5              0              5              7
 added                         0              1              3            300
 removed                       0              0              0              0
---------------------------------------------------------------------
SUM:
 same                       2986              0         172604         966700
 modified                    251              0            112           1193
 added                         3            459            321           3901
 removed                       2             66             60            796
---------------------------------------------------------------------
&lt;/pre&gt; 
&lt;p&gt;A pair of errors occurred. The first pair was caused by timing out when computing diffs of the file &lt;code&gt;Python-X/Mac/Modules/qt/_Qtmodule.c&lt;/code&gt; in each Python version. This file has &amp;gt; 26,000 lines of C code and takes more than 10 seconds--the default maximum duration for diff&#39;ing a single file--on my slow computer. (Note: this refers to performing differences with the &lt;code&gt;sdiff()&lt;/code&gt; function in the Perl &lt;code&gt;Algorithm::Diff&lt;/code&gt; module, not the command line &lt;code&gt;diff&lt;/code&gt; utility.) This error can be overcome by raising the time to, say, 20 seconds with &lt;code&gt;--diff-timeout 20&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;The second error is more problematic. The files &lt;code&gt;Python-X/Mac/Modules/qd/qdsupport.py&lt;/code&gt; include Python docstring (text between pairs of triple quotes) containing C comments. cloc treats docstrings as comments and handles them by first converting them to C comments, then using the C comment removing regular expression. Nested C comments yield erroneous results however.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/1%7D%7D%7D&quot;&gt;&lt;/a&gt; &lt;a name=&quot;custom_lang&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/%7B%7B%7B1&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#___top&quot; title=&quot;click to go to top of document&quot;&gt;Create Custom Language Definitions ▲&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;cloc can write its language comment definitions to a file or can read comment definitions from a file, overriding the built-in definitions. This can be useful when you want to use cloc to count lines of a language not yet included, to change association of file extensions to languages, or to modify the way existing languages are counted.&lt;/p&gt; 
&lt;p&gt;The easiest way to create a custom language definition file is to make cloc write its definitions to a file, then modify that file:&lt;/p&gt; 
&lt;pre&gt;&lt;i&gt;Unix&amp;gt;&lt;/i&gt; cloc --write-lang-def=my_definitions.txt
&lt;/pre&gt; 
&lt;p&gt;creates the file &lt;code&gt;my_definitions.txt&lt;/code&gt; which can be modified then read back in with either the &lt;code&gt;--read-lang-def&lt;/code&gt; or &lt;code&gt;--force-lang-def&lt;/code&gt; option. The difference between the options is former merges language definitions from the given file in with cloc&#39;s internal definitions with cloc&#39;s taking precedence if there are overlaps. The &lt;code&gt;--force-lang-def&lt;/code&gt; option, on the other hand, replaces cloc&#39;s definitions completely. This option has a disadvantage in preventing cloc from counting &lt;a class=&quot;u&quot; href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#extcollision&quot; name=&quot;extcollision&quot;&gt; languages whose extensions map to multiple languages &lt;/a&gt; as these languages require additional logic that is not easily expressed in a definitions file.&lt;/p&gt; 
&lt;pre&gt;&lt;i&gt;Unix&amp;gt;&lt;/i&gt; cloc --read-lang-def=my_definitions.txt  &lt;i&gt;file1 file2 dir1 ...&lt;/i&gt;
&lt;/pre&gt; 
&lt;p&gt;Each language entry has four parts:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;The language name starting in column 1.&lt;/li&gt; 
 &lt;li&gt;One or more comment &lt;em&gt;filters&lt;/em&gt; starting in column 5.&lt;/li&gt; 
 &lt;li&gt;One or more filename extensions starting in column 5.&lt;/li&gt; 
 &lt;li&gt;A 3rd generation scale factor starting in column 5. This entry must be provided but its value is not important unless you want to compare your language to a hypothetical third generation programming language.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;A filter defines a method to remove comment text from the source file. For example the entry for C++ looks like this&lt;/p&gt; 
&lt;pre&gt;C++
    filter call_regexp_common C++
    filter remove_inline //.*$
    extension C
    extension c++
    extension cc
    extension cpp
    extension cxx
    extension pcc
    3rd_gen_scale 1.51
    end_of_line_continuation \\$
&lt;/pre&gt; 
&lt;p&gt;C++ has two filters: first, remove lines matching Regexp::Common&#39;s C++ comment regex. The second filter using remove_inline is currently unused. Its intent is to identify lines with both code and comments and it may be implemented in the future.&lt;/p&gt; 
&lt;p&gt;A more complete discussion of the different filter options may appear here in the future. The output of cloc&#39;s &lt;code&gt;--write-lang-def&lt;/code&gt; option should provide enough examples for motivated individuals to modify or extend cloc&#39;s language definitions.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/1%7D%7D%7D&quot;&gt;&lt;/a&gt; &lt;a name=&quot;combine_reports&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/%7B%7B%7B1&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#___top&quot; title=&quot;click to go to top of document&quot;&gt;Combine Reports ▲&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;If you manage multiple software projects you might be interested in seeing line counts by project, not just by language. Say you manage three software projects called MariaDB, PostgreSQL, and SQLite. The teams responsible for each of these projects run cloc on their source code and provide you with the output. For example, the MariaDB team does&lt;/p&gt; 
&lt;pre&gt;cloc --out mariadb-10.1.txt mariadb-server-10.1.zip&lt;/pre&gt; 
&lt;p&gt;and provides you with the file &lt;code&gt;mariadb-10.1.txt&lt;/code&gt;. The contents of the three files you get are&lt;/p&gt; 
&lt;pre&gt;
&lt;i&gt;Unix&amp;gt;&lt;/i&gt; cat mariadb-10.1.txt
https://github.com/AlDanial/cloc v 1.65  T=45.36 s (110.5 files/s, 66411.4 lines/s)
-----------------------------------------------------------------------------------
Language                         files          blank        comment           code
-----------------------------------------------------------------------------------
C++                               1613         225338         290077         983026
C                                  853          62442          73017         715018
C/C++ Header                      1327          48300         114577         209394
Bourne Shell                       256          10224          10810          61943
Perl                               147          10342           8305          35562
Pascal                             107           4907           5237          32541
HTML                                56            195              6          16489
Javascript                           5           3309           3019          15540
m4                                  30           1599            359          14215
CMake                              190           1919           4097          12206
XML                                 35            648             56           5210
Ruby                                59            619            184           4998
Puppet                              10              0              1           3848
make                               134            724            360           3631
SQL                                 23            306            377           3405
Python                              34            371            122           2545
Bourne Again Shell                  27            299            380           1604
Windows Module Definition           37             27             13           1211
lex                                  4            394            166            991
yacc                                 2            152             64            810
DOS Batch                           19             89             82            700
Prolog                               1              9             40            448
RobotFramework                       1              0              0            441
CSS                                  2             33            155            393
JSON                                 5              0              0            359
dtrace                               9             59            179            306
Windows Resource File               10             61             89            250
Assembly                             2             70            284            237
WiX source                           1             18             10            155
Visual Basic                         6              0              0             88
YAML                                 2              4              4             65
PHP                                  1             11              2             24
SKILL                                1              8             15             16
sed                                  2              0              0             16
Windows Message File                 1              2              8              6
diff                                 1              1              4              4
D                                    1              4             11              4
-----------------------------------------------------------------------------------
SUM:                              5014         372484         512110        2127699
-----------------------------------------------------------------------------------

&lt;i&gt;Unix&amp;gt;&lt;/i&gt; cat sqlite-3081101.txt
https://github.com/AlDanial/cloc v 1.65  T=1.22 s (3.3 files/s, 143783.6 lines/s)
-------------------------------------------------------------------------------
Language                     files          blank        comment           code
-------------------------------------------------------------------------------
C                                2          11059          53924         101454
C/C++ Header                     2            211           6630           1546
-------------------------------------------------------------------------------
SUM:                             4          11270          60554         103000
-------------------------------------------------------------------------------

&lt;i&gt;Unix&amp;gt;&lt;/i&gt; cat postgresql-9.4.4.txt
https://github.com/AlDanial/cloc v 1.65  T=22.46 s (172.0 files/s, 96721.6 lines/s)
-----------------------------------------------------------------------------------
Language                         files          blank        comment           code
-----------------------------------------------------------------------------------
HTML                              1254           3725              0         785991
C                                 1139         139289         244045         736519
C/C++ Header                       667          12277          32488          57014
SQL                                410          13400           8745          51926
yacc                                 8           3163           2669          28491
Bourne Shell                        41           2647           2440          17170
Perl                                81           1702           1308           9456
lex                                  9            792           1631           4285
make                               205           1525           1554           4114
m4                                  12            218             25           1642
Windows Module Definition           13              4             17           1152
XSLT                                 5             76             55            294
DOS Batch                            7             29             30             92
CSS                                  1             20              7             69
Assembly                             3             17             38             69
D                                    1             14             14             66
Windows Resource File                3              4              0             62
Lisp                                 1              1              1             16
sed                                  1              1              7             15
Python                               1              5              0             13
Bourne Again Shell                   1              8              6             10
Windows Message File                 1              0              0              5
-----------------------------------------------------------------------------------
SUM:                              3864         178917         295080        1698471
-----------------------------------------------------------------------------------
&lt;/pre&gt; 
&lt;p&gt;While these three files are interesting, you also want to see the combined counts from all projects. That can be done with cloc&#39;s &lt;code&gt;--sum_reports&lt;/code&gt; option:&lt;/p&gt; 
&lt;pre&gt;
&lt;i&gt;Unix&amp;gt;&lt;/i&gt; cloc --sum-reports --out=databases mariadb-10.1.txt  sqlite-3081101.txt  postgresql-9.4.4.txt
Wrote databases.lang
Wrote databases.file
&lt;/pre&gt; 
&lt;p&gt;The report combination produces two output files, one for sums by programming language (&lt;code&gt;databases.lang&lt;/code&gt;) and one by project (&lt;code&gt;databases.file&lt;/code&gt;). Their contents are&lt;/p&gt; 
&lt;pre&gt;&lt;i&gt;Unix&amp;gt;&lt;/i&gt; cat databases.lang
https://github.com/AlDanial/cloc v 1.65
--------------------------------------------------------------------------------
Language                      files          blank        comment           code
--------------------------------------------------------------------------------
C                              1994         212790         370986        1552991
C++                            1613         225338         290077         983026
HTML                           1310           3920              6         802480
C/C++ Header                   1996          60788         153695         267954
Bourne Shell                    297          12871          13250          79113
SQL                             433          13706           9122          55331
Perl                            228          12044           9613          45018
Pascal                          107           4907           5237          32541
yacc                             10           3315           2733          29301
m4                               42           1817            384          15857
Javascript                        5           3309           3019          15540
CMake                           190           1919           4097          12206
make                            339           2249           1914           7745
lex                              13           1186           1797           5276
XML                              35            648             56           5210
Ruby                             59            619            184           4998
Puppet                           10              0              1           3848
Python                           35            376            122           2558
Windows Module Definition        50             31             30           2363
Bourne Again Shell               28            307            386           1614
DOS Batch                        26            118            112            792
CSS                               3             53            162            462
Prolog                            1              9             40            448
RobotFramework                    1              0              0            441
JSON                              5              0              0            359
Windows Resource File            13             65             89            312
Assembly                          5             87            322            306
dtrace                            9             59            179            306
XSLT                              5             76             55            294
WiX source                        1             18             10            155
Visual Basic                      6              0              0             88
D                                 2             18             25             70
YAML                              2              4              4             65
sed                               3              1              7             31
PHP                               1             11              2             24
SKILL                             1              8             15             16
Lisp                              1              1              1             16
Windows Message File              2              2              8             11
diff                              1              1              4              4
--------------------------------------------------------------------------------
SUM:                           8882         562671         867744        3929170
--------------------------------------------------------------------------------

&lt;i&gt;Unix&amp;gt;&lt;/i&gt; cat databases.file
----------------------------------------------------------------------------------
File                            files          blank        comment           code
----------------------------------------------------------------------------------
mariadb-10.1.txt                 5014         372484         512110        2127699
postgresql-9.4.4.txt             3864         178917         295080        1698471
sqlite-3081101.txt                  4          11270          60554         103000
----------------------------------------------------------------------------------
SUM:                             8882         562671         867744        3929170
----------------------------------------------------------------------------------
&lt;/pre&gt; 
&lt;p&gt;Report files themselves can be summed together. Say you also manage development of Perl and Python and you want to keep track of those line counts separately from your database projects. First create reports for Perl and Python separately:&lt;/p&gt; 
&lt;pre&gt;
cloc --out perl-5.22.0.txt   perl-5.22.0.tar.gz
cloc --out python-2.7.10.txt Python-2.7.10.tar.xz
&lt;/pre&gt; 
&lt;p&gt;then sum these together with&lt;/p&gt; 
&lt;pre&gt;
&lt;i&gt;Unix&amp;gt;&lt;/i&gt; cloc --sum-reports --out script_lang perl-5.22.0.txt python-2.7.10.txt
Wrote script_lang.lang
Wrote script_lang.file

&lt;i&gt;Unix&amp;gt;&lt;/i&gt; cat script_lang.lang
https://github.com/AlDanial/cloc v 1.65
-------------------------------------------------------------------------------
Language                     files          blank        comment           code
-------------------------------------------------------------------------------
Perl                          2892         136396         184362         536445
C                              680          75566          71211         531203
Python                        2141          89642         109524         434015
C/C++ Header                   408          16433          26938         214800
Bourne Shell                   154          11088          14496          87759
MSBuild script                  77              0              3          38910
m4                              20           1604            191          15559
Assembly                        51           3775           6794          12298
Pascal                           8            458           1603           8592
make                            16            897            828           4939
XML                             37            198              2           2484
HTML                            14            393             11           2344
C++                             12            338            295           2161
Windows Module Definition        9            171            187           2081
YAML                            49             20             15           2078
Prolog                          12            438              2           1146
JSON                            14              1              0           1037
yacc                             1             85             76            998
DOS Batch                       44            199            148            895
Objective-C                      7             98             61            635
Expect                           6            104            161            565
Windows Message File             1            102             11            489
CSS                              1             98             19            328
Windows Resource File            7             55             56            292
Javascript                       3             31             49            229
vim script                       1             36              7            106
diff                             1             17             87            105
NAnt script                      2              1              0             30
IDL                              1              0              0             24
Visual Basic                     2              1              1             12
D                                1              5              7              8
Lisp                             2              0              3              4
-------------------------------------------------------------------------------
SUM:                          6674         338250         417148        1902571
-------------------------------------------------------------------------------

&lt;i&gt;Unix&amp;gt;&lt;/i&gt; cat script_lang.file
-------------------------------------------------------------------------------
File                         files          blank        comment           code
-------------------------------------------------------------------------------
python-2.7.10.txt             3240         161276         173214         998697
perl-5.22.0.txt               3434         176974         243934         903874
-------------------------------------------------------------------------------
SUM:                          6674         338250         417148        1902571
-------------------------------------------------------------------------------
&lt;/pre&gt; 
&lt;p&gt;Finally, combine the combination files:&lt;/p&gt; 
&lt;pre&gt;
&lt;i&gt;Unix&amp;gt;&lt;/i&gt; cloc --sum-reports --report_file=everything databases.lang script_lang.lang
Wrote everything.lang
Wrote everything.file

&lt;i&gt;Unix&amp;gt;&lt;/i&gt; cat everything.lang
https://github.com/AlDanial/cloc v 1.65
---------------------------------------------------------------------------------
Language                       files          blank        comment           code
---------------------------------------------------------------------------------
C                               2674         288356         442197        2084194
C++                             1625         225676         290372         985187
HTML                            1324           4313             17         804824
Perl                            3120         148440         193975         581463
C/C++ Header                    2404          77221         180633         482754
Python                          2176          90018         109646         436573
Bourne Shell                     451          23959          27746         166872
SQL                              433          13706           9122          55331
Pascal                           115           5365           6840          41133
MSBuild script                    77              0              3          38910
m4                                62           3421            575          31416
yacc                              11           3400           2809          30299
Javascript                         8           3340           3068          15769
make                             355           3146           2742          12684
Assembly                          56           3862           7116          12604
CMake                            190           1919           4097          12206
XML                               72            846             58           7694
lex                               13           1186           1797           5276
Ruby                              59            619            184           4998
Windows Module Definition         59            202            217           4444
Puppet                            10              0              1           3848
YAML                              51             24             19           2143
DOS Batch                         70            317            260           1687
Bourne Again Shell                28            307            386           1614
Prolog                            13            447             42           1594
JSON                              19              1              0           1396
CSS                                4            151            181            790
Objective-C                        7             98             61            635
Windows Resource File             20            120            145            604
Expect                             6            104            161            565
Windows Message File               3            104             19            500
RobotFramework                     1              0              0            441
dtrace                             9             59            179            306
XSLT                               5             76             55            294
WiX source                         1             18             10            155
diff                               2             18             91            109
vim script                         1             36              7            106
Visual Basic                       8              1              1            100
D                                  3             23             32             78
sed                                3              1              7             31
NAnt script                        2              1              0             30
IDL                                1              0              0             24
PHP                                1             11              2             24
Lisp                               3              1              4             20
SKILL                              1              8             15             16
---------------------------------------------------------------------------------
SUM:                           15556         900921        1284892        5831741
---------------------------------------------------------------------------------

&lt;i&gt;Unix&amp;gt;&lt;/i&gt; cat everything.file
-------------------------------------------------------------------------------
File                         files          blank        comment           code
-------------------------------------------------------------------------------
databases.lang                8882         562671         867744        3929170
script_lang.lang              6674         338250         417148        1902571
-------------------------------------------------------------------------------
SUM:                         15556         900921        1284892        5831741
-------------------------------------------------------------------------------
&lt;/pre&gt; 
&lt;p&gt;One limitation of the &lt;code&gt;--sum-reports&lt;/code&gt; feature is that the individual counts must be saved in the plain text format. Counts saved as XML, JSON, YAML, or SQL will produce errors if used in a summation.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/1%7D%7D%7D&quot;&gt;&lt;/a&gt; &lt;a name=&quot;sql&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/%7B%7B%7B1&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#___top&quot; title=&quot;click to go to top of document&quot;&gt;SQL ▲&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;Cloc can write results in the form of SQL table create and insert statements for use with relational database programs such as SQLite, MySQL, PostgreSQL, Oracle, or Microsoft SQL. Once the code count information is in a database, the information can be interrogated and displayed in interesting ways.&lt;/p&gt; 
&lt;p&gt;A database created from cloc SQL output has two tables, &lt;strong&gt;metadata&lt;/strong&gt; and &lt;strong&gt;t&lt;/strong&gt;:&lt;/p&gt; 
&lt;p&gt;Table &lt;strong&gt;metadata&lt;/strong&gt;:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Field&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;id&lt;/td&gt; 
   &lt;td&gt;integer primary key&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;timestamp&lt;/td&gt; 
   &lt;td&gt;text&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;project&lt;/td&gt; 
   &lt;td&gt;text&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;elapsed_s&lt;/td&gt; 
   &lt;td&gt;text&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Table &lt;strong&gt;t&lt;/strong&gt;:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Field&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;project&lt;/td&gt; 
   &lt;td&gt;text&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;language&lt;/td&gt; 
   &lt;td&gt;text&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;file&lt;/td&gt; 
   &lt;td&gt;text&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;nBlank&lt;/td&gt; 
   &lt;td&gt;integer&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;nComment&lt;/td&gt; 
   &lt;td&gt;integer&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;nCode&lt;/td&gt; 
   &lt;td&gt;integer&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;nScaled&lt;/td&gt; 
   &lt;td&gt;real&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;foreign key (id)&lt;/td&gt; 
   &lt;td&gt;references metadata (id)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;The &lt;strong&gt;metadata&lt;/strong&gt; table contains information about when the cloc run was made. Run time is stored two ways: as Unix epoch seconds in &lt;code&gt;id&lt;/code&gt; and as an ISO 8601 formatted text string in the local time zone (for example &lt;code&gt;2024-03-01 14:19:41&lt;/code&gt;) in &lt;code&gt;timestamp&lt;/code&gt;. The &lt;code&gt;--sql-append&lt;/code&gt; switch allows one to combine many runs in a single database; each run adds a row to the metadata table. The code count information resides in table &lt;strong&gt;t&lt;/strong&gt;. The &lt;code&gt;id&lt;/code&gt; key makes it easy to associate a run&#39;s code count with its metadata.&lt;/p&gt; 
&lt;p&gt;Let&#39;s repeat the code count examples of Perl, Python, SQLite, MySQL and PostgreSQL tarballs shown in the &lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#combine_reports&quot;&gt;Combine Reports&lt;/a&gt; example above, this time using the SQL output options and the &lt;a href=&quot;http://www.sqlite.org/&quot;&gt;SQLite&lt;/a&gt; database engine.&lt;/p&gt; 
&lt;p&gt;The &lt;code&gt;--sql&lt;/code&gt; switch tells cloc to generate output in the form of SQL table &lt;code&gt;create&lt;/code&gt; and &lt;code&gt;insert&lt;/code&gt; commands. The switch takes an argument of a file name to write these SQL statements into, or, if the argument is 1 (numeric one), streams output to STDOUT. Since the SQLite command line program, &lt;code&gt;sqlite3&lt;/code&gt;, can read commands from STDIN, we can dispense with storing SQL statements to a file and use &lt;code&gt;--sql 1&lt;/code&gt; to pipe data directly into the SQLite executable:&lt;/p&gt; 
&lt;pre&gt;
cloc --sql 1 --sql-project mariadb mariadb-server-10.1.zip | sqlite3 code.db
&lt;/pre&gt; 
&lt;p&gt;The &lt;code&gt;--sql-project mariadb&lt;/code&gt; part is optional; there&#39;s no need to specify a project name when working with just one code base. However, since we&#39;ll be adding code counts from four other tarballs, we&#39;ll only be able to identify data by input source if we supply a project name for each run.&lt;/p&gt; 
&lt;p&gt;Now that we have a database we will need to pass in the &lt;code&gt;--sql-append&lt;/code&gt; switch to tell cloc not to wipe out this database but instead add more data:&lt;/p&gt; 
&lt;pre&gt;
cloc --sql 1 --sql-project postgresql --sql-append postgresql-9.4.4.tar.bz2        | sqlite3 code.db
cloc --sql 1 --sql-project sqlite     --sql-append sqlite-amalgamation-3081101.zip | sqlite3 code.db
cloc --sql 1 --sql-project python     --sql-append Python-2.7.10.tar.xz            | sqlite3 code.db
cloc --sql 1 --sql-project perl       --sql-append perl-5.22.0.tar.gz              | sqlite3 code.db
&lt;/pre&gt; 
&lt;p&gt;Now the fun begins--we have a database, &lt;code&gt;code.db&lt;/code&gt;, with lots of information about the five projects and can query it for all manner of interesting facts.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Which is the longest file over all projects?&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;
prompt&amp;gt; sqlite3 code.db &#39;select project,file,nBlank+nComment+nCode as nL from t
                                 where nL = (select max(nBlank+nComment+nCode) from t)&#39;

sqlite|sqlite-amalgamation-3081101/sqlite3.c|161623
&lt;/pre&gt; 
&lt;p&gt;&lt;code&gt;sqlite3&lt;/code&gt;&#39;s default output format leaves a bit to be desired. We can add an option to the program&#39;s rc file, &lt;code&gt;~/.sqliterc&lt;/code&gt;, to show column headers:&lt;/p&gt; 
&lt;pre&gt;
  .header on
&lt;/pre&gt; 
&lt;p&gt;One might be tempted to also include&lt;/p&gt; 
&lt;pre&gt;
  .mode column
&lt;/pre&gt; 
&lt;p&gt;in &lt;code&gt;~/.sqliterc&lt;/code&gt; but this causes problems when the output has more than one row since the widths of entries in the first row govern the maximum width for all subsequent rows. Often this leads to truncated output--not at all desirable. One option is to write a custom SQLite output formatter such as &lt;code&gt;sqlite_formatter&lt;/code&gt;, included with cloc.&lt;/p&gt; 
&lt;p&gt;To use it, simply pass &lt;code&gt;sqlite3&lt;/code&gt;&#39;s STDOUT into &lt;code&gt;sqlite_formatter&lt;/code&gt; via a pipe:&lt;/p&gt; 
&lt;pre&gt;
prompt&amp;gt; sqlite3 code.db &#39;select project,file,nBlank+nComment+nCode as nL from t
                         where nL = (select max(nBlank+nComment+nCode) from t)&#39; | ./sqlite_formatter
  &lt;font color=&quot;darkgreen&quot;&gt;
  -- Loading resources from ~/.sqliterc
  Project File                                  nL
  _______ _____________________________________ ______
  sqlite  sqlite-amalgamation-3081101/sqlite3.c 161623
  &lt;/font&gt;
&lt;/pre&gt; 
&lt;p&gt;If the &quot;Project File&quot; line doesn&#39;t appear, add &lt;code&gt;.header on&lt;/code&gt; to your &lt;code&gt;~/.sqliterc&lt;/code&gt; file as explained above.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;What is the longest file over all projects?&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;
prompt&amp;gt; sqlite3 code.db &#39;select project,file,nBlank+nComment+nCode as nL from t
                         where nL = (select max(nBlank+nComment+nCode) from t)&#39; | sqlite_formatter

Project File                                  nL
_______ _____________________________________ ______
sqlite  sqlite-amalgamation-3081101/sqlite3.c 161623
&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;What is the longest file in each project?&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;
prompt&amp;gt; sqlite3 code.db &#39;select project,file,max(nBlank+nComment+nCode) as nL from t
                          group by project order by nL;&#39; | sqlite_formatter

Project    File                                                             nL
__________ ________________________________________________________________ ______
python     Python-2.7.10/Mac/Modules/qt/_Qtmodule.c                          28091
postgresql postgresql-9.4.4/src/interfaces/ecpg/preproc/preproc.c            54623
mariadb    server-10.1/storage/mroonga/vendor/groonga/lib/nfkc.c             80246
perl       perl-5.22.0/cpan/Locale-Codes/lib/Locale/Codes/Language_Codes.pm 100747
sqlite     sqlite-amalgamation-3081101/sqlite3.c                            161623
&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Which files in each project have the most code lines?&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;
prompt&amp;gt; sqlite3 code.db &#39;select project,file,max(nCode) as nL from t
                         group by project order by nL desc;&#39; | sqlite_formatter

Project    File                                                             nL
__________ ________________________________________________________________ ______
perl       perl-5.22.0/cpan/Locale-Codes/lib/Locale/Codes/Language_Codes.pm 100735
sqlite     sqlite-amalgamation-3081101/sqlite3.c                             97469
mariadb    server-10.1/storage/mroonga/vendor/groonga/lib/nfkc.c             80221
postgresql postgresql-9.4.4/src/interfaces/ecpg/preproc/preproc.c            45297
python     Python-2.7.10/Mac/Modules/qt/_Qtmodule.c                          26705
&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Which C source files with more than 300 lines have a comment ratio below 1%?&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;
prompt&amp;gt; sqlite3 code.db &#39;select project, file, nCode, nComment,
                         (100.0*nComment)/(nComment+nCode) as comment_ratio from t
                         where language=&quot;C&quot; and nCode &amp;gt; 300 and
                         comment_ratio &amp;lt; 1 order by comment_ratio;&#39; | sqlite_formatter

Project    File                                                                                            nCode nComment comment_ratio
__________ _______________________________________________________________________________________________ _____ ________ __________________
mariadb    server-10.1/storage/mroonga/vendor/groonga/lib/nfkc.c                                           80221       14 0.0174487443135789
python     Python-2.7.10/Python/graminit.c                                                                  2175        1 0.0459558823529412
postgresql postgresql-9.4.4/src/backend/snowball/libstemmer/stem_UTF_8_turkish.c                            2095        1 0.0477099236641221
postgresql postgresql-9.4.4/src/backend/snowball/libstemmer/stem_UTF_8_french.c                             1211        1 0.0825082508250825
postgresql postgresql-9.4.4/src/backend/snowball/libstemmer/stem_ISO_8859_1_french.c                        1201        1 0.0831946755407654
postgresql postgresql-9.4.4/src/backend/snowball/libstemmer/stem_UTF_8_hungarian.c                          1182        1 0.084530853761623
postgresql postgresql-9.4.4/src/backend/snowball/libstemmer/stem_ISO_8859_1_hungarian.c                     1178        1 0.0848176420695505
mariadb    server-10.1/strings/ctype-eucjpms.c                                                             67466       60 0.0888546633889169
postgresql postgresql-9.4.4/src/backend/snowball/libstemmer/stem_UTF_8_english.c                            1072        1 0.0931966449207828
postgresql postgresql-9.4.4/src/backend/snowball/libstemmer/stem_ISO_8859_1_english.c                       1064        1 0.0938967136150235
postgresql postgresql-9.4.4/src/backend/snowball/libstemmer/stem_UTF_8_spanish.c                            1053        1 0.094876660341556
postgresql postgresql-9.4.4/src/backend/snowball/libstemmer/stem_ISO_8859_1_spanish.c                       1049        1 0.0952380952380952
postgresql postgresql-9.4.4/src/backend/snowball/libstemmer/stem_UTF_8_italian.c                            1031        1 0.0968992248062016
postgresql postgresql-9.4.4/src/backend/snowball/libstemmer/stem_ISO_8859_1_italian.c                       1023        1 0.09765625
postgresql postgresql-9.4.4/src/backend/snowball/libstemmer/stem_UTF_8_portuguese.c                          981        1 0.10183299389002
postgresql postgresql-9.4.4/src/backend/snowball/libstemmer/stem_ISO_8859_1_portuguese.c                     975        1 0.102459016393443
postgresql postgresql-9.4.4/src/backend/snowball/libstemmer/stem_UTF_8_romanian.c                            967        1 0.103305785123967
postgresql postgresql-9.4.4/src/backend/snowball/libstemmer/stem_ISO_8859_2_romanian.c                       961        1 0.103950103950104
mariadb    server-10.1/strings/ctype-ujis.c                                                                67177       79 0.117461639110265
postgresql postgresql-9.4.4/src/backend/snowball/libstemmer/stem_UTF_8_finnish.c                             720        1 0.13869625520111
postgresql postgresql-9.4.4/src/backend/snowball/libstemmer/stem_UTF_8_porter.c                              717        1 0.139275766016713
postgresql postgresql-9.4.4/src/backend/snowball/libstemmer/stem_ISO_8859_1_finnish.c                        714        1 0.13986013986014
postgresql postgresql-9.4.4/src/backend/snowball/libstemmer/stem_ISO_8859_1_porter.c                         711        1 0.140449438202247
postgresql postgresql-9.4.4/src/backend/snowball/libstemmer/stem_KOI8_R_russian.c                            660        1 0.151285930408472
postgresql postgresql-9.4.4/src/backend/snowball/libstemmer/stem_UTF_8_russian.c                             654        1 0.152671755725191
python     Python-2.7.10/Mac/Modules/qt/_Qtmodule.c                                                        26705       42 0.157026956294164
python     Python-2.7.10/Mac/Modules/icn/_Icnmodule.c                                                       1521        3 0.196850393700787
mariadb    server-10.1/strings/ctype-extra.c                                                                8282       18 0.216867469879518
postgresql postgresql-9.4.4/src/bin/psql/sql_help.c                                                         3576        8 0.223214285714286
mariadb    server-10.1/strings/ctype-sjis.c                                                                34006       86 0.252258594391646
python     Python-2.7.10/Python/Python-ast.c                                                                6554       17 0.258712524729874
mariadb    server-10.1/strings/ctype-cp932.c                                                               34609       92 0.265122042592432
perl       perl-5.22.0/keywords.c                                                                           2815        8 0.283386468296139
python     Python-2.7.10/Mac/Modules/menu/_Menumodule.c                                                     3263       10 0.305530094714329
postgresql postgresql-9.4.4/src/backend/snowball/libstemmer/stem_UTF_8_dutch.c                               596        2 0.334448160535117
postgresql postgresql-9.4.4/src/backend/snowball/libstemmer/stem_ISO_8859_1_dutch.c                          586        2 0.340136054421769
mariadb    server-10.1/strings/ctype-gbk.c                                                                 10684       38 0.354411490393583
python     Python-2.7.10/Mac/Modules/qd/_Qdmodule.c                                                         6694       24 0.357249181303959
python     Python-2.7.10/Mac/Modules/win/_Winmodule.c                                                       3056       11 0.358656667753505
postgresql postgresql-9.4.4/src/backend/snowball/libstemmer/stem_UTF_8_german.c                              476        2 0.418410041841004
postgresql postgresql-9.4.4/src/backend/snowball/libstemmer/stem_ISO_8859_1_german.c                         470        2 0.423728813559322
mariadb    server-10.1/strings/ctype-euc_kr.c                                                               9956       44 0.44
postgresql postgresql-9.4.4/src/backend/utils/fmgrtab.c                                                     4815       23 0.475403059115337
python     Python-2.7.10/Mac/Modules/ctl/_Ctlmodule.c                                                       5442       28 0.511882998171846
python     Python-2.7.10/Mac/Modules/ae/_AEmodule.c                                                         1347        7 0.51698670605613
python     Python-2.7.10/Mac/Modules/app/_Appmodule.c                                                       1712        9 0.52295177222545
mariadb    server-10.1/strings/ctype-gb2312.c                                                               6377       35 0.54585152838428
mariadb    server-10.1/storage/tokudb/ft-index/third_party/xz-4.999.9beta/src/liblzma/lzma/fastpos_table.c   516        3 0.578034682080925
python     Python-2.7.10/Mac/Modules/evt/_Evtmodule.c                                                        504        3 0.591715976331361
python     Python-2.7.10/Modules/expat/xmlrole.c                                                            1256        8 0.632911392405063
postgresql postgresql-9.4.4/src/backend/snowball/libstemmer/stem_UTF_8_danish.c                              312        2 0.636942675159236
postgresql postgresql-9.4.4/src/backend/snowball/libstemmer/stem_ISO_8859_1_danish.c                         310        2 0.641025641025641
python     Python-2.7.10/Mac/Modules/res/_Resmodule.c                                                       1621       12 0.734843845682792
python     Python-2.7.10/Mac/Modules/drag/_Dragmodule.c                                                     1046        8 0.759013282732448
python     Python-2.7.10/Mac/Modules/list/_Listmodule.c                                                     1021        8 0.777453838678329
python     Python-2.7.10/Mac/Modules/te/_TEmodule.c                                                         1198       10 0.827814569536424
python     Python-2.7.10/Mac/Modules/cg/_CGmodule.c                                                         1190       10 0.833333333333333
python     Python-2.7.10/Modules/clmodule.c                                                                 2379       23 0.957535387177352
python     Python-2.7.10/Mac/Modules/folder/_Foldermodule.c                                                  306        3 0.970873786407767
&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;What are the ten longest files (based on code lines) that have no comments at all? Exclude header, .html, and YAML files.&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;
prompt&amp;gt; sqlite3 code.db &#39;select project, file, nCode from t
                         where nComment = 0 and
                         language not in (&quot;C/C++ Header&quot;, &quot;YAML&quot;, &quot;HTML&quot;)
                         order by nCode desc limit 10;&#39; | sqlite_formatter

Project File                                                                 nCode
_______ ____________________________________________________________________ _____
perl    perl-5.22.0/cpan/Unicode-Collate/Collate/Locale/ja.pl                 1938
python  Python-2.7.10/PCbuild/pythoncore.vcproj                               1889
python  Python-2.7.10/PC/VS8.0/pythoncore.vcproj                              1889
mariadb server-10.1/mysql-test/extra/binlog_tests/mysqlbinlog_row_engine.inc  1862
perl    perl-5.22.0/cpan/Unicode-Collate/Collate/Locale/zh_strk.pl            1589
perl    perl-5.22.0/cpan/Unicode-Collate/Collate/Locale/zh_zhu.pl             1563
mariadb server-10.1/storage/mroonga/vendor/groonga/configure.ac               1526
perl    perl-5.22.0/cpan/Unicode-Collate/Collate/Locale/zh_pin.pl             1505
mariadb server-10.1/mysql-test/suite/funcs_1/storedproc/storedproc_02.inc     1465
python  Python-2.7.10/PC/VS8.0/_bsddb.vcproj                                  1463
&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;What are the most popular languages (in terms of lines of code) in each project?&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;
prompt&amp;gt; sqlite3 code.db &#39;select project, language, sum(nCode) as SumCode from t
                         group by project,language
                         order by project,SumCode desc;&#39; | sqlite_formatter
Project    Language                  SumCode
__________ _________________________ _______
mariadb    C++                        983026
mariadb    C                          715018
mariadb    C/C++ Header               209394
mariadb    Bourne Shell                61943
mariadb    Perl                        35562
mariadb    Pascal                      32541
mariadb    HTML                        16489
mariadb    Javascript                  15540
mariadb    m4                          14215
mariadb    CMake                       12206
mariadb    XML                          5210
mariadb    Ruby                         4998
mariadb    Puppet                       3848
mariadb    make                         3631
mariadb    SQL                          3405
mariadb    Python                       2545
mariadb    Bourne Again Shell           1604
mariadb    Windows Module Definition    1211
mariadb    lex                           991
mariadb    yacc                          810
mariadb    DOS Batch                     700
mariadb    Prolog                        448
mariadb    RobotFramework                441
mariadb    CSS                           393
mariadb    JSON                          359
mariadb    dtrace                        306
mariadb    Windows Resource File         250
mariadb    Assembly                      237
mariadb    WiX source                    155
mariadb    Visual Basic                   88
mariadb    YAML                           65
mariadb    PHP                            24
mariadb    SKILL                          16
mariadb    sed                            16
mariadb    Windows Message File            6
mariadb    D                               4
mariadb    diff                            4
perl       Perl                       536445
perl       C                          155648
perl       C/C++ Header               147858
perl       Bourne Shell                42668
perl       Pascal                       8592
perl       XML                          2410
perl       YAML                         2078
perl       C++                          2033
perl       make                         1986
perl       Prolog                       1146
perl       JSON                         1037
perl       yacc                          998
perl       Windows Message File          489
perl       DOS Batch                     389
perl       Windows Resource File          85
perl       D                               8
perl       Lisp                            4
postgresql HTML                       785991
postgresql C                          736519
postgresql C/C++ Header                57014
postgresql SQL                         51926
postgresql yacc                        28491
postgresql Bourne Shell                17170
postgresql Perl                         9456
postgresql lex                          4285
postgresql make                         4114
postgresql m4                           1642
postgresql Windows Module Definition    1152
postgresql XSLT                          294
postgresql DOS Batch                      92
postgresql Assembly                       69
postgresql CSS                            69
postgresql D                              66
postgresql Windows Resource File          62
postgresql Lisp                           16
postgresql sed                            15
postgresql Python                         13
postgresql Bourne Again Shell             10
postgresql Windows Message File            5
python     Python                     434015
python     C                          375555
python     C/C++ Header                66942
python     Bourne Shell                45091
python     MSBuild script              38910
python     m4                          15559
python     Assembly                    12298
python     make                         2953
python     HTML                         2344
python     Windows Module Definition    2081
python     Objective-C                   635
python     Expect                        565
python     DOS Batch                     506
python     CSS                           328
python     Javascript                    229
python     Windows Resource File         207
python     C++                           128
python     vim script                    106
python     diff                          105
python     XML                            74
python     NAnt script                    30
python     Prolog                         24
python     Visual Basic                   12
sqlite     C                          101454
sqlite     C/C++ Header                 1546
&lt;/pre&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/1%7D%7D%7D&quot;&gt;&lt;/a&gt; &lt;a name=&quot;custom_column_output&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/%7B%7B%7B1&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#___top&quot; title=&quot;click to go to top of document&quot;&gt;Custom Column Output ▲&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;Cloc&#39;s default output is a text table with five columns: language, file count, number of blank lines, number of comment lines and number of code lines. The switches &lt;code&gt;--by-file&lt;/code&gt;, &lt;code&gt;--3&lt;/code&gt;, and &lt;code&gt;--by-percent&lt;/code&gt; generate additional information but sometimes even those are insufficient.&lt;/p&gt; 
&lt;p&gt;The &lt;code&gt;--sql&lt;/code&gt; option described in the previous section offers the ability to create custom output. This section has a pair of examples that show how to create custom columns. The first example includes an extra column, &lt;strong&gt;Total&lt;/strong&gt;, which is the sum of the numbers of blank, comment, and code lines. The second shows how to include the language name when running with &lt;code&gt;--by-file&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Example 1: Add a &quot;Totals&quot; column.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;The first step is to run cloc and save the output to a relational database, SQLite in this case:&lt;/p&gt; 
&lt;pre&gt;
cloc --sql 1 --sql-project x yaml-cpp-yaml-cpp-0.5.3.tar.gz | sqlite3 counts.db
&lt;/pre&gt; 
&lt;p&gt;(the tar file comes from the &lt;a href=&quot;https://github.com/jbeder/yaml-cpp&quot;&gt;YAML-C++&lt;/a&gt; project).&lt;/p&gt; 
&lt;p&gt;Second, we craft an SQL query that returns the regular cloc output plus an extra column for totals, then save the SQL statement to a file, &lt;code&gt;query_with_totals.sql&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;
-- file query_with_totals.sql
select Language, count(File)   as files                       ,
                 sum(nBlank)   as blank                       ,
                 sum(nComment) as comment                     ,
                 sum(nCode)    as code                        ,
                 sum(nBlank)+sum(nComment)+sum(nCode) as Total
    from t group by Language order by code desc;
&lt;/pre&gt; 
&lt;p&gt;Third, we run this query through SQLite using the &lt;code&gt;counts.db&lt;/code&gt; database. We&#39;ll include the &lt;code&gt;-header&lt;/code&gt; switch so that SQLite prints the column names:&lt;/p&gt; 
&lt;pre&gt;
&amp;gt; cat query_with_totals.sql | sqlite3 -header counts.db
Language|files|blank|comment|code|Total
C++|141|12786|17359|60378|90523
C/C++ Header|110|8566|17420|51502|77488
Bourne Shell|10|6351|6779|38264|51394
m4|11|2037|260|17980|20277
Python|30|1613|2486|4602|8701
MSBuild script|11|0|0|1711|1711
CMake|7|155|285|606|1046
make|5|127|173|464|764
Markdown|2|30|0|39|69
&lt;/pre&gt; 
&lt;p&gt;The extra column for &lt;strong&gt;Total&lt;/strong&gt; is there but the format is unappealing. Running the output through &lt;code&gt;sqlite_formatter&lt;/code&gt; yields the desired result:&lt;/p&gt; 
&lt;pre&gt;
&amp;gt; cat query_with_totals.sql | sqlite3 -header counts.db | sqlite_formatter
Language       files blank comment code  Total
______________ _____ _____ _______ _____ _____
C++              141 12786   17359 60378 90523
C/C++ Header     110  8566   17420 51502 77488
Bourne Shell      10  6351    6779 38264 51394
m4                11  2037     260 17980 20277
Python            30  1613    2486  4602  8701
MSBuild script    11     0       0  1711  1711
CMake              7   155     285   606  1046
make               5   127     173   464   764
Markdown           2    30       0    39    69
&lt;/pre&gt; 
&lt;p&gt;The next section, &lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#wrapping-cloc-in-other-scripts-&quot;&gt;Wrapping cloc in other scripts&lt;/a&gt;, shows one way these commands can be combined into a new utility program.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Example 2: Include a column for &quot;Language&quot; when running with &lt;code&gt;--by-file&lt;/code&gt;.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Output from &lt;code&gt;--by-file&lt;/code&gt; omits each file&#39;s language to save screen real estate; file paths for large projects can be long and including an extra 20 or so characters for a Language column can be excessive.&lt;/p&gt; 
&lt;p&gt;As an example, here are the first few lines of output using the same code base as in Example 1:&lt;/p&gt; 
&lt;pre&gt;
&amp;gt; cloc --by-file yaml-cpp-yaml-cpp-0.5.3.tar.gz
github.com/AlDanial/cloc v 1.81  T=1.14 s (287.9 files/s, 221854.9 lines/s)
--------------------------------------------------------------------------------------------------------------------------------------------
File                                                                                                     blank        comment           code
--------------------------------------------------------------------------------------------------------------------------------------------
yaml-cpp-yaml-cpp-0.5.3/test/gmock-1.7.0/configure                                                        2580           2264          13691
yaml-cpp-yaml-cpp-0.5.3/test/gmock-1.7.0/gtest/configure                                                  2541           2235          13446
yaml-cpp-yaml-cpp-0.5.3/test/gmock-1.7.0/gtest/fused-src/gtest/gtest.h                                    1972           4681          13408
yaml-cpp-yaml-cpp-0.5.3/test/gmock-1.7.0/fused-src/gmock/gmock.h                                          1585           3397           9216
yaml-cpp-yaml-cpp-0.5.3/test/integration/gen_emitter_test.cpp                                              999              0           8760
yaml-cpp-yaml-cpp-0.5.3/test/gmock-1.7.0/aclocal.m4                                                        987            100           8712
yaml-cpp-yaml-cpp-0.5.3/test/gmock-1.7.0/gtest/m4/libtool.m4                                               760             65           7176
yaml-cpp-yaml-cpp-0.5.3/test/gmock-1.7.0/gtest/build-aux/ltmain.sh                                         959           1533           7169
yaml-cpp-yaml-cpp-0.5.3/test/gmock-1.7.0/fused-src/gmock-gtest-all.cc                                     1514           3539           6390
yaml-cpp-yaml-cpp-0.5.3/test/gmock-1.7.0/gtest/fused-src/gtest/gtest-all.cc                               1312           2896           5384
yaml-cpp-yaml-cpp-0.5.3/test/gmock-1.7.0/gtest/test/gtest_unittest.cc                                     1226           1091           5098
yaml-cpp-yaml-cpp-0.5.3/test/gmock-1.7.0/gtest/include/gtest/internal/gtest-param-util-generated.h         349            235           4559
&lt;/pre&gt; 
&lt;p&gt;The absence of language identification for each file is a bit disappointing, but this can be remedied with a custom column solution.&lt;/p&gt; 
&lt;p&gt;The first step, creating a database, matches that from Example 1 so we&#39;ll go straight to the second step of creating the desired SQL query. We&#39;ll store this one in the file &lt;code&gt;by_file_with_language.sql&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;
-- file by_file_with_language.sql
select File, Language, nBlank   as blank  ,
                       nComment as comment,
                       nCode    as code
    from t order by code desc;
&lt;/pre&gt; 
&lt;p&gt;Our desired extra column appears when we pass this custom SQL query through our database:&lt;/p&gt; 
&lt;pre&gt;
&amp;gt; cat by_file_with_language.sql | sqlite3 -header counts.db | sqlite_formatter
File                                                                                               Language       blank comment code
__________________________________________________________________________________________________ ______________ _____ _______ _____
yaml-cpp-yaml-cpp-0.5.3/test/gmock-1.7.0/configure                                                 Bourne Shell    2580    2264 13691
yaml-cpp-yaml-cpp-0.5.3/test/gmock-1.7.0/gtest/configure                                           Bourne Shell    2541    2235 13446
yaml-cpp-yaml-cpp-0.5.3/test/gmock-1.7.0/gtest/fused-src/gtest/gtest.h                             C/C++ Header    1972    4681 13408
yaml-cpp-yaml-cpp-0.5.3/test/gmock-1.7.0/fused-src/gmock/gmock.h                                   C/C++ Header    1585    3397  9216
yaml-cpp-yaml-cpp-0.5.3/test/integration/gen_emitter_test.cpp                                      C++              999       0  8760
yaml-cpp-yaml-cpp-0.5.3/test/gmock-1.7.0/aclocal.m4                                                m4               987     100  8712
yaml-cpp-yaml-cpp-0.5.3/test/gmock-1.7.0/gtest/m4/libtool.m4                                       m4               760      65  7176
yaml-cpp-yaml-cpp-0.5.3/test/gmock-1.7.0/gtest/build-aux/ltmain.sh                                 Bourne Shell     959    1533  7169
yaml-cpp-yaml-cpp-0.5.3/test/gmock-1.7.0/fused-src/gmock-gtest-all.cc                              C++             1514    3539  6390
yaml-cpp-yaml-cpp-0.5.3/test/gmock-1.7.0/gtest/fused-src/gtest/gtest-all.cc                        C++             1312    2896  5384
yaml-cpp-yaml-cpp-0.5.3/test/gmock-1.7.0/gtest/test/gtest_unittest.cc                              C++             1226    1091  5098
yaml-cpp-yaml-cpp-0.5.3/test/gmock-1.7.0/gtest/include/gtest/internal/gtest-param-util-generated.h C/C++ Header     349     235  4559
&lt;/pre&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/1%7D%7D%7D&quot;&gt;&lt;/a&gt; &lt;a name=&quot;wrapping_cloc_in_other_scripts&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/%7B%7B%7B1&quot;&gt;&lt;/a&gt; * &lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#wrapping-cloc-in-other-scripts-&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#___top&quot; title=&quot;click to go to top of document&quot;&gt;Wrapping cloc in other scripts ▲&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;More complex code counting solutions are possible by wrapping cloc in scripts or programs. The &quot;total lines&quot; column from example 1 of &lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#custom-column-output-&quot;&gt;Custom Column Output&lt;/a&gt; could be simplified to a single command with this shell script (on Linux):&lt;/p&gt; 
&lt;pre&gt;
#!/bin/sh
#
# These commands must be in the user&#39;s $PATH:
#   cloc
#   sqlite3
#   sqlite_formatter
#
if test $# -eq 0 ; then
    echo &quot;Usage: $0  [cloc arguments]&quot;
    echo &quot;       Run cloc to count lines of code with an additional&quot;
    echo &quot;       output column for total lines (code+comment+blank).&quot;
    exit
fi
DBFILE=`tempfile`
cloc --sql 1 --sql-project x $@ | sqlite3 ${DBFILE}
SQL=&quot;select Language, count(File)   as files                       ,
                      sum(nBlank)   as blank                       ,
                      sum(nComment) as comment                     ,
                      sum(nCode)    as code                        ,
                      sum(nBlank)+sum(nComment)+sum(nCode) as Total
         from t group by Language order by code desc;
&quot;
echo ${SQL} | sqlite3 -header ${DBFILE} | sqlite_formatter
rm ${DBFILE}
&lt;/pre&gt; 
&lt;p&gt;Saving the lines above to &lt;code&gt;total_columns.sh&lt;/code&gt; and making it executable (&lt;code&gt;chmod +x total_columns.sh&lt;/code&gt;) would let us do&lt;/p&gt; 
&lt;pre&gt;
./total_columns.sh yaml-cpp-yaml-cpp-0.5.3.tar.gz
&lt;/pre&gt; 
&lt;p&gt;to directly get&lt;/p&gt; 
&lt;pre&gt;
Language       files blank comment code  Total
______________ _____ _____ _______ _____ _____
C++              141 12786   17359 60378 90523
C/C++ Header     110  8566   17420 51502 77488
Bourne Shell      10  6351    6779 38264 51394
m4                11  2037     260 17980 20277
Python            30  1613    2486  4602  8701
MSBuild script    11     0       0  1711  1711
CMake              7   155     285   606  1046
make               5   127     173   464   764
Markdown           2    30       0    39    69
&lt;/pre&gt; 
&lt;p&gt;Other examples:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Count code from a specific branch of a web-hosted git repository and send the results as a .csv email attachment: &lt;a href=&quot;https://github.com/dannyloweatx/checkmarx&quot;&gt;https://github.com/dannyloweatx/checkmarx&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/1%7D%7D%7D&quot;&gt;&lt;/a&gt; &lt;a name=&quot;git_and_UTF8_pathnames&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/%7B%7B%7B1&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#___top&quot; title=&quot;click to go to top of document&quot;&gt;git and UTF8 pathnames ▲&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;cloc&#39;s &lt;code&gt;--git&lt;/code&gt; option may fail if you work with directory or file names with UTF-8 characters (for example, see &lt;a href=&quot;https://github.com/AlDanial/cloc/issues/457&quot;&gt;issue 457&lt;/a&gt;). The solution, &lt;a href=&quot;https://stackoverflow.com/questions/22827239/how-to-make-git-properly-display-utf-8-encoded-pathnames-in-the-console-window&quot;&gt;https://stackoverflow.com/questions/22827239/how-to-make-git-properly-display-utf-8-encoded-pathnames-in-the-console-window&lt;/a&gt;, is to apply this git configuration command:&lt;/p&gt; 
&lt;pre&gt;
git config --global core.quotepath off
&lt;/pre&gt; 
&lt;p&gt;Your console&#39;s font will need to be capable of displaying Unicode characters.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/1%7D%7D%7D&quot;&gt;&lt;/a&gt; &lt;a name=&quot;scale_factors&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/%7B%7B%7B1&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#___top&quot; title=&quot;click to go to top of document&quot;&gt;Third Generation Language Scale Factors ▲&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;cloc versions before 1.50 by default computed, for the provided inputs, a rough estimate of how many lines of code would be needed to write the same code in a hypothetical third-generation computer language. To produce this output one must now use the &lt;code&gt;--3&lt;/code&gt; switch.&lt;/p&gt; 
&lt;p&gt;Scale factors were derived from the 2006 version of language gearing ratios listed at Mayes Consulting web site, &lt;a href=&quot;http://softwareestimator.com/IndustryData2.htm&quot;&gt;http://softwareestimator.com/IndustryData2.htm&lt;/a&gt;, using this equation:&lt;/p&gt; 
&lt;p&gt;cloc scale factor for language X = 3rd generation default gearing ratio / language X gearing ratio&lt;/p&gt; 
&lt;p&gt;For example, cloc 3rd generation scale factor for DOS Batch = 80 / 128 = 0.625.&lt;/p&gt; 
&lt;p&gt;The biggest flaw with this approach is that gearing ratios are defined for logical lines of source code not physical lines (which cloc counts). The values in cloc&#39;s &#39;scale&#39; and &#39;3rd gen. equiv.&#39; columns should be taken with a large grain of salt.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/1%7D%7D%7D&quot;&gt;&lt;/a&gt; &lt;a name=&quot;options_txt&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/%7B%7B%7B1&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#___top&quot; title=&quot;click to go to top of document&quot;&gt;options.txt configuration file ▲&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;If you find yourself using the same command line switches every time you invoke cloc, you can save some typing by adding those switches to the &lt;code&gt;options.txt&lt;/code&gt; runtime configuration file. cloc will look for this file in the following default locations:&lt;/p&gt; 
&lt;pre&gt;
# Linux, NetBSD, FreeBSD, macOS:
/home/USERNAME/.config/cloc/options.txt

# Windows
C:\Users\USERNAME\AppData\Roaming\cloc
&lt;/pre&gt; 
&lt;p&gt;If you run cloc with &lt;code&gt;--help&lt;/code&gt;, cloc will tell you where it expects to find this config file file. The information appears by the explanation of the &lt;code&gt;--config&lt;/code&gt; switch after the text &lt;code&gt;the default location of&lt;/code&gt;. On Unix-like operating systems, this can be simplified to&lt;/p&gt; 
&lt;pre&gt;
&amp;gt; cloc --help | grep &quot;default location&quot;
             the default location of /home/al/.config/cloc/options.txt.
&lt;/pre&gt; 
&lt;p&gt;and in a Windows &lt;code&gt;cmd&lt;/code&gt; terminal with&lt;/p&gt; 
&lt;pre&gt;
&amp;gt; cloc --help | findstr default | findstr location
             the default location of C:\Users\al\AppData\Roaming\cloc
&lt;/pre&gt; 
&lt;p&gt;Place each switch and arguments, if any, on a line by itself. Lines prefixed with &lt;code&gt;#&lt;/code&gt; symbol are ignored as comments and blank lines are skipped. Leading hyphens on the switches are optional. Here&#39;s a sample file:&lt;/p&gt; 
&lt;pre&gt;
# options.txt
--vcs git
v      # verbose level 1
exclude-ext svg,html
&lt;/pre&gt; 
&lt;p&gt;The path to the &lt;code&gt;options.txt&lt;/code&gt; file can also be specified with the &lt;code&gt;--config FILE&lt;/code&gt; switch.&lt;/p&gt; 
&lt;p&gt;Finally, if cloc finds an &lt;code&gt;options.txt&lt;/code&gt; file in the same directory as files given by any of these switches (in the listed priority), it will use that configuration file from that location:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;code&gt;--list-file&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--exclude-list-file&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--read-lang-def&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--force-lang-def&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--diff-list-file&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Run with &lt;code&gt;--verbose&lt;/code&gt; to have cloc tell you which, if any, &lt;code&gt;options.txt&lt;/code&gt; file it uses.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/1%7D%7D%7D&quot;&gt;&lt;/a&gt; &lt;a name=&quot;Java Programmatic Interface&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/%7B%7B%7B1&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#___top&quot; title=&quot;click to go to top of document&quot;&gt;Java Programmatic Interface ▲&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/seart-group/jcloc/commits?author=dabico&quot;&gt;Ozren Dabić&lt;/a&gt; created a Java programmatic interface to cloc. It is available at &lt;a href=&quot;https://github.com/seart-group/jcloc&quot;&gt;https://github.com/seart-group/jcloc&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/1%7D%7D%7D&quot;&gt;&lt;/a&gt; &lt;a name=&quot;complex_regex_recursion&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/%7B%7B%7B1&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#___top&quot; title=&quot;click to go to top of document&quot;&gt;Complex regular subexpression recursion limit ▲&lt;/a&gt;&lt;/h1&gt; 
&lt;p&gt;cloc relies on the Regexp::Common module&#39;s regular expressions to remove comments from source code. If comments are malformed, for example the &lt;code&gt;/*&lt;/code&gt; start comment marker appears in a C program without a corresponding &lt;code&gt;*/&lt;/code&gt; marker, the regular expression engine could enter a recursive loop, eventually triggering the warning &lt;code&gt;Complex regular subexpression recursion limit&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;The most common cause for this warning is the existence of comment markers in string literals. While language compilers and interpreters are smart enough to recognize that &lt;code&gt;&quot;/*&quot;&lt;/code&gt; (for example) is a string and not a comment, cloc is fooled. File path globs, as in this line of JavaScript&lt;/p&gt; 
&lt;pre&gt;var paths = globArray(&quot;**/*.js&quot;, {cwd: srcPath});
&lt;/pre&gt; 
&lt;p&gt;are frequent culprits.&lt;/p&gt; 
&lt;p&gt;In an attempt to overcome this problem, a different algorithm which removes comment markers in strings can be enabled with the &lt;code&gt;--strip-str-comments&lt;/code&gt; switch. Doing so, however, has drawbacks: cloc will run more slowly and the output of &lt;code&gt;--strip-comments&lt;/code&gt; will contain strings that no longer match the input source.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/1%7D%7D%7D&quot;&gt;&lt;/a&gt; &lt;a name=&quot;Limitations&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/%7B%7B%7B1&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#___top&quot; title=&quot;click to go to top of document&quot;&gt;Limitations ▲&lt;/a&gt;&lt;/h1&gt; 
&lt;p&gt;Identifying comments within source code is trickier than one might expect. Many languages would need a complete parser to be counted correctly. cloc does not attempt to parse any of the languages it aims to count and therefore is an imperfect tool. The following are known problems:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; Lines containing both source code and comments are counted as lines of code. &lt;/li&gt; 
 &lt;li&gt; Comment markers within strings or &lt;a href=&quot;http://www.faqs.org/docs/abs/HTML/here-docs.html&quot;&gt;here-documents&lt;/a&gt; are treated as actual comment markers and not string literals. For example the following lines of C code &lt;pre&gt;printf(&quot; /* &quot;);
for (i = 0; i &amp;lt; 100; i++) {
    a += i;
}
printf(&quot; */ &quot;);
&lt;/pre&gt; look to cloc like this: &lt;pre&gt;printf(&quot; xxxxxxx
xxxxxxx
xxxxxxx
xxxxxxx
xxxxxxx     &quot;);
&lt;/pre&gt; where &lt;tt&gt;xxxxxxx&lt;/tt&gt; represents cloc&#39;s view of commented text. Therefore cloc counts the five lines as two lines of C code and three lines of comments (lines with both code and comment are counted as code). &lt;p&gt;If you suspect your code has such strings, use the switch &lt;code&gt;--strip-str-comments&lt;/code&gt; to switch to the algorithm which removes embedded comment markers. Its use will render the five lines above as&lt;/p&gt; &lt;pre&gt;printf(&quot;  &quot;);
for (i = 0; i &amp;lt; 100; i++) {
    a += i;
}
printf(&quot;  &quot;);
&lt;/pre&gt; &lt;p&gt;and therefore return a count of five lines of code. See the &lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#complex-regular-subexpression-recursion-limit-&quot;&gt;previous section&lt;/a&gt; on drawbacks to using &lt;code&gt;--strip-str-comments&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; Embedded languages are not recognized. For example, an HTML file containing JavaScript will be counted entirely as HTML. &lt;/li&gt; 
 &lt;li&gt; Python docstrings can serve several purposes. They may contain documentation, comment out blocks of code, or they can be regular strings (when they appear on the right hand side of an assignment or as a function argument). cloc is unable to infer the meaning of docstrings by context; by default cloc treats all docstrings as comments. The switch &lt;tt&gt;--docstring-as-code&lt;/tt&gt; treats all docstrings as code. &lt;/li&gt; 
 &lt;li&gt; Language definition files read with &lt;tt&gt;--read-lang-def&lt;/tt&gt; or &lt;tt&gt;--force-lang-def&lt;/tt&gt; must be plain ASCII text files. &lt;/li&gt; 
 &lt;li&gt; cloc treats compiler pragma&#39;s, for example &lt;tt&gt;#if&lt;/tt&gt; / &lt;tt&gt;#endif&lt;/tt&gt;, as code even if these are used to block lines of source from being compiled; the blocked lines still contribute to the code count. &lt;/li&gt; 
 &lt;li&gt; On Windows, cloc will fail with &lt;tt&gt;Can&#39;t cd to ... No such file or directory at 
   &lt;embedded&gt;
    /File/Find.pm
   &lt;/embedded&gt;&lt;/tt&gt; if the code being scanned has file paths longer than 255 characters. A work-around is to run cloc from the Windows Subsystem for Linux (WSL). &lt;/li&gt; 
 &lt;li&gt; cloc&#39;s comment match code uses regular expressions which cannot properly account for nested comments using the same comment markers (such as &lt;tt&gt;/* /* */ */&lt;/tt&gt;). &lt;/li&gt; 
 &lt;li&gt; XML comments embedded within &lt;tt&gt;CDATA&lt;/tt&gt; blocks are counted as comments rather than code. &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/1%7D%7D%7D&quot;&gt;&lt;/a&gt; &lt;a name=&quot;AdditionalLanguages&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/%7B%7B%7B1&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#___top&quot; title=&quot;click to go to top of document&quot;&gt;Requesting Support for Additional Languages ▲&lt;/a&gt;&lt;/h1&gt; 
&lt;p&gt;If cloc does not recognize a language you are interested in counting, create a &lt;a href=&quot;https://github.com/AlDanial/cloc/issues&quot;&gt;GitHub issue&lt;/a&gt; requesting support for your language. Include this information:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; File extensions associated with the language. If the language does not rely on file extensions and instead works with fixed file names or with `#!` style program invocations, explain what those are.&lt;/li&gt; 
 &lt;li&gt; A description of how comments are defined.&lt;/li&gt; 
 &lt;li&gt; Links to sample code.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/1%7D%7D%7D&quot;&gt;&lt;/a&gt; &lt;a name=&quot;reporting_problems&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/%7B%7B%7B1&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#___top&quot; title=&quot;click to go to top of document&quot;&gt;Reporting Problems ▲&lt;/a&gt;&lt;/h1&gt; 
&lt;p&gt;If you encounter a problem with cloc, first check to see if you&#39;re running with the latest version of the tool:&lt;/p&gt; 
&lt;pre&gt;
  cloc --version
&lt;/pre&gt; 
&lt;p&gt;If the version is older than the most recent release at &lt;a href=&quot;https://github.com/AlDanial/cloc/releases&quot;&gt;https://github.com/AlDanial/cloc/releases&lt;/a&gt;, download the latest version and see if it solves your problem.&lt;/p&gt; 
&lt;p&gt;If the problem happens with the latest release, submit a new issue at &lt;a href=&quot;https://github.com/AlDanial/cloc/issues&quot;&gt;https://github.com/AlDanial/cloc/issues&lt;/a&gt; &lt;em&gt;only&lt;/em&gt; if you can supply enough information for anyone reading the issue report to reproduce the problem. That means providing&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; the operating system you&#39;re running on&lt;/li&gt; 
 &lt;li&gt; the cloc command with all options&lt;/li&gt; 
 &lt;li&gt; the code you are counting (URL to a public git repo or zip file or tar file, et cetera)&lt;/li&gt; 
&lt;/ol&gt; The last item is generally problematic. If the code base is proprietary or amounts to more than a few dozen kilobytes, you&#39;ll need to try to reconstruct similar inputs or demonstrate the problem with an existing public code base. 
&lt;p&gt;Problem reports that cannot be reproduced will be ignored and eventually closed.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/1%7D%7D%7D&quot;&gt;&lt;/a&gt; &lt;a name=&quot;citation&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/%7B%7B%7B1&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#___top&quot; title=&quot;click to go to top of document&quot;&gt;Citation ▲&lt;/a&gt;&lt;/h1&gt; 
&lt;p&gt;Please use the following bibtex entry to cite cloc in a publication:&lt;/p&gt; 
&lt;pre&gt;
@software{adanial_cloc,
  author       = {Albert Danial},
  title        = {cloc: v1.92},
  month        = dec,
  year         = 2021,
  publisher    = {Zenodo},
  version      = {v1.92},
  doi          = {10.5281/zenodo.5760077},
  url          = {https://doi.org/10.5281/zenodo.5760077}
}
&lt;/pre&gt; 
&lt;p&gt;(Update the version number and corresponding year if this entry is outdated.)&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/1%7D%7D%7D&quot;&gt;&lt;/a&gt; &lt;a name=&quot;Acknowledgments&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/%7B%7B%7B1&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#___top&quot; title=&quot;click to go to top of document&quot;&gt;Acknowledgments ▲&lt;/a&gt;&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/wolframroesler&quot;&gt;Wolfram Rösler&lt;/a&gt; provided most of the code examples in the test suite. These examples come from his &lt;a href=&quot;http://helloworldcollection.de/&quot;&gt;Hello World collection&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Ismet Kursunoglu found errors with the MUMPS counter and provided access to a computer with a large body of MUMPS code to test cloc.&lt;/p&gt; 
&lt;p&gt;Tod Huggins gave helpful suggestions for the Visual Basic filters.&lt;/p&gt; 
&lt;p&gt;Anton Demichev found a flaw with the JSP counter in cloc v0.76 and wrote the XML output generator for the &lt;code&gt;--xml&lt;/code&gt; option.&lt;/p&gt; 
&lt;p&gt;Reuben Thomas pointed out that ISO C99 allows &lt;code&gt;//&lt;/code&gt; as a comment marker, provided code for the &lt;code&gt;--no3&lt;/code&gt; and &lt;code&gt;--stdin-name&lt;/code&gt; options, counting the m4 language, and suggested several user-interface enhancements.&lt;/p&gt; 
&lt;p&gt;Michael Bello provided code for the &lt;code&gt;--opt-match-f&lt;/code&gt;, &lt;code&gt;--opt-not-match-f&lt;/code&gt;, &lt;code&gt;--opt-match-d&lt;/code&gt;, and &lt;code&gt;--opt-not-match-d&lt;/code&gt; options.&lt;/p&gt; 
&lt;p&gt;Mahboob Hussain inspired the &lt;code&gt;--original-dir&lt;/code&gt; and &lt;code&gt;--skip-uniqueness&lt;/code&gt; options, found a bug in the duplicate file detection logic and improved the JSP filter.&lt;/p&gt; 
&lt;p&gt;Randy Sharo found and fixed an uninitialized variable bug for shell scripts having only one line.&lt;/p&gt; 
&lt;p&gt;Steven Baker found and fixed a problem with the YAML output generator.&lt;/p&gt; 
&lt;p&gt;Greg Toth provided code to improve blank line detection in COBOL.&lt;/p&gt; 
&lt;p&gt;Joel Oliveira provided code to let &lt;code&gt;--exclude-list-file&lt;/code&gt; handle directory name exclusion.&lt;/p&gt; 
&lt;p&gt;Blazej Kroll provided code to produce an XSLT file, &lt;code&gt;cloc-diff.xsl&lt;/code&gt;, when producing XML output for the &lt;code&gt;--diff&lt;/code&gt; option.&lt;/p&gt; 
&lt;p&gt;Denis Silakov enhanced the code which generates &lt;code&gt;cloc.xsl&lt;/code&gt; when using &lt;code&gt;--by-file&lt;/code&gt; and &lt;code&gt;--by-file-by-lang&lt;/code&gt; options, and provided an XSL file that works with &lt;code&gt;--diff&lt;/code&gt; output.&lt;/p&gt; 
&lt;p&gt;Andy (&lt;a href=&quot;mailto:awalshe@sf.net&quot;&gt;awalshe@sf.net&lt;/a&gt;) provided code to fix several bugs: correct output of &lt;code&gt;--counted&lt;/code&gt; so that only files that are used in the code count appear and that results are shown by language rather than file name; allow &lt;code&gt;--diff&lt;/code&gt; output from multiple runs to be summed together with &lt;code&gt;--sum-reports&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Jari Aalto created the initial version of &lt;code&gt;cloc.1.pod&lt;/code&gt; and maintains the Debian package for cloc.&lt;/p&gt; 
&lt;p&gt;Mikkel Christiansen (&lt;a href=&quot;mailto:mikkels@gmail.com&quot;&gt;mikkels@gmail.com&lt;/a&gt;) provided counter definitions for Clojure and ClojureScript.&lt;/p&gt; 
&lt;p&gt;Vera Djuraskovic from &lt;a href=&quot;http://webhostinggeeks.com/&quot;&gt;Webhostinggeeks.com&lt;/a&gt; provided the &lt;a href=&quot;http://science.webhostinggeeks.com/cloc&quot;&gt;Serbo-Croatian&lt;/a&gt; translation.&lt;/p&gt; 
&lt;p&gt;Gill Ajoft of &lt;a href=&quot;http://www.ajoft.com&quot;&gt;Ajoft Software&lt;/a&gt; provided the &lt;a href=&quot;http://www.ajoft.com/wpaper/aj-cloc.html&quot;&gt;Bulgarian&lt;/a&gt; translation.&lt;/p&gt; 
&lt;p&gt;The &lt;a href=&quot;http://newknowledgez.com/&quot;&gt;Knowledge Team&lt;/a&gt; provided the &lt;a href=&quot;http://newknowledgez.com/cloc.html&quot;&gt;Slovakian&lt;/a&gt; translation.&lt;/p&gt; 
&lt;p&gt;Erik Gooven Arellano Casillas provided an update to the MXML counter to recognize ActionScript comments.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;http://g14n.info&quot;&gt;Gianluca Casati&lt;/a&gt; created the &lt;a href=&quot;https://metacpan.org/pod/App::cloc&quot;&gt;cloc CPAN package&lt;/a&gt;.&lt;/p&gt; 
&lt;!-- broken link
Mary Stefanova provided the
[Polish](http://www.trevister.com/blog/cloc.html)
translation. ---&gt; 
&lt;p&gt;Ryan Lindeman implemented the &lt;code&gt;--by-percent&lt;/code&gt; feature.&lt;/p&gt; 
&lt;p&gt;Kent C. Dodds, &lt;a href=&quot;https://twitter.com/kentcdodd&quot;&gt;@kentcdodds&lt;/a&gt;, created and maintains the npm package of cloc.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;http://kudoybook.com&quot;&gt;Viktoria Parnak&lt;/a&gt; provided the &lt;a href=&quot;http://blog.kudoybook.com/cloc/&quot;&gt;Ukrainian&lt;/a&gt; translation.&lt;/p&gt; 
&lt;p&gt;Natalie Harmann provided the &lt;a href=&quot;http://www.besteonderdelen.nl/blog/?p=5426&quot;&gt;Belarussian&lt;/a&gt; translation.&lt;/p&gt; 
&lt;p&gt;Nithyal at &lt;a href=&quot;http://healthcareadministrationdegree.co/&quot;&gt;Healthcare Administration Portal&lt;/a&gt; provided the &lt;a href=&quot;http://healthcareadministrationdegree.co/socialwork/aldanial-cloc/&quot;&gt;Tamil&lt;/a&gt; translation.&lt;/p&gt; 
&lt;p&gt;Patricia Motosan provided the &lt;a href=&quot;http://www.bildelestore.dk/blog/cloc-contele-de-linii-de-cod/&quot;&gt;Romanian&lt;/a&gt; translation.&lt;/p&gt; 
&lt;!-- broken link
The [Garcinia Cambogia Review Team](http://www.garciniacambogiareviews.ca/)
provided the
[Arabic translation](http://www.garciniacambogiareviews.ca/translations/aldanial-cloc/). ---&gt; 
&lt;p&gt;Gajk Melikyan provided the provided the &lt;a href=&quot;http://students.studybay.com/?p=34&quot;&gt;Armenian translation&lt;/a&gt; for &lt;a href=&quot;http://studybay.com&quot;&gt;http://studybay.com&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;http://www.forallworld.com/cloc-grof-sornyi-kodot/&quot;&gt;Hungarian translation&lt;/a&gt; courtesy of &lt;a href=&quot;http://www.forallworld.com/&quot;&gt;Zsolt Boros&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/stsnel&quot;&gt;Sietse Snel&lt;/a&gt; implemented the parallel processing capability available with the &lt;tt&gt;--processes=&lt;i&gt;N&lt;/i&gt;&lt;/tt&gt; switch.&lt;/p&gt; 
&lt;p&gt;The development of cloc was partially funded by the Northrop Grumman Corporation.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/1%7D%7D%7D&quot;&gt;&lt;/a&gt; &lt;a name=&quot;Copyright&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/%7B%7B%7B1&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/#___top&quot; title=&quot;click to go to top of document&quot;&gt;Copyright ▲&lt;/a&gt;&lt;/h1&gt; 
&lt;p&gt;Copyright (c) 2006-2025, &lt;a href=&quot;https://github.com/AlDanial&quot;&gt;Al Danial&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/AlDanial/cloc/master/1%7D%7D%7D&quot;&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>darold/ora2pg</title>
      <link>https://github.com/darold/ora2pg</link>
      <description>&lt;p&gt;Ora2Pg is a free tool used to migrate an Oracle database to a PostgreSQL compatible schema. It connects your Oracle database, scan it automatically and extracts its structure or data, it then generates SQL scripts that you can load into PostgreSQL.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;NAME Ora2Pg - Oracle to PostgreSQL database schema converter&lt;/p&gt; 
&lt;p&gt;DESCRIPTION Ora2Pg is a free tool used to migrate an Oracle database to a PostgreSQL compatible schema. It connects your Oracle database, scans it automatically and extracts its structure or data, then generates SQL scripts that you can load into your PostgreSQL database.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Ora2Pg can be used for anything from reverse engineering Oracle database
to huge enterprise database migration or simply replicating some Oracle
data into a PostgreSQL database. It is really easy to use and doesn&#39;t
require any Oracle database knowledge other than providing the
parameters needed to connect to the Oracle database.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;FEATURES Ora2Pg consist of a Perl script (ora2pg) and a Perl module (Ora2Pg.pm), the only thing you have to modify is the configuration file ora2pg.conf by setting the DSN to the Oracle database and optionally the name of a schema. Once that&#39;s done you just have to set the type of export you want: TABLE with constraints, VIEW, MVIEW, TABLESPACE, SEQUENCE, INDEXES, TRIGGER, GRANT, FUNCTION, PROCEDURE, PACKAGE, PARTITION, TYPE, INSERT or COPY, FDW, QUERY, KETTLE, SYNONYM.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;By default Ora2Pg exports to a file that you can load into PostgreSQL
with the psql client, but you can also import directly into a PostgreSQL
database by setting its DSN into the configuration file. With all
configuration options of ora2pg.conf you have full control of what
should be exported and how.

Features included:

        - Export full database schema (tables, views, sequences, indexes), with
          unique, primary, foreign key and check constraints.
        - Export grants/privileges for users and groups.
        - Export range/list partitions and sub partitions.
        - Export a table selection (by specifying the table names).
        - Export Oracle schema to a PostgreSQL 8.4+ schema.
        - Export predefined functions, triggers, procedures, packages and
          package bodies.
        - Export full data or following a WHERE clause.
        - Full support of Oracle BLOB object as PG BYTEA.
        - Export Oracle views as PG tables.
        - Export Oracle user defined types.
        - Provide some basic automatic conversion of PLSQL code to PLPGSQL.
        - Works on any platform.
        - Export Oracle tables as foreign data wrapper tables.
        - Export materialized view.
        - Show a  report of an Oracle database content.
        - Migration cost assessment of an Oracle database.
        - Migration difficulty level assessment of an Oracle database.
        - Migration cost assessment of PL/SQL code from a file.
        - Migration cost assessment of Oracle SQL queries stored in a file.
        - Generate XML ktr files to be used with Penthalo Data Integrator (Kettle)
        - Export Oracle locator and spatial geometries into PostGis.
        - Export DBLINK as Oracle FDW.
        - Export SYNONYMS as views.
        - Export DIRECTORY as external table or directory for external_file extension.
        - Dispatch a list of SQL orders over multiple PostgreSQL connections
        - Perform a diff between Oracle and PostgreSQL database for test purpose.
        - MySQL/MariaDB and Microsoft SQL Server migration.

Ora2Pg does its best to automatically convert your Oracle database to
PostgreSQL but there&#39;s still manual works to do. The Oracle specific
PL/SQL code generated for functions, procedures, packages and triggers
has to be reviewed to match the PostgreSQL syntax. You will find some
useful recommendations on porting Oracle PL/SQL code to PostgreSQL
PL/PGSQL at &quot;Converting from other Databases to PostgreSQL&quot;, section:
Oracle (http://wiki.postgresql.org/wiki/Main_Page).

See http://ora2pg.darold.net/report.html for a HTML sample of an Oracle
database migration report.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;INSTALLATION All Perl modules can always be found at CPAN (&lt;a href=&quot;http://search.cpan.org/&quot;&gt;http://search.cpan.org/&lt;/a&gt;). Just type the full name of the module (ex: DBD::Oracle) into the search input box, it will brings you the page for download.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Releases of Ora2Pg stay at SF.net
(https://sourceforge.net/projects/ora2pg/).

Under Windows you should install Strawberry Perl
(http://strawberryperl.com/) and the OSes corresponding Oracle clients.
Since version 5.32 this Perl distribution include pre-compiled driver of
DBD::Oracle and DBD::Pg.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Requirement The Oracle Instant Client or a full Oracle installation must be installed on the system. You can download the RPM from Oracle download center:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    rpm -ivh oracle-instantclient12.2-basic-12.2.0.1.0-1.x86_64.rpm
    rpm -ivh oracle-instantclient12.2-devel-12.2.0.1.0-1.x86_64.rpm
    rpm -ivh oracle-instantclient12.2-jdbc-12.2.0.1.0-1.x86_64.rpm
    rpm -ivh oracle-instantclient12.2-sqlplus-12.2.0.1.0-1.x86_64.rpm

or simply download the corresponding ZIP archives from Oracle download
center and install them where you want, for example:
/opt/oracle/instantclient_12_2/

You also need a modern Perl distribution (perl 5.10 and more). To
connect to a database and proceed to his migration you need the DBI Perl
module &amp;gt; 1.614. To migrate an Oracle database you need the DBD::Oracle
Perl modules to be installed.

To install DBD::Oracle and have it working you need to have the Oracle
client libraries installed and the ORACLE_HOME environment variable must
be defined.

If you plan to export a MySQL database you need to install the Perl
module DBD::MySQL which requires that the mysql client libraries are
installed.

If you plan to export a SQL Server database you need to install the Perl
module DBD::ODBC which requires that the unixODBC package is installed.

On some Perl distribution you may need to install the Time::HiRes Perl
module.

If your distribution doesn&#39;t include these Perl modules you can install
them using CPAN:

        perl -MCPAN -e &#39;install DBD::Oracle&#39;
        perl -MCPAN -e &#39;install DBD::MySQL&#39;
        perl -MCPAN -e &#39;install DBD::ODBC&#39;
        perl -MCPAN -e &#39;install Time::HiRes&#39;

otherwise use the packages provided by your distribution.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Optional By default Ora2Pg dumps export to flat files, to load them into your PostgreSQL database you need the PostgreSQL client (psql). If you don&#39;t have it on the host running Ora2Pg you can always transfer these files to a host with the psql client installed. If you prefer to load export &#39;on the fly&#39;, the perl module DBD::Pg is required.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Ora2Pg allows you to dump all output in a compressed gzip file, to do
that you need the Compress::Zlib Perl module or if you prefer using
bzip2 compression, the program bzip2 must be available in your PATH.

If your distribution doesn&#39;t include these Perl modules you can install
them using CPAN:

        perl -MCPAN -e &#39;install DBD::Pg&#39;
        perl -MCPAN -e &#39;install Compress::Zlib&#39;

otherwise use the packages provided by your distribution.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Instruction for SQL Server For SQL Server you need to install the unixodbc package and the Perl DBD::ODBC driver:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;        sudo apt install unixodbc
        sudo apt install libdbd-odbc-perl

or

        sudo yum install unixodbc
        sudo yum install perl-DBD-ODBC
        sudo yum install perl-DBD-Pg

then install the Microsoft ODBC Driver for SQL Server. Follow the
instructions relative to your operating system from here:

        https://docs.microsoft.com/fr-fr/sql/connect/odbc/linux-mac/installing-the-microsoft-odbc-driver-for-sql-server?view=sql-server-ver16

Once it is done set the following in the /etc/odbcinst.ini file by
adjusting the SQL Server ODBC driver version:

        [msodbcsql18]
        Description=Microsoft ODBC Driver 18 for SQL Server
        Driver=/opt/microsoft/msodbcsql18/lib64/libmsodbcsql-18.0.so.1.1
        UsageCount=1

See ORACLE_DSN to know how to use the driver to connect to your MSSQL
database.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Installing Ora2Pg Like any other Perl Module Ora2Pg can be installed with the following commands:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;        tar xjf ora2pg-x.x.tar.bz2
        cd ora2pg-x.x/
        perl Makefile.PL
        make &amp;amp;&amp;amp; make install

This will install Ora2Pg.pm into your site Perl repository, ora2pg into
/usr/local/bin/ and ora2pg.conf into /etc/ora2pg/.

On Windows(tm) OSes you may use instead:

        perl Makefile.PL
        gmake &amp;amp;&amp;amp; gmake install

This will install scripts and libraries into your Perl site installation
directory and the ora2pg.conf file as well as all documentation files
into C:\ora2pg\

To install ora2pg in a different directory than the default one, simply
use this command:

        perl Makefile.PL PREFIX=&amp;lt;your_install_dir&amp;gt;
        make &amp;amp;&amp;amp; make install

then set PERL5LIB to the path to your installation directory before
using Ora2Pg.

        export PERL5LIB=&amp;lt;your_install_dir&amp;gt;
        ora2pg -c config/ora2pg.conf -t TABLE -b outdir/
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Packaging If you want to build the binary package for your preferred Linux distribution take a look at the packaging/ directory of the source tarball. There is everything to build RPM, Slackware and Debian packages. See README file in that directory.&lt;/p&gt; 
&lt;p&gt;Installing DBD::Oracle Ora2Pg needs the Perl module DBD::Oracle for connectivity to an Oracle database from perl DBI. To get DBD::Oracle get it from CPAN a perl module repository.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;After setting ORACLE_HOME and LD_LIBRARY_PATH environment variables as
root user, install DBD::Oracle. Proceed as follow:

        export LD_LIBRARY_PATH=/usr/lib/oracle/12.2/client64/lib
        export ORACLE_HOME=/usr/lib/oracle/12.2/client64
        perl -MCPAN -e &#39;install DBD::Oracle&#39;

If you are running for the first time it will ask many questions; you
can keep defaults by pressing ENTER key, but you need to give one
appropriate mirror site for CPAN to download the modules. Install
through CPAN manually if the above doesn&#39;t work:

        #perl -MCPAN -e shell
        cpan&amp;gt; get DBD::Oracle
        cpan&amp;gt; quit
        cd ~/.cpan/build/DBD-Oracle*
        export LD_LIBRARY_PATH=/usr/lib/oracle/11.2/client64/lib
        export ORACLE_HOME=/usr/lib/oracle/11.2/client64
        perl Makefile.PL
        make
        make install

Installing DBD::Oracle require that the three Oracle packages:
instant-client, SDK and SQLplus are installed as well as the libaio1
library.

If you are using Instant Client from ZIP archives, the LD_LIBRARY_PATH
and ORACLE_HOME will be the same and must be set to the directory where
you have installed the files. For example:
/opt/oracle/instantclient_12_2/
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;CONFIGURATION Ora2Pg configuration can be as simple as choosing the Oracle database to export and choose the export type. This can be done in a minute.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;By reading this documentation you will also be able to:

        - Select only certain tables and/or column for export.
        - Rename some tables and/or column during export.
        - Select data to export following a WHERE clause per table.
        - Delay database constraints during data loading.
        - Compress exported data to save disk space.
        - and much more.

The full control of the Oracle database migration is taken though a
single configuration file named ora2pg.conf. The format of this file
consist in a directive name in upper case followed by tab character and
a value. Comments are lines beginning with a #.

There&#39;s no specific order to place the configuration directives, they
are set at the time they are read in the configuration file.

For configuration directives that just take a single value, you can use
them multiple time in the configuration file but only the last
occurrence found in the file will be used. For configuration directives
that allow a list of value, you can use it multiple time, the values
will be appended to the list. If you use the IMPORT directive to load a
custom configuration file, directives defined in this file will be
stores from the place the IMPORT directive is found, so it is better to
put it at the end of the configuration file.

Values set in command line options will override values from the
configuration file.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Ora2Pg usage First of all be sure that libraries and binaries path include the Oracle Instant Client installation:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;        export LD_LIBRARY_PATH=/usr/lib/oracle/11.2/client64/lib
        export PATH=&quot;/usr/lib/oracle/11.2/client64/bin:$PATH&quot;

By default Ora2Pg will look for /etc/ora2pg/ora2pg.conf configuration
file, if the file exist you can simply execute:

        /usr/local/bin/ora2pg

or under Windows(tm) run ora2pg.bat file, located in your perl bin
directory. Windows(tm) users may also find a template configuration file
in C:\ora2pg

If you want to call another configuration file, just give the path as
command line argument:

        /usr/local/bin/ora2pg -c /etc/ora2pg/new_ora2pg.conf

Here are all command line parameters available when using ora2pg:

Usage: ora2pg [-dhpqv --estimate_cost --dump_as_html] [--option value]

    -a | --allow str  : Comma separated list of objects to allow from export.
                        Can be used with SHOW_COLUMN too.
    -b | --basedir dir: Set the default output directory, where files
                        resulting from exports will be stored.
    -c | --conf file  : Set an alternate configuration file other than the
                        default /etc/ora2pg/ora2pg.conf.
    -C | --cdc_file file: File used to store/read SCN per table during export.
                        default: TABLES_SCN.log in the current directory. This
                        is the file written by the --cdc_ready option.
    -d | --debug      : Enable verbose output.
    -D | --data_type str : Allow custom type replacement at command line.
    -e | --exclude str: Comma separated list of objects to exclude from export.
                        Can be used with SHOW_COLUMN too.
    -h | --help       : Print this short help.
    -g | --grant_object type : Extract privilege from the given object type.
                        See possible values with GRANT_OBJECT configuration.
    -i | --input file : File containing Oracle PL/SQL code to convert with
                        no Oracle database connection initiated.
    -j | --jobs num   : Number of parallel process to send data to PostgreSQL.
    -J | --copies num : Number of parallel connections to extract data from Oracle.
    -l | --log file   : Set a log file. Default is stdout.
    -L | --limit num  : Number of tuples extracted from Oracle and stored in
                        memory before writing, default: 10000.
    -m | --mysql      : Export a MySQL database instead of an Oracle schema.
    -M | --mssql      : Export a Microsoft SQL Server database.
    -n | --namespace schema : Set the Oracle schema to extract from.
    -N | --pg_schema schema : Set PostgreSQL&#39;s search_path.
    -o | --out file   : Set the path to the output file where SQL will
                        be written. Default: output.sql in running directory.
    -O | --options    : Used to override any configuration parameter, it can
                        be used multiple time. Syntax: -O &quot;PARAM_NAME=value&quot;
    -p | --plsql      : Enable PLSQL to PLPGSQL code conversion.
    -P | --parallel num: Number of parallel tables to extract at the same time.
    -q | --quiet      : Disable progress bar.
    -r | --relative   : use \ir instead of \i in the psql scripts generated.
    -s | --source DSN : Allow to set the Oracle DBI datasource.
    -S | --scn    SCN : Allow to set the Oracle System Change Number (SCN) to
                        use to export data. It will be used in the WHERE clause
                        to get the data. It is used with action COPY or INSERT.
    -t | --type export: Set the export type. It will override the one
                        given in the configuration file (TYPE).
    -T | --temp_dir dir: Set a distinct temporary directory when two
                        or more ora2pg are run in parallel.
    -u | --user name  : Set the Oracle database connection user.
                        ORA2PG_USER environment variable can be used instead.
    -v | --version    : Show Ora2Pg Version and exit.
    -w | --password pwd : Set the password of the Oracle database user.
                        ORA2PG_PASSWD environment variable can be used instead.
    -W | --where clause : Set the WHERE clause to apply to the Oracle query to
                        retrieve data. Can be used multiple time.
    --forceowner      : Force ora2pg to set tables and sequences owner like in
                  Oracle database. If the value is set to a username this one
                  will be used as the objects owner. By default it&#39;s the user
                  used to connect to the Pg database that will be the owner.
    --nls_lang code: Set the Oracle NLS_LANG client encoding.
    --client_encoding code: Set the PostgreSQL client encoding.
    --view_as_table str: Comma separated list of views to export as table.
    --estimate_cost   : Activate the migration cost evaluation with SHOW_REPORT
    --cost_unit_value minutes: Number of minutes for a cost evaluation unit.
                  default: 5 minutes, corresponds to a migration conducted by a
                  PostgreSQL expert. Set it to 10 if this is your first migration.
   --dump_as_html     : Force ora2pg to dump report in HTML, used only with
                        SHOW_REPORT. Default is to dump report as simple text.
   --dump_as_csv      : As above but force ora2pg to dump report in CSV.
   --dump_as_json     : As above but force ora2pg to dump report in JSON.
   --dump_as_sheet    : Report migration assessment with one CSV line per database.
   --init_project name: Initialise a typical ora2pg project tree. Top directory
                        will be created under project base dir.
   --project_base dir : Define the base dir for ora2pg project trees. Default
                        is current directory.
   --print_header     : Used with --dump_as_sheet to print the CSV header
                        especially for the first run of ora2pg.
   --human_days_limit num : Set the number of person-days limit where the migration
                        assessment level switch from B to C. Default is set to
                        5 person-days.
   --audit_user list  : Comma separated list of usernames to filter queries in
                        the DBA_AUDIT_TRAIL table. Used only with SHOW_REPORT
                        and QUERY export type.
   --pg_dsn DSN       : Set the datasource to PostgreSQL for direct import.
   --pg_user name     : Set the PostgreSQL user to use.
   --pg_pwd password  : Set the PostgreSQL password to use.
   --count_rows       : Force ora2pg to perform a real row count in TEST,
                        TEST_COUNT and SHOW_TABLE actions.
   --no_header        : Do not append Ora2Pg header to output file
   --oracle_speed     : Use to know at which speed Oracle is able to send
                        data. No data will be processed or written.
   --ora2pg_speed     : Use to know at which speed Ora2Pg is able to send
                        transformed data. Nothing will be written.
   --blob_to_lo       : export BLOB as large objects, can only be used with
                        action SHOW_COLUMN, TABLE and INSERT.
   --cdc_ready        : use current SCN per table to export data and register
                        them into a file named TABLES_SCN.log per default. It
                        can be changed using -C | --cdc_file.
   --lo_import        : use psql \lo_import command to import BLOB as large
                        object. Can be use to import data with COPY and import
                        large object manually in a second pass. It is recquired
                        for BLOB &amp;gt; 1GB. See documentation for more explanation.
   --mview_as_table str: Comma separated list of materialized views to export
                        as regular table.
   --drop_if_exists   : Drop the object before creation if it exists.
   --delete clause    : Set the DELETE clause to apply to the Oracle query to
                        be applied before importing data. Can be used multiple
                        time.
   --oracle_fdw_prefetch: Set the oracle_fdw prefetch value. Larger values
                        generally result in faster data transfer at the cost
                        of greater memory utilisation at the destination.

See full documentation at https://ora2pg.darold.net/ for more help or
see manpage with &#39;man ora2pg&#39;.

ora2pg will return 0 on success, 1 on error. It will return 2 when a
child process has been interrupted and you&#39;ve gotten the warning
message: &quot;WARNING: an error occurs during data export. Please check
what&#39;s happen.&quot; Most of the time this is an OOM issue, first try
reducing DATA_LIMIT value.

For developers, it is possible to add your own custom option(s) in the
Perl script ora2pg as any configuration directive from ora2pg.conf can
be passed in lower case to the new Ora2Pg object instance. See ora2pg
code on how to add your own option.

Note that performance might be improved by updating stats on oracle:

        BEGIN
        DBMS_STATS.GATHER_SCHEMA_STATS
        DBMS_STATS.GATHER_DATABASE_STATS 
        DBMS_STATS.GATHER_DICTIONARY_STATS
        END;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Generate a migration template The two options --project_base and --init_project when used indicate to ora2pg that he has to create a project template with a work tree, a configuration file and a script to export all objects from the Oracle database. Here a sample of the command usage:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;        ora2pg --project_base /app/migration/ --init_project test_project
        Creating project test_project.
        /app/migration/test_project/
                schema/
                        dblinks/
                        directories/
                        functions/
                        grants/
                        mviews/
                        packages/
                        partitions/
                        procedures/
                        sequences/
                        synonyms/
                        tables/
                        tablespaces/
                        triggers/
                        types/
                        views/
                sources/
                        functions/
                        mviews/
                        packages/
                        partitions/
                        procedures/
                        triggers/
                        types/
                        views/
                data/
                config/
                reports/

        Generating generic configuration file
        Creating script export_schema.sh to automate all exports.
        Creating script import_all.sh to automate all imports.

It create a generic config file where you just have to define the Oracle
database connection and a shell script called export_schema.sh. The
sources/ directory will contains the Oracle code, the schema/ will
contains the code ported to PostgreSQL. The reports/ directory will
contains the html reports with the migration cost assessment.

If you want to use your own default config file, use the -c option to
give the path to that file. Rename it with .dist suffix if you want
ora2pg to apply the generic configuration values otherwise, the
configuration file will be copied untouched.

Once you have set the connection to the Oracle Database you can execute
the script export_schema.sh that will export all object type from your
Oracle database and output DDL files into the schema&#39;s subdirectories.
At end of the export it will give you the command to export data later
when the import of the schema will be done and verified.

You can choose to load the DDL files generated manually or use the
second script import_all.sh to import those file interactively. If this
kind of migration is not something current for you it&#39;s recommended you
to use those scripts.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Oracle database connection There&#39;s 5 configuration directives to control the access to the Oracle database.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;ORACLE_HOME
    Used to set ORACLE_HOME environment variable to the Oracle libraries
    required by the DBD::Oracle Perl module.

ORACLE_DSN
    This directive is used to set the data source name in the form
    standard DBI DSN. For example:

            dbi:Oracle:host=oradb_host.myhost.com;sid=DB_SID;port=1521

    or

            dbi:Oracle:DB_SID

    On 18c this could be for example:

            dbi:Oracle:host=192.168.1.29;service_name=pdb1;port=1521

    for the second notation the SID should be declared in the well known
    file $ORACLE_HOME/network/admin/tnsnames.ora or in the path given to
    the TNS_ADMIN environment variable.

    For MySQL the DSN will lool like this:

            dbi:mysql:host=192.168.1.10;database=sakila;port=3306

    the &#39;sid&#39; part is replaced by &#39;database&#39;.

    For MS SQL Server it will look like this:

            dbi:ODBC:driver=msodbcsql18;server=mydb.database.windows.net;database=testdb;TrustServerCertificate=yes

ORACLE_USER et ORACLE_PWD
    These two directives are used to define the user and password for
    the Oracle database connection. Note that if you can it is better to
    login as Oracle super admin to avoid grants problem during the
    database scan and be sure that nothing is missing.

    If you do not supply a credential with ORACLE_PWD and you have
    installed the Term::ReadKey Perl module, Ora2Pg will ask for the
    password interactively. If ORACLE_USER is not set it will be asked
    interactively too.

    To connect to a local ORACLE instance with connections &quot;as sysdba&quot;
    you have to set ORACLE_USER to &quot;/&quot; and an empty password.

    To make a connection using an Oracle Secure External Password Store
    (SEPS), first configure the Oracle Wallet and then set both the
    ORACLE_USER and ORACLE_PWD directives to the special value of
    &quot;__SEPS__&quot; (without the quotes but with the double underscore).

USER_GRANTS
    Set this directive to 1 if you connect the Oracle database as simple
    user and do not have enough grants to extract things from the
    DBA_... tables. It will use tables ALL_... instead.

    Warning: if you use export type GRANT, you must set this
    configuration option to 0 or it will not work.

TRANSACTION
    This directive may be used if you want to change the default
    isolation level of the data export transaction. Default is now to
    set the level to a serializable transaction to ensure data
    consistency. The allowed values for this directive are:

            readonly: &#39;SET TRANSACTION READ ONLY&#39;,
            readwrite: &#39;SET TRANSACTION READ WRITE&#39;,
            serializable: &#39;SET TRANSACTION ISOLATION LEVEL SERIALIZABLE&#39;
            committed: &#39;SET TRANSACTION ISOLATION LEVEL READ COMMITTED&#39;,

    Releases before 6.2 used to set the isolation level to READ ONLY
    transaction but in some case this was breaking data consistency so
    now default is set to SERIALIZABLE.

INPUT_FILE
    This directive did not control the Oracle database connection or
    unless it purely disables the use of any Oracle database by
    accepting a file as argument. Set this directive to a file
    containing PL/SQL Oracle Code like function, procedure or full
    package body to prevent Ora2Pg from connecting to an Oracle database
    and just apply his conversion tool to the content of the file. This
    can be used with the most of export types: TABLE, TRIGGER,
    PROCEDURE, VIEW, FUNCTION or PACKAGE, etc.

ORA_INITIAL_COMMAND
    This directive can be used to send an initial command to Oracle,
    just after the connection. For example to unlock a policy before
    reading objects or to set some session parameters. This directive
    can be used multiple times.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Data encryption with Oracle server If your Oracle Client config file already includes the encryption method, then DBD:Oracle uses those settings to encrypt the connection while you extract the data. For example if you have configured the Oracle Client config file (sqlnet.ora or .sqlnet) with the following information:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;        # Configure encryption of connections to Oracle
        SQLNET.ENCRYPTION_CLIENT = required
        SQLNET.ENCRYPTION_TYPES_CLIENT = (AES256, RC4_256)
        SQLNET.CRYPTO_SEED = &#39;should be 10-70 random characters&#39;

Any tool that uses the Oracle client to talk to the database will be
encrypted if you setup session encryption like above.

For example, Perl&#39;s DBI uses DBD-Oracle, which uses the Oracle client
for actually handling database communication. If the installation of
Oracle client used by Perl is setup to request encrypted connections,
then your Perl connection to an Oracle database will also be encrypted.

Full details at
https://kb.berkeley.edu/jivekb/entry.jspa?externalID=1005
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Testing connection Once you have set the Oracle database DSN you can execute ora2pg to see if it works:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;        ora2pg -t SHOW_VERSION -c config/ora2pg.conf

will show the Oracle database server version. Take some time here to
test your installation as most problems take place here, the other
configuration steps are more technical.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Troubleshooting If the output.sql file has not exported anything other than the Pg transaction header and footer there&#39;s two possible reasons. The perl script ora2pg dump an ORA-XXX error, that mean that your DSN or login information are wrong, check the error and your settings and try again. The perl script says nothing and the output file is empty: the user lacks permission to extract something from the database. Try to connect to Oracle as super user or take a look at directive USER_GRANTS above and at next section, especially the SCHEMA directive.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;LOGFILE
    By default all messages are sent to the standard output. If you give
    a file path to that directive, all output will be appended to this
    file.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Oracle schema to export The Oracle database export can be limited to a specific Schema or Namespace, this can be mandatory following the database connection user.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;SCHEMA
    This directive is used to set the schema name to use during export.
    For example:

            SCHEMA  APPS

    will extract objects associated to the APPS schema.

    When no schema name is provided and EXPORT_SCHEMA is enabled, Ora2Pg
    will export all objects from all schema of the Oracle instance with
    their names prefixed with the schema name.

EXPORT_SCHEMA
    By default the Oracle schema is not exported into the PostgreSQL
    database and all objects are created under the default Pg namespace.
    If you want to also export this schema and create all objects under
    this namespace, set the EXPORT_SCHEMA directive to 1. This will set
    the schema search_path at top of export SQL file to the schema name
    set in the SCHEMA directive with the default pg_catalog schema. If
    you want to change this path, use the directive PG_SCHEMA.

CREATE_SCHEMA
    Enable/disable the CREATE SCHEMA SQL order at starting of the output
    file. It is enable by default and concern on TABLE export type.

COMPILE_SCHEMA
    By default Ora2Pg will only export valid PL/SQL code. You can force
    Oracle to compile again the invalidated code to get a chance to have
    it obtain the valid status and then be able to export it.

    Enable this directive to force Oracle to compile schema before
    exporting code. When this directive is enabled and SCHEMA is set to
    a specific schema name, only invalid objects in this schema will be
    recompiled. If SCHEMA is not set then all schema will be recompiled.
    To force recompile invalid object in a specific schema, set
    COMPILE_SCHEMA to the schema name you want to recompile.

    This will ask to Oracle to validate the PL/SQL that could have been
    invalidate after a export/import for example. The &#39;VALID&#39; or
    &#39;INVALID&#39; status applies to functions, procedures, packages and user
    defined types. It also concern disabled triggers.

EXPORT_INVALID
    If the above configuration directive is not enough to validate your
    PL/SQL code enable this configuration directive to allow export of
    all PL/SQL code even if it is marked as invalid. The &#39;VALID&#39; or
    &#39;INVALID&#39; status applies to functions, procedures, packages,
    triggers and user defined types.

PG_SCHEMA
    Allow you to defined/force the PostgreSQL schema to use. By default
    if you set EXPORT_SCHEMA to 1 the PostgreSQL search_path will be set
    to the schema name exported set as value of the SCHEMA directive.

    The value can be a comma delimited list of schema name but not when
    using TABLE export type because in this case it will generate the
    CREATE SCHEMA statement and it doesn&#39;t support multiple schema name.
    For example, if you set PG_SCHEMA to something like &quot;user_schema,
    public&quot;, the search path will be set like this:

            SET search_path = user_schema, public;

    forcing the use of an other schema (here user_schema) than the one
    from Oracle schema set in the SCHEMA directive.

    You can also set the default search_path for the PostgreSQL user you
    are using to connect to the destination database by using:

            ALTER ROLE username SET search_path TO user_schema, public;

    in this case you don&#39;t have to set PG_SCHEMA.

SYSUSERS
    Without explicit schema, Ora2Pg will export all objects that not
    belongs to system schema or role:

            SYSTEM,CTXSYS,DBSNMP,EXFSYS,LBACSYS,MDSYS,MGMT_VIEW,
            OLAPSYS,ORDDATA,OWBSYS,ORDPLUGINS,ORDSYS,OUTLN,
            SI_INFORMTN_SCHEMA,SYS,SYSMAN,WK_TEST,WKSYS,WKPROXY,
            WMSYS,XDB,APEX_PUBLIC_USER,DIP,FLOWS_020100,FLOWS_030000,
            FLOWS_040100,FLOWS_010600,FLOWS_FILES,MDDATA,ORACLE_OCM,
            SPATIAL_CSW_ADMIN_USR,SPATIAL_WFS_ADMIN_USR,XS$NULL,PERFSTAT,
            SQLTXPLAIN,DMSYS,TSMSYS,WKSYS,APEX_040000,APEX_040200,
            DVSYS,OJVMSYS,GSMADMIN_INTERNAL,APPQOSSYS,DVSYS,DVF,
            AUDSYS,APEX_030200,MGMT_VIEW,ODM,ODM_MTR,TRACESRV,MTMSYS,
            OWBSYS_AUDIT,WEBSYS,WK_PROXY,OSE$HTTP$ADMIN,
            AURORA$JIS$UTILITY$,AURORA$ORB$UNAUTHENTICATED,
            DBMS_PRIVILEGE_CAPTURE,CSMIG,MGDSYS,SDE,DBSFWUSER

    Following your Oracle installation you may have several other system
    role defined. To append these users to the schema exclusion list,
    just set the SYSUSERS configuration directive to a comma-separated
    list of system user to exclude. For example:

            SYSUSERS        INTERNAL,SYSDBA,BI,HR,IX,OE,PM,SH

    will add users INTERNAL and SYSDBA to the schema exclusion list.

FORCE_OWNER
    By default the owner of the database objects is the one you&#39;re using
    to connect to PostgreSQL using the psql command. If you use an other
    user (postgres for example) you can force Ora2Pg to set the object
    owner to be the one used in the Oracle database by setting the
    directive to 1, or to a completely different username by setting the
    directive value to that username.

FORCE_SECURITY_INVOKER
    Ora2Pg use the function&#39;s security privileges set in Oracle and it
    is often defined as SECURITY DEFINER. If you want to override those
    security privileges for all functions and use SECURITY DEFINER
    instead, enable this directive.

USE_TABLESPACE
    When enabled this directive force ora2pg to export all tables,
    indexes constraint and indexes using the tablespace name defined in
    Oracle database. This works only with tablespace that are not TEMP,
    USERS and SYSTEM.

WITH_OID
    Activating this directive will force Ora2Pg to add WITH (OIDS) when
    creating tables or views as tables. Default is same as PostgreSQL,
    disabled.

LOOK_FORWARD_FUNCTION
    List of schema to get functions/procedures meta information that are
    used in the current schema export. When replacing call to function
    with OUT parameters, if a function is declared in an other package
    then the function call rewriting can not be done because Ora2Pg only
    knows about functions declared in the current schema. By setting a
    comma separated list of schema as value of this directive, Ora2Pg
    will look forward in these packages for all
    functions/procedures/packages declaration before proceeding to
    current schema export.

NO_FUNCTION_METADATA
    Force Ora2Pg to not look for function declaration. Note that this
    will prevent Ora2Pg to rewrite function replacement call if needed.
    Do not enable it unless looking forward at function breaks other
    export.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Export type The export action is perform following a single configuration directive &#39;TYPE&#39;, some other add more control on what should be really exported.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;TYPE
    Here are the different values of the TYPE directive, default is
    TABLE:

            - TABLE: Extract all tables with indexes, primary keys, unique keys,
              foreign keys and check constraints.
            - VIEW: Extract only views.
            - GRANT: Extract roles converted to Pg groups, users and grants on all
              objects.
            - SEQUENCE: Extract all sequence and their last position.
            - TABLESPACE: Extract storage spaces for tables and indexes (Pg &amp;gt;= v8).
            - TRIGGER: Extract triggers defined following actions.
            - FUNCTION: Extract functions.
            - PROCEDURE: Extract procedures.
            - PACKAGE: Extract packages and package bodies.
            - INSERT: Extract data as INSERT statement.
            - COPY: Extract data as COPY statement.
            - PARTITION: Extract range and list Oracle partitions with subpartitions.
            - TYPE: Extract user defined Oracle type.
            - FDW: Export Oracle tables as foreign table for Oracle, MySQL and SQL Server FDW.
            - MVIEW: Export materialized view.
            - QUERY: Try to automatically convert Oracle SQL queries.
            - KETTLE: Generate XML ktr template files to be used by Kettle.
            - DBLINK: Generate oracle foreign data wrapper server to use as dblink.
            - SYNONYM: Export Oracle&#39;s synonyms as views on other schema&#39;s objects.
            - DIRECTORY: Export Oracle&#39;s directories as external_file extension objects.
            - LOAD: Dispatch a list of queries over multiple PostgreSQl connections.
            - TEST: perform a diff between Oracle and PostgreSQL database.
            - TEST_COUNT: perform a row count diff between Oracle and PostgreSQL table.
            - TEST_VIEW: perform a count on both side of number of rows returned by views.
            - TEST_DATA: perform data validation check on rows at both sides.
            - SEQUENCE_VALUES: export DDL to set the last values of sequences

    Only one type of export can be perform at the same time so the TYPE
    directive must be unique. If you have more than one only the last
    found in the file will be registered.

    Some export type can not or should not be load directly into the
    PostgreSQL database and still require little manual editing. This is
    the case for GRANT, TABLESPACE, TRIGGER, FUNCTION, PROCEDURE, TYPE,
    QUERY and PACKAGE export types especially if you have PLSQL code or
    Oracle specific SQL in it.

    For TABLESPACE you must ensure that file path exist on the system
    and for SYNONYM you may ensure that the object&#39;s owners and schemas
    correspond to the new PostgreSQL database design.

    Note that you can chained multiple export by giving to the TYPE
    directive a comma-separated list of export type, but in this case
    you must not use COPY or INSERT with other export type.

    Ora2Pg will convert Oracle partition using table inheritance,
    trigger and functions. See document at Pg site:
    http://www.postgresql.org/docs/current/interactive/ddl-partitioning.
    html

    The TYPE export allow export of user defined Oracle type. If you
    don&#39;t use the --plsql command line parameter it simply dump Oracle
    user type asis else Ora2Pg will try to convert it to PostgreSQL
    syntax.

    The KETTLE export type requires that the Oracle and PostgreSQL DNS
    are defined.

    Since Ora2Pg v8.1 there&#39;s three new export types:

            SHOW_VERSION : display Oracle version
            SHOW_SCHEMA  : display the list of schema available in the database.
            SHOW_TABLE   : display the list of tables available.
            SHOW_COLUMN  : display the list of tables columns available and the
                    Ora2PG conversion type from Oracle to PostgreSQL that will be
                    applied. It will also warn you if there&#39;s PostgreSQL reserved
                    words in Oracle object names.

    Here is an example of the SHOW_COLUMN output:

            [2] TABLE CURRENT_SCHEMA (1 rows) (Warning: &#39;CURRENT_SCHEMA&#39; is a reserved word in PostgreSQL)
                    CONSTRAINT : NUMBER(22) =&amp;gt; bigint (Warning: &#39;CONSTRAINT&#39; is a reserved word in PostgreSQL)
                    FREEZE : VARCHAR2(25) =&amp;gt; varchar(25) (Warning: &#39;FREEZE&#39; is a reserved word in PostgreSQL)
            ...
            [6] TABLE LOCATIONS (23 rows)
                    LOCATION_ID : NUMBER(4) =&amp;gt; smallint
                    STREET_ADDRESS : VARCHAR2(40) =&amp;gt; varchar(40)
                    POSTAL_CODE : VARCHAR2(12) =&amp;gt; varchar(12)
                    CITY : VARCHAR2(30) =&amp;gt; varchar(30)
                    STATE_PROVINCE : VARCHAR2(25) =&amp;gt; varchar(25)
                    COUNTRY_ID : CHAR(2) =&amp;gt; char(2)

    Those extraction keywords are use to only display the requested
    information and exit. This allows you to quickly know on what you
    are going to work.

    The SHOW_COLUMN allow an other ora2pg command line option: &#39;--allow
    relname&#39; or &#39;-a relname&#39; to limit the displayed information to the
    given table.

    The SHOW_ENCODING export type will display the NLS_LANG and
    CLIENT_ENCODING values that Ora2Pg will used and the real encoding
    of the Oracle database with the corresponding client encoding that
    could be used with PostgreSQL

    Ora2Pg allow you to export your Oracle, MySQL or MSSQL table
    definition to be use with the oracle_fdw, mysql_fdw or tds_fdw
    foreign data wrapper. By using type FDW your tables will be exported
    as follow:

            CREATE FOREIGN TABLE oratab (
                    id        integer           NOT NULL,
                    text      character varying(30),
                    floating  double precision  NOT NULL
            ) SERVER oradb OPTIONS (table &#39;ORATAB&#39;);

    Now you can use the table like a regular PostgreSQL table.

    Release 10 adds a new export type destined to evaluate the content
    of the database to migrate, in terms of objects and cost to end the
    migration:

            SHOW_REPORT  : show a detailed report of the Oracle database content.

    Here is a sample of report: http://ora2pg.darold.net/report.html

    There also a more advanced report with migration cost. See the
    dedicated chapter about Migration Cost Evaluation.

ESTIMATE_COST
    Activate the migration cost evaluation. Must only be used with
    SHOW_REPORT, FUNCTION, PROCEDURE, PACKAGE and QUERY export type.
    Default is disabled. You may want to use the --estimate_cost command
    line option instead to activate this functionality. Note that
    enabling this directive will force PLSQL_PGSQL activation.

COST_UNIT_VALUE
    Set the value in minutes of the migration cost evaluation unit.
    Default is five minutes per unit. See --cost_unit_value to change
    the unit value at command line.

DUMP_AS_HTML
    By default when using SHOW_REPORT the migration report is generated
    as simple text, enabling this directive will force ora2pg to create
    a report in HTML format.

    See http://ora2pg.darold.net/report.html for a sample report.

HUMAN_DAYS_LIMIT
    Use this directive to redefined the number of person-days limit
    where the migration assessment level must switch from B to C.
    Default is set to 10 person-days.

JOBS
    This configuration directive adds multiprocess support to COPY,
    FUNCTION and PROCEDURE export type, the value is the number of
    process to use. Default is multiprocess disable.

    This directive is used to set the number of cores to used to
    parallelize data import into PostgreSQL. During FUNCTION or
    PROCEDURE export type each function will be translated to plpgsql
    using a new process, the performances gain can be very important
    when you have tons of function to convert.

    There&#39;s no limitation in parallel processing than the number of
    cores and the PostgreSQL I/O performance capabilities.

    Doesn&#39;t work under Windows Operating System, it is simply disabled.

ORACLE_COPIES
    This configuration directive adds multiprocess support to extract
    data from Oracle. The value is the number of process to use to
    parallelize the select query. Default is parallel query disable.

    The parallelism is built on splitting the query following of the
    number of cores given as value to ORACLE_COPIES as follow:

            SELECT * FROM MYTABLE WHERE ABS(MOD(COLUMN, ORACLE_COPIES)) = CUR_PROC

    where COLUMN is a technical key like a primary or unique key where
    split will be based and the current core used by the query
    (CUR_PROC). You can also force the column name to use using the
    DEFINED_PK configuration directive.

    Doesn&#39;t work under Windows Operating System, it is simply disabled.

DEFINED_PK
    This directive is used to defined the technical key to used to split
    the query between number of cores set with the ORACLE_COPIES
    variable. For example:

            DEFINED_PK      EMPLOYEES:employee_id

    The parallel query that will be used supposing that -J or
    ORACLE_COPIES is set to 8:

            SELECT * FROM EMPLOYEES WHERE ABS(MOD(employee_id, 8)) = N

    where N is the current process forked starting from 0.

PARALLEL_TABLES
    This directive is used to defined the number of tables that will be
    processed in parallel for data extraction. The limit is the number
    of cores on your machine. Ora2Pg will open one database connection
    for each parallel table extraction. This directive, when upper than
    1, will invalidate ORACLE_COPIES but not JOBS, so the real number of
    process that will be used is PARALLEL_TABLES * JOBS.

    Note that this directive when set upper that 1 will also
    automatically enable the FILE_PER_TABLE directive if your are
    exporting to files. This is used to export tables and views in
    separate files.

    Use PARALLEL_TABLES to use parallelism with COPY, INSERT and
    TEST_DATA actions. It is also useful with TEST, TEST_COUNT, and
    SHOW_TABLE if --count_rows is used for real row count.

DEFAULT_PARALLELISM_DEGREE
    You can force Ora2Pg to use /*+ PARALLEL(tbname, degree) */ hint in
    each query used to export data from Oracle by setting a value upper
    than 1 to this directive. A value of 0 or 1 disable the use of
    parallel hint. Default is disabled.

FDW_SERVER
    This directive is used to set the name of the foreign data server
    that is used in the &quot;CREATE SERVER name FOREIGN DATA WRAPPER
    &amp;lt;fdw_extension&amp;gt; ...&quot; command. This name will then be used in the
    &quot;CREATE FOREIGN TABLE ...&quot; SQL commands and to import data using
    oracle_fdw. Default is no foreign server defined. This only concerns
    export type FDW, COPY and INSERT. For export type FDW the default
    value is orcl.

FDW_IMPORT_SCHEMA
    Schema where foreign tables for data migration will be created. If
    you use several instances of ora2pg for data migration through the
    foreign data wrapper, you might need to change the name of the
    schema for each instance. Default: ora2pg_fdw_import

ORACLE_FDW_PREFETCH
    The default behaviour of Ora2Pg is to NOT set the &quot;prefetch&quot; option
    for oracle_fdw when used for COPY and INSERT. This directive allows
    the prefetch to be set. See oracle_fdw documentation for the current
    default.

ORACLE_FDW_COPY_MODE
    When using Ora2Pg COPY with oracle_fdw it is possible to use two
    different modes: 1) &quot;local&quot;, which uses psql on the host running
    Ora2Pg for the &quot;TO&quot; binary stream; 2) &quot;server&quot;, which uses
    PostgreSQL server-side COPY for the &quot;TO&quot; binary stream. Both modes
    use psql for the &quot;FROM STDIN BINARY&quot;. However, &quot;local&quot; runs the psql
    &quot;FROM STDIN BINARY&quot; on host Ora2Pg is run from, whereas &quot;server&quot;
    runs the psql &quot;FROM STDIN BINARY&quot; on the PostgreSQL server. &quot;local&quot;
    mode should work on any PostgreSQL-based system, including managed
    offerings, which are not expected to support use of &quot;server&quot; mode
    due to permissions. The default is &quot;local&quot; as this is compatible
    with more configurations.

ORACLE_FDW_COPY_FORMAT
    When using Ora2Pg COPY with oracle_fdw it is possible to use either
    BINARY or CSV data format. BINARY provides better performance,
    however, requires exact data type matching between the FDW and
    destination table. CSV provides greater flexibiliity with respect to
    data type matching: if the FDW and destination data types are
    functionally-compatible the columns can be copied. The default is
    &quot;binary&quot;.

DROP_FOREIGN_SCHEMA
    By default Ora2Pg drops the temporary schema ora2pg_fdw_import used
    to import the Oracle foreign schema before each new import. If you
    want to preserve the existing schema because of modifications or the
    use of a third party server, disable this directive.

EXTERNAL_TO_FDW
    This directive, enabled by default, allow to export Oracle&#39;s
    External Tables as file_fdw foreign tables. To not export these
    tables at all, set the directive to 0.

INTERNAL_DATE_MAX
    Internal timestamp retrieves from custom type are extracted in the
    following format: 01-JAN-77 12.00.00.000000 AM. It is impossible to
    know the exact century that must be used, so by default any year
    below 49 will be added to 2000 and others to 1900. You can use this
    directive to change the default value 49. this is only relevant if
    you have user defined type with a column timestamp.

AUDIT_USER
    Set the comma separated list of username that must be used to filter
    queries from the DBA_AUDIT_TRAIL table. Default is to not scan this
    table and to never look for queries. This parameter is used only
    with SHOW_REPORT and QUERY export type with no input file for
    queries. Note that queries will be normalized before output unlike
    when a file is given at input using the -i option or INPUT
    directive.

FUNCTION_CHECK
    Disable this directive if you want to disable check_function_bodies.

            SET check_function_bodies = false;

    It disables validation of the function body string during CREATE
    FUNCTION. Default is to use de postgresql.conf setting that enable
    it by default.

ENABLE_BLOB_EXPORT
    Exporting BLOB takes time, in some circumstances you may want to
    export all data except the BLOB columns. In this case disable this
    directive and the BLOB columns will not be included into data
    export. Take care that the target bytea column do not have a NOT
    NULL constraint.

ENABLE_CLOB_EXPORT
    Same behavior as ENABLE_BLOB_EXPORT but for CLOB.

DATA_EXPORT_ORDER
    By default data export order will be done by sorting on table name.
    If you have huge tables at end of alphabetic order and you are using
    multiprocess, it can be better to set the sort order on size so that
    multiple small tables can be processed before the largest tables
    finish. In this case set this directive to size. Possible values are
    name and size. Note that export type SHOW_TABLE and SHOW_COLUMN will
    use this sort order too, not only COPY or INSERT export type. If you
    want to give you custom export order, just give a filename as value
    that contains the ordered list of table to export. Must be a list of
    one table per line, in uppercase for Oracle.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Limiting objects to export You may want to export only a part of an Oracle database, here are a set of configuration directives that will allow you to control what parts of the database should be exported.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;ALLOW
    This directive allows you to set a list of objects on which the
    export must be limited, excluding all other objects in the same type
    of export. The value is a space or comma-separated list of objects
    name to export. You can include valid regex into the list. For
    example:

            ALLOW           EMPLOYEES SALE_.* COUNTRIES .*_GEOM_SEQ

    will export objects with name EMPLOYEES, COUNTRIES, all objects
    beginning with &#39;SALE_&#39; and all objects with a name ending by
    &#39;_GEOM_SEQ&#39;. The object depends of the export type. Note that regex
    will not works with 8i database, you must use the % placeholder
    instead, Ora2Pg will use the LIKE operator.

    This is the manner to declare global filters that will be used with
    the current export type. You can also use extended filters that will
    be applied on specific objects or only on their related export type.
    For example:

            ora2pg -p -c ora2pg.conf -t TRIGGER -a &#39;TABLE[employees]&#39;

    will limit export of trigger to those defined on table employees. If
    you want to extract all triggers but not some INSTEAD OF triggers:

            ora2pg -c ora2pg.conf -t TRIGGER -e &#39;VIEW[trg_view_.*]&#39;

    Or a more complex form:

            ora2pg -p -c ora2pg.conf -t TABLE -a &#39;TABLE[EMPLOYEES]&#39; \
                    -e &#39;INDEX[emp_.*];CKEY[emp_salary_min]&#39;

    This command will export the definition of the employee table but
    will exclude all index beginning with &#39;emp_&#39; and the CHECK
    constraint called &#39;emp_salary_min&#39;.

    When exporting partition you can exclude some partition tables by
    using

            ora2pg -p -c ora2pg.conf -t PARTITION -e &#39;PARTITION[PART_199.* PART_198.*]&#39;

    This will exclude partitioned tables for year 1980 to 1999 from the
    export but not the main partition table. The trigger will also be
    adapted to exclude those table.

    With GRANT export you can use this extended form to exclude some
    users from the export or limit the export to some others:

            ora2pg -p -c ora2pg.conf -t GRANT -a &#39;USER1 USER2&#39;

    or

            ora2pg -p -c ora2pg.conf -t GRANT -a &#39;GRANT[USER1 USER2]&#39;

    will limit export grants to users USER1 and USER2. But if you don&#39;t
    want to export grants on some functions for these users, for
    example:

            ora2pg -p -c ora2pg.conf -t GRANT -a &#39;USER1 USER2&#39; -e &#39;FUNCTION[adm_.*];PROCEDURE[adm_.*]&#39;

    Advanced filters may need some learning.

    Oracle doesn&#39;t allow the use of lookahead expression so you may want
    to exclude some object that match the ALLOW regexp you have defined.
    For example if you want to export all table starting with E but not
    those starting with EXP it is not possible to do that in a single
    expression. This is why you can start a regular expression with the
    ! character to exclude object matching the regexp given just after.
    Our previous example can be written as follow:

            ALLOW   E.* !EXP.*

    it will be translated into:

             REGEXP_LIKE(..., &#39;^E.*$&#39;) AND NOT REGEXP_LIKE(..., &#39;^EXP.*$&#39;)

    in the object search expression.

EXCLUDE
    This directive is the opposite of the previous, it allow you to
    define a space or comma-separated list of object name to exclude
    from the export. You can include valid regex into the list. For
    example:

            EXCLUDE         EMPLOYEES TMP_.* COUNTRIES

    will exclude object with name EMPLOYEES, COUNTRIES and all tables
    beginning with &#39;tmp_&#39;.

    For example, you can ban from export some unwanted function with
    this directive:

            EXCLUDE         write_to_.* send_mail_.*

    this example will exclude all functions, procedures or functions in
    a package with the name beginning with those regex. Note that regex
    will not work with 8i database, you must use the % placeholder
    instead, Ora2Pg will use the NOT LIKE operator.

    See above (directive &#39;ALLOW&#39;) for the extended syntax.

NO_EXCLUDED_TABLE
    By default Ora2Pg exclude from export some Oracle &quot;garbage&quot; tables
    that should never be part of an export. This behavior generates a
    lot of REGEXP_LIKE expressions which are slowing down the export
    when looking at tables. To disable this behavior enable this
    directive, you will have to exclude or clean up later by yourself
    the unwanted tables. The regexp used to exclude the table are
    defined in the array @EXCLUDED_TABLES in lib/Ora2Pg.pm. Note this is
    behavior is independant to the EXCLUDE configuration directive.

VIEW_AS_TABLE
    Set which view to export as table. By default none. Value must be a
    list of view name or regexp separated by space or comma. If the
    object name is a view and the export type is TABLE, the view will be
    exported as a create table statement. If export type is COPY or
    INSERT, the corresponding data will be exported.

    See chapter &quot;Exporting views as PostgreSQL table&quot; for more details.

MVIEW_AS_TABLE
    Set which materialized view to export as table. By default none.
    Value must be a list of materialized view name or regexp separated
    by space or comma. If the object name is a materialized view and the
    export type is TABLE, the view will be exported as a create table
    statement. If export type is COPY or INSERT, the corresponding data
    will be exported.

NO_VIEW_ORDERING
    By default Ora2Pg try to order views to avoid error at import time
    with nested views. With a huge number of views this can take a very
    long time, you can bypass this ordering by enabling this directive.

GRANT_OBJECT
    When exporting GRANTs you can specify a comma separated list of
    objects for which privilege will be exported. Default is export for
    all objects. Here are the possibles values TABLE, VIEW, MATERIALIZED
    VIEW, SEQUENCE, PROCEDURE, FUNCTION, PACKAGE BODY, TYPE, SYNONYM,
    DIRECTORY. Only one object type is allowed at a time. For example
    set it to TABLE if you just want to export privilege on tables. You
    can use the -g option to overwrite it.

    When used this directive prevent the export of users unless it is
    set to USER. In this case only users definitions are exported.

WHERE
    This directive allows you to specify a WHERE clause filter when
    dumping the contents of tables. Value is constructs as follows:
    TABLE_NAME[WHERE_CLAUSE], or if you have only one where clause for
    each table just put the where clause as the value. Both are possible
    too. Here are some examples:

            # Global where clause applying to all tables included in the export
            WHERE  1=1

            # Apply the where clause only on table TABLE_NAME
            WHERE  TABLE_NAME[ID1=&#39;001&#39;]

            # Applies two different clause on tables TABLE_NAME and OTHER_TABLE
            # and a generic where clause on DATE_CREATE to all other tables
            WHERE  TABLE_NAME[ID1=&#39;001&#39; OR ID1=&#39;002] DATE_CREATE &amp;gt; &#39;2001-01-01&#39; OTHER_TABLE[NAME=&#39;test&#39;]

    Any where clause not included into a table name bracket clause will
    be applied to all exported table including the tables defined in the
    where clause. These WHERE clauses are very useful if you want to
    archive some data or at the opposite only export some recent data.

    To be able to quickly test data import it is useful to limit data
    export to the first thousand tuples of each table. For Oracle define
    the following clause:

            WHERE   ROWNUM &amp;lt; 1000

    and for MySQL, use the following:

            WHERE   1=1 LIMIT 1,1000

    This can also be restricted to some tables data export.

    Command line option -W or --where will override this directive for
    the global part and per table if the table names is the same.

TOP_MAX
    This directive is used to limit the number of item shown in the top
    N lists like the top list of tables per number of rows and the top
    list of largest tables in megabytes. By default it is set to 10
    items.

LOG_ON_ERROR
    Enable this directive if you want to continue direct data import on
    error. When Ora2Pg received an error in the COPY or INSERT statement
    from PostgreSQL it will log the statement to a file called
    TABLENAME_error.log in the output directory and continue to next
    bulk of data. Like this you can try to fix the statement and
    manually reload the error log file. Default is disabled: abort
    import on error.

REPLACE_QUERY
    Sometime you may want to extract data from an Oracle table but you
    need a custom query for that. Not just a &quot;SELECT * FROM table&quot; like
    Ora2Pg do but a more complex query. This directive allows you to
    overwrite the query used by Ora2Pg to extract data. The format is
    TABLENAME[SQL_QUERY]. If you have multiple table to extract by
    replacing the Ora2Pg query, you can define multiple REPLACE_QUERY
    lines.

            REPLACE_QUERY   EMPLOYEES[SELECT e.id,e.fisrtname,lastname FROM EMPLOYEES e JOIN EMP_UPDT u ON (e.id=u.id AND u.cdate&amp;gt;&#39;2014-08-01 00:00:00&#39;)]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Control of Full Text Search export Several directives can be used to control the way Ora2Pg will export the Oracle&#39;s Text search indexes. By default CONTEXT indexes will be exported to PostgreSQL FTS indexes but CTXCAT indexes will be exported as indexes using the pg_trgm extension.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;CONTEXT_AS_TRGM
    Force Ora2Pg to translate Oracle Text indexes into PostgreSQL
    indexes using pg_trgm extension. Default is to translate CONTEXT
    indexes into FTS indexes and CTXCAT indexes using pg_trgm. Most of
    the time using pg_trgm is enough, this is why this directive stand
    for. You need to create the pg_trgm extension into the destination
    database before importing the objects:

            CREATE EXTENSION pg_trgm;

FTS_INDEX_ONLY
    By default Ora2Pg creates a function-based index to translate Oracle
    Text indexes.

            CREATE INDEX ON t_document
                    USING gin(to_tsvector(&#39;pg_catalog.french&#39;, title));

    You will have to rewrite the CONTAIN() clause using to_tsvector(),
    example:

            SELECT id,title FROM t_document
                    WHERE to_tsvector(title)) @@ to_tsquery(&#39;search_word&#39;);

    To force Ora2Pg to create an extra tsvector column with a dedicated
    triggers for FTS indexes, disable this directive. In this case,
    Ora2Pg will add the column as follow: ALTER TABLE t_document ADD
    COLUMN tsv_title tsvector; Then update the column to compute FTS
    vectors if data have been loaded before UPDATE t_document SET
    tsv_title = to_tsvector(&#39;pg_catalog.french&#39;, coalesce(title,&#39;&#39;)); To
    automatically update the column when a modification in the title
    column appears, Ora2Pg adds the following trigger:

            CREATE FUNCTION tsv_t_document_title() RETURNS trigger AS $$
            BEGIN
                   IF TG_OP = &#39;INSERT&#39; OR new.title != old.title THEN
                           new.tsv_title :=
                           to_tsvector(&#39;pg_catalog.french&#39;, coalesce(new.title,&#39;&#39;));
                   END IF;
                   return new;
            END
            $$ LANGUAGE plpgsql;
            CREATE TRIGGER trig_tsv_t_document_title BEFORE INSERT OR UPDATE
             ON t_document
             FOR EACH ROW EXECUTE PROCEDURE tsv_t_document_title();

    When the Oracle text index is defined over multiple column, Ora2Pg
    will use setweight() to set a weight in the order of the column
    declaration.

FTS_CONFIG
    Use this directive to force text search configuration to use. When
    it is not set, Ora2Pg will autodetect the stemmer used by Oracle for
    each index and pg_catalog.english if the information is not found.

USE_UNACCENT
    If you want to perform your text search in an accent insensitive
    way, enable this directive. Ora2Pg will create an helper function
    over unaccent() and creates the pg_trgm indexes using this function.
    With FTS Ora2Pg will redefine your text search configuration, for
    example:

          CREATE TEXT SEARCH CONFIGURATION fr (COPY = french); 
          ALTER TEXT SEARCH CONFIGURATION fr
                  ALTER MAPPING FOR hword, hword_part, word WITH unaccent, french_stem;

    then set the FTS_CONFIG ora2pg.conf directive to fr instead of
    pg_catalog.english.

    When enabled, Ora2pg will create the wrapper function:

          CREATE OR REPLACE FUNCTION unaccent_immutable(text)
          RETURNS text AS
          $$
              SELECT public.unaccent(&#39;public.unaccent&#39;, $1);
          $$ LANGUAGE sql IMMUTABLE
             COST 1;

    the indexes are exported as follow:

          CREATE INDEX t_document_title_unaccent_trgm_idx ON t_document 
              USING gin (unaccent_immutable(title) gin_trgm_ops);

    In your queries you will need to use the same function in the search
    to be able to use the function-based index. Example:

            SELECT * FROM t_document
                    WHERE unaccent_immutable(title) LIKE &#39;%donnees%&#39;;

USE_LOWER_UNACCENT
    Same as above but call lower() in the unaccent_immutable() function:

          CREATE OR REPLACE FUNCTION unaccent_immutable(text)
          RETURNS text AS
          $$
              SELECT lower(public.unaccent(&#39;public.unaccent&#39;, $1));
          $$ LANGUAGE sql IMMUTABLE;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Modifying object structure One of the great usage of Ora2Pg is its flexibility to replicate Oracle database into PostgreSQL database with a different structure or schema. There&#39;s three configuration directives that allow you to map those differences.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;REORDERING_COLUMNS
    Enable this directive to reordering columns and minimized the
    footprint on disc, so that more rows fit on a data page, which is
    the most important factor for speed. Default is disabled, that mean
    the same order than in Oracle tables definition, that&#39;s should be
    enough for most usage. This directive is only used with TABLE
    export.

MODIFY_STRUCT
    This directive allows you to limit the columns to extract for a
    given table. The value consist in a space-separated list of table
    name with a set of column between parenthesis as follow:

            MODIFY_STRUCT   NOM_TABLE(nomcol1,nomcol2,...) ...

    for example:

            MODIFY_STRUCT   T_TEST1(id,dossier) T_TEST2(id,fichier)

    This will only extract columns &#39;id&#39; and &#39;dossier&#39; from table T_TEST1
    and columns &#39;id&#39; and &#39;fichier&#39; from the T_TEST2 table. This
    directive can only be used with TABLE, COPY or INSERT export. With
    TABLE export create table DDL will respect the new list of columns
    and all indexes or foreign key pointing to or from a column removed
    will not be exported.

EXCLUDE_COLUMNS
    Instead of redefining the table structure with MODIFY_STRUCT you may
    want to exclude some columns from the table export. The value
    consist in a space-separated list of table name with a set of column
    between parenthesis as follow:

            EXCLUDE_COLUMNS NOM_TABLE(nomcol1,nomcol2,...) ...

    for example:

            EXCLUDE_COLUMNS T_TEST1(id,dossier) T_TEST2(id,fichier)

    This will exclude from the export columns &#39;id&#39; and &#39;dossier&#39; from
    table T_TEST1 and columns &#39;id&#39; and &#39;fichier&#39; from the T_TEST2 table.
    This directive can only be used with TABLE, COPY or INSERT export.
    With TABLE export create table DDL will respect the new list of
    columns and all indexes or foreign key pointing to or from a column
    removed will not be exported.

REPLACE_TABLES
    This directive allows you to remap a list of Oracle table name to a
    PostgreSQL table name during export. The value is a list of
    space-separated values with the following structure:

            REPLACE_TABLES  ORIG_TBNAME1:DEST_TBNAME1 ORIG_TBNAME2:DEST_TBNAME2

    Oracle tables ORIG_TBNAME1 and ORIG_TBNAME2 will be respectively
    renamed into DEST_TBNAME1 and DEST_TBNAME2

REPLACE_COLS
    Like table name, the name of the column can be remapped to a
    different name using the following syntax:

            REPLACE_COLS    ORIG_TBNAME(ORIG_COLNAME1:NEW_COLNAME1,ORIG_COLNAME2:NEW_COLNAME2)

    For example:

            REPLACE_COLS    T_TEST(dico:dictionary,dossier:folder)

    will rename Oracle columns &#39;dico&#39; and &#39;dossier&#39; from table T_TEST
    into new name &#39;dictionary&#39; and &#39;folder&#39;.

REPLACE_AS_BOOLEAN
    If you want to change the type of some Oracle columns into
    PostgreSQL boolean during the export you can define here a list of
    tables and column separated by space as follow.

            REPLACE_AS_BOOLEAN     TB_NAME1:COL_NAME1 TB_NAME1:COL_NAME2 TB_NAME2:COL_NAME2

    The values set in the boolean columns list will be replaced with the
    &#39;t&#39; and &#39;f&#39; following the default replacement values and those
    additionally set in directive BOOLEAN_VALUES.

    Note that if you have modified the table name with REPLACE_TABLES
    and/or the column&#39;s name, you need to use the name of the original
    table and/or column.

            REPLACE_COLS            TB_NAME1(OLD_COL_NAME1:NEW_COL_NAME1)
            REPLACE_AS_BOOLEAN      TB_NAME1:OLD_COL_NAME1

    You can also give a type and a precision to automatically convert
    all fields of that type as a boolean. For example:

            REPLACE_AS_BOOLEAN      NUMBER:1 CHAR:1 TB_NAME1:COL_NAME1 TB_NAME1:COL_NAME2

    will also replace any field of type number(1) or char(1) as a
    boolean in all exported tables.

BOOLEAN_VALUES
    Use this to add additional definition of the possible boolean values
    used in Oracle fields. You must set a space-separated list of
    TRUE:FALSE values. By default here are the values recognized by
    Ora2Pg:

            BOOLEAN_VALUES          yes:no y:n 1:0 true:false enabled:disabled

    Any values defined here will be added to the default list.

REPLACE_ZERO_DATE
    When Ora2Pg find a &quot;zero&quot; date: 0000-00-00 00:00:00 it is replaced
    by a NULL. This could be a problem if your column is defined with
    NOT NULL constraint. If you can not remove the constraint, use this
    directive to set an arbitral date that will be used instead. You can
    also use -INFINITY if you don&#39;t want to use a fake date.

INDEXES_SUFFIX
    Add the given value as suffix to indexes names. Useful if you have
    indexes with same name as tables. For example:

            INDEXES_SUFFIX          _idx

    will add _idx at ed of all index name. Not so common but can help.

INDEXES_RENAMING
    Enable this directive to rename all indexes using
    tablename_columns_names. Could be very useful for database that have
    multiple time the same index name or that use the same name than a
    table, which is not allowed by PostgreSQL Disabled by default.

USE_INDEX_OPCLASS
    Operator classes text_pattern_ops, varchar_pattern_ops, and
    bpchar_pattern_ops support B-tree indexes on the corresponding
    types. The difference from the default operator classes is that the
    values are compared strictly character by character rather than
    according to the locale-specific collation rules. This makes these
    operator classes suitable for use by queries involving pattern
    matching expressions (LIKE or POSIX regular expressions) when the
    database does not use the standard &quot;C&quot; locale. If you enable, with
    value 1, this will force Ora2Pg to export all indexes defined on
    varchar2() and char() columns using those operators. If you set it
    to a value greater than 1 it will only change indexes on columns
    where the character limit is greater or equal than this value. For
    example, set it to 128 to create these kind of indexes on columns of
    type varchar2(N) where N &amp;gt;= 128.

RENAME_PARTITION
    Enable this directive if you want that your partition tables will be
    renamed. Disabled by default. If you have multiple partitioned
    table, when exported to PostgreSQL some partitions could have the
    same name but different parent tables. This is not allowed, table
    name must be unique, in this case enable this directive. A partition
    will be renamed following the rule: &quot;tablename&quot;_part&quot;pos&quot; where
    &quot;pos&quot; is the partition number. For subpartition this is:
    &quot;tablename&quot;_part&quot;pos&quot;_subpart&quot;pos&quot; If this is partition/subpartition
    default: &quot;tablename&quot;_part_default
    &quot;tablename&quot;_part&quot;pos&quot;_subpart_default

DISABLE_PARTITION
    If you don&#39;t want to reproduce the partitioning like in Oracle and
    want to export all partitioned Oracle data into the main single
    table in PostgreSQL enable this directive. Ora2Pg will export all
    data into the main table name. Default is to use partitioning,
    Ora2Pg will export data from each partition and import them into the
    PostgreSQL dedicated partition table.

PARTITION_BY_REFERENCE
    How to export partition by reference. Possible values are none,
    duplicate or the number of hash partition to create. Default is none
    to not export the partitions by reference.

    Value none mean no translation and export of partition by reference
    like before. Value &#39;duplicate&#39; will duplicate the referenced column
    in the partitioned table and apply the same partitioning from the
    referenced table to the partitioned table. If the value is a number,
    the table will be partitioned with the HASH method using the value
    as the modulo. For example if you set it to 4 it will create 4 HASH
    partitions.

DISABLE_UNLOGGED
    By default Ora2Pg export Oracle tables with the NOLOGGING attribute
    as UNLOGGED tables. You may want to fully disable this feature
    because you will lose all data from unlogged tables in case of a
    PostgreSQL crash. Set it to 1 to export all tables as normal tables.

DOUBLE_MAX_VARCHAR
    Increase varchar max character constraints to support PostgreSQL two
    bytes character encoding when the source database applies the length
    constraint on characters not bytes. Default disabled.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Oracle Spatial to PostGis Ora2Pg fully export Spatial object from Oracle database. There&#39;s some configuration directives that could be used to control the export.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;AUTODETECT_SPATIAL_TYPE
    By default Ora2Pg is looking at indexes to see the spatial
    constraint type and dimensions defined under Oracle. Those
    constraints are passed as at index creation using for example:

            CREATE INDEX ... INDEXTYPE IS MDSYS.SPATIAL_INDEX
            PARAMETERS(&#39;sdo_indx_dims=2, layer_gtype=point&#39;);

    If those Oracle constraints parameters are not set, the default is
    to export those columns as generic type GEOMETRY to be able to
    receive any spatial type.

    The AUTODETECT_SPATIAL_TYPE directive allows to force Ora2Pg to
    autodetect the real spatial type and dimension used in a spatial
    column otherwise a non- constrained &quot;geometry&quot; type is used.
    Enabling this feature will force Ora2Pg to scan a sample of 50000
    column to look at the GTYPE used. You can increase or reduce the
    sample size by setting the value of AUTODETECT_SPATIAL_TYPE to the
    desired number of line to scan. The directive is enabled by default.

    For example, in the case of a column named shape and defined with
    Oracle type SDO_GEOMETRY, with AUTODETECT_SPATIAL_TYPE disabled it
    will be converted as:

        shape geometry(GEOMETRY) or shape geometry(GEOMETRYZ, 4326)

    and if the directive is enabled and the column just contains a
    single geometry type that use a single dimension:

        shape geometry(POLYGON, 4326) or shape geometry(POLYGONZ, 4326)

    with a two or three dimensional polygon.

CONVERT_SRID
    This directive allows you to control the automatically conversion of
    Oracle SRID to standard EPSG. If enabled, Ora2Pg will use the Oracle
    function sdo_cs.map_oracle_srid_to_epsg() to convert all SRID.
    Enabled by default.

    If the SDO_SRID returned by Oracle is NULL, it will be replaced by
    the default value 8307 converted to its EPSG value: 4326 (see
    DEFAULT_SRID).

    If the value is upper than 1, all SRID will be forced to this value,
    in this case DEFAULT_SRID will not be used when Oracle returns a
    null value and the value will be forced to CONVERT_SRID.

    Note that it is also possible to set the EPSG value on Oracle side
    when sdo_cs.map_oracle_srid_to_epsg() return NULL if your want to
    force the value:

      system@db&amp;gt; UPDATE sdo_coord_ref_sys SET legacy_code=41014 WHERE srid = 27572;

DEFAULT_SRID
    Use this directive to override the default EPSG SRID to used: 4326.
    Can be overwritten by CONVERT_SRID, see above.

GEOMETRY_EXTRACT_TYPE
    This directive can take three values: WKT (default), WKB and
    INTERNAL. When it is set to WKT, Ora2Pg will use
    SDO_UTIL.TO_WKTGEOMETRY() to extract the geometry data. When it is
    set to WKB, Ora2Pg will use the binary output using
    SDO_UTIL.TO_WKBGEOMETRY(). If those two extract type are calls at
    Oracle side, they are slow and you can easily reach Out Of Memory
    when you have lot of rows. Also WKB is not able to export 3D
    geometry and some geometries like CURVEPOLYGON. In this case you may
    use the INTERNAL extraction type. It will use a Pure Perl library to
    convert the SDO_GEOMETRY data into a WKT representation, the
    translation is done on Ora2Pg side. This is a work in progress,
    please validate your exported data geometries before use. Default
    spatial object extraction type is INTERNAL.

POSTGIS_SCHEMA
    Use this directive to add a specific schema to the search path to
    look for PostGis functions.

ST_SRID_FUNCTION
    Oracle function to use to extract the srid from ST_Geometry meta
    information. Default: ST_SRID, for example it should be set to
    sde.st_srid for ArcSDE.

ST_DIMENSION_FUNCTION
    Oracle function to use to extract the dimension from ST_Geometry
    meta information. Default: ST_DIMENSION, for example it should be
    set to sde.st_dimention for ArcSDE.

ST_GEOMETRYTYPE_FUNCTION
    Oracle function to use to extract the geometry type from a
    ST_Geometry column Default: ST_GEOMETRYTYPE, for example it should
    be set to sde.st_geometrytype for ArcSDE.

ST_ASBINARY_FUNCTION
    Oracle function to used to convert an ST_Geometry value into WKB
    format. Default: ST_ASBINARY, for example it should be set to
    sde.st_asbinary for ArcSDE.

ST_ASTEXT_FUNCTION
    Oracle function to used to convert an ST_Geometry value into WKT
    format. Default: ST_ASTEXT, for example it should be set to
    sde.st_astext for ArcSDE.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;PostgreSQL Import By default conversion to PostgreSQL format is written to file &#39;output.sql&#39;. The command:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;        psql mydb &amp;lt; output.sql

will import content of file output.sql into PostgreSQL mydb database.

DATA_LIMIT
    When you are performing INSERT/COPY export Ora2Pg proceed by chunks
    of DATA_LIMIT tuples for speed improvement. Tuples are stored in
    memory before being written to disk, so if you want speed and have
    enough system resources you can grow this limit to an upper value
    for example: 100000 or 1000000. Before release 7.0 a value of 0 mean
    no limit so that all tuples are stored in memory before being
    flushed to disk. In 7.x branch this has been remove and chunk will
    be set to the default: 10000

BLOB_LIMIT
    When Ora2Pg detect a table with some BLOB it will automatically
    reduce the value of this directive by dividing it by 10 until his
    value is below 1000. You can control this value by setting
    BLOB_LIMIT. Exporting BLOB use lot of resources, setting it to a too
    high value can produce OOM.

CLOB_AS_BLOB
    Apply same behavior on CLOB than BLOB with BLOB_LIMIT settings. This
    is especially useful if you have large CLOB data. Default: enabled

OUTPUT
    The Ora2Pg output filename can be changed with this directive.
    Default value is output.sql. if you set the file name with extension
    .gz or .bz2 the output will be automatically compressed. This
    require that the Compress::Zlib Perl module is installed if the
    filename extension is .gz and that the bzip2 system command is
    installed for the .bz2 extension.

OUTPUT_DIR
    Since release 7.0, you can define a base directory where the file
    will be written. The directory must exists.

BZIP2
    This directive allows you to specify the full path to the bzip2
    program if it can not be found in the PATH environment variable.

FILE_PER_CONSTRAINT
    Allow object constraints to be saved in a separate file during
    schema export. The file will be named CONSTRAINTS_OUTPUT, where
    OUTPUT is the value of the corresponding configuration directive.
    You can use .gz xor .bz2 extension to enable compression. Default is
    to save all data in the OUTPUT file. This directive is usable only
    with TABLE export type.

    The constraints can be imported quickly into PostgreSQL using the
    LOAD export type to parallelize their creation over multiple (-j or
    JOBS) connections.

FILE_PER_INDEX
    Allow indexes to be saved in a separate file during schema export.
    The file will be named INDEXES_OUTPUT, where OUTPUT is the value of
    the corresponding configuration directive. You can use .gz xor .bz2
    file extension to enable compression. Default is to save all data in
    the OUTPUT file. This directive is usable only with TABLE AND
    TABLESPACE export type. With the TABLESPACE export, it is used to
    write &quot;ALTER INDEX ... TABLESPACE ...&quot; into a separate file named
    TBSP_INDEXES_OUTPUT that can be loaded at end of the migration after
    the indexes creation to move the indexes.

    The indexes can be imported quickly into PostgreSQL using the LOAD
    export type to parallelize their creation over multiple (-j or JOBS)
    connections.

FILE_PER_FKEYS
    Allow foreign key declaration to be saved in a separate file during
    schema export. By default foreign keys are exported into the main
    output file or in the CONSTRAINT_output.sql file. When enabled
    foreign keys will be exported into a file named FKEYS_output.sql

FILE_PER_TABLE
    Allow data export to be saved in one file per table/view. The files
    will be named as tablename_OUTPUT, where OUTPUT is the value of the
    corresponding configuration directive. You can still use .gz xor
    .bz2 extension in the OUTPUT directive to enable compression.
    Default 0 will save all data in one file, set it to 1 to enable this
    feature. This is usable only during INSERT or COPY export type.

FILE_PER_FUNCTION
    Allow functions, procedures and triggers to be saved in one file per
    object. The files will be named as objectname_OUTPUT. Where OUTPUT
    is the value of the corresponding configuration directive. You can
    still use .gz xor .bz2 extension in the OUTPUT directive to enable
    compression. Default 0 will save all in one single file, set it to 1
    to enable this feature. This is usable only during the corresponding
    export type, the package body export has a special behavior.

    When export type is PACKAGE and you&#39;ve enabled this directive,
    Ora2Pg will create a directory per package, named with the lower
    case name of the package, and will create one file per
    function/procedure into that directory. If the configuration
    directive is not enabled, it will create one file per package as
    packagename_OUTPUT, where OUTPUT is the value of the corresponding
    directive.

TRUNCATE_TABLE
    If this directive is set to 1, a TRUNCATE TABLE instruction will be
    add before loading data. This is usable only during INSERT or COPY
    export type.

    When activated, the instruction will be added only if there&#39;s no
    global DELETE clause or not one specific to the current table (see
    below).

DELETE
    Support for include a DELETE FROM ... WHERE clause filter before
    importing data and perform a delete of some lines instead of
    truncating tables. Value is construct as follow:
    TABLE_NAME[DELETE_WHERE_CLAUSE], or if you have only one where
    clause for all tables just put the delete clause as single value.
    Both are possible too. Here are some examples:

            DELETE  1=1    # Apply to all tables and delete all tuples
            DELETE  TABLE_TEST[ID1=&#39;001&#39;]   # Apply only on table TABLE_TEST
            DELETE  TABLE_TEST[ID1=&#39;001&#39; OR ID1=&#39;002] DATE_CREATE &amp;gt; &#39;2001-01-01&#39; TABLE_INFO[NAME=&#39;test&#39;]

    The last applies two different delete where clause on tables
    TABLE_TEST and TABLE_INFO and a generic delete where clause on
    DATE_CREATE to all other tables. If TRUNCATE_TABLE is enabled it
    will be applied to all tables not covered by the DELETE definition.

    These DELETE clauses might be useful with regular &quot;updates&quot;.

STOP_ON_ERROR
    Set this parameter to 0 to not include the call to \set
    ON_ERROR_STOP ON in all SQL scripts generated by Ora2Pg. By default
    this order is always present so that the script will immediately
    abort when an error is encountered.

COPY_FREEZE
    Enable this directive to use COPY FREEZE instead of a simple COPY to
    export data with rows already frozen. This is intended as a
    performance option for initial data loading. Rows will be frozen
    only if the table being loaded has been created or truncated in the
    current sub-transaction. This will only work with export to file and
    when -J or ORACLE_COPIES is not set or default to 1. It can be used
    with direct import into PostgreSQL under the same condition but -j
    or JOBS must also be unset or default to 1.

CREATE_OR_REPLACE
    By default Ora2Pg uses CREATE OR REPLACE in functions and views DDL,
    if you need not to override existing functions or views disable this
    configuration directive, DDL will not include OR REPLACE.

DROP_IF_EXISTS
    To add a DROP &amp;lt;OBJECT&amp;gt; IF EXISTS before creating the object, enable
    this directive. Can be useful in an iterative work. Default is
    disabled.

EXPORT_GTT
    PostgreSQL do not supports Global Temporary Table natively but you
    can use the pgtt extension to emulate this behavior. Enable this
    directive to export global temporary table.

PGTT_NOSUPERUSER
    By default the pgtt extension is loaded using the superuser
    privilege. Enabled it if you run the SQL scripts generated using a
    non superuser user. It will use:

        LOAD &#39;$libdir/plugins/pgtt&#39;;

    instead of default:

        LOAD &#39;pgtt&#39;;

NO_HEADER
    Enabling this directive will prevent Ora2Pg to print his header into
    output files. Only the translated code will be written.

PSQL_RELATIVE_PATH
    By default Ora2Pg use \i psql command to execute generated SQL files
    if you want to use a relative path following the script execution
    file enabling this option will use \ir. See psql help for more
    information.

DATA_VALIDATION_ROWS
    Number of rows that must be retrieved on both side for data
    validation. Default it to compare the 10000 first rows. A value of 0
    mean compare all rows.

DATA_VALIDATION_ORDERING
    Order of rows between both sides are different once the data have
    been modified. In this case data must be ordered using a primary key
    or a unique index, that mean that a table without such object can
    not be compared. If the validation is done just after the data
    migration without any data modification the validation can be done
    on all tables without any ordering.

DATA_VALIDATION_ERROR
    Stop validating data from a table after a certain amount of row
    mistmatch. Default is to stop after 10 rows validation errors.

TRANSFORM_VALUE
    Use this directive to precise which transformation should be applied
    to a column when exporting data. Value must be a semicolon separated
    list of

       TABLE[COLUMN_NAME, &amp;lt;replace code in SELECT target list&amp;gt;]

    For example to replace string &#39;Oracle&#39; by &#39;PostgreSQL&#39; in a varchar2
    column use the following.

       TRANSFORM_VALUE   ERROR_LOG_SAMPLE[DBMS_TYPE:regexp_replace(&quot;DBMS_TYPE&quot;,&#39;Oracle&#39;,&#39;PostgreSQL&#39;)]

    or to replace all Oracle char(0) in a string by a space character:

        TRANSFORM_VALUE   CLOB_TABLE[CHARDATA:translate(&quot;CHARDATA&quot;, chr(0), &#39; &#39;)]

    The expression will be applied in the SQL statemeent used to extract
    data from the source database.

When using Ora2Pg export type INSERT or COPY to dump data to file and
that FILE_PER_TABLE is enabled, you will be warned that Ora2Pg will not
export data again if the file already exists. This is to prevent
downloading twice table with huge amount of data. To force the download
of data from these tables you have to remove the existing output file
first.

If you want to import data on the fly to the PostgreSQL database you
have three configuration directives to set the PostgreSQL database
connection. This is only possible with COPY or INSERT export type as for
database schema there&#39;s no real interest to do that.

PG_DSN
    Use this directive to set the PostgreSQL data source namespace using
    DBD::Pg Perl module as follow:

            dbi:Pg:dbname=pgdb;host=localhost;port=5432

    will connect to database &#39;pgdb&#39; on localhost at tcp port 5432.

    Note that this directive is only used for data export, other export
    need to be imported manually through the use og psql or any other
    PostgreSQL client.

    To use SSL encrypted connection you must add sslmode=require to the
    connection string like follow:

            dbi:Pg:dbname=pgdb;host=localhost;port=5432;sslmode=require

PG_USER and PG_PWD
    These two directives are used to set the login user and password.

    If you do not supply a credential with PG_PWD and you have installed
    the Term::ReadKey Perl module, Ora2Pg will ask for the password
    interactively. If PG_USER is not set it will be asked interactively
    too.

SYNCHRONOUS_COMMIT
    Specifies whether transaction commit will wait for WAL records to be
    written to disk before the command returns a &quot;success&quot; indication to
    the client. This is the equivalent to set synchronous_commit
    directive of postgresql.conf file. This is only used when you load
    data directly to PostgreSQL, the default is off to disable
    synchronous commit to gain speed at writing data. Some modified
    version of PostgreSQL, like greenplum, do not have this setting, so
    in this set this directive to 1, ora2pg will not try to change the
    setting.

PG_INITIAL_COMMAND
    This directive can be used to send an initial command to PostgreSQL,
    just after the connection. For example to set some session
    parameters. This directive can be used multiple times.

INSERT_ON_CONFLICT
    When enabled this instruct Ora2Pg to add an ON CONFLICT DO NOTHING
    clause to all INSERT statements generated for this type of data
    export.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Column type control PG_NUMERIC_TYPE If set to 1 replace portable numeric type into PostgreSQL internal type. Oracle data type NUMBER(p,s) is approximatively converted to real and float PostgreSQL data type. If you have monetary fields or don&#39;t want rounding issues with the extra decimals you should preserve the same numeric(p,s) PostgreSQL data type. Do that only if you need exactness because using numeric(p,s) is slower than using real or double.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;PG_INTEGER_TYPE
    If set to 1 replace portable numeric type into PostgreSQL internal
    type. Oracle data type NUMBER(p) or NUMBER are converted to
    smallint, integer or bigint PostgreSQL data type following the value
    of the precision. If NUMBER without precision are set to
    DEFAULT_NUMERIC (see below).

DEFAULT_NUMERIC
    NUMBER without precision are converted by default to bigint only if
    PG_INTEGER_TYPE is true. You can overwrite this value to any PG
    type, like integer or float.

DATA_TYPE
    If you&#39;re experiencing any problem in data type schema conversion
    with this directive you can take full control of the correspondence
    between Oracle and PostgreSQL types to redefine data type
    translation used in Ora2pg. The syntax is a comma-separated list of
    &quot;Oracle datatype:Postgresql datatype&quot;. Here are the default list
    used:

            DATA_TYPE       VARCHAR2:varchar,NVARCHAR2:varchar,NVARCHAR:varchar,NCHAR:char,DATE:timestamp(0),LONG:text,LONG RAW:bytea,CLOB:text,NCLOB:text,BLOB:bytea,BFILE:bytea,RAW(16):uuid,RAW(32):uuid,RAW:bytea,UROWID:oid,ROWID:oid,FLOAT:double precision,DEC:decimal,DECIMAL:decimal,DOUBLE PRECISION:double precision,INT:integer,INTEGER:integer,REAL:real,SMALLINT:smallint,BINARY_FLOAT:double precision,BINARY_DOUBLE:double precision,TIMESTAMP:timestamp,XMLTYPE:xml,BINARY_INTEGER:integer,PLS_INTEGER:integer,TIMESTAMP WITH TIME ZONE:timestamp with time zone,TIMESTAMP WITH LOCAL TIME ZONE:timestamp with time zone

    The directive and the list definition must be a single line.

    Note that when a RAW(16) and RAW(32) columns is found or that the
    RAW column has &quot;SYS_GUID()&quot; as default value Ora2Pg will
    automatically translate the type of the column into uuid which might
    be the right translation in most of the case. In this case data will
    be automatically migrated as PostgreSQL uuid data type provided by
    the &quot;uuid-ossp&quot; extension.

    If you want to replace a type with a precision and scale you need to
    escape the coma with a backslash. For example, if you want to
    replace all NUMBER(*,0) into bigint instead of numeric(38) add the
    following:

           DATA_TYPE       NUMBER(*\,0):bigint

    You don&#39;t have to recopy all default type conversion but just the
    one you want to rewrite.

    There&#39;s a special case with BFILE when they are converted to type
    TEXT, they will just contains the full path to the external file. If
    you set the destination type to BYTEA, the default, Ora2Pg will
    export the content of the BFILE as bytea. The third case is when you
    set the destination type to EFILE, in this case, Ora2Pg will export
    it as an EFILE record: (DIRECTORY, FILENAME). Use the DIRECTORY
    export type to export the existing directories as well as privileges
    on those directories.

    There&#39;s no SQL function available to retrieve the path to the BFILE.
    Ora2Pg have to create one using the DBMS_LOB package.

            CREATE OR REPLACE FUNCTION ora2pg_get_bfilename( p_bfile IN BFILE )
            RETURN VARCHAR2
            AS
                l_dir   VARCHAR2(4000);
                l_fname VARCHAR2(4000);
                l_path  VARCHAR2(4000);
            BEGIN
                dbms_lob.FILEGETNAME( p_bfile, l_dir, l_fname );
                SELECT directory_path INTO l_path FROM all_directories
                    WHERE directory_name = l_dir;
                l_dir := rtrim(l_path,&#39;/&#39;);
                RETURN l_dir || &#39;/&#39; || l_fname;
            END;

    This function is only created if Ora2Pg found a table with a BFILE
    column and that the destination type is TEXT. The function is
    dropped at the end of the export. This concern both, COPY and INSERT
    export type.

    There&#39;s no SQL function available to retrieve BFILE as an EFILE
    record, then Ora2Pg have to create one using the DBMS_LOB package.

            CREATE OR REPLACE FUNCTION ora2pg_get_efile( p_bfile IN BFILE )
            RETURN VARCHAR2
            AS
                l_dir   VARCHAR2(4000);
                l_fname VARCHAR2(4000);
            BEGIN
                dbms_lob.FILEGETNAME( p_bfile, l_dir, l_fname );
                RETURN &#39;(&#39; || l_dir || &#39;,&#39; || l_fnamei || &#39;)&#39;;
            END;

    This function is only created if Ora2Pg found a table with a BFILE
    column and that the destination type is EFILE. The function is
    dropped at the end of the export. This concern both, COPY and INSERT
    export type.

    To set the destination type, use the DATA_TYPE configuration
    directive:

            DATA_TYPE       BFILE:EFILE

    for example.

    The EFILE type is a user defined type created by the PostgreSQL
    extension external_file that can be found here:
    https://github.com/darold/external_file This is a port of the BFILE
    Oracle type to PostgreSQL.

    There&#39;s no SQL function available to retrieve the content of a
    BFILE. Ora2Pg have to create one using the DBMS_LOB package.

            CREATE OR REPLACE FUNCTION ora2pg_get_bfile( p_bfile IN BFILE ) RETURN
            BLOB
              AS
                    filecontent BLOB := NULL;
                    src_file BFILE := NULL;
                    l_step PLS_INTEGER := 12000;
                    l_dir   VARCHAR2(4000);
                    l_fname VARCHAR2(4000);
                    offset NUMBER := 1;
              BEGIN
                IF p_bfile IS NULL THEN
                  RETURN NULL;
                END IF;

                DBMS_LOB.FILEGETNAME( p_bfile, l_dir, l_fname );
                src_file := BFILENAME( l_dir, l_fname );
                IF src_file IS NULL THEN
                    RETURN NULL;
                END IF;

                DBMS_LOB.FILEOPEN(src_file, DBMS_LOB.FILE_READONLY);
                DBMS_LOB.CREATETEMPORARY(filecontent, true);
                DBMS_LOB.LOADBLOBFROMFILE (filecontent, src_file, DBMS_LOB.LOBMAXSIZE, offset, offset);
                DBMS_LOB.FILECLOSE(src_file);
                RETURN filecontent;
            END;

    This function is only created if Ora2Pg found a table with a BFILE
    column and that the destination type is bytea (the default). The
    function is dropped at the end of the export. This concern both,
    COPY and INSERT export type.

    About the ROWID and UROWID, they are converted into OID by &quot;logical&quot;
    default but this will through an error at data import. There is no
    equivalent data type so you might want to use the DATA_TYPE
    directive to change the corresponding type in PostgreSQL. You should
    consider replacing this data type by a bigserial (autoincremented
    sequence), text or uuid data type.

MODIFY_TYPE
    Sometimes you need to force the destination type, for example a
    column exported as timestamp by Ora2Pg can be forced into type date.
    Value is a comma-separated list of TABLE:COLUMN:TYPE structure. If
    you need to use comma or space inside type definition you will have
    to backslash them.

            MODIFY_TYPE     TABLE1:COL3:varchar,TABLE1:COL4:decimal(9\,6)

    Type of table1.col3 will be replaced by a varchar and table1.col4 by
    a decimal with precision and scale.

    If the column&#39;s type is a user defined type Ora2Pg will autodetect
    the composite type and will export its data using ROW(). Some Oracle
    user defined types are just array of a native type, in this case you
    may want to transform this column in simple array of a PostgreSQL
    native type. To do so, just redefine the destination type as wanted
    and Ora2Pg will also transform the data as an array. For example,
    with the following definition in Oracle:

            CREATE OR REPLACE TYPE mem_type IS VARRAY(10) of VARCHAR2(15);
            CREATE TABLE club (Name VARCHAR2(10),
                    Address VARCHAR2(20),
                    City VARCHAR2(20),
                    Phone VARCHAR2(8),
                    Members mem_type
            );

    custom type &quot;mem_type&quot; is just a string array and can be translated
    into the following in PostgreSQL:

            CREATE TABLE club (
                    name varchar(10),
                    address varchar(20),
                    city varchar(20),
                    phone varchar(8),
                    members text[]
            ) ;

    To do so, just use the directive as follow:

            MODIFY_TYPE     CLUB:MEMBERS:text[]

    Ora2Pg will take care to transform all data of this column in the
    correct format. Only arrays of characters and numerics types are
    supported.

TO_NUMBER_CONVERSION
    By default Oracle call to function TO_NUMBER will be translated as a
    cast into numeric. For example, TO_NUMBER(&#39;10.1234&#39;) is converted
    into PostgreSQL call to_number(&#39;10.1234&#39;)::numeric. If you want you
    can cast the call to integer or bigint by changing the value of the
    configuration directive. If you need better control of the format,
    just set it as value, for example: TO_NUMBER_CONVERSION
    99999999999999999999.9999999999 will convert the code above as:
    TO_NUMBER(&#39;10.1234&#39;, &#39;99999999999999999999.9999999999&#39;) Any value of
    the directive that it is not numeric, integer or bigint will be
    taken as a mask format. If set to none, no conversion will be done.

VARCHAR_TO_TEXT
    By default varchar2 without size constraint are tranlated into text.
    If you want to keep the varchar name, disable this directive.

FORCE_IDENTITY_BIGINT
    Usually identity column must be bigint to correspond to an auto
    increment sequence so Ora2Pg always force it to be a bigint. If, for
    any reason you want Ora2Pg to respect the DATA_TYPE you have set for
    identity column then disable this directive.

TO_CHAR_NOTIMEZONE
    If you want Ora2Pg to remove any timezone information into the
    format part of the TO_CHAR() function, enable this directive.
    Disabled by default.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Taking export under control The following other configuration directives interact directly with the export process and give you fine granularity in database export control.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;SKIP
    For TABLE export you may not want to export all schema constraints,
    the SKIP configuration directive allows you to specify a
    space-separated list of constraints that should not be exported.
    Possible values are:

            - fkeys: turn off foreign key constraints
            - pkeys: turn off primary keys
            - ukeys: turn off unique column constraints
            - indexes: turn off all other index types
            - checks: turn off check constraints

    For example:

            SKIP    indexes,checks

    will removed indexes and check constraints from export.

PKEY_IN_CREATE
    Enable this directive if you want to add primary key definition
    inside the create table statement. If disabled (the default) primary
    key definition will be added with an alter table statement. Enable
    it if you are exporting to GreenPlum PostgreSQL database.

KEEP_PKEY_NAMES
    By default names of the primary and unique key in the source Oracle
    database are ignored and key names are autogenerated in the target
    PostgreSQL database with the PostgreSQL internal default naming
    rules. If you want to preserve Oracle primary and unique key names
    set this option to 1.

FKEY_ADD_UPDATE
    This directive allows you to add an ON UPDATE CASCADE option to a
    foreign key when a ON DELETE CASCADE is defined or always. Oracle do
    not support this feature, you have to use trigger to operate the ON
    UPDATE CASCADE. As PostgreSQL has this feature, you can choose how
    to add the foreign key option. There are three values to this
    directive: never, the default that mean that foreign keys will be
    declared exactly like in Oracle. The second value is delete, that
    mean that the ON UPDATE CASCADE option will be added only if the ON
    DELETE CASCADE is already defined on the foreign Keys. The last
    value, always, will force all foreign keys to be defined using the
    update option.

FKEY_DEFERRABLE
    When exporting tables, Ora2Pg normally exports constraints as they
    are, if they are non-deferrable they are exported as non-deferrable.
    However, non-deferrable constraints will probably cause problems
    when attempting to import data to Pg. The FKEY_DEFERRABLE option set
    to 1 will cause all foreign key constraints to be exported as
    deferrable.

DEFER_FKEY
    In addition to exporting data when the DEFER_FKEY option set to 1,
    it will add a command to defer all foreign key constraints during
    data export and the import will be done in a single transaction.
    This will work only if foreign keys have been exported as deferrable
    and you are not using direct import to PostgreSQL (PG_DSN is not
    defined). Constraints will then be checked at the end of the
    transaction.

    This directive can also be enabled if you want to force all foreign
    keys to be created as deferrable and initially deferred during
    schema export (TABLE export type).

DROP_FKEY
    If deferring foreign keys is not possible due to the amount of data
    in a single transaction, you&#39;ve not exported foreign keys as
    deferrable or you are using direct import to PostgreSQL, you can use
    the DROP_FKEY directive.

    It will drop all foreign keys before all data import and recreate
    them at the end of the import.

DROP_INDEXES
    This directive allows you to gain lot of speed improvement during
    data import by removing all indexes that are not an automatic index
    (indexes of primary keys) and recreate them at the end of data
    import. Of course it is far better to not import indexes and
    constraints before having imported all data.

DISABLE_TRIGGERS
    This directive is used to disable triggers on all tables in COPY or
    INSERT export modes. Available values are USER (disable user-defined
    triggers only) and ALL (includes RI system triggers). Default is 0:
    do not add SQL statements to disable trigger before data import.

    If you want to disable triggers during data migration, set the value
    to USER if your are connected as non superuser and ALL if you are
    connected as PostgreSQL superuser. A value of 1 is equal to USER.

DISABLE_SEQUENCE
    If set to 1 it disables alter of sequences on all tables during COPY
    or INSERT export mode. This is used to prevent the update of
    sequence during data migration. Default is 0, alter sequences.

NOESCAPE
    By default all data that are not of type date or time are escaped.
    If you experience any problem with that you can set it to 1 to
    disable character escaping during data export. This directive is
    only used during a COPY export. See STANDARD_CONFORMING_STRINGS for
    enabling/disabling escape with INSERT statements.

STANDARD_CONFORMING_STRINGS
    This controls whether ordinary string literals (&#39;...&#39;) treat
    backslashes literally, as specified in SQL standard. This was the
    default before Ora2Pg v8.5 so that all strings was escaped first,
    now this is currently on, causing Ora2Pg to use the escape string
    syntax (E&#39;...&#39;) if this parameter is not set to 0. This is the exact
    behavior of the same option in PostgreSQL. This directive is only
    used during data export to build INSERT statements. See NOESCAPE for
    enabling/disabling escape in COPY statements.

TRIM_TYPE
    If you want to convert CHAR(n) from Oracle into varchar(n) or text
    on PostgreSQL using directive DATA_TYPE, you might want to do some
    trimming on the data. By default Ora2Pg will auto-detect this
    conversion and remove any whitespace at both leading and trailing
    position. If you just want to remove the leadings character set the
    value to LEADING. If you just want to remove the trailing character,
    set the value to TRAILING. Default value is BOTH.

TRIM_CHAR
    The default trimming character is space, use this directive if you
    need to change the character that will be removed. For example, set
    it to - if you have leading - in the char(n) field. To use space as
    trimming charger, comment this directive, this is the default value.

PRESERVE_CASE
    If you want to preserve the case of Oracle object name set this
    directive to 1. By default Ora2Pg will convert all Oracle object
    names to lower case. I do not recommend to enable this unless you
    will always have to double-quote object names on all your SQL
    scripts.

ORA_RESERVED_WORDS
    Allow escaping of column name using Oracle reserved words. Value is
    a list of comma-separated reserved word. Default:
    audit,comment,references.

USE_RESERVED_WORDS
    Enable this directive if you have table or column names that are a
    reserved word for PostgreSQL. Ora2Pg will double quote the name of
    the object.

GEN_USER_PWD
    Set this directive to 1 to replace default password by a random
    password for all extracted user during a GRANT export.

PG_SUPPORTS_MVIEW
    Since PostgreSQL 9.3, materialized view are supported with the SQL
    syntax &#39;CREATE MATERIALIZED VIEW&#39;. To force Ora2Pg to use the native
    PostgreSQL support you must enable this configuration - enable by
    default. If you want to use the old style with table and a set of
    function, you should disable it.

PG_SUPPORTS_IFEXISTS
    PostgreSQL version below 9.x do not support IF EXISTS in DDL
    statements. Disabling the directive with value 0 will prevent Ora2Pg
    to add those keywords in all generated statements. Default value is
    1, enabled.

PG_VERSION
    Set the PostgreSQL major version number of the target database. Ex:
    9.6 or 13 Default is current major version at time of a new release.
    This replace the old and deprecadted PG_SUPPORTS_* configuration
    directives described bellow.

PG_SUPPORTS_ROLE (Deprecated)
    This option is deprecated since Ora2Pg release v7.3.

    By default Oracle roles are translated into PostgreSQL groups. If
    you have PostgreSQL 8.1 or more consider the use of ROLES and set
    this directive to 1 to export roles.

PG_SUPPORTS_INOUT (Deprecated)
    This option is deprecated since Ora2Pg release v7.3.

    If set to 0, all IN, OUT or INOUT parameters will not be used into
    the generated PostgreSQL function declarations (disable it for
    PostgreSQL database version lower than 8.1), This is now enable by
    default.

PG_SUPPORTS_DEFAULT
    This directive enable or disable the use of default parameter value
    in function export. Until PostgreSQL 8.4 such a default value was
    not supported, this feature is now enable by default.

PG_SUPPORTS_WHEN (Deprecated)
    Add support to WHEN clause on triggers as PostgreSQL v9.0 now
    support it. This directive is enabled by default, set it to 0
    disable this feature.

PG_SUPPORTS_INSTEADOF (Deprecated)
    Add support to INSTEAD OF usage on triggers (used with PG &amp;gt;= 9.1),
    if this directive is disabled the INSTEAD OF triggers will be
    rewritten as Pg rules.

PG_SUPPORTS_CHECKOPTION
    When enabled, export views with CHECK OPTION. Disable it if you have
    PostgreSQL version prior to 9.4. Default: 1, enabled.

PG_SUPPORTS_IFEXISTS
    If disabled, do not export object with IF EXISTS statements. Enabled
    by default.

PG_SUPPORTS_PARTITION
    PostgreSQL version prior to 10.0 do not have native partitioning.
    Enable this directive if you want to use declarative partitioning.
    Enable by default.

PG_SUPPORTS_SUBSTR
    Some versions of PostgreSQL like Redshift doesn&#39;t support substr()
    and it need to be replaced by a call to substring(). In this case,
    disable it.

PG_SUPPORTS_NAMED_OPERATOR
    Disable this directive if you are using PG &amp;lt; 9.5, PL/SQL operator
    used in named parameter =&amp;gt; will be replaced by PostgreSQL
    proprietary operator := Enable by default.

PG_SUPPORTS_IDENTITY
    Enable this directive if you have PostgreSQL &amp;gt;= 10 to use IDENTITY
    columns instead of serial or bigserial data type. If
    PG_SUPPORTS_IDENTITY is disabled and there is IDENTITY column in the
    Oracle table, they are exported as serial or bigserial columns. When
    it is enabled they are exported as IDENTITY columns like:

          CREATE TABLE identity_test_tab (
                  id bigint GENERATED ALWAYS AS IDENTITY,
                  description varchar(30)
          ) ;

    If there is non default sequence options set in Oracle, they will be
    appended after the IDENTITY keyword. Additionally in both cases,
    Ora2Pg will create a file AUTOINCREMENT_output.sql with a embedded
    function to update the associated sequences with the restart value
    set to &quot;SELECT max(colname)+1 FROM tablename&quot;. Of course this file
    must be imported after data import otherwise sequence will be kept
    to start value. Enabled by default.

PG_SUPPORTS_PROCEDURE
    PostgreSQL v11 adds support of PROCEDURE, enable it if you use such
    version.

BITMAP_AS_GIN
    Use btree_gin extension to create bitmap like index with pg &amp;gt;= 9.4
    You will need to create the extension by yourself: create extension
    btree_gin; Default is to create GIN index, when disabled, a btree
    index will be created

PG_BACKGROUND
    Use pg_background extension to create an autonomous transaction
    instead of using a dblink wrapper. With pg &amp;gt;= 9.5 only. Default is
    to use dblink. See https://github.com/vibhorkum/pg_background about
    this extension.

DBLINK_CONN
    By default if you have an autonomous transaction translated using
    dblink extension instead of pg_background the connection is defined
    using the values set with PG_DSN, PG_USER and PG_PWD. If you want to
    fully override the connection string use this directive as follow to
    set the connection in the autonomous transaction wrapper function.
    For example:

            DBLINK_CONN    port=5432 dbname=pgdb host=localhost user=pguser password=pgpass

LONGREADLEN
    Use this directive to set the database handle&#39;s &#39;LongReadLen&#39;
    attribute to a value that will be the larger than the expected size
    of the LOBs. The default is 1MB witch may not be enough to extract
    BLOBs or CLOBs. If the size of the LOB exceeds the &#39;LongReadLen&#39;
    DBD::Oracle will return a &#39;ORA-24345: A Truncation&#39; error. Default:
    1023*1024 bytes.

    Take a look at this page to learn more:
    http://search.cpan.org/~pythian/DBD-Oracle-1.22/Oracle.pm#Data_Inter
    face_for_Persistent_LOBs

    Important note: If you increase the value of this directive take
    care that DATA_LIMIT will probably needs to be reduced. Even if you
    only have a 1MB blob, trying to read 10000 of them (the default
    DATA_LIMIT) all at once will require 10GB of memory. You may extract
    data from those table separately and set a DATA_LIMIT to 500 or
    lower, otherwise you may experience some out of memory.

LONGTRUNKOK
    If you want to bypass the &#39;ORA-24345: A Truncation&#39; error, set this
    directive to 1, it will truncate the data extracted to the
    LongReadLen value. Disable by default so that you will be warned if
    your LongReadLen value is not high enough.

USE_LOB_LOCATOR
    Disable this if you want to load full content of BLOB and CLOB and
    not use LOB locators. In this case you will have to set LONGREADLEN
    to the right value. Note that this will not improve speed of BLOB
    export as most of the time is always consumed by the bytea escaping
    and in this case export is done line by line and not by chunk of
    DATA_LIMIT rows. For more information on how it works, see
    http://search.cpan.org/~pythian/DBD-Oracle-1.74/lib/DBD/Oracle.pm#Da
    ta_Interface_for_LOB_Locators

    Default is enabled, it use LOB locators.

LOB_CHUNK_SIZE
    Oracle recommends reading from and writing to a LOB in batches using
    a multiple of the LOB chunk size. This chunk size defaults to 8k
    (8192). Recent tests shown that the best performances can be reach
    with higher value like 512K or 4Mb.

    A quick benchmark with 30120 rows with different size of BLOB
    (200x5Mb, 19800x212k, 10000x942K, 100x17Mb, 20x156Mb), with
    DATA_LIMIT=100, LONGREADLEN=170Mb and a total table size of 20GB
    gives:

           no lob locator  : 22m46,218s (1365 sec., avg: 22 recs/sec)
           chunk size 8k   : 15m50,886s (951 sec., avg: 31 recs/sec)
           chunk size 512k : 1m28,161s (88 sec., avg: 342 recs/sec)
           chunk size 4Mb  : 1m23,717s (83 sec., avg: 362 recs/sec)

    In conclusion it can be more than 10 time faster with LOB_CHUNK_SIZE
    set to 4Mb. Depending of the size of most BLOB you may want to
    adjust the value here. For example if you have a majority of small
    lobs bellow 8K, using 8192 is better to not waste space. Default
    value for LOB_CHUNK_SIZE is 512000.

XML_PRETTY
    Force the use getStringVal() instead of getClobVal() for XML data
    export. Default is 1, enabled for backward compatibility. Set it to
    0 to use extract method a la CLOB. Note that XML value extracted
    with getStringVal() must not exceed VARCHAR2 size limit (4000)
    otherwise it will return an error.

ENABLE_MICROSECOND
    Set it to O if you want to disable export of millisecond from Oracle
    timestamp columns. By default milliseconds are exported with the use
    of following format:

            &#39;YYYY-MM-DD HH24:MI:SS.FF&#39;

    Disabling will force the use of the following Oracle format:

            to_char(..., &#39;YYYY-MM-DD HH24:MI:SS&#39;)

    By default milliseconds are exported.

DISABLE_COMMENT
    Set this to 1 if you don&#39;t want to export comment associated to
    tables and columns definition. Default is enabled.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Control MySQL export behavior MYSQL_PIPES_AS_CONCAT Enable this if double pipe and double ampersand (|| and &amp;amp;&amp;amp;) should not be taken as equivalent to OR and AND. It depend of the variable @sql_mode, Use it only if Ora2Pg fail on auto detecting this behavior.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;MYSQL_INTERNAL_EXTRACT_FORMAT
    Enable this directive if you want EXTRACT() replacement to use the
    internal format returned as an integer, for example DD HH24:MM:SS
    will be replaced with format; DDHH24MMSS::bigint, this depend of
    your apps usage.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Control SQL Server export behavior DROP_ROWVERSION PostgreSQL has no equivalent to rowversion datatype and feature, if you want to remove these useless columns, enable this directive. Columns of datatype &#39;rowversion&#39; or &#39;timestamp&#39; will not be exported.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;CASE_INSENSITIVE_SEARCH
    Emulate the same behavior of MSSQL with case insensitive search. If
    the value is citext it will use the citext data type instead of
    char/varchar/text in tables DDL (Ora2Pg will add a CHECK constraint
    for columns with a precision). Instead of citext you can also set a
    collation name that will be used in the columns definitions. To
    disable case insensitive search set it to: none.

SELECT_TOP
    Append a TOP N clause to the SELECT command used to extract the data
    from SQL Server. This is the equivalent to a WHERE ROWNUM &amp;lt; 1000
    clause for Oracle.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Special options to handle character encoding NLS_LANG and NLS_NCHAR By default Ora2Pg will set NLS_LANG to AMERICAN_AMERICA.AL32UTF8 and NLS_NCHAR to AL32UTF8. It is not recommended to change those settings but in some case it could be useful. Using your own settings with those configuration directive will change the client encoding at Oracle side by setting the environment variables $ENV{NLS_LANG} and $ENV{NLS_NCHAR}.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;BINMODE
    By default Ora2Pg will force Perl to use utf8 I/O encoding. This is
    done through a call to the Perl pragma:

            use open &#39;:utf8&#39;;

    You can override this encoding by using the BINMODE directive, for
    example you can set it to :locale to use your locale or iso-8859-7,
    it will respectively use

            use open &#39;:locale&#39;;
            use open &#39;:encoding(iso-8859-7)&#39;;

    If you have change the NLS_LANG in non UTF8 encoding, you might want
    to set this directive. See http://perldoc.perl.org/5.14.2/open.html
    for more information. Most of the time, leave this directive
    commented.

CLIENT_ENCODING
    By default PostgreSQL client encoding is automatically set to UTF8
    to avoid encoding issue. If you have changed the value of NLS_LANG
    you might have to change the encoding of the PostgreSQL client.

    You can take a look at the PostgreSQL supported character sets here:
    http://www.postgresql.org/docs/9.0/static/multibyte.html

FORCE_PLSQL_ENCODING
    To force utf8 encoding of the PL/SQL code exported, enable this
    directive. Could be helpful in some rare condition.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;PLSQL to PLPGSQL conversion Automatic code conversion from Oracle PLSQL to PostgreSQL PLPGSQL is a work in progress in Ora2Pg and surely you will always have manual work. The Perl code used for automatic conversion is all stored in a specific Perl Module named Ora2Pg/PLSQL.pm feel free to modify/add you own code and send me patches. The main work in on function, procedure, package and package body headers and parameters rewrite.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;PLSQL_PGSQL
    Enable/disable PLSQL to PLPGSQL conversion. Enabled by default.

NULL_EQUAL_EMPTY
    Ora2Pg can replace all conditions with a test on NULL by a call to
    the coalesce() function to mimic the Oracle behavior where empty
    string are considered equal to NULL.

            (field1 IS NULL) is replaced by (coalesce(field1::text, &#39;&#39;) = &#39;&#39;)
            (field2 IS NOT NULL) is replaced by (field2 IS NOT NULL AND field2::text &amp;lt;&amp;gt; &#39;&#39;)

    You might want this replacement to be sure that your application
    will have the same behavior but if you have control on you
    application a better way is to change it to transform empty string
    into NULL because PostgreSQL makes the difference.

EMPTY_LOB_NULL
    Force empty_clob() and empty_blob() to be exported as NULL instead
    as empty string for the first one and &#39;\x&#39; for the second. If NULL
    is allowed in your column this might improve data export speed if
    you have lot of empty lob. Default is to preserve the exact data
    from Oracle.

PACKAGE_AS_SCHEMA
    If you don&#39;t want to export package as schema but as simple
    functions you might also want to replace all call to
    package_name.function_name. If you disable the PACKAGE_AS_SCHEMA
    directive then Ora2Pg will replace all call to
    package_name.function_name() by package_name_function_name().
    Default is to use a schema to emulate package.

    The replacement will be done in all kind of DDL or code that is
    parsed by the PLSQL to PLPGSQL converter. PLSQL_PGSQL must be
    enabled or -p used in command line.

REWRITE_OUTER_JOIN
    Enable this directive if the rewrite of Oracle native syntax (+) of
    OUTER JOIN is broken. This will force Ora2Pg to not rewrite such
    code, default is to try to rewrite simple form of right outer join
    for the moment.

UUID_FUNCTION
    By default Ora2Pg will convert call to SYS_GUID() Oracle function
    with a call to uuid_generate_v4 from uuid-ossp extension. You can
    redefined it to use the gen_random_uuid function from pgcrypto
    extension by changing the function name. Default to
    uuid_generate_v4.

    Note that when a RAW(16) and RAW(32) columns is found or that the
    RAW column has &quot;SYS_GUID()&quot; as default value Ora2Pg will
    automatically translate the type of the column into uuid which might
    be the right translation in most of the case. In this case data will
    be automatically migrated as PostgreSQL uuid data type provided by
    the &quot;uuid-ossp&quot; extension.

FUNCTION_STABLE
    By default Oracle functions are marked as STABLE as they can not
    modify data unless when used in PL/SQL with variable assignment or
    as conditional expression. You can force Ora2Pg to create these
    function as VOLATILE by disabling this configuration directive.

COMMENT_COMMIT_ROLLBACK
    By default call to COMMIT/ROLLBACK are kept untouched by Ora2Pg to
    force the user to review the logic of the function. Once it is fixed
    in Oracle source code or you want to comment this calls enable the
    following directive.

COMMENT_SAVEPOINT
    It is common to see SAVEPOINT call inside PL/SQL procedure together
    with a ROLLBACK TO savepoint_name. When COMMENT_COMMIT_ROLLBACK is
    enabled you may want to also comment SAVEPOINT calls, in this case
    enable it.

STRING_CONSTANT_REGEXP
    Ora2Pg replace all string constant during the pl/sql to plpgsql
    translation, string constant are all text include between single
    quote. If you have some string placeholder used in dynamic call to
    queries you can set a list of regexp to be temporary replaced to not
    break the parser. For example:

            STRING_CONSTANT_REGEXP         &amp;lt;placeholder value=&quot;.*&quot;&amp;gt;

    The list of regexp must use the semi colon as separator.

ALTERNATIVE_QUOTING_REGEXP
    To support the Alternative Quoting Mechanism (&#39;Q&#39; or &#39;q&#39;) for String
    Literals set the regexp with the text capture to use to extract the
    text part. For example with a variable declared as

            c_sample VARCHAR2(100 CHAR) := q&#39;{This doesn&#39;t work.}&#39;;

    the regexp to use must be:

            ALTERNATIVE_QUOTING_REGEXP     q&#39;{(.*)}&#39;

    ora2pg will use the $$ delimiter, with the example the result will
    be:

            c_sample varchar(100) := $$This doesn&#39;t work.$$;

    The value of this configuration directive can be a list of regexp
    separated by a semi colon. The capture part (between parenthesis) is
    mandatory in each regexp if you want to restore the string constant.

USE_ORAFCE
    If you want to use functions defined in the Orafce library and
    prevent Ora2Pg to translate call to these functions, enable this
    directive. The Orafce library can be found here:
    https://github.com/orafce/orafce

    By default Ora2pg rewrite add_month(), add_year(), date_trunc() and
    to_char() functions, but you may prefer to use the orafce version of
    these function that do not need any code transformation.

AUTONOMOUS_TRANSACTION
    Enable translation of autonomous transactions into a wrapper
    function using dblink or pg_background extension. If you don&#39;t want
    to use this translation and just want the function to be exported as
    a normal one without the pragma call, disable this directive.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Materialized view Materialized views are exported as snapshot &quot;Snapshot Materialized Views&quot; as PostgreSQL only supports full refresh.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;If you want to import the materialized views in PostgreSQL prior to 9.3
you have to set configuration directive PG_SUPPORTS_MVIEW to 0. In this
case Ora2Pg will export all materialized views as explain in this
document:

        http://tech.jonathangardner.net/wiki/PostgreSQL/Materialized_Views.

When exporting materialized view Ora2Pg will first add the SQL code to
create the &quot;materialized_views&quot; table:

        CREATE TABLE materialized_views (
                mview_name text NOT NULL PRIMARY KEY,
                view_name text NOT NULL,
                iname text,
                last_refresh TIMESTAMP WITH TIME ZONE
        );

all materialized views will have an entry in this table. It then adds
the plpgsql code to create tree functions:

        create_materialized_view(text, text, text) used to create a materialized view
        drop_materialized_view(text) used to delete a materialized view
        refresh_full_materialized_view(text) used to refresh a view

then it adds the SQL code to create the view and the materialized view:

        CREATE VIEW mviewname_mview AS
        SELECT ... FROM ...;

        SELECT create_materialized_view(&#39;mviewname&#39;,&#39;mviewname_mview&#39;, change with the name of the column to used for the index);

The first argument is the name of the materialized view, the second the
name of the view on which the materialized view is based and the third
is the column name on which the index should be build (aka most of the
time the primary key). This column is not automatically deduced so you
need to replace its name.

As said above Ora2Pg only supports snapshot materialized views so the
table will be entirely refreshed by issuing first a truncate of the
table and then by load again all data from the view:

         refresh_full_materialized_view(&#39;mviewname&#39;);

To drop the materialized view you just have to call the
drop_materialized_view() function with the name of the materialized view
as parameter.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Other configuration directives DEBUG Set it to 1 will enable verbose output.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;IMPORT
    You can define common Ora2Pg configuration directives into a single
    file that can be imported into other configuration files with the
    IMPORT configuration directive as follow:

            IMPORT  commonfile.conf

    will import all configuration directives defined into
    commonfile.conf into the current configuration file.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Exporting views as PostgreSQL tables You can export any Oracle view as a PostgreSQL table simply by setting TYPE configuration option to TABLE to have the corresponding create table statement. Or use type COPY or INSERT to export the corresponding data. To allow that you have to specify your views in the VIEW_AS_TABLE configuration option.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Then if Ora2Pg finds the view it will extract its schema (if TYPE=TABLE)
into a PG create table form, then it will extract the data (if TYPE=COPY
or INSERT) following the view schema.

For example, with the following view:

        CREATE OR REPLACE VIEW product_prices (category_id, product_count, low_price, high_price) AS
        SELECT  category_id, COUNT(*) as product_count,
            MIN(list_price) as low_price,
            MAX(list_price) as high_price
         FROM   product_information
        GROUP BY category_id;

Setting VIEW_AS_TABLE to product_prices and using export type TABLE,
will force Ora2Pg to detect columns returned types and to generate a
create table statement:

        CREATE TABLE product_prices (
                category_id bigint,
                product_count integer,
                low_price numeric,
                high_price numeric
        );

Data will be loaded following the COPY or INSERT export type and the
view declaration.

You can use the ALLOW and EXCLUDE directive in addition to filter other
objects to export.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Export as Kettle transformation XML files The KETTLE export type is useful if you want to use Penthalo Data Integrator (Kettle) to import data to PostgreSQL. With this type of export Ora2Pg will generate one XML Kettle transformation files (.ktr) per table and add a line to manually execute the transformation in the output.sql file. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;        ora2pg -c ora2pg.conf -t KETTLE -j 12 -a MYTABLE -o load_mydata.sh

will generate one file called &#39;HR.MYTABLE.ktr&#39; and add a line to the
output file (load_mydata.sh):

        #!/bin/sh

        KETTLE_TEMPLATE_PATH=&#39;.&#39;

        JAVAMAXMEM=4096 ./pan.sh -file $KETTLE_TEMPLATE_PATH/HR.MYTABLE.ktr -level Detailed

The -j 12 option will create a template with 12 processes to insert data
into PostgreSQL. It is also possible to specify the number of parallel
queries used to extract data from the Oracle with the -J command line
option as follow:

        ora2pg -c ora2pg.conf -t KETTLE -J 4 -j 12 -a EMPLOYEES -o load_mydata.sh

This is only possible if there is a unique key defined on a numeric
column or that you have defined the technical key to used to split the
query between cores in the DEFINED_PKEY configuration directive. For
example:

        DEFINED_PK      EMPLOYEES:employee_id

will force the number of Oracle connection copies to 4 and defined the
SQL query as follow in the Kettle XML transformation file:

        &amp;lt;sql&amp;gt;SELECT * FROM HR.EMPLOYEES WHERE ABS(MOD(employee_id,${Internal.Step.Unique.Count}))=${Internal.Step.Unique.Number}&amp;lt;/sql&amp;gt;

The KETTLE export type requires that the Oracle and PostgreSQL DSN are
defined. You can also activate the TRUNCATE_TABLE directive to force a
truncation of the table before data import.

The KETTLE export type is an original work of Marc Cousin.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Migration cost assessment Estimating the cost of a migration process from Oracle to PostgreSQL is not easy. To obtain a good assessment of this migration cost, Ora2Pg will inspect all database objects, all functions and stored procedures to detect if there&#39;s still some objects and PL/SQL code that can not be automatically converted by Ora2Pg.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Ora2Pg has a content analysis mode that inspect the Oracle database to
generate a text report on what the Oracle database contains and what can
not be exported.

To activate the &quot;analysis and report&quot; mode, you have to use the export
de type SHOW_REPORT like in the following command:

        ora2pg -t SHOW_REPORT

Here is a sample report obtained with this command:

        --------------------------------------
        Ora2Pg: Oracle Database Content Report
        --------------------------------------
        Version Oracle Database 10g Enterprise Edition Release 10.2.0.1.0
        Schema  HR
        Size  880.00 MB
     
        --------------------------------------
        Object  Number  Invalid Comments
        --------------------------------------
        CLUSTER   2 0 Clusters are not supported and will not be exported.
        FUNCTION  40  0 Total size of function code: 81992.
        INDEX     435 0 232 index(es) are concerned by the export, others are automatically generated and will
                                        do so on PostgreSQL. 1 bitmap index(es). 230 b-tree index(es). 1 reversed b-tree index(es)
                                        Note that bitmap index(es) will be exported as b-tree index(es) if any. Cluster, domain,
                                        bitmap join and IOT indexes will not be exported at all. Reverse indexes are not exported
                                        too, you may use a trigram-based index (see pg_trgm) or a reverse() function based index
                                        and search. You may also use &#39;varchar_pattern_ops&#39;, &#39;text_pattern_ops&#39; or &#39;bpchar_pattern_ops&#39;
                                        operators in your indexes to improve search with the LIKE operator respectively into
                                        varchar, text or char columns.
        MATERIALIZED VIEW 1 0 All materialized view will be exported as snapshot materialized views, they
                                        are only updated when fully refreshed.
        PACKAGE BODY  2 1 Total size of package code: 20700.
        PROCEDURE 7 0 Total size of procedure code: 19198.
        SEQUENCE  160 0 Sequences are fully supported, but all call to sequence_name.NEXTVAL or sequence_name.CURRVAL
                                        will be transformed into NEXTVAL(&#39;sequence_name&#39;) or CURRVAL(&#39;sequence_name&#39;).
        TABLE     265 0 1 external table(s) will be exported as standard table. See EXTERNAL_TO_FDW configuration
                                        directive to export as file_fdw foreign tables or use COPY in your code if you just
                                        want to load data from external files. 2 binary columns. 4 unknown types.
        TABLE PARTITION 8 0 Partitions are exported using table inheritance and check constraint. 1 HASH partitions.
                                        2 LIST partitions. 6 RANGE partitions. Note that Hash partitions are not supported.
        TRIGGER   30  0 Total size of trigger code: 21677.
        TYPE      7 1 5 type(s) are concerned by the export, others are not supported. 2 Nested Tables.
                                        2 Object type. 1 Subtype. 1 Type Boby. 1 Type inherited. 1 Varrays. Note that Type
                                        inherited and Subtype are converted as table, type inheritance is not supported.
        TYPE BODY 0 3 Export of type with member method are not supported, they will not be exported.
        VIEW      7 0 Views are fully supported, but if you have updatable views you will need to use
                                        INSTEAD OF triggers.
        DATABASE LINK 1 0 Database links will not be exported. You may try the dblink perl contrib module or use
                                        the SQL/MED PostgreSQL features with the different Foreign Data Wrapper (FDW) extensions.
                                    
        Note: Invalid code will not be exported unless the EXPORT_INVALID configuration directive is activated.

Once the database can be analysed, Ora2Pg, by his ability to convert SQL
and PL/SQL code from Oracle syntax to PostgreSQL, can go further by
estimating the code difficulties and estimate the time necessary to
operate a full database migration.

To estimate the migration cost in person-days, Ora2Pg allow you to use a
configuration directive called ESTIMATE_COST that you can also enabled
at command line:

        --estimate_cost

This feature can only be used with the SHOW_REPORT, FUNCTION, PROCEDURE,
PACKAGE and QUERY export type.

        ora2pg -t SHOW_REPORT  --estimate_cost

The generated report is same as above but with a new &#39;Estimated cost&#39;
column as follow:

        --------------------------------------
        Ora2Pg: Oracle Database Content Report
        --------------------------------------
        Version Oracle Database 10g Express Edition Release 10.2.0.1.0
        Schema  HR
        Size  890.00 MB
     
        --------------------------------------
        Object  Number  Invalid Estimated cost  Comments
        --------------------------------------
        DATABASE LINK  3 0 9 Database links will be exported as SQL/MED PostgreSQL&#39;s Foreign Data Wrapper (FDW) extensions
                                        using oracle_fdw.
        FUNCTION  2 0 7 Total size of function code: 369 bytes. HIGH_SALARY: 2, VALIDATE_SSN: 3.
        INDEX 21  0 11  11 index(es) are concerned by the export, others are automatically generated and will do so
                                        on PostgreSQL. 11 b-tree index(es). Note that bitmap index(es) will be exported as b-tree
                                        index(es) if any. Cluster, domain, bitmap join and IOT indexes will not be exported at all.
                                        Reverse indexes are not exported too, you may use a trigram-based index (see pg_trgm) or a
                                        reverse() function based index and search. You may also use &#39;varchar_pattern_ops&#39;, &#39;text_pattern_ops&#39;
                                        or &#39;bpchar_pattern_ops&#39; operators in your indexes to improve search with the LIKE operator
                                        respectively into varchar, text or char columns.
        JOB 0 0 0 Job are not exported. You may set external cron job with them.
        MATERIALIZED VIEW 1 0 3 All materialized view will be exported as snapshot materialized views, they
                                                are only updated when fully refreshed.
        PACKAGE BODY  0 2 54  Total size of package code: 2487 bytes. Number of procedures and functions found
                                                inside those packages: 7. two_proc.get_table: 10, emp_mgmt.create_dept: 4,
                                                emp_mgmt.hire: 13, emp_mgmt.increase_comm: 4, emp_mgmt.increase_sal: 4,
                                                emp_mgmt.remove_dept: 3, emp_mgmt.remove_emp: 2.
        PROCEDURE 4 0 39  Total size of procedure code: 2436 bytes. TEST_COMMENTAIRE: 2, SECURE_DML: 3,
                                                PHD_GET_TABLE: 24, ADD_JOB_HISTORY: 6.
        SEQUENCE  3 0 0 Sequences are fully supported, but all call to sequence_name.NEXTVAL or sequence_name.CURRVAL
                                                will be transformed into NEXTVAL(&#39;sequence_name&#39;) or CURRVAL(&#39;sequence_name&#39;).
        SYNONYM   3 0 4 SYNONYMs will be exported as views. SYNONYMs do not exists with PostgreSQL but a common workaround
                                                is to use views or set the PostgreSQL search_path in your session to access
                                                object outside the current schema.
                                                user1.emp_details_view_v is an alias to hr.emp_details_view.
                                                user1.emp_table is an alias to hr.employees@other_server.
                                                user1.offices is an alias to hr.locations.
        TABLE 17  0 8.5 1 external table(s) will be exported as standard table. See EXTERNAL_TO_FDW configuration
                                        directive to export as file_fdw foreign tables or use COPY in your code if you just want to
                                        load data from external files. 2 binary columns. 4 unknown types.
        TRIGGER 1 1 4 Total size of trigger code: 123 bytes. UPDATE_JOB_HISTORY: 2.
        TYPE  7 1 5 5 type(s) are concerned by the export, others are not supported. 2 Nested Tables. 2 Object type.
                                        1 Subtype. 1 Type Boby. 1 Type inherited. 1 Varrays. Note that Type inherited and Subtype are
                                        converted as table, type inheritance is not supported.
        TYPE BODY 0 3 30  Export of type with member method are not supported, they will not be exported.
        VIEW  1 1 1 Views are fully supported, but if you have updatable views you will need to use INSTEAD OF triggers.
        --------------------------------------
        Total 65  8 162.5 162.5 cost migration units means approximatively 2 man day(s).

The last line shows the total estimated migration cost in person-days
following the number of migration units estimated for each object. This
migration unit represent around five minutes for a PostgreSQL expert. If
this is your first migration you can get it higher with the
configuration directive COST_UNIT_VALUE or the --cost_unit_value command
line option:

        ora2pg -t SHOW_REPORT  --estimate_cost --cost_unit_value 10

Ora2Pg is also able to give you a migration difficulty level assessment,
here a sample:

Migration level: B-5

    Migration levels:
        A - Migration that might be run automatically
        B - Migration with code rewrite and a person-days cost up to 5 days
        C - Migration with code rewrite and a person-days cost above 5 days
    Technical levels:
        1 = trivial: no stored functions and no triggers
        2 = easy: no stored functions but with triggers, no manual rewriting
        3 = simple: stored functions and/or triggers, no manual rewriting
        4 = manual: no stored functions but with triggers or views with code rewriting
        5 = difficult: stored functions and/or triggers with code rewriting

This assessment consist in a letter A or B to specify if the migration
needs manual rewriting or not. And a number from 1 up to 5 to give you a
technical difficulty level. You have an additional option
--human_days_limit to specify the number of person-days limit where the
migration level should be set to C to indicate that it need a huge
amount of work and a full project management with migration support.
Default is 10 person-days. You can use the configuration directive
HUMAN_DAYS_LIMIT to change this default value permanently.

This feature has been developed to help you or your boss to decide which
database to migrate first and the team that must be mobilized to operate
the migration.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Global Oracle and MySQL migration assessment Ora2Pg come with a script ora2pg_scanner that can be used when you have a huge number of instances and schema to scan for migration assessment.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Usage: ora2pg_scanner -l CSVFILE [-o OUTDIR]

   -b | --binpath DIR: full path to directory where the ora2pg binary stays.
                Might be useful only on Windows OS.
   -c | --config FILE: set custom configuration file to use otherwise ora2pg
                will use the default: /etc/ora2pg/ora2pg.conf.
   -l | --list FILE : CSV file containing a list of databases to scan with
                all required information. The first line of the file
                can contain the following header that describes the
                format that must be used:

                &quot;type&quot;,&quot;schema/database&quot;,&quot;dsn&quot;,&quot;user&quot;,&quot;password&quot;

   -o | --outdir DIR : (optional) by default all reports will be dumped to a
                directory named &#39;output&#39;, it will be created automatically.
                If you want to change the name of this directory, set the name
                at second argument.

   -t | --test : just try all connections by retrieving the required schema
                 or database name. Useful to validate your CSV list file.
   -u | --unit MIN : redefine globally the migration cost unit value in minutes.
                 Default is taken from the ora2pg.conf (default 5 minutes).

   Here is a full example of a CSV databases list file:

        &quot;type&quot;,&quot;schema/database&quot;,&quot;dsn&quot;,&quot;user&quot;,&quot;password&quot;
        &quot;MYSQL&quot;,&quot;sakila&quot;,&quot;dbi:mysql:host=192.168.1.10;database=sakila;port=3306&quot;,&quot;root&quot;,&quot;secret&quot;
        &quot;ORACLE&quot;,&quot;HR&quot;,&quot;dbi:Oracle:host=192.168.1.10;sid=XE;port=1521&quot;,&quot;system&quot;,&quot;manager&quot;
        &quot;MSSQL&quot;,&quot;HR&quot;,&quot;dbi:ODBC:driver=msodbcsql18;server=srv.database.windows.net;database=testdb&quot;,&quot;system&quot;,&quot;manager&quot;

   The CSV field separator must be a comma.

   Note that if you want to scan all schemas from an Oracle instance you just
   have to leave the schema field empty, Ora2Pg will automatically detect all
   available schemas and generate a report for each one. Of course you need to
   use a connection user with enough privileges to be able to scan all schemas.
   For example:

        &quot;ORACLE&quot;,&quot;&quot;,&quot;dbi:Oracle:host=192.168.1.10;sid=XE;port=1521&quot;,&quot;system&quot;,&quot;manager&quot;
        &quot;MSSQL&quot;,&quot;&quot;,&quot;dbi:ODBC:driver=msodbcsql18;server=srv.database.windows.net;database=testdb&quot;,&quot;usrname&quot;,&quot;passwd&quot;

   will generate a report for all schema in the XE instance. Note that in this
   case the SCHEMA directive in ora2pg.conf must not be set.

It will generate a CSV file with the assessment result, one line per
schema or database and a detailed HTML report for each database scanned.

Hint: Use the -t | --test option before to test all your connections in
your CSV file.

For Windows users you must use the -b command line option to set the
directory where ora2pg_scanner stays otherwise the ora2pg command calls
will fail.

In the migration assessment details about functions Ora2Pg always
include per default 2 migration units for TEST and 1 unit for SIZE per
1000 characters in the code. This mean that by default it will add 15
minutes in the migration assessment per function. Obviously if you have
unitary tests or very simple functions this will not represent the real
migration time.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Migration assessment method Migration unit scores given to each type of Oracle database object are defined in the Perl library lib/Ora2Pg/PLSQL.pm in the %OBJECT_SCORE variable definition.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;The number of PL/SQL lines associated to a migration unit is also
defined in this file in the $SIZE_SCORE variable value.

The number of migration units associated to each PL/SQL code
difficulties can be found in the same Perl library lib/Ora2Pg/PLSQL.pm
in the hash %UNCOVERED_SCORE initialization.

This assessment method is a work in progress so I&#39;m expecting feedbacks
on migration experiences to polish the scores/units attributed in those
variables.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Improving indexes and constraints creation speed Using the LOAD export type and a file containing SQL orders to perform, it is possible to dispatch those orders over multiple PostgreSQL connections. To be able to use this feature, the PG_DSN, PG_USER and PG_PWD must be set. Then:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;        ora2pg -t LOAD -c config/ora2pg.conf -i schema/tables/INDEXES_table.sql -j 4

will dispatch indexes creation over 4 simultaneous PostgreSQL
connections.

This will considerably accelerate this part of the migration process
with huge data size.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Exporting LONG RAW If you still have columns defined as LONG RAW, Ora2Pg will not be able to export these kind of data. The OCI library fail to export them and always return the same first record. To be able to export the data you need to transform the field as BLOB by creating a temporary table before migrating data. For example, the Oracle table:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;        SQL&amp;gt; DESC TEST_LONGRAW
         Name                 NULL ?   Type
         -------------------- -------- ----------------------------
         ID                            NUMBER
         C1                            LONG RAW

need to be &quot;translated&quot; into a table using BLOB as follow:

        CREATE TABLE test_blob (id NUMBER, c1 BLOB);

And then copy the data with the following INSERT query:

        INSERT INTO test_blob SELECT id, to_lob(c1) FROM test_longraw;

Then you just have to exclude the original table from the export (see
EXCLUDE directive) and to renamed the new temporary table on the fly
using the REPLACE_TABLES configuration directive.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Global variables Oracle allow the use of global variables defined in packages. Ora2Pg will export these variables for PostgreSQL as user defined custom variables available in a session. Oracle variables assignment are exported as call to:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    PERFORM set_config(&#39;pkgname.varname&#39;, value, false);

Use of these variables in the code is replaced by:

    current_setting(&#39;pkgname.varname&#39;)::global_variables_type;

where global_variables_type is the type of the variable extracted from
the package definition.

If the variable is a constant or have a default value assigned at
declaration, Ora2Pg will create a file global_variables.conf with the
definition to include in the postgresql.conf file so that their values
will already be set at database connection. Note that the value can
always modified by the user so you can not have exactly a constant.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Hints Converting your queries with Oracle style outer join (+) syntax to ANSI standard SQL at the Oracle side can save you lot of time for the migration. You can use TOAD Query Builder can re-write these using the proper ANSI syntax, see: &lt;a href=&quot;http://www.toadworld.com/products/toad-for-oracle/f/10/t/9518.aspx&quot;&gt;http://www.toadworld.com/products/toad-for-oracle/f/10/t/9518.aspx&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;There&#39;s also an alternative with SQL Developer Data Modeler, see
http://www.thatjeffsmith.com/archive/2012/01/sql-developer-data-modeler-
quick-tip-use-oracle-join-syntax-or-ansi/

Toad is also able to rewrite the native Oracle DECODE() syntax into ANSI
standard SQL CASE statement. You can find some slide about this in a
presentation given at PgConf.RU:
http://ora2pg.darold.net/slides/ora2pg_the_hard_way.pdf
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Test the migration The type of action called TEST allow you to check that all objects from Oracle database have been created under PostgreSQL. Of course PG_DSN must be set to be able to check PostgreSQL side.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Note that this feature respect the schema name limitation if
EXPORT_SCHEMA and SCHEMA or PG_SCHEMA are defined. If only EXPORT_SCHEMA
is set all schemes from Oracle and PostgreSQL are scanned. You can
filter to a single schema using SCHEMA and/or PG_SCHEMA but you can not
filter on a list of schema. To test a list of schema you will have to
repeat the calls to Ora2Pg by specifying a single schema each time.

For example command:

        ora2pg -t TEST -c config/ora2pg.conf &amp;gt; migration_diff.txt

Will create a file containing the report of all object and row count on
both side, Oracle and PostgreSQL, with an error section giving you the
detail of the differences for each kind of object. Here is a sample
result:

        [TEST INDEXES COUNT]
        ORACLEDB:DEPARTMENTS:2
        POSTGRES:departments:1
        ORACLEDB:EMPLOYEES:6
        POSTGRES:employees:6
        [ERRORS INDEXES COUNT]
        Table departments doesn&#39;t have the same number of indexes in Oracle (2) and in PostgreSQL (1).

        [TEST UNIQUE CONSTRAINTS COUNT]
        ORACLEDB:DEPARTMENTS:1
        POSTGRES:departments:1
        ORACLEDB:EMPLOYEES:1
        POSTGRES:employees:1
        [ERRORS UNIQUE CONSTRAINTS COUNT]
        OK, Oracle and PostgreSQL have the same number of unique constraints.

        [TEST PRIMARY KEYS COUNT]
        ORACLEDB:DEPARTMENTS:1
        POSTGRES:departments:1
        ORACLEDB:EMPLOYEES:1
        POSTGRES:employees:1
        [ERRORS PRIMARY KEYS COUNT]
        OK, Oracle and PostgreSQL have the same number of primary keys.

        [TEST CHECK CONSTRAINTS COUNT]
        ORACLEDB:DEPARTMENTS:1
        POSTGRES:departments:1
        ORACLEDB:EMPLOYEES:1
        POSTGRES:employees:1
        [ERRORS CHECK CONSTRAINTS COUNT]
        OK, Oracle and PostgreSQL have the same number of check constraints.

        [TEST NOT NULL CONSTRAINTS COUNT]
        ORACLEDB:DEPARTMENTS:1
        POSTGRES:departments:1
        ORACLEDB:EMPLOYEES:1
        POSTGRES:employees:1
        [ERRORS NOT NULL CONSTRAINTS COUNT]
        OK, Oracle and PostgreSQL have the same number of not null constraints.

        [TEST COLUMN DEFAULT VALUE COUNT]
        ORACLEDB:DEPARTMENTS:1
        POSTGRES:departments:1
        ORACLEDB:EMPLOYEES:1
        POSTGRES:employees:1
        [ERRORS COLUMN DEFAULT VALUE COUNT]
        OK, Oracle and PostgreSQL have the same number of column default value.

        [TEST IDENTITY COLUMN COUNT]
        ORACLEDB:DEPARTMENTS:1
        POSTGRES:departments:1
        ORACLEDB:EMPLOYEES:0
        POSTGRES:employees:0
        [ERRORS IDENTITY COLUMN COUNT]
        OK, Oracle and PostgreSQL have the same number of identity column.

        [TEST FOREIGN KEYS COUNT]
        ORACLEDB:DEPARTMENTS:0
        POSTGRES:departments:0
        ORACLEDB:EMPLOYEES:1
        POSTGRES:employees:1
        [ERRORS FOREIGN KEYS COUNT]
        OK, Oracle and PostgreSQL have the same number of foreign keys.

        [TEST TABLE COUNT]
        ORACLEDB:TABLE:2
        POSTGRES:TABLE:2
        [ERRORS TABLE COUNT]
        OK, Oracle and PostgreSQL have the same number of TABLE.

        [TEST TABLE TRIGGERS COUNT]
        ORACLEDB:DEPARTMENTS:0
        POSTGRES:departments:0
        ORACLEDB:EMPLOYEES:1
        POSTGRES:employees:1
        [ERRORS TABLE TRIGGERS COUNT]
        OK, Oracle and PostgreSQL have the same number of table triggers.

        [TEST TRIGGER COUNT]
        ORACLEDB:TRIGGER:2
        POSTGRES:TRIGGER:2
        [ERRORS TRIGGER COUNT]
        OK, Oracle and PostgreSQL have the same number of TRIGGER.

        [TEST VIEW COUNT]
        ORACLEDB:VIEW:1
        POSTGRES:VIEW:1
        [ERRORS VIEW COUNT]
        OK, Oracle and PostgreSQL have the same number of VIEW.

        [TEST MVIEW COUNT]
        ORACLEDB:MVIEW:0
        POSTGRES:MVIEW:0
        [ERRORS MVIEW COUNT]
        OK, Oracle and PostgreSQL have the same number of MVIEW.

        [TEST SEQUENCE COUNT]
        ORACLEDB:SEQUENCE:1
        POSTGRES:SEQUENCE:0
        [ERRORS SEQUENCE COUNT]
        SEQUENCE does not have the same count in Oracle (1) and in PostgreSQL (0).

        [TEST TYPE COUNT]
        ORACLEDB:TYPE:1
        POSTGRES:TYPE:0
        [ERRORS TYPE COUNT]
        TYPE does not have the same count in Oracle (1) and in PostgreSQL (0).

        [TEST FDW COUNT]
        ORACLEDB:FDW:0
        POSTGRES:FDW:0
        [ERRORS FDW COUNT]
        OK, Oracle and PostgreSQL have the same number of FDW.

        [TEST FUNCTION COUNT]
        ORACLEDB:FUNCTION:3
        POSTGRES:FUNCTION:3
        [ERRORS FUNCTION COUNT]
        OK, Oracle and PostgreSQL have the same number of functions.

        [TEST SEQUENCE VALUES]
        ORACLEDB:EMPLOYEES_NUM_SEQ:1285
        POSTGRES:employees_num_seq:1285
        [ERRORS SEQUENCE VALUES COUNT]
        OK, Oracle and PostgreSQL have the same values for sequences

        [TEST ROWS COUNT]
        ORACLEDB:DEPARTMENTS:27
        POSTGRES:departments:27
        ORACLEDB:EMPLOYEES:854
        POSTGRES:employees:854
        [ERRORS ROWS COUNT]
        OK, Oracle and PostgreSQL have the same number of rows.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Data validation Data validation consists in comparing data retrieved from a foreign table pointing to the source Oracle table and a local PostgreSQL table resulting from the data export.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;To run data validation you can use a direct connection like any other
Ora2Pg action but you can also use the oracle_fdw, mysql_fdw ior tds_fdw
extension provided that FDW_SERVER and PG_DSN configuration directives
are set.

By default Ora2Pg will extract the 10000 first rows from both side, you
can change this value using directive DATA_VALIDATION_ROWS. When it is
set to zero all rows of the tables will be compared.

Data validation requires that the table has a primary key or unique
index and that the key columns is not a LOB. Rows will be sorted using
this unique key. Due to differences in sort behavior between Oracle and
PostgreSQL, if the collation of unique key columns in PostgreSQL is not
&#39;C&#39;, the sort order can be different compared to Oracle. In this case
the data validation will fail.

Data validation must be done before any data is modified.

Ora2Pg will stop comparing two tables after DATA_VALIDATION_ROWS is
reached or that 10 errors has been encountered, result is dumped in a
file named &quot;data_validation.log&quot; written in the current directory by
default. The number of error before stopping the diff between rows can
be controlled using the configuration directive DATA_VALIDATION_ERROR.
All rows in errors are printed to the output file for your analyze.

It is possible to parallelize data validation by using -P option or the
corresponding configuration directive PARALLEL_TABLES in ora2pg.conf.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Use of System Change Number (SCN) Ora2Pg is able to export data as of a specific SCN. You can set it at command line using the -S or --scn option. You can give a specific SCN or if you want to use the current SCN at first connection time set the value to &#39;current&#39;. In this last case the connection user has the &quot;SELECT ANY DICTIONARY&quot; or the &quot;SELECT_CATALOG_ROLE&quot; role, the current SCN is looked at the v$database view.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Example of use:

    ora2pg -c ora2pg.conf -t COPY --scn 16605281

This adds the following clause to the query used to retrieve data for
example:

    AS OF SCN 16605281

You can also use th --scn option to use the Oracle flashback capabality
by specifying a timestamp expression instead of a SCN. For example:

    ora2pg -c ora2pg.conf -t COPY --scn &quot;TO_TIMESTAMP(&#39;2021-12-01 00:00:00&#39;, &#39;YYYY-MM-DD HH:MI:SS&#39;)&quot;

This will add the following clause to the query used to retrieve data:

    AS OF TIMESTAMP TO_TIMESTAMP(&#39;2021-12-01 00:00:00&#39;, &#39;YYYY-MM-DD HH:MI:SS&#39;)

or for example to only retrive yesterday&#39;s data:

    ora2pg -c ora2pg.conf -t COPY --scn &quot;SYSDATE - 1&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Change Data Capture (CDC) Ora2Pg do not have such feature which allow to import data and to only apply changes after the first import. But you can use the --cdc_ready option to export data with registration of the SCN at the time of the table export. All SCN per tables are written to a file named TABLES_SCN.log by default, it can be changed using -C | --cdc_file option.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;These SCN registered per table during COPY or INSERT export can be used
with a CDC tool. The format of the file is tablename:SCN per line.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Importing BLOB as large objects By default Ora2Pg imports Oracle BLOB as bytea, the destination column is created using the bytea data type. If you want to use large object instead of bytea, just add the --blob_to_lo option to the ora2pg command. It will create the destination column as data type Oid and will save the BLOB as a large object using the lo_from_bytea() function. The Oid returned by the call to lo_from_bytea() is inserted in the destination column instead of a bytea. Because of the use of the function this option can only be used with actions SHOW_COLUMN, TABLE and INSERT. Action COPY is not allowed.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;If you want to use COPY or have huge size BLOB ( &amp;gt; 1GB) than can not be
imported using lo_from_bytea() you can add option --lo_import to the
ora2pg command. This will allow to import data in two passes.

1) Export data using COPY or INSERT will set the Oid destination column
for BLOB to value 0 and save the BLOB value into a dedicated file. It
will also create a Shell script to import the BLOB files into the
database using psql command \lo_import and to update the table Oid
column to the returned large object Oid. The script is named
lo_import-TABLENAME.sh

2) Execute all scripts lo_import-TABLENAME.sh after setting the
environment variables PGDATABASE and optionally PGHOST, PGPORT, PGUSER,
etc. if they do not correspond to the default values for libpq.

You might also execute manually a VACUUM FULL on the table to remove the
bloat created by the table update.

Limitation: the table must have a primary key, it is used to set the
WHERE clause to update the Oid column after the large object import.
Importing BLOB using this second method (--lo_import) is very slow so it
should be reserved to rows where the BLOB &amp;gt; 1GB for all other rows use
the option --blob_to_lo. To filter the rows you can use the WHERE
configuration directive in ora2pg.conf.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;SUPPORT Author / Maintainer Gilles Darold 
 &lt;gilles at darold dot net&gt;&lt;/gilles&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Please report any bugs, patches, help, etc. to &amp;lt;gilles AT darold DOT
net&amp;gt;.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Feature request If you need new features let me know at 
 &lt;gilles at darold dot net&gt;
  . This help a lot to develop a better/useful tool.
 &lt;/gilles&gt;&lt;/p&gt; 
&lt;p&gt;How to contribute ? Any contribution to build a better tool is welcome, you just have to send me your ideas, features request or patches and there will be applied.&lt;/p&gt; 
&lt;p&gt;LICENSE Copyright (c) 2000-2024 Gilles Darold - All rights reserved.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;        This program is free software: you can redistribute it and/or modify
        it under the terms of the GNU General Public License as published by
        the Free Software Foundation, either version 3 of the License, or
        any later version.

        This program is distributed in the hope that it will be useful,
        but WITHOUT ANY WARRANTY; without even the implied warranty of
        MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
        GNU General Public License for more details.

        You should have received a copy of the GNU General Public License
        along with this program.  If not, see &amp;lt; http://www.gnu.org/licenses/ &amp;gt;.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;ACKNOWLEDGEMENT I must thanks a lot all the great contributors, see changelog for all acknowledgments.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ocpi/ocpi</title>
      <link>https://github.com/ocpi/ocpi</link>
      <description>&lt;p&gt;The Open Charge Point Interface (OCPI) allows for a scalable, automated roaming setup between Charge Point Operators and e-Mobility Service Providers. It supports authorisation, charge point information exchange (incl transaction events), charge detail record exchange and finally, the exchange of smart-charging commands between parties.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;This repository contains the OCPI specification, latest release: &lt;a href=&quot;https://evroaming.org/app/uploads/2021/11/OCPI-2.2.1.pdf&quot;&gt;&lt;code&gt;OCPI 2.2.1&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;The branch with the latest fixes to the 2.2.1 documentation is &lt;a href=&quot;https://github.com/ocpi/ocpi/tree/release-2.2.1-bugfixes&quot;&gt;&lt;code&gt;release-2.2.1-bugfixes&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;-The branch with the latest fixes to the 2.2 documentation is &lt;a href=&quot;https://github.com/ocpi/ocpi/tree/release-2.2-bugfixes&quot;&gt;&lt;code&gt;release-2.2-bugfixes&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;The branch with the latest fixes to the 2.1.1 documentation is &lt;a href=&quot;https://github.com/ocpi/ocpi/tree/release-2.1.1-bugfixes&quot;&gt;&lt;code&gt;release-2.1.1-bugfixes&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;The &lt;code&gt;master&lt;/code&gt; branch always contains the latest official release.&lt;/p&gt; 
&lt;p&gt;Development of the next version of OCPI, new functionality, is done in the &lt;a href=&quot;https://github.com/ocpi/ocpi-3/&quot;&gt;ocpi-3 repository&lt;/a&gt;, which is only accessible to Contributors of the &lt;a href=&quot;https://evroaming.org/how-to-join/&quot;&gt;EV Roaming Foundation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ocpi/ocpi/master/version_history.asciidoc&quot;&gt;&lt;strong&gt;Version History&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ocpi/ocpi/master/introduction.asciidoc&quot;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ocpi/ocpi/master/terminology.asciidoc&quot;&gt;Terminology and Definitions&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ocpi/ocpi/master/topology.asciidoc&quot;&gt;Supported Topologies&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Protocol Meta Information&lt;/strong&gt;, describes the connections between the parties&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ocpi/ocpi/master/transport_and_format.asciidoc&quot;&gt;Transport and Format&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ocpi/ocpi/master/status_codes.asciidoc&quot;&gt;Status codes&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ocpi/ocpi/master/version_information_endpoint.asciidoc&quot;&gt;Version information endpoint&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ocpi/ocpi/master/credentials.asciidoc&quot;&gt;Credentials &amp;amp; registration&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Overview of Modules&lt;/strong&gt;, each section describes one module.&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ocpi/ocpi/master/mod_locations.asciidoc&quot;&gt;Locations&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ocpi/ocpi/master/mod_sessions.asciidoc&quot;&gt;Sessions&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ocpi/ocpi/master/mod_cdrs.asciidoc&quot;&gt;CDRs&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ocpi/ocpi/master/mod_tariffs.asciidoc&quot;&gt;Tariffs&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ocpi/ocpi/master/mod_tokens.asciidoc&quot;&gt;Tokens&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ocpi/ocpi/master/mod_commands.asciidoc&quot;&gt;Commands&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ocpi/ocpi/master/mod_charging_profiles.asciidoc&quot;&gt;Charging Profiles&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ocpi/ocpi/master/mod_hub_client_info.asciidoc&quot;&gt;Hub Client Info&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Generic Types&lt;/strong&gt;, describing all data types that are used by multiple objects&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ocpi/ocpi/master/types.asciidoc&quot;&gt;Types&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ocpi/ocpi/master/changelog.asciidoc&quot;&gt;&lt;strong&gt;Changelog&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Current versions&lt;/h3&gt; 
&lt;h4&gt;Release 2.2.1&lt;/h4&gt; 
&lt;p&gt;Only minor changes, but breaking compatibility with 2.2 in order to support signed data exchange so that parties using OCPI can comply with consumer protection legislation. A more detailed overview is inside &lt;a href=&quot;https://evroaming.org/app/uploads/2021/11/OCPI-2.2.1.pdf&quot;&gt;the specification document itself&lt;/a&gt;.&lt;/p&gt; 
&lt;h4&gt;Release 2.2-d2&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Support for Hubs 
  &lt;ul&gt; 
   &lt;li&gt;Message routing headers&lt;/li&gt; 
   &lt;li&gt;Hub Client Info&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Support Platforms with multiple/different roles, additional roles&lt;/li&gt; 
 &lt;li&gt;Charging Profiles&lt;/li&gt; 
 &lt;li&gt;based Smart Charging&lt;/li&gt; 
 &lt;li&gt;Improvements: 
  &lt;ul&gt; 
   &lt;li&gt;CDRs: Credit CDRs, VAT, Calibration law/Eichrecht support, Session_id, AuthorizationReference, CdrLocation, CdrToken&lt;/li&gt; 
   &lt;li&gt;Sessions: VAT, CdrToken, How to add a Charging Period&lt;/li&gt; 
   &lt;li&gt;Tariffs: Tariff types, Min/Max price, reservation tariff, Much more examples&lt;/li&gt; 
   &lt;li&gt;Locations: Multiple Tariffs, Lost of small improvements&lt;/li&gt; 
   &lt;li&gt;Tokens: Group_id, energy contract&lt;/li&gt; 
   &lt;li&gt;Commands: Cancel Reservation added&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;fixes some bugs of 2.1.1&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Release 2.1.1-d2&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Improvements from rel. 2.0&lt;/li&gt; 
 &lt;li&gt;Chargepoint commands&lt;/li&gt; 
 &lt;li&gt;realtime authorization&lt;/li&gt; 
 &lt;li&gt;fixes some bugs of 2.1 (2.1 is now deprecated)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Release 2.0&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Charge Point Exchange Static &amp;amp; Dynamic (with tariffing covering only start/kWh/time)&lt;/li&gt; 
 &lt;li&gt;Authorization &amp;amp; token data exchange&lt;/li&gt; 
 &lt;li&gt;Tariffing&lt;/li&gt; 
 &lt;li&gt;Session Info exchange (cdr &amp;amp; ndr)&lt;/li&gt; 
 &lt;li&gt;Registration (How to connect) &amp;amp; Security&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Planned releases&lt;/h3&gt; 
&lt;h4&gt;Release 3.0&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;ISO 15118 Plug&amp;amp;Charge&lt;/li&gt; 
 &lt;li&gt;Eichrecht support&lt;/li&gt; 
 &lt;li&gt;Performance improvements&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Building Process&lt;/h2&gt; 
&lt;p&gt;The OCPI Build Process has been improved. OCPI 2.0/2.1.1 was in markdown format, and diagrams where Plantuml.&lt;/p&gt; 
&lt;p&gt;For OCPI 2.2, the text of OCPI has been converted to asciidoc. Asciidoc is easier to format the output, and chapter numbering and internal links are much easier to work with.&lt;/p&gt; 
&lt;p&gt;The Plantuml is no longer converted to PNG images, but the SVG, making them much better readable, and even searchable in the PDF.&lt;/p&gt; 
&lt;p&gt;In OCPI 2.0 and 2.1.1, the JSON examples contained a lot of mistakes, where outdated compared to the text, or not even valid JSON. To prevent issues with the examples in the specification, the examples are not placed in separate JSON files. At the moment, the JSON files are check if they are valid JSON.&lt;/p&gt; 
&lt;hr&gt; 
&lt;p&gt;1 Dec 2014 &lt;a href=&quot;https://raw.githubusercontent.com/ocpi/ocpi/master/releases/old/OCPI-Draftv4.pdf&quot;&gt;Draft v4&lt;/a&gt; is published 17 June 2015 [Draft v5] is moved to a new branch that will be used as a reference as the OCPI specifications are being redefined and the specifications are restructured in different files, a file per chapter&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>centreon/centreon-plugins</title>
      <link>https://github.com/centreon/centreon-plugins</link>
      <description>&lt;p&gt;Collection of standard plugins to discover and gather cloud-to-edge metrics and status across your whole IT infrastructure.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;centreon-plugins&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/centreon/centreon-plugins/raw/master/LICENSE.txt&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/License-APACHE2-brightgreen.svg?sanitize=true&quot; alt=&quot;License&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;!-- SHIELDS --&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/centreon/centreon-plugins/graphs/contributors&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/contributors/centreon/centreon-plugins?color=%2384BD00&amp;amp;label=CONTRIBUTORS&amp;amp;style=for-the-badge&quot; alt=&quot;Contributors&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/centreon/centreon-plugins/stargazers&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/centreon/centreon-plugins?color=%23433b02a&amp;amp;label=STARS&amp;amp;style=for-the-badge&quot; alt=&quot;Stars&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/centreon/centreon-plugins/network/members&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/forks/centreon/centreon-plugins?color=%23009fdf&amp;amp;label=FORKS&amp;amp;style=for-the-badge&quot; alt=&quot;Forks&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/centreon/centreon-plugins/issues&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/issues/centreon/centreon-plugins?color=%230072ce&amp;amp;label=ISSUES&amp;amp;style=for-the-badge&quot; alt=&quot;Issues&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;What are Centreon Plugins&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/centreon/centreon-plugins/&quot;&gt;Centreon plugins&lt;/a&gt; is a free and open source project to monitor systems. The project can be used with Centreon and all monitoring softwares compatible with Nagios plugins.&lt;/p&gt; 
&lt;h3&gt;Principles&lt;/h3&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/centreon/centreon-plugins/&quot;&gt;Centreon plugins&lt;/a&gt; should comply with &lt;a href=&quot;https://www.monitoring-plugins.org/doc/guidelines.html&quot;&gt;Monitoring Plugins Development Guidelines&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;In short, they return:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;An error code: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;0&lt;/code&gt; for &lt;code&gt;OK&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;1&lt;/code&gt; for &lt;code&gt;WARNING&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;2&lt;/code&gt; for &lt;code&gt;CRITICAL&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;3&lt;/code&gt; for &lt;code&gt;UNKNOWN&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;A human understandable output message (example: &lt;code&gt;OK: CPU(s) average usage is 2.66 % - CPU &#39;0&#39; usage : 2.66 %&lt;/code&gt;).&lt;/li&gt; 
 &lt;li&gt;A set of metrics provided as &lt;em&gt;perfdata&lt;/em&gt; after a &lt;code&gt;|&lt;/code&gt; character (example: &lt;code&gt;&#39;cpu.utilization.percentage&#39;=2.66%;;;0;100 &#39;0#core.cpu.utilization.percentage&#39;=2.66%;;;0;100&lt;/code&gt;).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;What can Centreon Plugins monitor?&lt;/h3&gt; 
&lt;p&gt;You can monitor many systems:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Application&lt;/strong&gt;: Apache, Asterisk, Elasticsearch, Github, Jenkins, Kafka, Nginx, Pfsense, Redis, Tomcat, Varnish, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Cloud&lt;/strong&gt;: AWS, Azure, Docker, Office365, Nutanix, Prometheus, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Databases&lt;/strong&gt;: Firebird, Informix, MS SQL, MySQL, Oracle, Postgres, Cassandra.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Hardware&lt;/strong&gt;: printers (RFC3805), UPS (Powerware, Mge, Standard), Sun Hardware, Cisco UCS, SensorIP, HP Proliant, HP Bladechassis, Dell Openmanage, Dell CMC, Raritan, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Network&lt;/strong&gt;: Aruba, Brocade, Bluecoat, Brocade, Checkpoint, Cisco AP/IronPort/ASA/Standard, Extreme, Fortigate, H3C, Hirschmann, HP Procurve, F5 BIG-IP, Juniper, PaloAlto, Redback, Riverbed, Ruggedcom, Stonesoft, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Operating systems&lt;/strong&gt;: Linux (SNMP, NRPE), Freebsd (SNMP), AIX (SNMP), Solaris (SNMP), etc.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Storage&lt;/strong&gt;: EMC Clariion, Netapp, Nimble, HP MSA p2000, Dell EqualLogic, Qnap, Panzura, Synology, etc.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;To get a complete list, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;perl src/centreon_plugins.pl --list-plugin
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Basic Usage&lt;/h3&gt; 
&lt;p&gt;We&#39;ll use a basic example to show you how to monitor a system. I have finished the install section and I want to monitor a Linux in SNMP. First, I need to find the plugin to use in the list:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;perl centreon_plugins.pl --list-plugin | grep -i linux | grep &#39;PLUGIN&#39;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;It will return:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;PLUGIN: os::linux::local::plugin
PLUGIN: os::linux::snmp::plugin
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;It seems that &#39;os::linux::snmp::plugin&#39; is the good one. So I verify with the option &lt;code&gt;--help&lt;/code&gt; to be sure:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ perl centreon_plugins.pl --plugin=os::linux::snmp::plugin --help
...
Plugin Description:
  Check Linux operating systems in SNMP.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;It&#39;s exactly what I need. Now I&#39;ll add the option &lt;code&gt;--list-mode&lt;/code&gt; to know what can I do with it:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ perl centreon_plugins.pl --plugin=os::linux::snmp::plugin --list-mode
...
Modes Available:
 processcount
 time
 list-storages
 disk-usage
 diskio
 uptime
 swap
 cpu-detailed
 load
 traffic
 cpu
 inodes
 list-diskspath
 list-interfaces
 packet-errors
 memory
 tcpcon
 storage
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;I would like to test the &#39;load&#39; mode:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ perl centreon_plugins.pl --plugin=os::linux::snmp::plugin --mode=load
UNKNOWN: Missing parameter --hostname.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;It&#39;s not working because some options are missing. I can have a description of the mode and options with the option &lt;code&gt;--help&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ perl centreon_plugins.pl --plugin=os::linux::snmp::plugin --mode=load --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Eventually, I have to configure some SNMP options:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ perl centreon_plugins.pl --plugin=os::linux::snmp::plugin --mode=load --hostname=127.0.0.1 --snmp-version=2c --snmp-community=public
OK: Load average: 0.00, 0.00, 0.00 | &#39;load1&#39;=0.00;;;0; &#39;load5&#39;=0.00;;;0; &#39;load15&#39;=0.00;;;0;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;I can set threshold with options &lt;code&gt;--warning&lt;/code&gt; and &lt;code&gt;--critical&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ perl centreon_plugins.pl --plugin=os::linux::snmp::plugin --mode=load --hostname=127.0.0.1 --snmp-version=2c --snmp-community=public --warning=1,2,3 --critical=2,3,4
OK: Load average: 0.00, 0.00, 0.00 | &#39;load1&#39;=0.00;0:1;0:2;0; &#39;load5&#39;=0.00;0:2;0:3;0; &#39;load15&#39;=0.00;0:3;0:4;0;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more information or help, please read &lt;a href=&quot;https://raw.githubusercontent.com/centreon/centreon-plugins/develop/doc/en/user/guide.rst&quot;&gt;&#39;doc/en/user/guide.rst&#39;&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributions&lt;/h2&gt; 
&lt;h3&gt;Code contributions/pull requests&lt;/h3&gt; 
&lt;p&gt;If you want to contribute by submitting new functionalities, enhancements or bug fixes, first thank you for participating :-) Then have a look, if not already done, to our &lt;strong&gt;&lt;a href=&quot;https://github.com/centreon/centreon-plugins/raw/develop/doc/en/developer/guide.md&quot;&gt;development guide&lt;/a&gt;&lt;/strong&gt;. Then create a &lt;a href=&quot;https://github.com/centreon/centreon-plugins/fork&quot;&gt;fork&lt;/a&gt; and a development branch, and once it&#39;s done, you may submit a &lt;a href=&quot;https://github.com/centreon/centreon-plugins/pulls&quot;&gt;pull request&lt;/a&gt; that the corporate development team will examine.&lt;/p&gt; 
&lt;h3&gt;Issues/bug reports&lt;/h3&gt; 
&lt;p&gt;If you encounter a behaviour that is clearly a bug or a regression, you are welcome to submit an &lt;a href=&quot;https://github.com/centreon/centreon-plugins/issues&quot;&gt;issue&lt;/a&gt;. Please be aware that this is an open source project and that there is no guaranteed response time.&lt;/p&gt; 
&lt;h3&gt;Questions/search for help&lt;/h3&gt; 
&lt;p&gt;If you have trouble using our plugins, but are not sure whether it&#39;s due to a bug or a misuse, please take the time to ask for help on &lt;a href=&quot;https://thewatch.centreon.com/data-collection-6&quot;&gt;The Watch, Data Collection section&lt;/a&gt; and become certain that it is a bug before submitting it here.&lt;/p&gt; 
&lt;h3&gt;Feature/enhancement request&lt;/h3&gt; 
&lt;p&gt;There is high demand for new plugins and new functionalities on existing plugins, so we have to rely on our community to help us prioritize them. How? Post your suggestion on &lt;a href=&quot;https://thewatch.centreon.com/ideas&quot;&gt;The Watch Ideas&lt;/a&gt; with as much detail as possible and we will pick the most voted topics to add them to our product roadmap.&lt;/p&gt; 
&lt;p&gt;To develop a plugin/mode, we need the following information, depending on the protocol:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;SNMP&lt;/strong&gt;: MIB files and full snmpwalk of enterprise branch (&lt;code&gt;snmpwalk -ObentU -v 2c -c public address .1.3.6.1.4.1 &amp;gt; equipment.snmpwalk&lt;/code&gt;) or &lt;a href=&quot;https://thewatch.centreon.com/product-how-to-21/snmp-collection-tutorial-132&quot;&gt;SNMP collections&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;HTTP API (SOAP, Rest/Json, XML-RPC)&lt;/strong&gt;: the documentation and some curl examples or HTTP &lt;a href=&quot;https://thewatch.centreon.com/data-collection-6/centreon-plugins-discover-collection-modes-131&quot;&gt;collections&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;CLI&lt;/strong&gt;: command line examples (command + result).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;SQL&lt;/strong&gt;: queries + results + column types or &lt;a href=&quot;https://thewatch.centreon.com/product-how-to-21/sql-collection-tutorial-134&quot;&gt;SQL collections&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;JMX&lt;/strong&gt;: mbean names and attributes.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If some information is confidential, such as logins or IP addresses, obfuscate them in what is sent publicly and we&#39;ll get in touch with you by private message if this information is needed.&lt;/p&gt; 
&lt;p&gt;Please note that all the developments are open source, we will not commit to a release date. If it is an emergency for you, please contact &lt;a href=&quot;https://www.centreon.com/contact/&quot;&gt;Centreon&#39;s sales team&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Continuous integration&lt;/h3&gt; 
&lt;p&gt;Please follow documentation &lt;a href=&quot;https://raw.githubusercontent.com/centreon/centreon-plugins/develop/doc/CI.md&quot;&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;!-- URL AND IMAGES FOR SHIELDS --&gt;</description>
    </item>
    
  </channel>
</rss>
