<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>GitHub Python Daily Trending</title>
    <description>Daily Trending of Python in GitHub</description>
    <pubDate>Sun, 16 Mar 2025 01:36:48 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>tensortrade-org/tensortrade</title>
      <link>https://github.com/tensortrade-org/tensortrade</link>
      <description>&lt;p&gt;An open source reinforcement learning framework for training, evaluating, and deploying robust trading agents.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;a href=&quot;https://towardsdatascience.com/trade-smarter-w-reinforcement-learning-a5e91163f315?source=friends_link&amp;amp;sk=ea3afd0a305141eb9147be4718826dfb&quot;&gt;TensorTrade: Trade Efficiently with Reinforcement Learning&lt;/a&gt;&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://travis-ci.org/tensortrade-org/tensortrade&quot;&gt;&lt;img src=&quot;https://travis-ci.com/tensortrade-org/tensortrade.svg?branch=master&quot; alt=&quot;Build Status&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://tensortrade.org&quot;&gt;&lt;img src=&quot;https://readthedocs.org/projects/tensortrade/badge/?version=latest&quot; alt=&quot;Documentation Status&quot;&gt;&lt;/a&gt; &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/license/tensortrade-org/tensortrade.svg?color=brightgreen&quot; alt=&quot;Apache License&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://discord.gg/ZZ7BGWh&quot;&gt;&lt;img src=&quot;https://img.shields.io/discord/592446624882491402.svg?color=brightgreen&quot; alt=&quot;Discord&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://www.python.org/downloads/release/python-3110/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/python-3.11-blue.svg?sanitize=true&quot; alt=&quot;Python 3.11&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;hr&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;img src=&quot;https://github.com/notadamking/tensortrade/raw/master/docs/source/_static/logo.jpg&quot;&gt; 
&lt;/div&gt; 
&lt;hr&gt; 
&lt;p&gt;&lt;strong&gt;TensorTrade is still in Beta, meaning it should be used very cautiously if used in production, as it may contain bugs.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;TensorTrade is an open source Python framework for building, training, evaluating, and deploying robust trading algorithms using reinforcement learning. The framework focuses on being highly composable and extensible, to allow the system to scale from simple trading strategies on a single CPU, to complex investment strategies run on a distribution of HPC machines.&lt;/p&gt; 
&lt;p&gt;Under the hood, the framework uses many of the APIs from existing machine learning libraries to maintain high quality data pipelines and learning models. One of the main goals of TensorTrade is to enable fast experimentation with algorithmic trading strategies, by leveraging the existing tools and pipelines provided by &lt;code&gt;numpy&lt;/code&gt;, &lt;code&gt;pandas&lt;/code&gt;, &lt;code&gt;gym&lt;/code&gt;, &lt;code&gt;keras&lt;/code&gt;, and &lt;code&gt;tensorflow&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Every piece of the framework is split up into re-usable components, allowing you to take advantage of the general use components built by the community, while keeping your proprietary features private. The aim is to simplify the process of testing and deploying robust trading agents using deep reinforcement learning, to allow you and I to focus on creating profitable strategies.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;The goal of this framework is to enable fast experimentation, while maintaining production-quality data pipelines.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;Read &lt;a href=&quot;https://www.tensortrade.org/en/latest/&quot;&gt;the documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Guiding principles&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;Inspired by &lt;a href=&quot;https://github.com/keras-team/keras&quot;&gt;Keras&#39; guiding principles&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;User friendliness.&lt;/strong&gt; TensorTrade is an API designed for human beings, not machines. It puts user experience front and center. TensorTrade follows best practices for reducing cognitive load: it offers consistent &amp;amp; simple APIs, it minimizes the number of user actions required for common use cases, and it provides clear and actionable feedback upon user error.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Modularity.&lt;/strong&gt; A trading environment is a conglomeration of fully configurable modules that can be plugged together with as few restrictions as possible. In particular, exchanges, feature pipelines, action schemes, reward schemes, trading agents, and performance reports are all standalone modules that you can combine to create new trading environments.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Easy extensibility.&lt;/strong&gt; New modules are simple to add (as new classes and functions), and existing modules provide ample examples. To be able to easily create new modules allows for total expressiveness, making TensorTrade suitable for advanced research and production use.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;You can get started testing on Google Colab or your local machine, by viewing our &lt;a href=&quot;https://github.com/tensortrade-org/tensortrade/tree/master/examples&quot;&gt;many examples&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;TensorTrade requires Python &amp;gt;= 3.11.9 for all functionality to work as expected. You can install TensorTrade both as a pre-packaged solution by running the default setup command.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install tensortrade
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can then alternatively install TensorTrade directly from the master code repository, pulling directly from the latest commits. This will give you the latest features\fixes, but it is highly untested code, so proceed at your own risk.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install git+https://github.com/tensortrade-org/tensortrade.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively you can clone\download the repository in your local environment an manually install the requirements, either the &quot;base&quot; ones, or the ones that also include requirements to run the examples in the documentation.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install -r requirements.txt
pip install -r examples/requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Docker&lt;/h2&gt; 
&lt;p&gt;To run the commands below, ensure Docker is installed. Visit &lt;a href=&quot;https://docs.docker.com/install/&quot;&gt;https://docs.docker.com/install/&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h3&gt;Run Jupyter Notebooks&lt;/h3&gt; 
&lt;p&gt;To run a jupyter notebook in your browser, execute the following command and visit the &lt;code&gt;http://127.0.0.1:8888/?token=...&lt;/code&gt; link printed to the command line.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;make run-notebook
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Build Documentation&lt;/h3&gt; 
&lt;p&gt;To build the HTML documentation, execute the following command.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;make run-docs
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Run Test Suite&lt;/h3&gt; 
&lt;p&gt;To run the test suite, execute the following command.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;make run-tests
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Support&lt;/h2&gt; 
&lt;p&gt;You can ask questions and join the development discussion:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;On the &lt;a href=&quot;https://discord.gg/ZZ7BGWh&quot;&gt;TensorTrade Discord server&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;On the &lt;a href=&quot;https://gitter.im/tensortrade-framework/community&quot;&gt;TensorTrade Gitter&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can also post &lt;strong&gt;bug reports and feature requests&lt;/strong&gt; in &lt;a href=&quot;https://github.com/notadamking/tensortrade/issues&quot;&gt;GitHub issues&lt;/a&gt;. Make sure to read &lt;a href=&quot;https://github.com/notadamking/tensortrade/raw/master/CONTRIBUTING.md&quot;&gt;our guidelines&lt;/a&gt; first.&lt;/p&gt; 
&lt;h2&gt;Contributors&lt;/h2&gt; 
&lt;p&gt;Contributions are encouraged and welcomed. This project is meant to grow as the community around it grows. Let me know on Discord in the #suggestions channel if there is anything that you would like to see in the future, or if there is anything you feel is missing.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Working on your first Pull Request?&lt;/strong&gt; You can learn how from this &lt;em&gt;free&lt;/em&gt; series &lt;a href=&quot;https://egghead.io/series/how-to-contribute-to-an-open-source-project-on-github&quot;&gt;How to Contribute to an Open Source Project on GitHub&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://contributors-img.firebaseapp.com/image?repo=notadamking/tensortrade&quot; alt=&quot;https://github.com/notadamking/tensortrade/graphs/contributors&quot;&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>eosphoros-ai/DB-GPT</title>
      <link>https://github.com/eosphoros-ai/DB-GPT</link>
      <description>&lt;p&gt;AI Native Data App Development framework with AWEL(Agentic Workflow Expression Language) and Agents&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;DB-GPT: AI Native Data App Development framework with AWEL(Agentic Workflow Expression Language) and Agents&lt;/h1&gt; 
&lt;p align=&quot;left&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/eosphoros-ai/DB-GPT/main/assets/LOGO.png&quot; width=&quot;100%&quot;&gt; &lt;/p&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;p&gt; &lt;a href=&quot;https://github.com/eosphoros-ai/DB-GPT&quot;&gt; &lt;img alt=&quot;stars&quot; src=&quot;https://img.shields.io/github/stars/eosphoros-ai/db-gpt?style=social&quot;&gt; &lt;/a&gt; &lt;a href=&quot;https://github.com/eosphoros-ai/DB-GPT&quot;&gt; &lt;img alt=&quot;forks&quot; src=&quot;https://img.shields.io/github/forks/eosphoros-ai/db-gpt?style=social&quot;&gt; &lt;/a&gt; &lt;a href=&quot;https://opensource.org/licenses/MIT&quot;&gt; &lt;img alt=&quot;License: MIT&quot; src=&quot;https://img.shields.io/badge/License-MIT-yellow.svg?sanitize=true&quot;&gt; &lt;/a&gt; &lt;a href=&quot;https://github.com/eosphoros-ai/DB-GPT/releases&quot;&gt; &lt;img alt=&quot;Release Notes&quot; src=&quot;https://img.shields.io/github/release/eosphoros-ai/DB-GPT&quot;&gt; &lt;/a&gt; &lt;a href=&quot;https://github.com/eosphoros-ai/DB-GPT/issues&quot;&gt; &lt;img alt=&quot;Open Issues&quot; src=&quot;https://img.shields.io/github/issues-raw/eosphoros-ai/DB-GPT&quot;&gt; &lt;/a&gt; &lt;a href=&quot;https://discord.gg/7uQnPuveTY&quot;&gt; &lt;img alt=&quot;Discord&quot; src=&quot;https://dcbadge.vercel.app/api/server/7uQnPuveTY?compact=true&amp;amp;style=flat&quot;&gt; &lt;/a&gt; &lt;a href=&quot;https://join.slack.com/t/slack-inu2564/shared_invite/zt-29rcnyw2b-N~ubOD9kFc7b7MDOAM1otA&quot;&gt; &lt;img alt=&quot;Slack&quot; src=&quot;https://badgen.net/badge/Slack/Join%20DB-GPT/0abd59?icon=slack&quot;&gt; &lt;/a&gt; &lt;a href=&quot;https://codespaces.new/eosphoros-ai/DB-GPT&quot;&gt; &lt;img alt=&quot;Open in GitHub Codespaces&quot; src=&quot;https://github.com/codespaces/badge.svg?sanitize=true&quot;&gt; &lt;/a&gt; &lt;/p&gt; 
 &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/eosphoros-ai/DB-GPT/main/README.zh.md&quot;&gt;&lt;strong&gt;简体中文&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/eosphoros-ai/DB-GPT/main/README.ja.md&quot;&gt;&lt;strong&gt;日本語&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&quot;https://discord.gg/7uQnPuveTY&quot;&gt;&lt;strong&gt;Discord&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&quot;https://docs.dbgpt.site&quot;&gt;&lt;strong&gt;Documents&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&quot;https://github.com/eosphoros-ai/DB-GPT/raw/main/README.zh.md#%E8%81%94%E7%B3%BB%E6%88%91%E4%BB%AC&quot;&gt;&lt;strong&gt;微信&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&quot;https://github.com/eosphoros-ai/community&quot;&gt;&lt;strong&gt;Community&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&quot;https://arxiv.org/pdf/2312.17449.pdf&quot;&gt;&lt;strong&gt;Paper&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;What is DB-GPT?&lt;/h2&gt; 
&lt;p&gt;🤖 &lt;strong&gt;DB-GPT is an open source AI native data app development framework with AWEL(Agentic Workflow Expression Language) and agents&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;The purpose is to build infrastructure in the field of large models, through the development of multiple technical capabilities such as multi-model management (SMMF), Text2SQL effect optimization, RAG framework and optimization, Multi-Agents framework collaboration, AWEL (agent workflow orchestration), etc. Which makes large model applications with data simpler and more convenient.&lt;/p&gt; 
&lt;p&gt;🚀 &lt;strong&gt;In the Data 3.0 era, based on models and databases, enterprises and developers can build their own bespoke applications with less code.&lt;/strong&gt;&lt;/p&gt; 
&lt;h3&gt;DISCKAIMER&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/eosphoros-ai/DB-GPT/main/DISCKAIMER.md&quot;&gt;disckaimer&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;AI-Native Data App&lt;/h3&gt; 
&lt;hr&gt; 
&lt;ul&gt; 
 &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://docs.dbgpt.cn/docs/changelog/Released_V0.6.0&quot;&gt;Released V0.6.0 | A set of significant upgrades&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;&quot;&gt;The AWEL upgrade to 2.0&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;&quot;&gt;GraphRAG&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;&quot;&gt;AI Native Data App construction and management&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;&quot;&gt;The GPT-Vis upgrade, supporting a variety of visualization charts&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;&quot;&gt;Support Text2NLU and Text2GQL fine-tuning&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;&quot;&gt;Support Intent recognition, slot filling, and Prompt management&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr&gt; 
&lt;p&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/a2f0a875-df8c-4f0d-89a3-eed321c02113&quot; alt=&quot;app_chat_v0 6&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/c8cc85bb-e3c2-4fab-8fb9-7b4b469d0611&quot; alt=&quot;app_manage_chat_data_v0 6&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/b15d6ebe-54c4-4527-a16d-02fbbaf20dc9&quot; alt=&quot;chat_dashboard_display_v0 6&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/40761507-a1e1-49d4-b49a-3dd9a5ea41cc&quot; alt=&quot;agent_prompt_awel_v0 6&quot;&gt;&lt;/p&gt; 
&lt;h2&gt;Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/eosphoros-ai/DB-GPT/main/#introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/eosphoros-ai/DB-GPT/main/#install&quot;&gt;Install&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/eosphoros-ai/DB-GPT/main/#features&quot;&gt;Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/eosphoros-ai/DB-GPT/main/#contribution&quot;&gt;Contribution&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/eosphoros-ai/DB-GPT/main/#contact-information&quot;&gt;Contact&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;The architecture of DB-GPT is shown in the following figure:&lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/eosphoros-ai/DB-GPT/main/assets/dbgpt.png&quot; width=&quot;800&quot;&gt; &lt;/p&gt; 
&lt;p&gt;The core capabilities include the following parts:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;RAG (Retrieval Augmented Generation)&lt;/strong&gt;: RAG is currently the most practically implemented and urgently needed domain. DB-GPT has already implemented a framework based on RAG, allowing users to build knowledge-based applications using the RAG capabilities of DB-GPT.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;GBI (Generative Business Intelligence)&lt;/strong&gt;: Generative BI is one of the core capabilities of the DB-GPT project, providing the foundational data intelligence technology to build enterprise report analysis and business insights.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Fine-tuning Framework&lt;/strong&gt;: Model fine-tuning is an indispensable capability for any enterprise to implement in vertical and niche domains. DB-GPT provides a complete fine-tuning framework that integrates seamlessly with the DB-GPT project. In recent fine-tuning efforts, an accuracy rate based on the Spider dataset has been achieved at 82.5%.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Data-Driven Multi-Agents Framework&lt;/strong&gt;: DB-GPT offers a data-driven self-evolving multi-agents framework, aiming to continuously make decisions and execute based on data.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Data Factory&lt;/strong&gt;: The Data Factory is mainly about cleaning and processing trustworthy knowledge and data in the era of large models.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Data Sources&lt;/strong&gt;: Integrating various data sources to seamlessly connect production business data to the core capabilities of DB-GPT.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;SubModule&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/eosphoros-ai/DB-GPT-Hub&quot;&gt;DB-GPT-Hub&lt;/a&gt; Text-to-SQL workflow with high performance by applying Supervised Fine-Tuning (SFT) on Large Language Models (LLMs).&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/eosphoros-ai/dbgpts&quot;&gt;dbgpts&lt;/a&gt; dbgpts is the official repository which contains some data apps、AWEL operators、AWEL workflow templates and agents which build upon DB-GPT.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Text2SQL Finetune&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;support llms&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; LLaMA&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; LLaMA-2&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; BLOOM&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; BLOOMZ&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; Falcon&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; Baichuan&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; Baichuan2&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; InternLM&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; Qwen&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; XVERSE&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; ChatGLM2&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;SFT Accuracy As of October 10, 2023, through the fine-tuning of an open-source model with 13 billion parameters using this project, we have achieved execution accuracy on the Spider dataset that surpasses even GPT-4!&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/eosphoros-ai/DB-GPT-Hub&quot;&gt;More Information about Text2SQL finetune&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/eosphoros-ai/DB-GPT-Plugins&quot;&gt;DB-GPT-Plugins&lt;/a&gt; DB-GPT Plugins that can run Auto-GPT plugin directly&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/eosphoros-ai/GPT-Vis&quot;&gt;GPT-Vis&lt;/a&gt; Visualization protocol&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Install&lt;/h2&gt; 
&lt;p&gt;&lt;img src=&quot;https://img.shields.io/badge/docker-%230db7ed.svg?style=for-the-badge&amp;amp;logo=docker&amp;amp;logoColor=white&quot; alt=&quot;Docker&quot;&gt; &lt;img src=&quot;https://img.shields.io/badge/Linux-FCC624?style=for-the-badge&amp;amp;logo=linux&amp;amp;logoColor=black&quot; alt=&quot;Linux&quot;&gt; &lt;img src=&quot;https://img.shields.io/badge/mac%20os-000000?style=for-the-badge&amp;amp;logo=macos&amp;amp;logoColor=F0F0F0&quot; alt=&quot;macOS&quot;&gt; &lt;img src=&quot;https://img.shields.io/badge/Windows-0078D6?style=for-the-badge&amp;amp;logo=windows&amp;amp;logoColor=white&quot; alt=&quot;Windows&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;http://docs.dbgpt.cn/docs/overview&quot;&gt;&lt;strong&gt;Usage Tutorial&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;http://docs.dbgpt.cn/docs/installation&quot;&gt;&lt;strong&gt;Install&lt;/strong&gt;&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;http://docs.dbgpt.cn/docs/installation/docker&quot;&gt;Docker&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;http://docs.dbgpt.cn/docs/installation/sourcecode&quot;&gt;Source Code&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;http://docs.dbgpt.cn/docs/quickstart&quot;&gt;&lt;strong&gt;Quickstart&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;http://docs.dbgpt.cn/docs/operation_manual&quot;&gt;&lt;strong&gt;Application&lt;/strong&gt;&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;http://docs.dbgpt.cn/docs/cookbook/app/data_analysis_app_develop&quot;&gt;Development Guide&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;http://docs.dbgpt.cn/docs/application/app_usage&quot;&gt;App Usage&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;http://docs.dbgpt.cn/docs/application/awel_flow_usage&quot;&gt;AWEL Flow Usage&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;http://docs.dbgpt.cn/docs/operation_manual/advanced_tutorial/debugging&quot;&gt;&lt;strong&gt;Debugging&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;http://docs.dbgpt.cn/docs/application/advanced_tutorial/cli&quot;&gt;&lt;strong&gt;Advanced Usage&lt;/strong&gt;&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;http://docs.dbgpt.cn/docs/application/advanced_tutorial/smmf&quot;&gt;SMMF&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;http://docs.dbgpt.cn/docs/application/fine_tuning_manual/dbgpt_hub&quot;&gt;Finetune&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;http://docs.dbgpt.cn/docs/awel/tutorial&quot;&gt;AWEL&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;p&gt;At present, we have introduced several key features to showcase our current capabilities:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Private Domain Q&amp;amp;A &amp;amp; Data Processing&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The DB-GPT project offers a range of functionalities designed to improve knowledge base construction and enable efficient storage and retrieval of both structured and unstructured data. These functionalities include built-in support for uploading multiple file formats, the ability to integrate custom data extraction plug-ins, and unified vector storage and retrieval capabilities for effectively managing large volumes of information.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Multi-Data Source &amp;amp; GBI(Generative Business intelligence)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The DB-GPT project facilitates seamless natural language interaction with diverse data sources, including Excel, databases, and data warehouses. It simplifies the process of querying and retrieving information from these sources, empowering users to engage in intuitive conversations and gain insights. Moreover, DB-GPT supports the generation of analytical reports, providing users with valuable data summaries and interpretations.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Multi-Agents&amp;amp;Plugins&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;It offers support for custom plug-ins to perform various tasks and natively integrates the Auto-GPT plug-in model. The Agents protocol adheres to the Agent Protocol standard.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Automated Fine-tuning text2SQL&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We&#39;ve also developed an automated fine-tuning lightweight framework centred on large language models (LLMs), Text2SQL datasets, LoRA/QLoRA/Pturning, and other fine-tuning methods. This framework simplifies Text-to-SQL fine-tuning, making it as straightforward as an assembly line process. &lt;a href=&quot;https://github.com/eosphoros-ai/DB-GPT-Hub&quot;&gt;DB-GPT-Hub&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;SMMF(Service-oriented Multi-model Management Framework)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We offer extensive model support, including dozens of large language models (LLMs) from both open-source and API agents, such as LLaMA/LLaMA2, Baichuan, ChatGLM, Wenxin, Tongyi, Zhipu, and many more.&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;News 
    &lt;ul&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/Qwen/Qwen2.5-72B-Instruct&quot;&gt;Qwen2.5-72B-Instruct&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/Qwen/Qwen2.5-32B-Instruct&quot;&gt;Qwen2.5-32B-Instruct&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/Qwen/Qwen2.5-14B-Instruct&quot;&gt;Qwen2.5-14B-Instruct&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/Qwen/Qwen2.5-7B-Instruct&quot;&gt;Qwen2.5-7B-Instruct&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/Qwen/Qwen2.5-3B-Instruct&quot;&gt;Qwen2.5-3B-Instruct&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct&quot;&gt;Qwen2.5-1.5B-Instruct&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct&quot;&gt;Qwen2.5-0.5B-Instruct&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct&quot;&gt;Qwen2.5-Coder-7B-Instruct&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/Qwen/Qwen2.5-Coder-1.5B-Instruct&quot;&gt;Qwen2.5-Coder-1.5B-Instruct&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/meta-llama/Meta-Llama-3.1-405B-Instruct&quot;&gt;Meta-Llama-3.1-405B-Instruct&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct&quot;&gt;Meta-Llama-3.1-70B-Instruct&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct&quot;&gt;Meta-Llama-3.1-8B-Instruct&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/google/gemma-2-27b-it&quot;&gt;gemma-2-27b-it&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/google/gemma-2-9b-it&quot;&gt;gemma-2-9b-it&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Instruct&quot;&gt;DeepSeek-Coder-V2-Instruct&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct&quot;&gt;DeepSeek-Coder-V2-Lite-Instruct&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/Qwen/Qwen2-57B-A14B-Instruct&quot;&gt;Qwen2-57B-A14B-Instruct&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/Qwen/Qwen2-72B-Instruct&quot;&gt;Qwen2-72B-Instruct&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/Qwen/Qwen2-7B-Instruct&quot;&gt;Qwen2-7B-Instruct&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/Qwen/Qwen2-1.5B-Instruct&quot;&gt;Qwen2-1.5B-Instruct&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/Qwen/Qwen2-0.5B-Instruct&quot;&gt;Qwen2-0.5B-Instruct&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/THUDM/glm-4-9b-chat&quot;&gt;glm-4-9b-chat&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/collections/microsoft/phi-3-6626e15e9585a200d2d761e3&quot;&gt;Phi-3&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/01-ai/Yi-1.5-34B-Chat&quot;&gt;Yi-1.5-34B-Chat&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/01-ai/Yi-1.5-9B-Chat&quot;&gt;Yi-1.5-9B-Chat&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/01-ai/Yi-1.5-6B-Chat&quot;&gt;Yi-1.5-6B-Chat&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/Qwen/Qwen1.5-110B-Chat&quot;&gt;Qwen1.5-110B-Chat&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/Qwen/Qwen1.5-MoE-A2.7B-Chat&quot;&gt;Qwen1.5-MoE-A2.7B-Chat&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct&quot;&gt;Meta-Llama-3-70B-Instruct&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct&quot;&gt;Meta-Llama-3-8B-Instruct&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/Qwen/CodeQwen1.5-7B-Chat&quot;&gt;CodeQwen1.5-7B-Chat&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/Qwen/Qwen1.5-32B-Chat&quot;&gt;Qwen1.5-32B-Chat&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/Nexusflow/Starling-LM-7B-beta&quot;&gt;Starling-LM-7B-beta&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/google/gemma-7b-it&quot;&gt;gemma-7b-it&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/google/gemma-2b-it&quot;&gt;gemma-2b-it&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/upstage/SOLAR-10.7B-Instruct-v1.0&quot;&gt;SOLAR-10.7B&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1&quot;&gt;Mixtral-8x7B&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/Qwen/Qwen-72B-Chat&quot;&gt;Qwen-72B-Chat&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/01-ai/Yi-34B-Chat&quot;&gt;Yi-34B-Chat&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;http://docs.dbgpt.site/docs/modules/smmf&quot;&gt;More Supported LLMs&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Privacy and Security&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We ensure the privacy and security of data through the implementation of various technologies, including privatized large models and proxy desensitization.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Support Datasources&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;http://docs.dbgpt.cn/docs/modules/connections&quot;&gt;Datasources&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Image&lt;/h2&gt; 
&lt;p&gt;🌐 &lt;a href=&quot;https://www.codewithgpu.com/i/eosphoros-ai/DB-GPT/dbgpt&quot;&gt;AutoDL Image&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Language Switching&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;In the .env configuration file, modify the LANGUAGE parameter to switch to different languages. The default is English (Chinese: zh, English: en, other languages to be added later).
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Contribution&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;To check detailed guidelines for new contributions, please refer &lt;a href=&quot;https://github.com/eosphoros-ai/DB-GPT/raw/main/CONTRIBUTING.md&quot;&gt;how to contribute&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Contributors Wall&lt;/h3&gt; 
&lt;a href=&quot;https://github.com/eosphoros-ai/DB-GPT/graphs/contributors&quot;&gt; &lt;img src=&quot;https://contrib.rocks/image?repo=eosphoros-ai/DB-GPT&amp;amp;max=200&quot;&gt; &lt;/a&gt; 
&lt;h2&gt;Licence&lt;/h2&gt; 
&lt;p&gt;The MIT License (MIT)&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you want to understand the overall architecture of DB-GPT, please cite &lt;a href=&quot;https://arxiv.org/abs/2312.17449&quot; target=&quot;_blank&quot;&gt;paper&lt;/a&gt; and &lt;a href=&quot;https:// arxiv.org/abs/2404.10209&quot; target=&quot;_blank&quot;&gt;Paper&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;If you want to learn about using DB-GPT for Agent development, please cite the &lt;a href=&quot;https://arxiv.org/abs/2412.13520&quot; target=&quot;_blank&quot;&gt;paper&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bibtex&quot;&gt;@article{xue2023dbgpt,
      title={DB-GPT: Empowering Database Interactions with Private Large Language Models}, 
      author={Siqiao Xue and Caigao Jiang and Wenhui Shi and Fangyin Cheng and Keting Chen and Hongjun Yang and Zhiping Zhang and Jianshan He and Hongyang Zhang and Ganglin Wei and Wang Zhao and Fan Zhou and Danrui Qi and Hong Yi and Shaodong Liu and Faqiang Chen},
      year={2023},
      journal={arXiv preprint arXiv:2312.17449},
      url={https://arxiv.org/abs/2312.17449}
}
@misc{huang2024romasrolebasedmultiagentdatabase,
      title={ROMAS: A Role-Based Multi-Agent System for Database monitoring and Planning}, 
      author={Yi Huang and Fangyin Cheng and Fan Zhou and Jiahui Li and Jian Gong and Hongjun Yang and Zhidong Fan and Caigao Jiang and Siqiao Xue and Faqiang Chen},
      year={2024},
      eprint={2412.13520},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2412.13520}, 
}
@inproceedings{xue2024demonstration,
      title={Demonstration of DB-GPT: Next Generation Data Interaction System Empowered by Large Language Models}, 
      author={Siqiao Xue and Danrui Qi and Caigao Jiang and Wenhui Shi and Fangyin Cheng and Keting Chen and Hongjun Yang and Zhiping Zhang and Jianshan He and Hongyang Zhang and Ganglin Wei and Wang Zhao and Fan Zhou and Hong Yi and Shaodong Liu and Hongjun Yang and Faqiang Chen},
      year={2024},
      booktitle = &quot;Proceedings of the VLDB Endowment&quot;,
      url={https://arxiv.org/abs/2404.10209}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Contact Information&lt;/h2&gt; 
&lt;p&gt;We are working on building a community, if you have any ideas for building the community, feel free to contact us. &lt;a href=&quot;https://discord.gg/7uQnPuveTY&quot;&gt;&lt;img src=&quot;https://dcbadge.vercel.app/api/server/7uQnPuveTY?compact=true&amp;amp;style=flat&quot; alt=&quot;&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://star-history.com/#csunny/DB-GPT&quot;&gt;&lt;img src=&quot;https://api.star-history.com/svg?repos=csunny/DB-GPT&amp;amp;type=Date&quot; alt=&quot;Star History Chart&quot;&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>microsoft/RD-Agent</title>
      <link>https://github.com/microsoft/RD-Agent</link>
      <description>&lt;p&gt;Research and development (R&amp;D) is crucial for the enhancement of industrial productivity, especially in the AI era, where the core aspects of R&amp;D are mainly focused on data and models. We are committed to automating these high-value generic R&amp;D processes through our open source R&amp;D automation tool RD-Agent, which lets AI drive data-driven AI.&lt;/p&gt;&lt;hr&gt;&lt;h4 align=&quot;center&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/microsoft/RD-Agent/main/docs/_static/logo.png&quot; alt=&quot;RA-Agent logo&quot; style=&quot;width:70%; &quot;&gt; &lt;p&gt;&lt;a href=&quot;https://rdagent.azurewebsites.net&quot; target=&quot;_blank&quot;&gt;🖥️ Live Demo&lt;/a&gt; | &lt;a href=&quot;https://rdagent.azurewebsites.net/factor_loop&quot; target=&quot;_blank&quot;&gt;🎥 Demo Video&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=JJ4JYO3HscM&amp;amp;list=PLALmKB0_N3_i52fhUmPQiL4jsO354uopR&quot; target=&quot;_blank&quot;&gt;▶️YouTube&lt;/a&gt; | &lt;a href=&quot;https://rdagent.readthedocs.io/en/latest/index.html&quot; target=&quot;_blank&quot;&gt;📖 Documentation&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/RD-Agent/main/#-paperwork-list&quot;&gt; 📃 Papers &lt;/a&gt;&lt;/p&gt; &lt;/h4&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/microsoft/RD-Agent/actions/workflows/ci.yml&quot;&gt;&lt;img src=&quot;https://github.com/microsoft/RD-Agent/actions/workflows/ci.yml/badge.svg?sanitize=true&quot; alt=&quot;CI&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/microsoft/RD-Agent/actions/workflows/github-code-scanning/codeql&quot;&gt;&lt;img src=&quot;https://github.com/microsoft/RD-Agent/actions/workflows/github-code-scanning/codeql/badge.svg?sanitize=true&quot; alt=&quot;CodeQL&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/microsoft/RD-Agent/actions/workflows/dependabot/dependabot-updates&quot;&gt;&lt;img src=&quot;https://github.com/microsoft/RD-Agent/actions/workflows/dependabot/dependabot-updates/badge.svg?sanitize=true&quot; alt=&quot;Dependabot Updates&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/microsoft/RD-Agent/actions/workflows/pr.yml&quot;&gt;&lt;img src=&quot;https://github.com/microsoft/RD-Agent/actions/workflows/pr.yml/badge.svg?sanitize=true&quot; alt=&quot;Lint PR Title&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/microsoft/RD-Agent/actions/workflows/release.yml&quot;&gt;&lt;img src=&quot;https://github.com/microsoft/RD-Agent/actions/workflows/release.yml/badge.svg?sanitize=true&quot; alt=&quot;Release.yml&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://pypi.org/project/rdagent/#files&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/platform-Linux-blue&quot; alt=&quot;Platform&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://pypi.org/project/rdagent/&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/v/rdagent&quot; alt=&quot;PyPI&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://pypi.org/project/rdagent/&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/pyversions/rdagent&quot; alt=&quot;PyPI - Python Version&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/microsoft/RD-Agent/releases&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/v/release/microsoft/RD-Agent&quot; alt=&quot;Release&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/microsoft/RD-Agent/raw/main/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/license/microsoft/RD-Agent&quot; alt=&quot;GitHub&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/pre-commit/pre-commit&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&quot; alt=&quot;pre-commit&quot;&gt;&lt;/a&gt; &lt;a href=&quot;http://mypy-lang.org/&quot;&gt;&lt;img src=&quot;https://www.mypy-lang.org/static/mypy_badge.svg?sanitize=true&quot; alt=&quot;Checked with mypy&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/astral-sh/ruff&quot;&gt;&lt;img src=&quot;https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json&quot; alt=&quot;Ruff&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://discord.gg/ybQ97B6Jjy&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/chat-discord-blue&quot; alt=&quot;Chat&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://rdagent.readthedocs.io/en/latest/?badge=latest&quot;&gt;&lt;img src=&quot;https://readthedocs.org/projects/rdagent/badge/?version=latest&quot; alt=&quot;Documentation Status&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/microsoft/RD-Agent/actions/workflows/readthedocs-preview.yml&quot;&gt;&lt;img src=&quot;https://github.com/microsoft/RD-Agent/actions/workflows/readthedocs-preview.yml/badge.svg?sanitize=true&quot; alt=&quot;Readthedocs Preview&quot;&gt;&lt;/a&gt; 
 &lt;!-- this badge is too long, please place it in the last one to make it pretty --&gt;&lt;/p&gt; 
&lt;h1&gt;📰 News&lt;/h1&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;🗞️ News&lt;/th&gt; 
   &lt;th&gt;📝 Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Kaggle Scenario release&lt;/td&gt; 
   &lt;td&gt;We release &lt;strong&gt;&lt;a href=&quot;https://rdagent.readthedocs.io/en/latest/scens/kaggle_agent.html&quot;&gt;Kaggle Agent&lt;/a&gt;&lt;/strong&gt;, try the new features!&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Official WeChat group release&lt;/td&gt; 
   &lt;td&gt;We created a WeChat group, welcome to join! (🗪&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/RD-Agent/main/docs/WeChat_QR_code.jpg&quot;&gt;QR Code&lt;/a&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Official Discord release&lt;/td&gt; 
   &lt;td&gt;We launch our first chatting channel in Discord (🗪&lt;a href=&quot;https://discord.gg/ybQ97B6Jjy&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/chat-discord-blue&quot; alt=&quot;Chat&quot;&gt;&lt;/a&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;First release&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;RDAgent&lt;/strong&gt; is released on GitHub&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;🌟 Introduction&lt;/h1&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;img src=&quot;https://raw.githubusercontent.com/microsoft/RD-Agent/main/docs/_static/scen.png&quot; alt=&quot;Our focused scenario&quot; style=&quot;width:80%; &quot;&gt; 
&lt;/div&gt; 
&lt;p&gt;RDAgent aims to automate the most critical and valuable aspects of the industrial R&amp;amp;D process, and we begin with focusing on the data-driven scenarios to streamline the development of models and data. Methodologically, we have identified a framework with two key components: &#39;R&#39; for proposing new ideas and &#39;D&#39; for implementing them. We believe that the automatic evolution of R&amp;amp;D will lead to solutions of significant industrial value.&lt;/p&gt; 
&lt;!-- Tag Cloud --&gt; 
&lt;p&gt;R&amp;amp;D is a very general scenario. The advent of RDAgent can be your&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;💰 &lt;strong&gt;Automatic Quant Factory&lt;/strong&gt; (&lt;a href=&quot;https://rdagent.azurewebsites.net/factor_loop&quot;&gt;🎥Demo Video&lt;/a&gt;|&lt;a href=&quot;https://www.youtube.com/watch?v=X4DK2QZKaKY&amp;amp;t=6s&quot;&gt;▶️YouTube&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;🤖 &lt;strong&gt;Data Mining Agent:&lt;/strong&gt; Iteratively proposing data &amp;amp; models (&lt;a href=&quot;https://rdagent.azurewebsites.net/model_loop&quot;&gt;🎥Demo Video 1&lt;/a&gt;|&lt;a href=&quot;https://www.youtube.com/watch?v=dm0dWL49Bc0&amp;amp;t=104s&quot;&gt;▶️YouTube&lt;/a&gt;) (&lt;a href=&quot;https://rdagent.azurewebsites.net/dmm&quot;&gt;🎥Demo Video 2&lt;/a&gt;|&lt;a href=&quot;https://www.youtube.com/watch?v=VIaSTZuoZg4&quot;&gt;▶️YouTube&lt;/a&gt;) and implementing them by gaining knowledge from data.&lt;/li&gt; 
 &lt;li&gt;🦾 &lt;strong&gt;Research Copilot:&lt;/strong&gt; Auto read research papers (&lt;a href=&quot;https://rdagent.azurewebsites.net/report_model&quot;&gt;🎥Demo Video&lt;/a&gt;|&lt;a href=&quot;https://www.youtube.com/watch?v=BiA2SfdKQ7o&quot;&gt;▶️YouTube&lt;/a&gt;) / financial reports (&lt;a href=&quot;https://rdagent.azurewebsites.net/report_factor&quot;&gt;🎥Demo Video&lt;/a&gt;|&lt;a href=&quot;https://www.youtube.com/watch?v=ECLTXVcSx-c&quot;&gt;▶️YouTube&lt;/a&gt;) and implement model structures or building datasets.&lt;/li&gt; 
 &lt;li&gt;🤖 &lt;strong&gt;Kaggle Agent:&lt;/strong&gt; Auto Model Tuning and Feature Engineering(&lt;a href=&quot;&quot;&gt;🎥Demo Video Coming Soon...&lt;/a&gt;) and implementing them to achieve more in competitions.&lt;/li&gt; 
 &lt;li&gt;...&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can click the links above to view the demo. We&#39;re continuously adding more methods and scenarios to the project to enhance your R&amp;amp;D processes and boost productivity.&lt;/p&gt; 
&lt;p&gt;Additionally, you can take a closer look at the examples in our &lt;strong&gt;&lt;a href=&quot;https://rdagent.azurewebsites.net/&quot;&gt;🖥️ Live Demo&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;a href=&quot;https://rdagent.azurewebsites.net/&quot; target=&quot;_blank&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/microsoft/RD-Agent/main/docs/_static/demo.png&quot; alt=&quot;Watch the demo&quot; width=&quot;80%&quot;&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h1&gt;⚡ Quick start&lt;/h1&gt; 
&lt;p&gt;You can try above demos by running the following command:&lt;/p&gt; 
&lt;h3&gt;🐳 Docker installation.&lt;/h3&gt; 
&lt;p&gt;Users must ensure Docker is installed before attempting most scenarios. Please refer to the &lt;a href=&quot;https://docs.docker.com/engine/install/&quot;&gt;official 🐳Docker page&lt;/a&gt; for installation instructions.&lt;/p&gt; 
&lt;h3&gt;🐍 Create a Conda Environment&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Create a new conda environment with Python (3.10 and 3.11 are well-tested in our CI): &lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;conda create -n rdagent python=3.10
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Activate the environment: &lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;conda activate rdagent
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;🛠️ Install the RDAgent&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;You can directly install the RDAgent package from PyPI: &lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;pip install rdagent
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;💊 Health check&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;rdagent provides a health check that currently checks two things. 
  &lt;ul&gt; 
   &lt;li&gt;whether the docker installation was successful.&lt;/li&gt; 
   &lt;li&gt;whether the default port used by the &lt;a href=&quot;https://github.com/microsoft/RD-Agent?tab=readme-ov-file#%EF%B8%8F-monitor-the-application-results&quot;&gt;rdagent ui&lt;/a&gt; is occupied.&lt;/li&gt; 
  &lt;/ul&gt; &lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;rdagent health_check
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;⚙️ Configuration&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;The demos requires following ability:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;ChatCompletion&lt;/li&gt; 
   &lt;li&gt;json_mode&lt;/li&gt; 
   &lt;li&gt;embedding query&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;For example: If you are using the &lt;code&gt;OpenAI API&lt;/code&gt;, you have to configure your GPT model in the &lt;code&gt;.env&lt;/code&gt; file like this.&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;cat &amp;lt;&amp;lt; EOF  &amp;gt; .env
OPENAI_API_KEY=&amp;lt;replace_with_your_openai_api_key&amp;gt;
# EMBEDDING_MODEL=text-embedding-3-small
CHAT_MODEL=gpt-4-turbo
EOF
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;However, not every API services support these features by devault. For example: &lt;code&gt;AZURE OpenAI&lt;/code&gt;, you have to configure your GPT model in the &lt;code&gt;.env&lt;/code&gt; file like this.&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;cat &amp;lt;&amp;lt; EOF  &amp;gt; .env
USE_AZURE=True
EMBEDDING_OPENAI_API_KEY=&amp;lt;replace_with_your_azure_openai_api_key&amp;gt;
EMBEDDING_AZURE_API_BASE=&amp;lt;replace_with_your_azure_endpoint&amp;gt;
EMBEDDING_AZURE_API_VERSION=&amp;lt;replace_with_the_version_of_your_azure_openai_api&amp;gt;
EMBEDDING_MODEL=text-embedding-3-small
CHAT_OPENAI_API_KEY=&amp;lt;replace_with_your_azure_openai_api_key&amp;gt;
CHAT_AZURE_API_BASE=&amp;lt;replace_with_your_azure_endpoint&amp;gt;
CHAT_AZURE_API_VERSION=&amp;lt;replace_with_the_version_of_your_azure_openai_api&amp;gt;
CHAT_MODEL=&amp;lt;replace_it_with_the_name_of_your_azure_chat_model&amp;gt;
EOF
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;For more configuration information, please refer to the &lt;a href=&quot;https://rdagent.readthedocs.io/en/latest/installation_and_configuration.html&quot;&gt;documentation&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;🚀 Run the Application&lt;/h3&gt; 
&lt;p&gt;The &lt;strong&gt;&lt;a href=&quot;https://rdagent.azurewebsites.net/&quot;&gt;🖥️ Live Demo&lt;/a&gt;&lt;/strong&gt; is implemented by the following commands(each item represents one demo, you can select the one you prefer):&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Run the &lt;strong&gt;Automated Quantitative Trading &amp;amp; Iterative Factors Evolution&lt;/strong&gt;: &lt;a href=&quot;http://github.com/microsoft/qlib&quot;&gt;Qlib&lt;/a&gt; self-loop factor proposal and implementation application&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;rdagent fin_factor
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run the &lt;strong&gt;Automated Quantitative Trading &amp;amp; Iterative Model Evolution&lt;/strong&gt;: &lt;a href=&quot;http://github.com/microsoft/qlib&quot;&gt;Qlib&lt;/a&gt; self-loop model proposal and implementation application&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;rdagent fin_model
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run the &lt;strong&gt;Automated Medical Prediction Model Evolution&lt;/strong&gt;: Medical self-loop model proposal and implementation application&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;(1) Apply for an account at &lt;a href=&quot;https://physionet.org/&quot;&gt;PhysioNet&lt;/a&gt;. &lt;br&gt; (2) Request access to FIDDLE preprocessed data: &lt;a href=&quot;https://physionet.org/content/mimic-eicu-fiddle-feature/1.0.0/&quot;&gt;FIDDLE Dataset&lt;/a&gt;. &lt;br&gt; (3) Place your username and password in &lt;code&gt;.env&lt;/code&gt;.&lt;/p&gt; 
  &lt;/blockquote&gt; &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;cat &amp;lt;&amp;lt; EOF  &amp;gt;&amp;gt; .env
DM_USERNAME=&amp;lt;your_username&amp;gt;
DM_PASSWORD=&amp;lt;your_password&amp;gt;
EOF
&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;rdagent med_model
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run the &lt;strong&gt;Automated Quantitative Trading &amp;amp; Factors Extraction from Financial Reports&lt;/strong&gt;: Run the &lt;a href=&quot;http://github.com/microsoft/qlib&quot;&gt;Qlib&lt;/a&gt; factor extraction and implementation application based on financial reports&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;# 1. Generally, you can run this scenario using the following command:
rdagent fin_factor_report --report_folder=&amp;lt;Your financial reports folder path&amp;gt;

# 2. Specifically, you need to prepare some financial reports first. You can follow this concrete example:
wget https://github.com/SunsetWolf/rdagent_resource/releases/download/reports/all_reports.zip
unzip all_reports.zip -d git_ignore_folder/reports
rdagent fin_factor_report --report_folder=git_ignore_folder/reports
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run the &lt;strong&gt;Automated Model Research &amp;amp; Development Copilot&lt;/strong&gt;: model extraction and implementation application&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;# 1. Generally, you can run your own papers/reports with the following command:
rdagent general_model &amp;lt;Your paper URL&amp;gt;

# 2. Specifically, you can do it like this. For more details and additional paper examples, use `rdagent general_model -h`:
rdagent general_model  &quot;https://arxiv.org/pdf/2210.09789&quot;
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run the &lt;strong&gt;Automated Kaggle Model Tuning &amp;amp; Feature Engineering&lt;/strong&gt;: self-loop model proposal and feature engineering implementation application &lt;br&gt;&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;Using &lt;strong&gt;sf-crime&lt;/strong&gt; &lt;em&gt;(San Francisco Crime Classification)&lt;/em&gt; as an example. &lt;br&gt;&lt;/p&gt; 
   &lt;ol&gt; 
    &lt;li&gt;Register and login on the &lt;a href=&quot;https://www.kaggle.com/&quot;&gt;Kaggle&lt;/a&gt; website. &lt;br&gt;&lt;/li&gt; 
    &lt;li&gt;Configuring the Kaggle API. &lt;br&gt; (1) Click on the avatar (usually in the top right corner of the page) -&amp;gt; &lt;code&gt;Settings&lt;/code&gt; -&amp;gt; &lt;code&gt;Create New Token&lt;/code&gt;, A file called &lt;code&gt;kaggle.json&lt;/code&gt; will be downloaded. &lt;br&gt; (2) Move &lt;code&gt;kaggle.json&lt;/code&gt; to &lt;code&gt;~/.config/kaggle/&lt;/code&gt; &lt;br&gt; (3) Modify the permissions of the kaggle.json file. Reference command: &lt;code&gt;chmod 600 ~/.config/kaggle/kaggle.json&lt;/code&gt; &lt;br&gt;&lt;/li&gt; 
    &lt;li&gt;Join the competition: Click &lt;code&gt;Join the competition&lt;/code&gt; -&amp;gt; &lt;code&gt;I Understand and Accept&lt;/code&gt; at the bottom of the &lt;a href=&quot;https://www.kaggle.com/competitions/sf-crime/data&quot;&gt;competition details page&lt;/a&gt;.&lt;/li&gt; 
   &lt;/ol&gt; 
  &lt;/blockquote&gt; &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# Generally, you can run the Kaggle competition program with the following command:
rdagent kaggle --competition &amp;lt;your competition name&amp;gt;

# Specifically, you will need to first prepare some competition description files and configure the competition description file path, which you can follow for this specific example:

# 1. Prepare the competition description files
wget https://github.com/SunsetWolf/rdagent_resource/releases/download/kaggle_data/kaggle_data.zip
unzip kaggle_data.zip -d git_ignore_folder/kaggle_data

# 2. Add the competition description file path to the `.env` file.
dotenv set KG_LOCAL_DATA_PATH &quot;$(pwd)/git_ignore_folder/kaggle_data&quot;

# 3. run the application
rdagent kaggle --competition sf-crime
&lt;/code&gt;&lt;/pre&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;&lt;strong&gt;Description of the above example:&lt;/strong&gt; &lt;br&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Kaggle competition data, contains two parts: competition description file (json file) and competition dataset (zip file). We prepare the competition description file for you, the competition dataset will be downloaded automatically when you run the program, as in the example. &lt;br&gt;&lt;/li&gt; 
    &lt;li&gt;If you want to download the competition description file automatically, you need to install chromedriver, The instructions for installing chromedriver can be found in the &lt;a href=&quot;https://rdagent.readthedocs.io/en/latest/scens/kaggle_agent.html#example-guide&quot;&gt;documentation&lt;/a&gt;. &lt;br&gt;&lt;/li&gt; 
    &lt;li&gt;The &lt;strong&gt;Competition List Available&lt;/strong&gt; can be found &lt;a href=&quot;https://rdagent.readthedocs.io/en/latest/scens/kaggle_agent.html#competition-list-available&quot;&gt;here&lt;/a&gt;. &lt;br&gt;&lt;/li&gt; 
   &lt;/ul&gt; 
  &lt;/blockquote&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;🖥️ Monitor the Application Results&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;You can run the following command for our demo program to see the run logs.&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;rdagent ui --port 19899 --log_dir &amp;lt;your log folder like &quot;log/&quot;&amp;gt;
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Although port 19899 is not commonly used, but before you run this demo, you need to check if port 19899 is occupied. If it is, please change it to another port that is not occupied.&lt;/p&gt; &lt;p&gt;You can check if a port is occupied by running the following command.&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;rdagent health_check
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;🏭 Scenarios&lt;/h1&gt; 
&lt;p&gt;We have applied RD-Agent to multiple valuable data-driven industrial scenarios.&lt;/p&gt; 
&lt;h2&gt;🎯 Goal: Agent for Data-driven R&amp;amp;D&lt;/h2&gt; 
&lt;p&gt;In this project, we are aiming to build an Agent to automate Data-Driven R&amp;amp;D that can&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;📄 Read real-world material (reports, papers, etc.) and &lt;strong&gt;extract&lt;/strong&gt; key formulas, descriptions of interested &lt;strong&gt;features&lt;/strong&gt; and &lt;strong&gt;models&lt;/strong&gt;, which are the key components of data-driven R&amp;amp;D .&lt;/li&gt; 
 &lt;li&gt;🛠️ &lt;strong&gt;Implement&lt;/strong&gt; the extracted formulas (e.g., features, factors, and models) in runnable codes. 
  &lt;ul&gt; 
   &lt;li&gt;Due to the limited ability of LLM in implementing at once, build an evolving process for the agent to improve performance by learning from feedback and knowledge.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;💡 Propose &lt;strong&gt;new ideas&lt;/strong&gt; based on current knowledge and observations.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- ![Data-Centric R&amp;D Overview](docs/_static/overview.png) --&gt; 
&lt;h2&gt;📈 Scenarios/Demos&lt;/h2&gt; 
&lt;p&gt;In the two key areas of data-driven scenarios, model implementation and data building, our system aims to serve two main roles: 🦾Copilot and 🤖Agent.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;The 🦾Copilot follows human instructions to automate repetitive tasks.&lt;/li&gt; 
 &lt;li&gt;The 🤖Agent, being more autonomous, actively proposes ideas for better results in the future.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The supported scenarios are listed below:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Scenario/Target&lt;/th&gt; 
   &lt;th&gt;Model Implementation&lt;/th&gt; 
   &lt;th&gt;Data Building&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;💹 Finance&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;🤖 &lt;a href=&quot;https://rdagent.azurewebsites.net/model_loop&quot;&gt;Iteratively Proposing Ideas &amp;amp; Evolving&lt;/a&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=dm0dWL49Bc0&amp;amp;t=104s&quot;&gt;▶️YouTube&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;🤖 &lt;a href=&quot;https://rdagent.azurewebsites.net/factor_loop&quot;&gt;Iteratively Proposing Ideas &amp;amp; Evolving&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=X4DK2QZKaKY&amp;amp;t=6s&quot;&gt;▶️YouTube&lt;/a&gt; &lt;br&gt; 🦾 &lt;a href=&quot;https://rdagent.azurewebsites.net/report_factor&quot;&gt;Auto reports reading &amp;amp; implementation&lt;/a&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=ECLTXVcSx-c&quot;&gt;▶️YouTube&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;🩺 Medical&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;🤖 &lt;a href=&quot;https://rdagent.azurewebsites.net/dmm&quot;&gt;Iteratively Proposing Ideas &amp;amp; Evolving&lt;/a&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=VIaSTZuoZg4&quot;&gt;▶️YouTube&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;🏭 General&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;🦾 &lt;a href=&quot;https://rdagent.azurewebsites.net/report_model&quot;&gt;Auto paper reading &amp;amp; implementation&lt;/a&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=BiA2SfdKQ7o&quot;&gt;▶️YouTube&lt;/a&gt; &lt;br&gt; 🤖 Auto Kaggle Model Tuning&lt;/td&gt; 
   &lt;td&gt;🤖Auto Kaggle feature Engineering&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://rdagent.readthedocs.io/en/latest/scens/kaggle_agent.html#roadmap&quot;&gt;RoadMap&lt;/a&gt;&lt;/strong&gt;: Currently, we are working hard to add new features to the Kaggle scenario.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Different scenarios vary in entrance and configuration. Please check the detailed setup tutorial in the scenarios documents.&lt;/p&gt; 
&lt;p&gt;Here is a gallery of &lt;a href=&quot;https://github.com/SunsetWolf/rdagent_resource/releases/download/demo_traces/demo_traces.zip&quot;&gt;successful explorations&lt;/a&gt; (5 traces showed in &lt;strong&gt;&lt;a href=&quot;https://rdagent.azurewebsites.net/&quot;&gt;🖥️ Live Demo&lt;/a&gt;&lt;/strong&gt;). You can download and view the execution trace using &lt;a href=&quot;https://github.com/microsoft/RD-Agent?tab=readme-ov-file#%EF%B8%8F-monitor-the-application-results&quot;&gt;this command&lt;/a&gt; from the documentation.&lt;/p&gt; 
&lt;p&gt;Please refer to &lt;strong&gt;&lt;a href=&quot;https://rdagent.readthedocs.io/en/latest/scens/catalog.html&quot;&gt;📖readthedocs_scen&lt;/a&gt;&lt;/strong&gt; for more details of the scenarios.&lt;/p&gt; 
&lt;h1&gt;⚙️ Framework&lt;/h1&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;img src=&quot;https://raw.githubusercontent.com/microsoft/RD-Agent/main/docs/_static/Framework-RDAgent.png&quot; alt=&quot;Framework-RDAgent&quot; width=&quot;85%&quot;&gt; 
&lt;/div&gt; 
&lt;p&gt;Automating the R&amp;amp;D process in data science is a highly valuable yet underexplored area in industry. We propose a framework to push the boundaries of this important research field.&lt;/p&gt; 
&lt;p&gt;The research questions within this framework can be divided into three main categories:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Research Area&lt;/th&gt; 
   &lt;th&gt;Paper/Work List&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Benchmark the R&amp;amp;D abilities&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/RD-Agent/main/#benchmark&quot;&gt;Benchmark&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Idea proposal:&lt;/strong&gt; Explore new ideas or refine existing ones&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/RD-Agent/main/#research&quot;&gt;Research&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Ability to realize ideas:&lt;/strong&gt; Implement and execute ideas&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/RD-Agent/main/#development&quot;&gt;Development&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;We believe that the key to delivering high-quality solutions lies in the ability to evolve R&amp;amp;D capabilities. Agents should learn like human experts, continuously improving their R&amp;amp;D skills.&lt;/p&gt; 
&lt;p&gt;More documents can be found in the &lt;strong&gt;&lt;a href=&quot;https://rdagent.readthedocs.io/&quot;&gt;📖 readthedocs&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; 
&lt;h1&gt;📃 Paper/Work list&lt;/h1&gt; 
&lt;h2&gt;📊 Benchmark&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.11276&quot;&gt;Towards Data-Centric Automatic R&amp;amp;D&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class=&quot;language-BibTeX&quot;&gt;@misc{chen2024datacentric,
    title={Towards Data-Centric Automatic R&amp;amp;D},
    author={Haotian Chen and Xinjie Shen and Zeqi Ye and Wenjun Feng and Haoxue Wang and Xiao Yang and Xu Yang and Weiqing Liu and Jiang Bian},
    year={2024},
    eprint={2404.11276},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/494f55d3-de9e-4e73-ba3d-a787e8f9e841&quot; alt=&quot;image&quot;&gt;&lt;/p&gt; 
&lt;h2&gt;🔍 Research&lt;/h2&gt; 
&lt;p&gt;In a data mining expert&#39;s daily research and development process, they propose a hypothesis (e.g., a model structure like RNN can capture patterns in time-series data), design experiments (e.g., finance data contains time-series and we can verify the hypothesis in this scenario), implement the experiment as code (e.g., Pytorch model structure), and then execute the code to get feedback (e.g., metrics, loss curve, etc.). The experts learn from the feedback and improve in the next iteration.&lt;/p&gt; 
&lt;p&gt;Based on the principles above, we have established a basic method framework that continuously proposes hypotheses, verifies them, and gets feedback from the real-world practice. This is the first scientific research automation framework that supports linking with real-world verification.&lt;/p&gt; 
&lt;p&gt;For more detail, please refer to our &lt;strong&gt;&lt;a href=&quot;https://rdagent.azurewebsites.net&quot;&gt;🖥️ Live Demo page&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; 
&lt;h2&gt;🛠️ Development&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2407.18690&quot;&gt;Collaborative Evolving Strategy for Automatic Data-Centric Development&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class=&quot;language-BibTeX&quot;&gt;@misc{yang2024collaborative,
    title={Collaborative Evolving Strategy for Automatic Data-Centric Development},
    author={Xu Yang and Haotian Chen and Wenjun Feng and Haoxue Wang and Zeqi Ye and Xinjie Shen and Xiao Yang and Shizhao Sun and Weiqing Liu and Jiang Bian},
    year={2024},
    eprint={2407.18690},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/75d9769b-0edd-4caf-9d45-57d1e577054b&quot; alt=&quot;image&quot;&gt;&lt;/p&gt; 
&lt;h1&gt;🤝 Contributing&lt;/h1&gt; 
&lt;h2&gt;📝 Guidelines&lt;/h2&gt; 
&lt;p&gt;This project welcomes contributions and suggestions. Contributing to this project is straightforward and rewarding. Whether it&#39;s solving an issue, addressing a bug, enhancing documentation, or even correcting a typo, every contribution is valuable and helps improve RDAgent.&lt;/p&gt; 
&lt;p&gt;To get started, you can explore the issues list, or search for &lt;code&gt;TODO:&lt;/code&gt; comments in the codebase by running the command &lt;code&gt;grep -r &quot;TODO:&quot;&lt;/code&gt;.&lt;/p&gt; 
&lt;img src=&quot;https://img.shields.io/github/contributors-anon/microsoft/RD-Agent&quot;&gt; 
&lt;a href=&quot;https://github.com/microsoft/RD-Agent/graphs/contributors&quot;&gt; &lt;img src=&quot;https://contrib.rocks/image?repo=microsoft/RD-Agent&amp;amp;max=100&amp;amp;columns=15&quot;&gt; &lt;/a&gt; 
&lt;p&gt;Before we released RD-Agent as an open-source project on GitHub, it was an internal project within our group. Unfortunately, the internal commit history was not preserved when we removed some confidential code. As a result, some contributions from our group members, including Haotian Chen, Wenjun Feng, Haoxue Wang, Zeqi Ye, Xinjie Shen, and Jinhui Li, were not included in the public commits.&lt;/p&gt; 
&lt;h1&gt;⚖️ Legal disclaimer&lt;/h1&gt; 
&lt;p style=&quot;line-height: 1; font-style: italic;&quot;&gt;The RD-agent is provided “as is”, without warranty of any kind, express or implied, including but not limited to the warranties of merchantability, fitness for a particular purpose and noninfringement. The RD-agent is aimed to facilitate research and development process in the financial industry and not ready-to-use for any financial investment or advice. Users shall independently assess and test the risks of the RD-agent in a specific use scenario, ensure the responsible use of AI technology, including but not limited to developing and integrating risk mitigation measures, and comply with all applicable laws and regulations in all applicable jurisdictions. The RD-agent does not provide financial opinions or reflect the opinions of Microsoft, nor is it designed to replace the role of qualified financial professionals in formulating, assessing, and approving finance products. The inputs and outputs of the RD-agent belong to the users and users shall assume all liability under any theory of liability, whether in contract, torts, regulatory, negligence, products liability, or otherwise, associated with use of the RD-agent and any inputs and outputs thereof.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>jiji262/douyin-downloader</title>
      <link>https://github.com/jiji262/douyin-downloader</link>
      <description>&lt;p&gt;抖音批量下载工具，去水印，支持视频、图集、合集、音乐(原声)。免费！免费！免费！&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;DouYin Downloader&lt;/h1&gt; 
&lt;p&gt;DouYin Downloader 是一个用于批量下载抖音内容的工具。基于抖音 API 实现，支持命令行参数或 YAML 配置文件方式运行，可满足大部分抖音内容的下载需求。&lt;/p&gt; 
&lt;h2&gt;✨ 特性&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;多种内容支持&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;视频、图集、音乐、直播信息下载&lt;/li&gt; 
   &lt;li&gt;支持个人主页、作品分享、直播、合集、音乐集合等多种链接&lt;/li&gt; 
   &lt;li&gt;支持去水印下载&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;批量下载能力&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;多线程并发下载&lt;/li&gt; 
   &lt;li&gt;支持多链接批量下载&lt;/li&gt; 
   &lt;li&gt;自动跳过已下载内容&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;灵活配置&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;支持命令行参数和配置文件两种方式&lt;/li&gt; 
   &lt;li&gt;可自定义下载路径、线程数等&lt;/li&gt; 
   &lt;li&gt;支持下载数量限制&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;增量更新&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;支持主页作品增量更新&lt;/li&gt; 
   &lt;li&gt;支持数据持久化到数据库&lt;/li&gt; 
   &lt;li&gt;可根据时间范围过滤&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🚀 快速开始&lt;/h2&gt; 
&lt;h3&gt;安装&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;安装 Python 依赖：&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start=&quot;2&quot;&gt; 
 &lt;li&gt;复制配置文件：&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;cp config.example.yml config.yml
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;配置&lt;/h3&gt; 
&lt;p&gt;编辑 &lt;code&gt;config.yml&lt;/code&gt; 文件，设置：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;下载链接&lt;/li&gt; 
 &lt;li&gt;保存路径&lt;/li&gt; 
 &lt;li&gt;Cookie 信息（从浏览器开发者工具获取）&lt;/li&gt; 
 &lt;li&gt;其他下载选项&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;运行&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;方式一：使用配置文件（推荐）&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python DouYinCommand.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;方式二：使用命令行&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python DouYinCommand.py -C True -l &quot;抖音分享链接&quot; -p &quot;下载路径&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;使用交流群&lt;/h2&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jiji262/douyin-downloader/main/img/fuye.png&quot; alt=&quot;fuye&quot;&gt;&lt;/p&gt; 
&lt;h2&gt;使用截图&lt;/h2&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jiji262/douyin-downloader/main/img/DouYinCommand1.png&quot; alt=&quot;DouYinCommand1&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jiji262/douyin-downloader/main/img/DouYinCommand2.png&quot; alt=&quot;DouYinCommand2&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jiji262/douyin-downloader/main/img/DouYinCommanddownload.jpg&quot; alt=&quot;DouYinCommand download&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jiji262/douyin-downloader/main/img/DouYinCommanddownloaddetail.jpg&quot; alt=&quot;DouYinCommand download detail&quot;&gt;&lt;/p&gt; 
&lt;h2&gt;📝 支持的链接类型&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;作品分享链接：&lt;code&gt;https://v.douyin.com/xxx/&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;个人主页：&lt;code&gt;https://www.douyin.com/user/xxx&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;单个视频：&lt;code&gt;https://www.douyin.com/video/xxx&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;图集：&lt;code&gt;https://www.douyin.com/note/xxx&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;合集：&lt;code&gt;https://www.douyin.com/collection/xxx&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;音乐原声：&lt;code&gt;https://www.douyin.com/music/xxx&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;直播：&lt;code&gt;https://live.douyin.com/xxx&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🛠️ 高级用法&lt;/h2&gt; 
&lt;h3&gt;命令行参数&lt;/h3&gt; 
&lt;p&gt;基础参数：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;-C, --cmd            使用命令行模式
-l, --link          下载链接
-p, --path          保存路径
-t, --thread        线程数（默认5）
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;下载选项：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;-m, --music         下载音乐（默认True）
-c, --cover         下载封面（默认True）
-a, --avatar        下载头像（默认True）
-j, --json          保存JSON数据（默认True）
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;更多参数说明请使用 &lt;code&gt;-h&lt;/code&gt; 查看帮助信息。&lt;/p&gt; 
&lt;h3&gt;示例命令&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;下载单个视频：&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python DouYinCommand.py -C True -l &quot;https://v.douyin.com/xxx/&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start=&quot;2&quot;&gt; 
 &lt;li&gt;下载主页作品：&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python DouYinCommand.py -C True -l &quot;https://v.douyin.com/xxx/&quot; -M post
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start=&quot;3&quot;&gt; 
 &lt;li&gt;批量下载：&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python DouYinCommand.py -C True -l &quot;链接1&quot; -l &quot;链接2&quot; -p &quot;./downloads&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;更多示例请参考&lt;a href=&quot;https://raw.githubusercontent.com/jiji262/douyin-downloader/main/docs/examples.md&quot;&gt;使用示例文档&lt;/a&gt;。&lt;/p&gt; 
&lt;h2&gt;📋 注意事项&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;本项目仅供学习交流使用&lt;/li&gt; 
 &lt;li&gt;使用前请确保已安装所需依赖&lt;/li&gt; 
 &lt;li&gt;Cookie 信息需要自行获取&lt;/li&gt; 
 &lt;li&gt;建议适当调整线程数，避免请求过于频繁&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;🤝 贡献&lt;/h2&gt; 
&lt;p&gt;欢迎提交 Issue 和 Pull Request。&lt;/p&gt; 
&lt;h2&gt;📜 许可证&lt;/h2&gt; 
&lt;p&gt;本项目采用 &lt;a href=&quot;https://raw.githubusercontent.com/jiji262/douyin-downloader/main/LICENSE&quot;&gt;MIT&lt;/a&gt; 许可证。&lt;/p&gt; 
&lt;h2&gt;🙏 鸣谢&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Johnserf-Seed/TikTokDownload&quot;&gt;TikTokDownload&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;本项目使用了 ChatGPT 辅助开发，如有问题请提 Issue&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;📊 Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://star-history.com/#jiji262/douyin-downloader&amp;amp;Date&quot;&gt;&lt;img src=&quot;https://api.star-history.com/svg?repos=jiji262/douyin-downloader&amp;amp;type=Date&quot; alt=&quot;Star History Chart&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;License&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://opensource.org/licenses/MIT&quot;&gt;MIT&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>X-PLUG/MobileAgent</title>
      <link>https://github.com/X-PLUG/MobileAgent</link>
      <description>&lt;p&gt;Mobile-Agent: The Powerful Mobile Device Operation Assistant Family&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/X-PLUG/MobileAgent/main/assets/logo.png?v=1&amp;amp;type=image&quot; alt=&quot;&quot;&gt;&lt;/p&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;h3&gt;Mobile-Agent: The Powerful Mobile Device Operation Assistant Family&lt;/h3&gt;
 &lt;h3&gt; 
  &lt;div align=&quot;center&quot;&gt; 
   &lt;a href=&quot;https://huggingface.co/spaces/junyangwang0410/Mobile-Agent&quot;&gt;&lt;img src=&quot;https://huggingface.co/datasets/huggingface/badges/raw/main/open-in-hf-spaces-sm-dark.svg?sanitize=true&quot; alt=&quot;Open in Spaces&quot;&gt;&lt;/a&gt; 
   &lt;a href=&quot;https://modelscope.cn/studios/wangjunyang/Mobile-Agent-v2&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/X-PLUG/MobileAgent/main/assets/Demo-ModelScope-brightgreen.svg?sanitize=true&quot; alt=&quot;Demo ModelScope&quot;&gt;&lt;/a&gt; 
   &lt;a href=&quot;https://arxiv.org/abs/2502.14282%20&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Arxiv-2502.14282-b31b1b.svg?logo=arXiv&quot; alt=&quot;&quot;&gt;&lt;/a&gt; 
   &lt;a href=&quot;https://arxiv.org/abs/2501.11733&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Arxiv-2501.11733-b31b1b.svg?logo=arXiv&quot; alt=&quot;&quot;&gt;&lt;/a&gt; 
   &lt;a href=&quot;https://arxiv.org/abs/2406.01014%20&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Arxiv-2406.01014-b31b1b.svg?logo=arXiv&quot; alt=&quot;&quot;&gt;&lt;/a&gt; 
   &lt;a href=&quot;https://arxiv.org/abs/2401.16158&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Arxiv-2401.16158-b31b1b.svg?logo=arXiv&quot; alt=&quot;&quot;&gt;&lt;/a&gt; 
  &lt;/div&gt; &lt;p align=&quot;center&quot;&gt; &lt;a href=&quot;https://trendshift.io/repositories/7423&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/7423&quot; alt=&quot;MobileAgent | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;&gt;&lt;/a&gt; &lt;/p&gt; &lt;/h3&gt;
&lt;/div&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;a href=&quot;https://raw.githubusercontent.com/X-PLUG/MobileAgent/main/README.md&quot;&gt;English&lt;/a&gt; | 
 &lt;a href=&quot;https://raw.githubusercontent.com/X-PLUG/MobileAgent/main/README_zh.md&quot;&gt;简体中文&lt;/a&gt; | 
 &lt;a href=&quot;https://raw.githubusercontent.com/X-PLUG/MobileAgent/main/README_ja.md&quot;&gt;日本語&lt;/a&gt; 
 &lt;hr&gt; 
&lt;/div&gt; 
&lt;h2&gt;📺Demo&lt;/h2&gt; 
&lt;h3&gt;Newest PC-Agent&lt;/h3&gt; 
&lt;p&gt;See &lt;a href=&quot;https://arxiv.org/abs/2502.14282&quot;&gt;paper&lt;/a&gt; for details.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/user-attachments/assets/b13bbb14-b39a-4c6b-b4a6-3df97de517dc&quot;&gt;https://github.com/user-attachments/assets/b13bbb14-b39a-4c6b-b4a6-3df97de517dc&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Mobile-Agent-E&lt;/h3&gt; 
&lt;p&gt;See the &lt;a href=&quot;https://x-plug.github.io/MobileAgent&quot;&gt;project page&lt;/a&gt; for video demos.&lt;/p&gt; 
&lt;!-- &lt;div style=&quot;display: flex; justify-content: space-between; gap: 10px; flex-wrap: wrap;&quot;&gt;
  &lt;video width=&quot;30%&quot; controls&gt;
    &lt;source src=&quot;https://raw.githubusercontent.com/X-PLUG/MobileAgent/main/Mobile-Agent-E/static/videos/bouldering_gym.mp4&quot; type=&quot;video/mp4&quot;&gt;
  &lt;/video&gt;
  &lt;video width=&quot;30%&quot; controls&gt;
    &lt;source src=&quot;https://raw.githubusercontent.com/X-PLUG/MobileAgent/main/Mobile-Agent-E/static/videos/shopping.mp4&quot; type=&quot;video/mp4&quot;&gt;
  &lt;/video&gt;
  &lt;video width=&quot;30%&quot; controls&gt;
    &lt;source src=&quot;https://raw.githubusercontent.com/X-PLUG/MobileAgent/main/Mobile-Agent-E/static/videos/survey.mp4&quot; type=&quot;video/mp4&quot;&gt;
  &lt;/video&gt;
&lt;/div&gt; --&gt; 
&lt;h3&gt;Mobile-Agent-v3 (Note: The video is not accelerated)&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;YouTube&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=EMbIpzqJld0&quot;&gt;&lt;img src=&quot;https://img.youtube.com/vi/EMbIpzqJld0/0.jpg&quot; alt=&quot;YouTube&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Bilibili&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.bilibili.com/video/BV1pPvyekEsa/?share_source=copy_web&amp;amp;vd_source=47ffcd57083495a8965c8cdbe1a751ae&quot;&gt;&lt;img src=&quot;https://img.youtube.com/vi/EMbIpzqJld0/0.jpg&quot; alt=&quot;Bilibili&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;PC-Agent&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Chrome and DingTalk&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/user-attachments/assets/b890a08f-8a2f-426d-9458-aa3699185030&quot;&gt;https://github.com/user-attachments/assets/b890a08f-8a2f-426d-9458-aa3699185030&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Word&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/user-attachments/assets/37f0a0a5-3d21-4232-9d1d-0fe845d0f77d&quot;&gt;https://github.com/user-attachments/assets/37f0a0a5-3d21-4232-9d1d-0fe845d0f77d&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Mobile-Agent-v2&lt;/h3&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/X-PLUG/MobileAgent/assets/127390760/d907795d-b5b9-48bf-b1db-70cf3f45d155&quot;&gt;https://github.com/X-PLUG/MobileAgent/assets/127390760/d907795d-b5b9-48bf-b1db-70cf3f45d155&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Mobile-Agent&lt;/h3&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/X-PLUG/MobileAgent/assets/127390760/26c48fb0-67ed-4df6-97b2-aa0c18386d31&quot;&gt;https://github.com/X-PLUG/MobileAgent/assets/127390760/26c48fb0-67ed-4df6-97b2-aa0c18386d31&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;📢News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;🔥🔥[2.21.25] We have released an updated version of PC-Agent. Check the &lt;a href=&quot;https://arxiv.org/abs/2502.14282&quot;&gt;paper&lt;/a&gt; for details. The code will be updated soon.&lt;/li&gt; 
 &lt;li&gt;🔥🔥[1.20.25] We propose &lt;a href=&quot;https://x-plug.github.io/MobileAgent&quot;&gt;Mobile-Agent-E&lt;/a&gt;, a hierarchical multi-agent framework capable of self-evolution through past experience, achieving stronger performance on complex, multi-app tasks.&lt;/li&gt; 
 &lt;li&gt;🔥🔥[9.26] Mobile-Agent-v2 has been accepted by &lt;strong&gt;The Thirty-eighth Annual Conference on Neural Information Processing Systems (NeurIPS 2024)&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;🔥[8.23] We proposed PC-Agent, a &lt;strong&gt;PC&lt;/strong&gt; operation assistant supporting both &lt;strong&gt;Mac and Windows&lt;/strong&gt; platforms.&lt;/li&gt; 
 &lt;li&gt;🔥[7.29] Mobile-Agent won the &lt;strong&gt;best demo award&lt;/strong&gt; at the &lt;em&gt;&lt;strong&gt;The 23rd China National Conference on Computational Linguistics&lt;/strong&gt;&lt;/em&gt; (CCL 2024). On the CCL 2024, we displayed the upcoming Mobile-Agent-v3. It has smaller memory overhead (8 GB), faster reasoning speed (10s-15s per operation), and all uses open source models. Video demo, please see the last section 📺Demo.&lt;/li&gt; 
 &lt;li&gt;[6.27] We proposed Demo that can upload mobile phone screenshots to experience Mobile-Agent-V2 in &lt;a href=&quot;https://huggingface.co/spaces/junyangwang0410/Mobile-Agent&quot;&gt;Hugging Face&lt;/a&gt; and &lt;a href=&quot;https://modelscope.cn/studios/wangjunyang/Mobile-Agent-v2&quot;&gt;ModelScope&lt;/a&gt;. You don’t need to configure models and devices, and you can experience it immediately.&lt;/li&gt; 
 &lt;li&gt;[6. 4] Modelscope-Agent has supported Mobile-Agent-V2, based on Android Adb Env, please check in the &lt;a href=&quot;https://github.com/modelscope/modelscope-agent/tree/master/apps/mobile_agent&quot;&gt;application&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;[6. 4] We proposed Mobile-Agent-v2, a mobile device operation assistant with effective navigation via multi-agent collaboration.&lt;/li&gt; 
 &lt;li&gt;[3.10] Mobile-Agent has been accepted by the &lt;strong&gt;ICLR 2024 Workshop on Large Language Model (LLM) Agents&lt;/strong&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;📱Version&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/X-PLUG/MobileAgent/main/PC-Agent/README.md&quot;&gt;PC-Agent&lt;/a&gt; - A Hierarchical Multi-Agent Collaboration Framework for Complex Task Automation on PC&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/X-PLUG/MobileAgent/main/Mobile-Agent-E/README.md&quot;&gt;Mobile-Agent-E&lt;/a&gt; - Stronger performance on complex, long-horizon, reasoning-intensive tasks, with self-evolution capability&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/X-PLUG/MobileAgent/main/Mobile-Agent-v3/README.md&quot;&gt;Mobile-Agent-v3&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/X-PLUG/MobileAgent/main/Mobile-Agent-v2/README.md&quot;&gt;Mobile-Agent-v2&lt;/a&gt; - Mobile Device Operation Assistant with Effective Navigation via Multi-Agent Collaboration&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/X-PLUG/MobileAgent/main/Mobile-Agent/README.md&quot;&gt;Mobile-Agent&lt;/a&gt; - Autonomous Multi-Modal Mobile Device Agent with Visual Perception&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;⭐Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://star-history.com/#X-PLUG/MobileAgent&amp;amp;Date&quot;&gt;&lt;img src=&quot;https://api.star-history.com/svg?repos=X-PLUG/MobileAgent&amp;amp;type=Date&quot; alt=&quot;Star History Chart&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;📑Citation&lt;/h2&gt; 
&lt;p&gt;If you find Mobile-Agent useful for your research and applications, please cite using this BibTeX:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@article{liu2025pc,
  title={PC-Agent: A Hierarchical Multi-Agent Collaboration Framework for Complex Task Automation on PC},
  author={Liu, Haowei and Zhang, Xi and Xu, Haiyang and Wanyan, Yuyang and Wang, Junyang and Yan, Ming and Zhang, Ji and Yuan, Chunfeng and Xu, Changsheng and Hu, Weiming and Huang, Fei},
  journal={arXiv preprint arXiv:2502.14282},
  year={2025}
}

@article{wang2025mobile,
  title={Mobile-Agent-E: Self-Evolving Mobile Assistant for Complex Tasks},
  author={Wang, Zhenhailong and Xu, Haiyang and Wang, Junyang and Zhang, Xi and Yan, Ming and Zhang, Ji and Huang, Fei and Ji, Heng},
  journal={arXiv preprint arXiv:2501.11733},
  year={2025}
}

@article{wang2024mobile2,
  title={Mobile-Agent-v2: Mobile Device Operation Assistant with Effective Navigation via Multi-Agent Collaboration},
  author={Wang, Junyang and Xu, Haiyang and Jia, Haitao and Zhang, Xi and Yan, Ming and Shen, Weizhou and Zhang, Ji and Huang, Fei and Sang, Jitao},
  journal={arXiv preprint arXiv:2406.01014},
  year={2024}
}

@article{wang2024mobile,
  title={Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception},
  author={Wang, Junyang and Xu, Haiyang and Ye, Jiabo and Yan, Ming and Shen, Weizhou and Zhang, Ji and Huang, Fei and Sang, Jitao},
  journal={arXiv preprint arXiv:2401.16158},
  year={2024}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;📦Related Projects&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/mnotgod96/AppAgent&quot;&gt;AppAgent: Multimodal Agents as Smartphone Users&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/X-PLUG/mPLUG-Owl&quot;&gt;mPLUG-Owl &amp;amp; mPLUG-Owl2: Modularized Multimodal Large Language Model&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen-VL&quot;&gt;Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/IDEA-Research/GroundingDINO&quot;&gt;GroundingDINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/openai/CLIP&quot;&gt;CLIP: Contrastive Language-Image Pretraining&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>opendatalab/MinerU</title>
      <link>https://github.com/opendatalab/MinerU</link>
      <description>&lt;p&gt;A high-quality tool for convert PDF to Markdown and JSON.一站式开源高质量数据提取工具，将PDF转换成Markdown和JSON格式。&lt;/p&gt;&lt;hr&gt;&lt;div align=&quot;center&quot; xmlns=&quot;http://www.w3.org/1999/html&quot;&gt; 
 &lt;!-- logo --&gt; 
 &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/opendatalab/MinerU/master/docs/images/MinerU-logo.png&quot; width=&quot;300px&quot; style=&quot;vertical-align:middle;&quot;&gt; &lt;/p&gt; 
 &lt;!-- icon --&gt; 
 &lt;p&gt;&lt;a href=&quot;https://github.com/opendatalab/MinerU&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/opendatalab/MinerU.svg?sanitize=true&quot; alt=&quot;stars&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/opendatalab/MinerU&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/forks/opendatalab/MinerU.svg?sanitize=true&quot; alt=&quot;forks&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/opendatalab/MinerU/issues&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/issues-raw/opendatalab/MinerU&quot; alt=&quot;open issues&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/opendatalab/MinerU/issues&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/issues-closed-raw/opendatalab/MinerU&quot; alt=&quot;issue resolution&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://badge.fury.io/py/magic-pdf&quot;&gt;&lt;img src=&quot;https://badge.fury.io/py/magic-pdf.svg?sanitize=true&quot; alt=&quot;PyPI version&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://pepy.tech/project/magic-pdf&quot;&gt;&lt;img src=&quot;https://static.pepy.tech/badge/magic-pdf&quot; alt=&quot;Downloads&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://pepy.tech/project/magic-pdf&quot;&gt;&lt;img src=&quot;https://static.pepy.tech/badge/magic-pdf/month&quot; alt=&quot;Downloads&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href=&quot;https://mineru.net/OpenSourceTools/Extractor?source=github&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Demo_on_OpenDataLab-blue?logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTM0IiBoZWlnaHQ9IjEzNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBkPSJtMTIyLDljMCw1LTQsOS05LDlzLTktNC05LTksNC05LDktOSw5LDQsOSw5eiIgZmlsbD0idXJsKCNhKSIvPjxwYXRoIGQ9Im0xMjIsOWMwLDUtNCw5LTksOXMtOS00LTktOSw0LTksOS05LDksNCw5LDl6IiBmaWxsPSIjMDEwMTAxIi8+PHBhdGggZD0ibTkxLDE4YzAsNS00LDktOSw5cy05LTQtOS05LDQtOSw5LTksOSw0LDksOXoiIGZpbGw9InVybCgjYikiLz48cGF0aCBkPSJtOTEsMThjMCw1LTQsOS05LDlzLTktNC05LTksNC05LDktOSw5LDQsOSw5eiIgZmlsbD0iIzAxMDEwMSIvPjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJtMzksNjJjMCwxNiw4LDMwLDIwLDM4LDctNiwxMi0xNiwxMi0yNlY0OWMwLTQsMy03LDYtOGw0Ni0xMmM1LTEsMTEsMywxMSw4djMxYzAsMzctMzAsNjYtNjYsNjYtMzcsMC02Ni0zMC02Ni02NlY0NmMwLTQsMy03LDYtOGwyMC02YzUtMSwxMSwzLDExLDh2MjF6bS0yOSw2YzAsMTYsNiwzMCwxNyw0MCwzLDEsNSwxLDgsMSw1LDAsMTAtMSwxNS0zQzM3LDk1LDI5LDc5LDI5LDYyVjQybC0xOSw1djIweiIgZmlsbD0idXJsKCNjKSIvPjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJtMzksNjJjMCwxNiw4LDMwLDIwLDM4LDctNiwxMi0xNiwxMi0yNlY0OWMwLTQsMy03LDYtOGw0Ni0xMmM1LTEsMTEsMywxMSw4djMxYzAsMzctMzAsNjYtNjYsNjYtMzcsMC02Ni0zMC02Ni02NlY0NmMwLTQsMy03LDYtOGwyMC02YzUtMSwxMSwzLDExLDh2MjF6bS0yOSw2YzAsMTYsNiwzMCwxNyw0MCwzLDEsNSwxLDgsMSw1LDAsMTAtMSwxNS0zQzM3LDk1LDI5LDc5LDI5LDYyVjQybC0xOSw1djIweiIgZmlsbD0iIzAxMDEwMSIvPjxkZWZzPjxsaW5lYXJHcmFkaWVudCBpZD0iYSIgeDE9Ijg0IiB5MT0iNDEiIHgyPSI3NSIgeTI9IjEyMCIgZ3JhZGllbnRVbml0cz0idXNlclNwYWNlT25Vc2UiPjxzdG9wIHN0b3AtY29sb3I9IiNmZmYiLz48c3RvcCBvZmZzZXQ9IjEiIHN0b3AtY29sb3I9IiMyZTJlMmUiLz48L2xpbmVhckdyYWRpZW50PjxsaW5lYXJHcmFkaWVudCBpZD0iYiIgeDE9Ijg0IiB5MT0iNDEiIHgyPSI3NSIgeTI9IjEyMCIgZ3JhZGllbnRVbml0cz0idXNlclNwYWNlT25Vc2UiPjxzdG9wIHN0b3AtY29sb3I9IiNmZmYiLz48c3RvcCBvZmZzZXQ9IjEiIHN0b3AtY29sb3I9IiMyZTJlMmUiLz48L2xpbmVhckdyYWRpZW50PjxsaW5lYXJHcmFkaWVudCBpZD0iYyIgeDE9Ijg0IiB5MT0iNDEiIHgyPSI3NSIgeTI9IjEyMCIgZ3JhZGllbnRVbml0cz0idXNlclNwYWNlT25Vc2UiPjxzdG9wIHN0b3AtY29sb3I9IiNmZmYiLz48c3RvcCBvZmZzZXQ9IjEiIHN0b3AtY29sb3I9IiMyZTJlMmUiLz48L2xpbmVhckdyYWRpZW50PjwvZGVmcz48L3N2Zz4=&amp;amp;labelColor=white&quot; alt=&quot;OpenDataLab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://huggingface.co/spaces/opendatalab/MinerU&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Demo_on_HuggingFace-yellow.svg?logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAF8AAABYCAMAAACkl9t/AAAAk1BMVEVHcEz/nQv/nQv/nQr/nQv/nQr/nQv/nQv/nQr/wRf/txT/pg7/yRr/rBD/zRz/ngv/oAz/zhz/nwv/txT/ngv/0B3+zBz/nQv/0h7/wxn/vRb/thXkuiT/rxH/pxD/ogzcqyf/nQvTlSz/czCxky7/SjifdjT/Mj3+Mj3wMj15aTnDNz+DSD9RTUBsP0FRO0Q6O0WyIxEIAAAAGHRSTlMADB8zSWF3krDDw8TJ1NbX5efv8ff9/fxKDJ9uAAAGKklEQVR42u2Z63qjOAyGC4RwCOfB2JAGqrSb2WnTw/1f3UaWcSGYNKTdf/P+mOkTrE+yJBulvfvLT2A5ruenaVHyIks33npl/6C4s/ZLAM45SOi/1FtZPyFur1OYofBX3w7d54Bxm+E8db+nDr12ttmESZ4zludJEG5S7TO72YPlKZFyE+YCYUJTBZsMiNS5Sd7NlDmKM2Eg2JQg8awbglfqgbhArjxkS7dgp2RH6hc9AMLdZYUtZN5DJr4molC8BfKrEkPKEnEVjLbgW1fLy77ZVOJagoIcLIl+IxaQZGjiX597HopF5CkaXVMDO9Pyix3AFV3kw4lQLCbHuMovz8FallbcQIJ5Ta0vks9RnolbCK84BtjKRS5uA43hYoZcOBGIG2Epbv6CvFVQ8m8loh66WNySsnN7htL58LNp+NXT8/PhXiBXPMjLSxtwp8W9f/1AngRierBkA+kk/IpUSOeKByzn8y3kAAAfh//0oXgV4roHm/kz4E2z//zRc3/lgwBzbM2mJxQEa5pqgX7d1L0htrhx7LKxOZlKbwcAWyEOWqYSI8YPtgDQVjpB5nvaHaSnBaQSD6hweDi8PosxD6/PT09YY3xQA7LTCTKfYX+QHpA0GCcqmEHvr/cyfKQTEuwgbs2kPxJEB0iNjfJcCTPyocx+A0griHSmADiC91oNGVwJ69RudYe65vJmoqfpul0lrqXadW0jFKH5BKwAeCq+Den7s+3zfRJzA61/Uj/9H/VzLKTx9jFPPdXeeP+L7WEvDLAKAIoF8bPTKT0+TM7W8ePj3Rz/Yn3kOAp2f1Kf0Weony7pn/cPydvhQYV+eFOfmOu7VB/ViPe34/EN3RFHY/yRuT8ddCtMPH/McBAT5s+vRde/gf2c/sPsjLK+m5IBQF5tO+h2tTlBGnP6693JdsvofjOPnnEHkh2TnV/X1fBl9S5zrwuwF8NFrAVJVwCAPTe8gaJlomqlp0pv4Pjn98tJ/t/fL++6unpR1YGC2n/KCoa0tTLoKiEeUPDl94nj+5/Tv3/eT5vBQ60X1S0oZr+IWRR8Ldhu7AlLjPISlJcO9vrFotky9SpzDequlwEir5beYAc0R7D9KS1DXva0jhYRDXoExPdc6yw5GShkZXe9QdO/uOvHofxjrV/TNS6iMJS+4TcSTgk9n5agJdBQbB//IfF/HpvPt3Tbi7b6I6K0R72p6ajryEJrENW2bbeVUGjfgoals4L443c7BEE4mJO2SpbRngxQrAKRudRzGQ8jVOL2qDVjjI8K1gc3TIJ5KiFZ1q+gdsARPB4NQS4AjwVSt72DSoXNyOWUrU5mQ9nRYyjp89Xo7oRI6Bga9QNT1mQ/ptaJq5T/7WcgAZywR/XlPGAUDdet3LE+qS0TI+g+aJU8MIqjo0Kx8Ly+maxLjJmjQ18rA0YCkxLQbUZP1WqdmyQGJLUm7VnQFqodmXSqmRrdVpqdzk5LvmvgtEcW8PMGdaS23EOWyDVbACZzUJPaqMbjDxpA3Qrgl0AikimGDbqmyT8P8NOYiqrldF8rX+YN7TopX4UoHuSCYY7cgX4gHwclQKl1zhx0THf+tCAUValzjI7Wg9EhptrkIcfIJjA94evOn8B2eHaVzvBrnl2ig0So6hvPaz0IGcOvTHvUIlE2+prqAxLSQxZlU2stql1NqCCLdIiIN/i1DBEHUoElM9dBravbiAnKqgpi4IBkw+utSPIoBijDXJipSVV7MpOEJUAc5Qmm3BnUN+w3hteEieYKfRZSIUcXKMVf0u5wD4EwsUNVvZOtUT7A2GkffHjByWpHqvRBYrTV72a6j8zZ6W0DTE86Hn04bmyWX3Ri9WH7ZU6Q7h+ZHo0nHUAcsQvVhXRDZHChwiyi/hnPuOsSEF6Exk3o6Y9DT1eZ+6cASXk2Y9k+6EOQMDGm6WBK10wOQJCBwren86cPPWUcRAnTVjGcU1LBgs9FURiX/e6479yZcLwCBmTxiawEwrOcleuu12t3tbLv/N4RLYIBhYexm7Fcn4OJcn0+zc+s8/VfPeddZHAGN6TT8eGczHdR/Gts1/MzDkThr23zqrVfAMFT33Nx1RJsx1k5zuWILLnG/vsH+Fv5D4NTVcp1Gzo8AAAAAElFTkSuQmCC&amp;amp;labelColor=white&quot; alt=&quot;HuggingFace&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://www.modelscope.cn/studios/OpenDataLab/MinerU&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Demo_on_ModelScope-purple?logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIzIiBoZWlnaHQ9IjIwMCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCiA8Zz4KICA8dGl0bGU+TGF5ZXIgMTwvdGl0bGU+CiAgPHBhdGggaWQ9InN2Z18xNCIgZmlsbD0iIzYyNGFmZiIgZD0ibTAsODkuODRsMjUuNjUsMGwwLDI1LjY0OTk5bC0yNS42NSwwbDAsLTI1LjY0OTk5eiIvPgogIDxwYXRoIGlkPSJzdmdfMTUiIGZpbGw9IiM2MjRhZmYiIGQ9Im05OS4xNCwxMTUuNDlsMjUuNjUsMGwwLDI1LjY1bC0yNS42NSwwbDAsLTI1LjY1eiIvPgogIDxwYXRoIGlkPSJzdmdfMTYiIGZpbGw9IiM2MjRhZmYiIGQ9Im0xNzYuMDksMTQxLjE0bC0yNS42NDk5OSwwbDAsMjIuMTlsNDcuODQsMGwwLC00Ny44NGwtMjIuMTksMGwwLDI1LjY1eiIvPgogIDxwYXRoIGlkPSJzdmdfMTciIGZpbGw9IiMzNmNmZDEiIGQ9Im0xMjQuNzksODkuODRsMjUuNjUsMGwwLDI1LjY0OTk5bC0yNS42NSwwbDAsLTI1LjY0OTk5eiIvPgogIDxwYXRoIGlkPSJzdmdfMTgiIGZpbGw9IiMzNmNmZDEiIGQ9Im0wLDY0LjE5bDI1LjY1LDBsMCwyNS42NWwtMjUuNjUsMGwwLC0yNS42NXoiLz4KICA8cGF0aCBpZD0ic3ZnXzE5IiBmaWxsPSIjNjI0YWZmIiBkPSJtMTk4LjI4LDg5Ljg0bDI1LjY0OTk5LDBsMCwyNS42NDk5OWwtMjUuNjQ5OTksMGwwLC0yNS42NDk5OXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIwIiBmaWxsPSIjMzZjZmQxIiBkPSJtMTk4LjI4LDY0LjE5bDI1LjY0OTk5LDBsMCwyNS42NWwtMjUuNjQ5OTksMGwwLC0yNS42NXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIxIiBmaWxsPSIjNjI0YWZmIiBkPSJtMTUwLjQ0LDQybDAsMjIuMTlsMjUuNjQ5OTksMGwwLDI1LjY1bDIyLjE5LDBsMCwtNDcuODRsLTQ3Ljg0LDB6Ii8+CiAgPHBhdGggaWQ9InN2Z18yMiIgZmlsbD0iIzM2Y2ZkMSIgZD0ibTczLjQ5LDg5Ljg0bDI1LjY1LDBsMCwyNS42NDk5OWwtMjUuNjUsMGwwLC0yNS42NDk5OXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIzIiBmaWxsPSIjNjI0YWZmIiBkPSJtNDcuODQsNjQuMTlsMjUuNjUsMGwwLC0yMi4xOWwtNDcuODQsMGwwLDQ3Ljg0bDIyLjE5LDBsMCwtMjUuNjV6Ii8+CiAgPHBhdGggaWQ9InN2Z18yNCIgZmlsbD0iIzYyNGFmZiIgZD0ibTQ3Ljg0LDExNS40OWwtMjIuMTksMGwwLDQ3Ljg0bDQ3Ljg0LDBsMCwtMjIuMTlsLTI1LjY1LDBsMCwtMjUuNjV6Ii8+CiA8L2c+Cjwvc3ZnPg==&amp;amp;labelColor=white&quot; alt=&quot;ModelScope&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/gist/myhloli/3b3a00a4a0a61577b6c30f989092d20d/mineru_demo.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2409.18839&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Paper-arXiv-green&quot; alt=&quot;Paper&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href=&quot;https://trendshift.io/repositories/11174&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/11174&quot; alt=&quot;opendatalab%2FMinerU | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;!-- language --&gt; 
 &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/opendatalab/MinerU/master/README.md&quot;&gt;English&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/opendatalab/MinerU/master/README_zh-CN.md&quot;&gt;简体中文&lt;/a&gt;&lt;/p&gt; 
 &lt;!-- hot link --&gt; 
 &lt;p align=&quot;center&quot;&gt; &lt;a href=&quot;https://github.com/opendatalab/PDF-Extract-Kit&quot;&gt;PDF-Extract-Kit: High-Quality PDF Extraction Toolkit&lt;/a&gt;🔥🔥🔥 &lt;br&gt; &lt;br&gt; &lt;a href=&quot;https://mineru.net/client?source=github&quot;&gt; Easier to use: Just grab MinerU Desktop. No coding, no login, just a simple interface and smooth interactions. Enjoy it without any fuss!&lt;/a&gt;🚀🚀🚀 &lt;/p&gt; 
 &lt;!-- join us --&gt; 
 &lt;p align=&quot;center&quot;&gt; 👋 join us on &lt;a href=&quot;https://discord.gg/Tdedn9GTXq&quot; target=&quot;_blank&quot;&gt;Discord&lt;/a&gt; and &lt;a href=&quot;http://mineru.space/s/V85Yl&quot; target=&quot;_blank&quot;&gt;WeChat&lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h1&gt;Changelog&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;2025/03/03 1.2.1 released, fixed several bugs: 
  &lt;ul&gt; 
   &lt;li&gt;Fixed the impact on punctuation marks during full-width to half-width conversion of letters and numbers&lt;/li&gt; 
   &lt;li&gt;Fixed caption matching inaccuracies in certain scenarios&lt;/li&gt; 
   &lt;li&gt;Fixed formula span loss issues in certain scenarios&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;2025/02/24 1.2.0 released. This version includes several fixes and improvements to enhance parsing efficiency and accuracy: 
  &lt;ul&gt; 
   &lt;li&gt;Performance Optimization 
    &lt;ul&gt; 
     &lt;li&gt;Increased classification speed for PDF documents in auto mode.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Parsing Optimization 
    &lt;ul&gt; 
     &lt;li&gt;Improved parsing logic for documents containing watermarks, significantly enhancing the parsing results for such documents.&lt;/li&gt; 
     &lt;li&gt;Enhanced the matching logic for multiple images/tables and captions within a single page, improving the accuracy of image-text matching in complex layouts.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Bug Fixes 
    &lt;ul&gt; 
     &lt;li&gt;Fixed an issue where image/table spans were incorrectly filled into text blocks under certain conditions.&lt;/li&gt; 
     &lt;li&gt;Resolved an issue where title blocks were empty in some cases.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;2025/01/22 1.1.0 released. In this version we have focused on improving parsing accuracy and efficiency: 
  &lt;ul&gt; 
   &lt;li&gt;Model capability upgrade (requires re-executing the &lt;a href=&quot;https://raw.githubusercontent.com/opendatalab/MinerU/master/docs/how_to_download_models_en.md&quot;&gt;model download process&lt;/a&gt; to obtain incremental updates of model files) 
    &lt;ul&gt; 
     &lt;li&gt;The layout recognition model has been upgraded to the latest &lt;code&gt;doclayout_yolo(2501)&lt;/code&gt; model, improving layout recognition accuracy.&lt;/li&gt; 
     &lt;li&gt;The formula parsing model has been upgraded to the latest &lt;code&gt;unimernet(2501)&lt;/code&gt; model, improving formula recognition accuracy.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Performance optimization 
    &lt;ul&gt; 
     &lt;li&gt;On devices that meet certain configuration requirements (16GB+ VRAM), by optimizing resource usage and restructuring the processing pipeline, overall parsing speed has been increased by more than 50%.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Parsing effect optimization 
    &lt;ul&gt; 
     &lt;li&gt;Added a new heading classification feature (testing version, enabled by default) to the online demo(&lt;a href=&quot;https://mineru.net/OpenSourceTools/Extractor&quot;&gt;mineru.net&lt;/a&gt;/&lt;a href=&quot;https://huggingface.co/spaces/opendatalab/MinerU&quot;&gt;huggingface&lt;/a&gt;/&lt;a href=&quot;https://www.modelscope.cn/studios/OpenDataLab/MinerU&quot;&gt;modelscope&lt;/a&gt;), which supports hierarchical classification of headings, thereby enhancing document structuring.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;2025/01/10 1.0.1 released. This is our first official release, where we have introduced a completely new API interface and enhanced compatibility through extensive refactoring, as well as a brand new automatic language identification feature: 
  &lt;ul&gt; 
   &lt;li&gt;New API Interface 
    &lt;ul&gt; 
     &lt;li&gt;For the data-side API, we have introduced the Dataset class, designed to provide a robust and flexible data processing framework. This framework currently supports a variety of document formats, including images (.jpg and .png), PDFs, Word documents (.doc and .docx), and PowerPoint presentations (.ppt and .pptx). It ensures effective support for data processing tasks ranging from simple to complex.&lt;/li&gt; 
     &lt;li&gt;For the user-side API, we have meticulously designed the MinerU processing workflow as a series of composable Stages. Each Stage represents a specific processing step, allowing users to define new Stages according to their needs and creatively combine these stages to customize their data processing workflows.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Enhanced Compatibility 
    &lt;ul&gt; 
     &lt;li&gt;By optimizing the dependency environment and configuration items, we ensure stable and efficient operation on ARM architecture Linux systems.&lt;/li&gt; 
     &lt;li&gt;We have deeply integrated with Huawei Ascend NPU acceleration, providing autonomous and controllable high-performance computing capabilities. This supports the localization and development of AI application platforms in China. &lt;a href=&quot;https://raw.githubusercontent.com/opendatalab/MinerU/master/docs/README_Ascend_NPU_Acceleration_zh_CN.md&quot;&gt;Ascend NPU Acceleration&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Automatic Language Identification 
    &lt;ul&gt; 
     &lt;li&gt;By introducing a new language recognition model, setting the &lt;code&gt;lang&lt;/code&gt; configuration to &lt;code&gt;auto&lt;/code&gt; during document parsing will automatically select the appropriate OCR language model, improving the accuracy of scanned document parsing.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;2024/11/22 0.10.0 released. Introducing hybrid OCR text extraction capabilities, 
  &lt;ul&gt; 
   &lt;li&gt;Significantly improved parsing performance in complex text distribution scenarios such as dense formulas, irregular span regions, and text represented by images.&lt;/li&gt; 
   &lt;li&gt;Combines the dual advantages of accurate content extraction and faster speed in text mode, and more precise span/line region recognition in OCR mode.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;2024/11/15 0.9.3 released. Integrated &lt;a href=&quot;https://github.com/RapidAI/RapidTable&quot;&gt;RapidTable&lt;/a&gt; for table recognition, improving single-table parsing speed by more than 10 times, with higher accuracy and lower GPU memory usage.&lt;/li&gt; 
 &lt;li&gt;2024/11/06 0.9.2 released. Integrated the &lt;a href=&quot;https://huggingface.co/U4R/StructTable-InternVL2-1B&quot;&gt;StructTable-InternVL2-1B&lt;/a&gt; model for table recognition functionality.&lt;/li&gt; 
 &lt;li&gt;2024/10/31 0.9.0 released. This is a major new version with extensive code refactoring, addressing numerous issues, improving performance, reducing hardware requirements, and enhancing usability: 
  &lt;ul&gt; 
   &lt;li&gt;Refactored the sorting module code to use &lt;a href=&quot;https://github.com/ppaanngggg/layoutreader&quot;&gt;layoutreader&lt;/a&gt; for reading order sorting, ensuring high accuracy in various layouts.&lt;/li&gt; 
   &lt;li&gt;Refactored the paragraph concatenation module to achieve good results in cross-column, cross-page, cross-figure, and cross-table scenarios.&lt;/li&gt; 
   &lt;li&gt;Refactored the list and table of contents recognition functions, significantly improving the accuracy of list blocks and table of contents blocks, as well as the parsing of corresponding text paragraphs.&lt;/li&gt; 
   &lt;li&gt;Refactored the matching logic for figures, tables, and descriptive text, greatly enhancing the accuracy of matching captions and footnotes to figures and tables, and reducing the loss rate of descriptive text to near zero.&lt;/li&gt; 
   &lt;li&gt;Added multi-language support for OCR, supporting detection and recognition of 84 languages.For the list of supported languages, see &lt;a href=&quot;https://paddlepaddle.github.io/PaddleOCR/latest/en/ppocr/blog/multi_languages.html#5-support-languages-and-abbreviations&quot;&gt;OCR Language Support List&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;Added memory recycling logic and other memory optimization measures, significantly reducing memory usage. The memory requirement for enabling all acceleration features except table acceleration (layout/formula/OCR) has been reduced from 16GB to 8GB, and the memory requirement for enabling all acceleration features has been reduced from 24GB to 10GB.&lt;/li&gt; 
   &lt;li&gt;Optimized configuration file feature switches, adding an independent formula detection switch to significantly improve speed and parsing results when formula detection is not needed.&lt;/li&gt; 
   &lt;li&gt;Integrated &lt;a href=&quot;https://github.com/opendatalab/PDF-Extract-Kit&quot;&gt;PDF-Extract-Kit 1.0&lt;/a&gt;: 
    &lt;ul&gt; 
     &lt;li&gt;Added the self-developed &lt;code&gt;doclayout_yolo&lt;/code&gt; model, which speeds up processing by more than 10 times compared to the original solution while maintaining similar parsing effects, and can be freely switched with &lt;code&gt;layoutlmv3&lt;/code&gt; via the configuration file.&lt;/li&gt; 
     &lt;li&gt;Upgraded formula parsing to &lt;code&gt;unimernet 0.2.1&lt;/code&gt;, improving formula parsing accuracy while significantly reducing memory usage.&lt;/li&gt; 
     &lt;li&gt;Due to the repository change for &lt;code&gt;PDF-Extract-Kit 1.0&lt;/code&gt;, you need to re-download the model. Please refer to &lt;a href=&quot;https://raw.githubusercontent.com/opendatalab/MinerU/master/docs/how_to_download_models_en.md&quot;&gt;How to Download Models&lt;/a&gt; for detailed steps.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;2024/09/27 Version 0.8.1 released, Fixed some bugs, and providing a &lt;a href=&quot;https://raw.githubusercontent.com/opendatalab/MinerU/master/projects/web_demo/README.md&quot;&gt;localized deployment version&lt;/a&gt; of the &lt;a href=&quot;https://opendatalab.com/OpenSourceTools/Extractor/PDF/&quot;&gt;online demo&lt;/a&gt; and the &lt;a href=&quot;https://raw.githubusercontent.com/opendatalab/MinerU/master/projects/web/README.md&quot;&gt;front-end interface&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;2024/09/09: Version 0.8.0 released, supporting fast deployment with Dockerfile, and launching demos on Huggingface and Modelscope.&lt;/li&gt; 
 &lt;li&gt;2024/08/30: Version 0.7.1 released, add paddle tablemaster table recognition option&lt;/li&gt; 
 &lt;li&gt;2024/08/09: Version 0.7.0b1 released, simplified installation process, added table recognition functionality&lt;/li&gt; 
 &lt;li&gt;2024/08/01: Version 0.6.2b1 released, optimized dependency conflict issues and installation documentation&lt;/li&gt; 
 &lt;li&gt;2024/07/05: Initial open-source release&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- TABLE OF CONTENT --&gt; 
&lt;details open&gt; 
 &lt;summary&gt;&lt;h2 style=&quot;display: inline-block&quot;&gt;Table of Contents&lt;/h2&gt;&lt;/summary&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;a href=&quot;https://raw.githubusercontent.com/opendatalab/MinerU/master/#mineru&quot;&gt;MinerU&lt;/a&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/opendatalab/MinerU/master/#project-introduction&quot;&gt;Project Introduction&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/opendatalab/MinerU/master/#key-features&quot;&gt;Key Features&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/opendatalab/MinerU/master/#quick-start&quot;&gt;Quick Start&lt;/a&gt; 
     &lt;ul&gt; 
      &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/opendatalab/MinerU/master/#online-demo&quot;&gt;Online Demo&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/opendatalab/MinerU/master/#quick-cpu-demo&quot;&gt;Quick CPU Demo&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/opendatalab/MinerU/master/#using-gpu&quot;&gt;Using GPU&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/opendatalab/MinerU/master/#using-npu&quot;&gt;Using NPU&lt;/a&gt;&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
    &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/opendatalab/MinerU/master/#usage&quot;&gt;Usage&lt;/a&gt; 
     &lt;ul&gt; 
      &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/opendatalab/MinerU/master/#command-line&quot;&gt;Command Line&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/opendatalab/MinerU/master/#api&quot;&gt;API&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/opendatalab/MinerU/master/#deploy-derived-projects&quot;&gt;Deploy Derived Projects&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/opendatalab/MinerU/master/#development-guide&quot;&gt;Development Guide&lt;/a&gt;&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/opendatalab/MinerU/master/#todo&quot;&gt;TODO&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/opendatalab/MinerU/master/#known-issues&quot;&gt;Known Issues&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/opendatalab/MinerU/master/#faq&quot;&gt;FAQ&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/opendatalab/MinerU/master/#all-thanks-to-our-contributors&quot;&gt;All Thanks To Our Contributors&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/opendatalab/MinerU/master/#license-information&quot;&gt;License Information&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/opendatalab/MinerU/master/#acknowledgments&quot;&gt;Acknowledgments&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/opendatalab/MinerU/master/#citation&quot;&gt;Citation&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/opendatalab/MinerU/master/#star-history&quot;&gt;Star History&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/opendatalab/MinerU/master/#magic-doc&quot;&gt;Magic-doc&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/opendatalab/MinerU/master/#magic-html&quot;&gt;Magic-html&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/opendatalab/MinerU/master/#links&quot;&gt;Links&lt;/a&gt;&lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/details&gt; 
&lt;h1&gt;MinerU&lt;/h1&gt; 
&lt;h2&gt;Project Introduction&lt;/h2&gt; 
&lt;p&gt;MinerU is a tool that converts PDFs into machine-readable formats (e.g., markdown, JSON), allowing for easy extraction into any format. MinerU was born during the pre-training process of &lt;a href=&quot;https://github.com/InternLM/InternLM&quot;&gt;InternLM&lt;/a&gt;. We focus on solving symbol conversion issues in scientific literature and hope to contribute to technological development in the era of large models. Compared to well-known commercial products, MinerU is still young. If you encounter any issues or if the results are not as expected, please submit an issue on &lt;a href=&quot;https://github.com/opendatalab/MinerU/issues&quot;&gt;issue&lt;/a&gt; and &lt;strong&gt;attach the relevant PDF&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/user-attachments/assets/4bea02c9-6d54-4cd6-97ed-dff14340982c&quot;&gt;https://github.com/user-attachments/assets/4bea02c9-6d54-4cd6-97ed-dff14340982c&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Remove headers, footers, footnotes, page numbers, etc., to ensure semantic coherence.&lt;/li&gt; 
 &lt;li&gt;Output text in human-readable order, suitable for single-column, multi-column, and complex layouts.&lt;/li&gt; 
 &lt;li&gt;Preserve the structure of the original document, including headings, paragraphs, lists, etc.&lt;/li&gt; 
 &lt;li&gt;Extract images, image descriptions, tables, table titles, and footnotes.&lt;/li&gt; 
 &lt;li&gt;Automatically recognize and convert formulas in the document to LaTeX format.&lt;/li&gt; 
 &lt;li&gt;Automatically recognize and convert tables in the document to HTML format.&lt;/li&gt; 
 &lt;li&gt;Automatically detect scanned PDFs and garbled PDFs and enable OCR functionality.&lt;/li&gt; 
 &lt;li&gt;OCR supports detection and recognition of 84 languages.&lt;/li&gt; 
 &lt;li&gt;Supports multiple output formats, such as multimodal and NLP Markdown, JSON sorted by reading order, and rich intermediate formats.&lt;/li&gt; 
 &lt;li&gt;Supports various visualization results, including layout visualization and span visualization, for efficient confirmation of output quality.&lt;/li&gt; 
 &lt;li&gt;Supports running in a pure CPU environment, and also supports GPU(CUDA)/NPU(CANN)/MPS acceleration&lt;/li&gt; 
 &lt;li&gt;Compatible with Windows, Linux, and Mac platforms.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;p&gt;If you encounter any installation issues, please first consult the &lt;a href=&quot;https://raw.githubusercontent.com/opendatalab/MinerU/master/#faq&quot;&gt;FAQ&lt;/a&gt;. &lt;br&gt; If the parsing results are not as expected, refer to the &lt;a href=&quot;https://raw.githubusercontent.com/opendatalab/MinerU/master/#known-issues&quot;&gt;Known Issues&lt;/a&gt;. &lt;br&gt; There are three different ways to experience MinerU:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/opendatalab/MinerU/master/#online-demo&quot;&gt;Online Demo (No Installation Required)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/opendatalab/MinerU/master/#quick-cpu-demo&quot;&gt;Quick CPU Demo (Windows, Linux, Mac)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Accelerate inference by using CUDA/CANN/MPS 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/opendatalab/MinerU/master/#Using-GPU&quot;&gt;Linux/Windows + CUDA&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/opendatalab/MinerU/master/#using-npu&quot;&gt;Linux + CANN&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/opendatalab/MinerU/master/#using-mps&quot;&gt;MacOS + MPS&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING] &lt;strong&gt;Pre-installation Notice—Hardware and Software Environment Support&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;To ensure the stability and reliability of the project, we only optimize and test for specific hardware and software environments during development. This ensures that users deploying and running the project on recommended system configurations will get the best performance with the fewest compatibility issues.&lt;/p&gt; 
 &lt;p&gt;By focusing resources on the mainline environment, our team can more efficiently resolve potential bugs and develop new features.&lt;/p&gt; 
 &lt;p&gt;In non-mainline environments, due to the diversity of hardware and software configurations, as well as third-party dependency compatibility issues, we cannot guarantee 100% project availability. Therefore, for users who wish to use this project in non-recommended environments, we suggest carefully reading the documentation and FAQ first. Most issues already have corresponding solutions in the FAQ. We also encourage community feedback to help us gradually expand support.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td colspan=&quot;3&quot; rowspan=&quot;2&quot;&gt;Operating System&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Linux after 2019&lt;/td&gt; 
   &lt;td&gt;Windows 10 / 11&lt;/td&gt; 
   &lt;td&gt;macOS 11+&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td colspan=&quot;3&quot;&gt;CPU&lt;/td&gt; 
   &lt;td&gt;x86_64 / arm64&lt;/td&gt; 
   &lt;td&gt;x86_64(unsupported ARM Windows)&lt;/td&gt; 
   &lt;td&gt;x86_64 / arm64&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td colspan=&quot;3&quot;&gt;Memory Requirements&lt;/td&gt; 
   &lt;td colspan=&quot;3&quot;&gt;16GB or more, recommended 32GB+&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td colspan=&quot;3&quot;&gt;Storage Requirements&lt;/td&gt; 
   &lt;td colspan=&quot;3&quot;&gt;20GB or more, with a preference for SSD&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td colspan=&quot;3&quot;&gt;Python Version&lt;/td&gt; 
   &lt;td colspan=&quot;3&quot;&gt;3.10(Please make sure to create a Python 3.10 virtual environment using conda)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td colspan=&quot;3&quot;&gt;Nvidia Driver Version&lt;/td&gt; 
   &lt;td&gt;latest (Proprietary Driver)&lt;/td&gt; 
   &lt;td&gt;latest&lt;/td&gt; 
   &lt;td&gt;None&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td colspan=&quot;3&quot;&gt;CUDA Environment&lt;/td&gt; 
   &lt;td&gt;Automatic installation [12.1 (pytorch) + 11.8 (paddle)]&lt;/td&gt; 
   &lt;td&gt;11.8 (manual installation) + cuDNN v8.7.0 (manual installation)&lt;/td&gt; 
   &lt;td&gt;None&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td colspan=&quot;3&quot;&gt;CANN Environment(NPU support)&lt;/td&gt; 
   &lt;td&gt;8.0+(Ascend 910b)&lt;/td&gt; 
   &lt;td&gt;None&lt;/td&gt; 
   &lt;td&gt;None&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan=&quot;2&quot;&gt;GPU Hardware Support List&lt;/td&gt; 
   &lt;td colspan=&quot;2&quot;&gt;GPU VRAM 8GB or more&lt;/td&gt; 
   &lt;td colspan=&quot;2&quot;&gt;2080~2080Ti / 3060Ti~3090Ti / 4060~4090&lt;br&gt; 8G VRAM can enable all acceleration features&lt;/td&gt; 
   &lt;td rowspan=&quot;2&quot;&gt;None&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h3&gt;Online Demo&lt;/h3&gt; 
&lt;p&gt;Synced with dev branch updates:&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://mineru.net/OpenSourceTools/Extractor?source=github&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Demo_on_OpenDataLab-blue?logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTM0IiBoZWlnaHQ9IjEzNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBkPSJtMTIyLDljMCw1LTQsOS05LDlzLTktNC05LTksNC05LDktOSw5LDQsOSw5eiIgZmlsbD0idXJsKCNhKSIvPjxwYXRoIGQ9Im0xMjIsOWMwLDUtNCw5LTksOXMtOS00LTktOSw0LTksOS05LDksNCw5LDl6IiBmaWxsPSIjMDEwMTAxIi8+PHBhdGggZD0ibTkxLDE4YzAsNS00LDktOSw5cy05LTQtOS05LDQtOSw5LTksOSw0LDksOXoiIGZpbGw9InVybCgjYikiLz48cGF0aCBkPSJtOTEsMThjMCw1LTQsOS05LDlzLTktNC05LTksNC05LDktOSw5LDQsOSw5eiIgZmlsbD0iIzAxMDEwMSIvPjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJtMzksNjJjMCwxNiw4LDMwLDIwLDM4LDctNiwxMi0xNiwxMi0yNlY0OWMwLTQsMy03LDYtOGw0Ni0xMmM1LTEsMTEsMywxMSw4djMxYzAsMzctMzAsNjYtNjYsNjYtMzcsMC02Ni0zMC02Ni02NlY0NmMwLTQsMy03LDYtOGwyMC02YzUtMSwxMSwzLDExLDh2MjF6bS0yOSw2YzAsMTYsNiwzMCwxNyw0MCwzLDEsNSwxLDgsMSw1LDAsMTAtMSwxNS0zQzM3LDk1LDI5LDc5LDI5LDYyVjQybC0xOSw1djIweiIgZmlsbD0idXJsKCNjKSIvPjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJtMzksNjJjMCwxNiw4LDMwLDIwLDM4LDctNiwxMi0xNiwxMi0yNlY0OWMwLTQsMy03LDYtOGw0Ni0xMmM1LTEsMTEsMywxMSw4djMxYzAsMzctMzAsNjYtNjYsNjYtMzcsMC02Ni0zMC02Ni02NlY0NmMwLTQsMy03LDYtOGwyMC02YzUtMSwxMSwzLDExLDh2MjF6bS0yOSw2YzAsMTYsNiwzMCwxNyw0MCwzLDEsNSwxLDgsMSw1LDAsMTAtMSwxNS0zQzM3LDk1LDI5LDc5LDI5LDYyVjQybC0xOSw1djIweiIgZmlsbD0iIzAxMDEwMSIvPjxkZWZzPjxsaW5lYXJHcmFkaWVudCBpZD0iYSIgeDE9Ijg0IiB5MT0iNDEiIHgyPSI3NSIgeTI9IjEyMCIgZ3JhZGllbnRVbml0cz0idXNlclNwYWNlT25Vc2UiPjxzdG9wIHN0b3AtY29sb3I9IiNmZmYiLz48c3RvcCBvZmZzZXQ9IjEiIHN0b3AtY29sb3I9IiMyZTJlMmUiLz48L2xpbmVhckdyYWRpZW50PjxsaW5lYXJHcmFkaWVudCBpZD0iYiIgeDE9Ijg0IiB5MT0iNDEiIHgyPSI3NSIgeTI9IjEyMCIgZ3JhZGllbnRVbml0cz0idXNlclNwYWNlT25Vc2UiPjxzdG9wIHN0b3AtY29sb3I9IiNmZmYiLz48c3RvcCBvZmZzZXQ9IjEiIHN0b3AtY29sb3I9IiMyZTJlMmUiLz48L2xpbmVhckdyYWRpZW50PjxsaW5lYXJHcmFkaWVudCBpZD0iYyIgeDE9Ijg0IiB5MT0iNDEiIHgyPSI3NSIgeTI9IjEyMCIgZ3JhZGllbnRVbml0cz0idXNlclNwYWNlT25Vc2UiPjxzdG9wIHN0b3AtY29sb3I9IiNmZmYiLz48c3RvcCBvZmZzZXQ9IjEiIHN0b3AtY29sb3I9IiMyZTJlMmUiLz48L2xpbmVhckdyYWRpZW50PjwvZGVmcz48L3N2Zz4=&amp;amp;labelColor=white&quot; alt=&quot;OpenDataLab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://huggingface.co/spaces/opendatalab/MinerU&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Demo_on_HuggingFace-yellow.svg?logo=data:image/png;base64,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&amp;amp;labelColor=white&quot; alt=&quot;HuggingFace&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://www.modelscope.cn/studios/OpenDataLab/MinerU&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Demo_on_ModelScope-purple?logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIzIiBoZWlnaHQ9IjIwMCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCiA8Zz4KICA8dGl0bGU+TGF5ZXIgMTwvdGl0bGU+CiAgPHBhdGggaWQ9InN2Z18xNCIgZmlsbD0iIzYyNGFmZiIgZD0ibTAsODkuODRsMjUuNjUsMGwwLDI1LjY0OTk5bC0yNS42NSwwbDAsLTI1LjY0OTk5eiIvPgogIDxwYXRoIGlkPSJzdmdfMTUiIGZpbGw9IiM2MjRhZmYiIGQ9Im05OS4xNCwxMTUuNDlsMjUuNjUsMGwwLDI1LjY1bC0yNS42NSwwbDAsLTI1LjY1eiIvPgogIDxwYXRoIGlkPSJzdmdfMTYiIGZpbGw9IiM2MjRhZmYiIGQ9Im0xNzYuMDksMTQxLjE0bC0yNS42NDk5OSwwbDAsMjIuMTlsNDcuODQsMGwwLC00Ny44NGwtMjIuMTksMGwwLDI1LjY1eiIvPgogIDxwYXRoIGlkPSJzdmdfMTciIGZpbGw9IiMzNmNmZDEiIGQ9Im0xMjQuNzksODkuODRsMjUuNjUsMGwwLDI1LjY0OTk5bC0yNS42NSwwbDAsLTI1LjY0OTk5eiIvPgogIDxwYXRoIGlkPSJzdmdfMTgiIGZpbGw9IiMzNmNmZDEiIGQ9Im0wLDY0LjE5bDI1LjY1LDBsMCwyNS42NWwtMjUuNjUsMGwwLC0yNS42NXoiLz4KICA8cGF0aCBpZD0ic3ZnXzE5IiBmaWxsPSIjNjI0YWZmIiBkPSJtMTk4LjI4LDg5Ljg0bDI1LjY0OTk5LDBsMCwyNS42NDk5OWwtMjUuNjQ5OTksMGwwLC0yNS42NDk5OXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIwIiBmaWxsPSIjMzZjZmQxIiBkPSJtMTk4LjI4LDY0LjE5bDI1LjY0OTk5LDBsMCwyNS42NWwtMjUuNjQ5OTksMGwwLC0yNS42NXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIxIiBmaWxsPSIjNjI0YWZmIiBkPSJtMTUwLjQ0LDQybDAsMjIuMTlsMjUuNjQ5OTksMGwwLDI1LjY1bDIyLjE5LDBsMCwtNDcuODRsLTQ3Ljg0LDB6Ii8+CiAgPHBhdGggaWQ9InN2Z18yMiIgZmlsbD0iIzM2Y2ZkMSIgZD0ibTczLjQ5LDg5Ljg0bDI1LjY1LDBsMCwyNS42NDk5OWwtMjUuNjUsMGwwLC0yNS42NDk5OXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIzIiBmaWxsPSIjNjI0YWZmIiBkPSJtNDcuODQsNjQuMTlsMjUuNjUsMGwwLC0yMi4xOWwtNDcuODQsMGwwLDQ3Ljg0bDIyLjE5LDBsMCwtMjUuNjV6Ii8+CiAgPHBhdGggaWQ9InN2Z18yNCIgZmlsbD0iIzYyNGFmZiIgZD0ibTQ3Ljg0LDExNS40OWwtMjIuMTksMGwwLDQ3Ljg0bDQ3Ljg0LDBsMCwtMjIuMTlsLTI1LjY1LDBsMCwtMjUuNjV6Ii8+CiA8L2c+Cjwvc3ZnPg==&amp;amp;labelColor=white&quot; alt=&quot;ModelScope&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Quick CPU Demo&lt;/h3&gt; 
&lt;h4&gt;1. Install magic-pdf&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;conda create -n mineru python=3.10
conda activate mineru
pip install -U &quot;magic-pdf[full]&quot; --extra-index-url https://wheels.myhloli.com
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;2. Download model weight files&lt;/h4&gt; 
&lt;p&gt;Refer to &lt;a href=&quot;https://raw.githubusercontent.com/opendatalab/MinerU/master/docs/how_to_download_models_en.md&quot;&gt;How to Download Model Files&lt;/a&gt; for detailed instructions.&lt;/p&gt; 
&lt;h4&gt;3. Modify the Configuration File for Additional Configuration&lt;/h4&gt; 
&lt;p&gt;After completing the &lt;a href=&quot;https://raw.githubusercontent.com/opendatalab/MinerU/master/#2-download-model-weight-files&quot;&gt;2. Download model weight files&lt;/a&gt; step, the script will automatically generate a &lt;code&gt;magic-pdf.json&lt;/code&gt; file in the user directory and configure the default model path. You can find the &lt;code&gt;magic-pdf.json&lt;/code&gt; file in your 【user directory】.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] The user directory for Windows is &quot;C:\Users\username&quot;, for Linux it is &quot;/home/username&quot;, and for macOS it is &quot;/Users/username&quot;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;You can modify certain configurations in this file to enable or disable features, such as table recognition:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] If the following items are not present in the JSON, please manually add the required items and remove the comment content (standard JSON does not support comments).&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class=&quot;language-json&quot;&gt;{
    // other config
    &quot;layout-config&quot;: {
        &quot;model&quot;: &quot;doclayout_yolo&quot; // Please change to &quot;layoutlmv3&quot; when using layoutlmv3.
    },
    &quot;formula-config&quot;: {
        &quot;mfd_model&quot;: &quot;yolo_v8_mfd&quot;,
        &quot;mfr_model&quot;: &quot;unimernet_small&quot;,
        &quot;enable&quot;: true  // The formula recognition feature is enabled by default. If you need to disable it, please change the value here to &quot;false&quot;.
    },
    &quot;table-config&quot;: {
        &quot;model&quot;: &quot;rapid_table&quot;,  // Default to using &quot;rapid_table&quot;, can be switched to &quot;tablemaster&quot; or &quot;struct_eqtable&quot;.
        &quot;sub_model&quot;: &quot;slanet_plus&quot;,  // When the model is &quot;rapid_table&quot;, you can choose a sub_model. The options are &quot;slanet_plus&quot; and &quot;unitable&quot;
        &quot;enable&quot;: true, // The table recognition feature is enabled by default. If you need to disable it, please change the value here to &quot;false&quot;.
        &quot;max_time&quot;: 400
    }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Using GPU&lt;/h3&gt; 
&lt;p&gt;If your device supports CUDA and meets the GPU requirements of the mainline environment, you can use GPU acceleration. Please select the appropriate guide based on your system:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/opendatalab/MinerU/master/docs/README_Ubuntu_CUDA_Acceleration_en_US.md&quot;&gt;Ubuntu 22.04 LTS + GPU&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/opendatalab/MinerU/master/docs/README_Windows_CUDA_Acceleration_en_US.md&quot;&gt;Windows 10/11 + GPU&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Quick Deployment with Docker&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] Docker requires a GPU with at least 8GB of VRAM, and all acceleration features are enabled by default.&lt;/p&gt; 
 &lt;p&gt;Before running this Docker, you can use the following command to check if your device supports CUDA acceleration on Docker.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;docker run --rm --gpus=all nvidia/cuda:12.1.0-base-ubuntu22.04 nvidia-smi
&lt;/code&gt;&lt;/pre&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;wget https://github.com/opendatalab/MinerU/raw/master/docker/global/Dockerfile -O Dockerfile
docker build -t mineru:latest .
docker run -it --name mineru --gpus=all mineru:latest /bin/bash -c &quot;echo &#39;source /opt/mineru_venv/bin/activate&#39; &amp;gt;&amp;gt; ~/.bashrc &amp;amp;&amp;amp; exec bash&quot;
magic-pdf --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Using NPU&lt;/h3&gt; 
&lt;p&gt;If your device has NPU acceleration hardware, you can follow the tutorial below to use NPU acceleration:&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/opendatalab/MinerU/master/docs/README_Ascend_NPU_Acceleration_zh_CN.md&quot;&gt;Ascend NPU Acceleration&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Using MPS&lt;/h3&gt; 
&lt;p&gt;If your device uses Apple silicon chips, you can enable MPS acceleration for certain supported tasks (such as layout detection and formula detection).&lt;/p&gt; 
&lt;p&gt;You can enable MPS acceleration by setting the &lt;code&gt;device-mode&lt;/code&gt; parameter to &lt;code&gt;mps&lt;/code&gt; in the &lt;code&gt;magic-pdf.json&lt;/code&gt; configuration file.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-json&quot;&gt;{
    // other config
    &quot;device-mode&quot;: &quot;mps&quot;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] Since the formula recognition task cannot utilize MPS acceleration, you can disable the formula recognition feature in tasks where it is not needed to achieve optimal performance.&lt;/p&gt; 
 &lt;p&gt;You can disable the formula recognition feature by setting the &lt;code&gt;enable&lt;/code&gt; parameter in the &lt;code&gt;formula-config&lt;/code&gt; section to &lt;code&gt;false&lt;/code&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;h3&gt;Command Line&lt;/h3&gt; 
&lt;p&gt;&lt;a href=&quot;https://mineru.readthedocs.io/en/latest/user_guide/usage/command_line.html&quot;&gt;Using MinerU via Command Line&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] For more information about the output files, please refer to the &lt;a href=&quot;https://raw.githubusercontent.com/opendatalab/MinerU/master/docs/output_file_en_us.md&quot;&gt;Output File Description&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;API&lt;/h3&gt; 
&lt;p&gt;&lt;a href=&quot;https://mineru.readthedocs.io/en/latest/user_guide/usage/api.html&quot;&gt;Using MinerU via Python API&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Deploy Derived Projects&lt;/h3&gt; 
&lt;p&gt;Derived projects include secondary development projects based on MinerU by project developers and community developers,&lt;br&gt; such as application interfaces based on Gradio, RAG based on llama, web demos similar to the official website, lightweight multi-GPU load balancing client/server ends, etc. These projects may offer more features and a better user experience.&lt;br&gt; For specific deployment methods, please refer to the &lt;a href=&quot;https://raw.githubusercontent.com/opendatalab/MinerU/master/projects/README.md&quot;&gt;Derived Project README&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Development Guide&lt;/h3&gt; 
&lt;p&gt;TODO&lt;/p&gt; 
&lt;h1&gt;TODO&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; Reading order based on the model&lt;/li&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; Recognition of &lt;code&gt;index&lt;/code&gt; and &lt;code&gt;list&lt;/code&gt; in the main text&lt;/li&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; Table recognition&lt;/li&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; Heading Classification&lt;/li&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; disabled&gt; Code block recognition in the main text&lt;/li&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; disabled&gt; &lt;a href=&quot;https://raw.githubusercontent.com/opendatalab/MinerU/master/docs/chemical_knowledge_introduction/introduction.pdf&quot;&gt;Chemical formula recognition&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; disabled&gt; Geometric shape recognition&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Known Issues&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;Reading order is determined by the model based on the spatial distribution of readable content, and may be out of order in some areas under extremely complex layouts.&lt;/li&gt; 
 &lt;li&gt;Vertical text is not supported.&lt;/li&gt; 
 &lt;li&gt;Tables of contents and lists are recognized through rules, and some uncommon list formats may not be recognized.&lt;/li&gt; 
 &lt;li&gt;Code blocks are not yet supported in the layout model.&lt;/li&gt; 
 &lt;li&gt;Comic books, art albums, primary school textbooks, and exercises cannot be parsed well.&lt;/li&gt; 
 &lt;li&gt;Table recognition may result in row/column recognition errors in complex tables.&lt;/li&gt; 
 &lt;li&gt;OCR recognition may produce inaccurate characters in PDFs of lesser-known languages (e.g., diacritical marks in Latin script, easily confused characters in Arabic script).&lt;/li&gt; 
 &lt;li&gt;Some formulas may not render correctly in Markdown.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;FAQ&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/opendatalab/MinerU/master/docs/FAQ_zh_cn.md&quot;&gt;FAQ in Chinese&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/opendatalab/MinerU/master/docs/FAQ_en_us.md&quot;&gt;FAQ in English&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;All Thanks To Our Contributors&lt;/h1&gt; 
&lt;a href=&quot;https://github.com/opendatalab/MinerU/graphs/contributors&quot;&gt; &lt;img src=&quot;https://contrib.rocks/image?repo=opendatalab/MinerU&quot;&gt; &lt;/a&gt; 
&lt;h1&gt;License Information&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/opendatalab/MinerU/master/LICENSE.md&quot;&gt;LICENSE.md&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;This project currently uses PyMuPDF to achieve advanced functionality. However, since it adheres to the AGPL license, it may impose restrictions on certain usage scenarios. In future iterations, we plan to explore and replace it with a more permissive PDF processing library to enhance user-friendliness and flexibility.&lt;/p&gt; 
&lt;h1&gt;Acknowledgments&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/opendatalab/PDF-Extract-Kit&quot;&gt;PDF-Extract-Kit&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/opendatalab/DocLayout-YOLO&quot;&gt;DocLayout-YOLO&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/UniModal4Reasoning/StructEqTable-Deploy&quot;&gt;StructEqTable&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/RapidAI/RapidTable&quot;&gt;RapidTable&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR&quot;&gt;PaddleOCR&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/pymupdf/PyMuPDF&quot;&gt;PyMuPDF&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ppaanngggg/layoutreader&quot;&gt;layoutreader&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/LlmKira/fast-langdetect&quot;&gt;fast-langdetect&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/pdfminer/pdfminer.six&quot;&gt;pdfminer.six&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Citation&lt;/h1&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bibtex&quot;&gt;@misc{wang2024mineruopensourcesolutionprecise,
      title={MinerU: An Open-Source Solution for Precise Document Content Extraction}, 
      author={Bin Wang and Chao Xu and Xiaomeng Zhao and Linke Ouyang and Fan Wu and Zhiyuan Zhao and Rui Xu and Kaiwen Liu and Yuan Qu and Fukai Shang and Bo Zhang and Liqun Wei and Zhihao Sui and Wei Li and Botian Shi and Yu Qiao and Dahua Lin and Conghui He},
      year={2024},
      eprint={2409.18839},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2409.18839}, 
}

@article{he2024opendatalab,
  title={Opendatalab: Empowering general artificial intelligence with open datasets},
  author={He, Conghui and Li, Wei and Jin, Zhenjiang and Xu, Chao and Wang, Bin and Lin, Dahua},
  journal={arXiv preprint arXiv:2407.13773},
  year={2024}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;Star History&lt;/h1&gt; 
&lt;a&gt; 
 &lt;picture&gt; 
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://api.star-history.com/svg?repos=opendatalab/MinerU&amp;amp;type=Date&amp;amp;theme=dark&quot;&gt; 
  &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://api.star-history.com/svg?repos=opendatalab/MinerU&amp;amp;type=Date&quot;&gt; 
  &lt;img alt=&quot;Star History Chart&quot; src=&quot;https://api.star-history.com/svg?repos=opendatalab/MinerU&amp;amp;type=Date&quot;&gt; 
 &lt;/picture&gt; &lt;/a&gt; 
&lt;h1&gt;Magic-doc&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/InternLM/magic-doc&quot;&gt;Magic-Doc&lt;/a&gt; Fast speed ppt/pptx/doc/docx/pdf extraction tool&lt;/p&gt; 
&lt;h1&gt;Magic-html&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/opendatalab/magic-html&quot;&gt;Magic-HTML&lt;/a&gt; Mixed web page extraction tool&lt;/p&gt; 
&lt;h1&gt;Links&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/opendatalab/labelU&quot;&gt;LabelU (A Lightweight Multi-modal Data Annotation Tool)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/opendatalab/LabelLLM&quot;&gt;LabelLLM (An Open-source LLM Dialogue Annotation Platform)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/opendatalab/PDF-Extract-Kit&quot;&gt;PDF-Extract-Kit (A Comprehensive Toolkit for High-Quality PDF Content Extraction)&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>QiuChenly/InjectLib</title>
      <link>https://github.com/QiuChenly/InjectLib</link>
      <description>&lt;p&gt;你知道我要说什么&lt;/p&gt;&lt;hr&gt;&lt;a href=&quot;https://github.com/QiuChenly/InjectLib&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/QiuChenly/InjectLib/main/style.svg?sanitize=true&quot; width=&quot;800&quot; height=&quot;590&quot;&gt; &lt;/a&gt; 
&lt;h2&gt;荣誉贡献榜&lt;/h2&gt; 
&lt;p&gt;&lt;img src=&quot;https://img.shields.io/github/contributors/QiuChenly/InjectLib.svg?style=flat-square&quot; alt=&quot;GitHub contributors&quot;&gt;&lt;/p&gt; 
&lt;a href=&quot;https://github.com/QiuChenly/InjectLib/graphs/contributors&quot;&gt; &lt;img src=&quot;https://contrib.rocks/image?repo=QiuChenly/InjectLib&quot;&gt; &lt;/a&gt; 
&lt;br&gt; 
&lt;br&gt; 
&lt;p align=&quot;center&quot;&gt;🔞全球各❤️地❤️服❤️务🔞&lt;br&gt; 🔞①线至①⑧线城市齐全🔞&lt;br&gt; 🔞汇❤️编 🚗与🤤ASM约❤️会🔞&lt;br&gt; 🔞 一❤️个❤️人独自在家❤️火❤️热❤️难❤️耐 玩🔞逆🔞向 🔞&lt;br&gt; 🔞找Qiu❤️Chen❤️l❤️y❤️Open❤️Source🔞&lt;br&gt;&lt;/p&gt; 
&lt;h1&gt;你这玩意我怎么使用？&lt;/h1&gt; 
&lt;p&gt;下载仓库zip，解压后终端cd到这个目录，执行&quot;python3 main.py&quot; 选择你中意的程序注入即可。&lt;/p&gt; 
&lt;p&gt;仍然遇到问题？你有两种选择。&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;加群询问获取帮助: &lt;strong&gt;&lt;a href=&quot;https://t.me/+VvqTr-2EFaZhYzA1&quot;&gt;https://t.me/+VvqTr-2EFaZhYzA1&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;继续向下阅读并访问文档说明使用:&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;点下面的链接进入在线文档，有操作指南。&lt;/p&gt; 
&lt;p&gt;目前还没有写完，如果有没有的、看不懂的内容可以提Issues。&lt;/p&gt; 
&lt;p&gt;点我查看➡️&lt;a href=&quot;https://qiuchenlyopensource.github.io/Documentaions/&quot;&gt;使用文档&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;加入小团体&lt;/h1&gt; 
&lt;p&gt;关注我的频道，进群获取最新的推送资讯。&lt;/p&gt; 
&lt;p&gt;頻道: &lt;strong&gt;&lt;a href=&quot;https://t.me/qiuchenlymac&quot;&gt;https://t.me/qiuchenlymac&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;群組: &lt;strong&gt;&lt;a href=&quot;https://t.me/+VvqTr-2EFaZhYzA1&quot;&gt;https://t.me/+VvqTr-2EFaZhYzA1&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Twitter: &lt;a href=&quot;https://twitter.com/QiuChenly&quot;&gt;https://twitter.com/QiuChenly&lt;/a&gt;&lt;/p&gt; 
&lt;br&gt; 關注QiuChenly喵，關注落葉的Twitter喵。謝謝大家喵。 
&lt;h1&gt;原神!&lt;/h1&gt; 
&lt;p&gt;點擊圖片進入新世界.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/QiuChenly/InjectLib/main/imgs/bengbuzhule.mp4&quot;&gt;&lt;img src=&quot;https://i2.hdslb.com/bfs/archive/966fe6fe2c1329919bb8972d69fb8c09d17047cc.jpg@100w_100h_1c.png&quot; alt=&quot;启动&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;操作系统要求 &amp;amp; 代码编译环境要求&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;最低运行 macOS High Sierra 10.13&lt;/li&gt; 
 &lt;li&gt;编译SDK macOS 14.0&lt;/li&gt; 
 &lt;li&gt;目标部署平台 macOS 10.13&lt;/li&gt; 
 &lt;li&gt;CMakeLists 环境变量 
  &lt;ul&gt; 
   &lt;li&gt;set(CMAKE_OSX_DEPLOYMENT_TARGET &quot;10.13&quot;)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;检查二进制文件的最低macOS版本兼容性 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;find . -name &quot;*.*&quot; | xargs otool -l | grep -E &quot;(minos|sdk)&quot;&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr&gt; 
&lt;h1&gt;项目存在的目的&lt;/h1&gt; 
&lt;p&gt;本项目是Free的、开源的、基于互联网最原始的共享精神的、不接受任何打赏的、无所不包的、令人感叹的、无与伦比的、精妙绝伦的、化腐朽为神奇的、逆天的、养生的、抽象的、二次元的、OP的。&lt;/p&gt; 
&lt;p&gt;在2023年，所有人都逐渐觉得打赏、付费才是理所应当的，哪怕是某些人只做了一件从外网搬运到国内的工作，也应该得到鼓励。 我不能说这种行为是完全错误的，只能说有些人恬不知耻见利忘义。哪怕是打赏也应该基于双方意愿的基础上，而不是用“打赏后才能下载”这种理由强奸用户的使用习惯，把用户变成必须付费的蠢驴，并辅以几十元的超低价注册会员费用钝刀割肉式的强奸用户。&lt;/p&gt; 
&lt;p&gt;当然，这种用户也确实是个蠢货。有这种钱你买正版得了，别跟我说太贵，你出去跟朋友吃一顿好点的饭200起步，大部分好软件正版才不到100块钱。抽包烟软中煊赫门起步，面对19.9年费会员时却面露难色，好像杀了你的🐎一样。相信我，你也并不是真的需要这些软件，只是人云亦云盲目从众罢了。&lt;/p&gt; 
&lt;p&gt;我认为，共享精神不应该建立在物质上，我深刻的理解金钱对人的吸引和动力，但这种精神本身就超越了物质。&lt;/p&gt; 
&lt;h1&gt;&lt;del&gt;免责声明&lt;/del&gt;wo ze ni ma de b&lt;/h1&gt; 
&lt;p&gt;致来自中国大陆的各位学习研究爱好者:&lt;br&gt; 根据大陆中华人民共和国《计算机软件保护条例》第十七条规定：“为了学习和研究软件内含的设计思想和原理，通过安装、显示、传输或者存储软件等方式使用软件的，可以不经软件著作权人许可，不向其支付报酬。”您需知晓本仓库所有内容资源均来源于网络，仅供用户交流学习与研究使用，版权归属原版权方所有，版权争议与本仓库本作者无关，用户本人下载后不能用作商业或非法用途，需在24小时之内删除，否则后果均由用户承担责任。 如果你不删那就让这些喜欢发律师函的事务所一对一指导你。&lt;/p&gt; 
&lt;p&gt;我是來自北美的獨立IOS應用程式開發者,專注於開發有趣又富有創意的應用。對於法律問題,我只能說明技術原理,不能提供任何法律意見。希望大家都能以和平、理性的態度來探討各種課題。&lt;/p&gt; 
&lt;p&gt;同時, 我也是二次元南桐. 从台灣國立大學毕业的那一天, 我的青春永遠留在了高雄.&lt;/p&gt; 
&lt;p&gt;對於肆意濫用法律的組織和個人,請將律師函發送至: 华盛顿特区第35大道林肯大街15号-501, John Albet收.&lt;/p&gt; 
&lt;h1&gt;&lt;del&gt;停更&lt;/del&gt;&lt;/h1&gt; 
&lt;p&gt;&lt;del&gt;最近想追个19岁的小妹妹。&lt;br&gt; 项目基本上不会更新了，增加的新项目基本上是工作💻需要才做的。&lt;br&gt; 也不会去维护下面App的新版本了，等我追到手🧑‍🤝‍🧑再说罢！&lt;br&gt;&lt;/del&gt;&lt;/p&gt; 
&lt;p&gt;&lt;del&gt;为了追💗妹妹👧，MD，跟米哈游原神铁道星穹崩坏王者荣耀蛋仔二次元拼了😡👊！&lt;br&gt; 这下不得不成为农P/原P/穹P了🙏🙏&lt;br&gt;&lt;/del&gt;&lt;/p&gt; 
&lt;p&gt;&lt;del&gt;无知时诋毁原神🫤🙏&lt;br&gt; 成熟时理解原神😭🙏&lt;br&gt; 恋爱时成为原神😋🙏&lt;br&gt;&lt;/del&gt;&lt;/p&gt; 
&lt;p&gt;&lt;del&gt;原神助我！喝唉！🖐大荒天陨！️&lt;br&gt; 任何邪恶！终将绳之以法👮！&lt;br&gt;&lt;/del&gt;&lt;/p&gt; 
&lt;p&gt;&lt;del&gt;原神，启动！&lt;/del&gt;&lt;/p&gt; 
&lt;p&gt;失败了，大家别问了。 &lt;br&gt; 她不是不喜欢玩游戏，她只是不想和不喜欢的人玩游戏。&lt;br&gt;&lt;/p&gt; 
&lt;p&gt;这段Repo不会删，警钟长鸣。但是你要问我如果再给我一次机会还会不会选18岁妹妹，我的回答是“yes i do.”&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>crewAIInc/crewAI</title>
      <link>https://github.com/crewAIInc/crewAI</link>
      <description>&lt;p&gt;Framework for orchestrating role-playing, autonomous AI agents. By fostering collaborative intelligence, CrewAI empowers agents to work together seamlessly, tackling complex tasks.&lt;/p&gt;&lt;hr&gt;&lt;div align=&quot;center&quot;&gt; 
 &lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/crewAIInc/crewAI/main/docs/crewai_logo.png&quot; alt=&quot;Logo of CrewAI&quot;&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h3&gt;Fast and Flexible Multi-Agent Automation Framework&lt;/h3&gt; 
&lt;p&gt;CrewAI is a lean, lightning-fast Python framework built entirely from scratch—completely &lt;strong&gt;independent of LangChain or other agent frameworks&lt;/strong&gt;. It empowers developers with both high-level simplicity and precise low-level control, ideal for creating autonomous AI agents tailored to any scenario.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;CrewAI Crews&lt;/strong&gt;: Optimize for autonomy and collaborative intelligence.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;CrewAI Flows&lt;/strong&gt;: Enable granular, event-driven control, single LLM calls for precise task orchestration and supports Crews natively&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;With over 100,000 developers certified through our community courses at &lt;a href=&quot;https://learn.crewai.com&quot;&gt;learn.crewai.com&lt;/a&gt;, CrewAI is rapidly becoming the standard for enterprise-ready AI automation.&lt;/p&gt; 
&lt;h1&gt;CrewAI Enterprise Suite&lt;/h1&gt; 
&lt;p&gt;CrewAI Enterprise Suite is a comprehensive bundle tailored for organizations that require secure, scalable, and easy-to-manage agent-driven automation.&lt;/p&gt; 
&lt;p&gt;You can try one part of the suite the &lt;a href=&quot;https://app.crewai.com&quot;&gt;Crew Control Plane for free&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Crew Control Plane Key Features:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Tracing &amp;amp; Observability&lt;/strong&gt;: Monitor and track your AI agents and workflows in real-time, including metrics, logs, and traces.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Unified Control Plane&lt;/strong&gt;: A centralized platform for managing, monitoring, and scaling your AI agents and workflows.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Seamless Integrations&lt;/strong&gt;: Easily connect with existing enterprise systems, data sources, and cloud infrastructure.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Advanced Security&lt;/strong&gt;: Built-in robust security and compliance measures ensuring safe deployment and management.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Actionable Insights&lt;/strong&gt;: Real-time analytics and reporting to optimize performance and decision-making.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;24/7 Support&lt;/strong&gt;: Dedicated enterprise support to ensure uninterrupted operation and quick resolution of issues.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;On-premise and Cloud Deployment Options&lt;/strong&gt;: Deploy CrewAI Enterprise on-premise or in the cloud, depending on your security and compliance requirements.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;CrewAI Enterprise is designed for enterprises seeking a powerful, reliable solution to transform complex business processes into efficient, intelligent automations.&lt;/p&gt; 
&lt;h3&gt; &lt;p&gt;&lt;a href=&quot;https://www.crewai.com/&quot;&gt;Homepage&lt;/a&gt; | &lt;a href=&quot;https://docs.crewai.com/&quot;&gt;Documentation&lt;/a&gt; | &lt;a href=&quot;https://chatg.pt/DWjSBZn&quot;&gt;Chat with Docs&lt;/a&gt; | &lt;a href=&quot;https://community.crewai.com&quot;&gt;Discourse&lt;/a&gt;&lt;/p&gt; &lt;/h3&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/crewAIInc/crewAI&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/joaomdmoura/crewAI&quot; alt=&quot;GitHub Repo stars&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://opensource.org/licenses/MIT&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/License-MIT-green.svg?sanitize=true&quot; alt=&quot;License: MIT&quot;&gt;&lt;/a&gt;&lt;/p&gt;  
&lt;h2&gt;Table of contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/crewAIInc/crewAI/main/#why-crewai&quot;&gt;Why CrewAI?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/crewAIInc/crewAI/main/#getting-started&quot;&gt;Getting Started&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/crewAIInc/crewAI/main/#key-features&quot;&gt;Key Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/crewAIInc/crewAI/main/#understanding-flows-and-crews&quot;&gt;Understanding Flows and Crews&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/crewAIInc/crewAI/main/#how-crewai-compares&quot;&gt;CrewAI vs LangGraph&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/crewAIInc/crewAI/main/#examples&quot;&gt;Examples&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/crewAIInc/crewAI/main/#quick-tutorial&quot;&gt;Quick Tutorial&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/crewAIInc/crewAI/main/#write-job-descriptions&quot;&gt;Write Job Descriptions&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/crewAIInc/crewAI/main/#trip-planner&quot;&gt;Trip Planner&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/crewAIInc/crewAI/main/#stock-analysis&quot;&gt;Stock Analysis&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/crewAIInc/crewAI/main/#using-crews-and-flows-together&quot;&gt;Using Crews and Flows Together&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/crewAIInc/crewAI/main/#connecting-your-crew-to-a-model&quot;&gt;Connecting Your Crew to a Model&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/crewAIInc/crewAI/main/#how-crewai-compares&quot;&gt;How CrewAI Compares&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/crewAIInc/crewAI/main/#frequently-asked-questions-faq&quot;&gt;Frequently Asked Questions (FAQ)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/crewAIInc/crewAI/main/#contribution&quot;&gt;Contribution&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/crewAIInc/crewAI/main/#telemetry&quot;&gt;Telemetry&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/crewAIInc/crewAI/main/#license&quot;&gt;License&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Why CrewAI?&lt;/h2&gt; 
&lt;div align=&quot;center&quot; style=&quot;margin-bottom: 30px;&quot;&gt; 
 &lt;img src=&quot;https://raw.githubusercontent.com/crewAIInc/crewAI/main/docs/asset.png&quot; alt=&quot;CrewAI Logo&quot; width=&quot;100%&quot;&gt; 
&lt;/div&gt; 
&lt;p&gt;CrewAI unlocks the true potential of multi-agent automation, delivering the best-in-class combination of speed, flexibility, and control with either Crews of AI Agents or Flows of Events:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Standalone Framework&lt;/strong&gt;: Built from scratch, independent of LangChain or any other agent framework.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;High Performance&lt;/strong&gt;: Optimized for speed and minimal resource usage, enabling faster execution.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Flexible Low Level Customization&lt;/strong&gt;: Complete freedom to customize at both high and low levels - from overall workflows and system architecture to granular agent behaviors, internal prompts, and execution logic.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Ideal for Every Use Case&lt;/strong&gt;: Proven effective for both simple tasks and highly complex, real-world, enterprise-grade scenarios.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Robust Community&lt;/strong&gt;: Backed by a rapidly growing community of over &lt;strong&gt;100,000 certified&lt;/strong&gt; developers offering comprehensive support and resources.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;CrewAI empowers developers and enterprises to confidently build intelligent automations, bridging the gap between simplicity, flexibility, and performance.&lt;/p&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;h3&gt;Learning Resources&lt;/h3&gt; 
&lt;p&gt;Learn CrewAI through our comprehensive courses:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.deeplearning.ai/short-courses/multi-ai-agent-systems-with-crewai/&quot;&gt;Multi AI Agent Systems with CrewAI&lt;/a&gt; - Master the fundamentals of multi-agent systems&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.deeplearning.ai/short-courses/practical-multi-ai-agents-and-advanced-use-cases-with-crewai/&quot;&gt;Practical Multi AI Agents and Advanced Use Cases&lt;/a&gt; - Deep dive into advanced implementations&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Understanding Flows and Crews&lt;/h3&gt; 
&lt;p&gt;CrewAI offers two powerful, complementary approaches that work seamlessly together to build sophisticated AI applications:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Crews&lt;/strong&gt;: Teams of AI agents with true autonomy and agency, working together to accomplish complex tasks through role-based collaboration. Crews enable:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Natural, autonomous decision-making between agents&lt;/li&gt; 
   &lt;li&gt;Dynamic task delegation and collaboration&lt;/li&gt; 
   &lt;li&gt;Specialized roles with defined goals and expertise&lt;/li&gt; 
   &lt;li&gt;Flexible problem-solving approaches&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Flows&lt;/strong&gt;: Production-ready, event-driven workflows that deliver precise control over complex automations. Flows provide:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Fine-grained control over execution paths for real-world scenarios&lt;/li&gt; 
   &lt;li&gt;Secure, consistent state management between tasks&lt;/li&gt; 
   &lt;li&gt;Clean integration of AI agents with production Python code&lt;/li&gt; 
   &lt;li&gt;Conditional branching for complex business logic&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;The true power of CrewAI emerges when combining Crews and Flows. This synergy allows you to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Build complex, production-grade applications&lt;/li&gt; 
 &lt;li&gt;Balance autonomy with precise control&lt;/li&gt; 
 &lt;li&gt;Handle sophisticated real-world scenarios&lt;/li&gt; 
 &lt;li&gt;Maintain clean, maintainable code structure&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Getting Started with Installation&lt;/h3&gt; 
&lt;p&gt;To get started with CrewAI, follow these simple steps:&lt;/p&gt; 
&lt;h3&gt;1. Installation&lt;/h3&gt; 
&lt;p&gt;Ensure you have Python &amp;gt;=3.10 &amp;lt;3.13 installed on your system. CrewAI uses &lt;a href=&quot;https://docs.astral.sh/uv/&quot;&gt;UV&lt;/a&gt; for dependency management and package handling, offering a seamless setup and execution experience.&lt;/p&gt; 
&lt;p&gt;First, install CrewAI:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;pip install crewai
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you want to install the &#39;crewai&#39; package along with its optional features that include additional tools for agents, you can do so by using the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;pip install &#39;crewai[tools]&#39;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The command above installs the basic package and also adds extra components which require more dependencies to function.&lt;/p&gt; 
&lt;h3&gt;Troubleshooting Dependencies&lt;/h3&gt; 
&lt;p&gt;If you encounter issues during installation or usage, here are some common solutions:&lt;/p&gt; 
&lt;h4&gt;Common Issues&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ModuleNotFoundError: No module named &#39;tiktoken&#39;&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Install tiktoken explicitly: &lt;code&gt;pip install &#39;crewai[embeddings]&#39;&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;If using embedchain or other tools: &lt;code&gt;pip install &#39;crewai[tools]&#39;&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Failed building wheel for tiktoken&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Ensure Rust compiler is installed (see installation steps above)&lt;/li&gt; 
   &lt;li&gt;For Windows: Verify Visual C++ Build Tools are installed&lt;/li&gt; 
   &lt;li&gt;Try upgrading pip: &lt;code&gt;pip install --upgrade pip&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;If issues persist, use a pre-built wheel: &lt;code&gt;pip install tiktoken --prefer-binary&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;2. Setting Up Your Crew with the YAML Configuration&lt;/h3&gt; 
&lt;p&gt;To create a new CrewAI project, run the following CLI (Command Line Interface) command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;crewai create crew &amp;lt;project_name&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This command creates a new project folder with the following structure:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;my_project/
├── .gitignore
├── pyproject.toml
├── README.md
├── .env
└── src/
    └── my_project/
        ├── __init__.py
        ├── main.py
        ├── crew.py
        ├── tools/
        │   ├── custom_tool.py
        │   └── __init__.py
        └── config/
            ├── agents.yaml
            └── tasks.yaml
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can now start developing your crew by editing the files in the &lt;code&gt;src/my_project&lt;/code&gt; folder. The &lt;code&gt;main.py&lt;/code&gt; file is the entry point of the project, the &lt;code&gt;crew.py&lt;/code&gt; file is where you define your crew, the &lt;code&gt;agents.yaml&lt;/code&gt; file is where you define your agents, and the &lt;code&gt;tasks.yaml&lt;/code&gt; file is where you define your tasks.&lt;/p&gt; 
&lt;h4&gt;To customize your project, you can:&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Modify &lt;code&gt;src/my_project/config/agents.yaml&lt;/code&gt; to define your agents.&lt;/li&gt; 
 &lt;li&gt;Modify &lt;code&gt;src/my_project/config/tasks.yaml&lt;/code&gt; to define your tasks.&lt;/li&gt; 
 &lt;li&gt;Modify &lt;code&gt;src/my_project/crew.py&lt;/code&gt; to add your own logic, tools, and specific arguments.&lt;/li&gt; 
 &lt;li&gt;Modify &lt;code&gt;src/my_project/main.py&lt;/code&gt; to add custom inputs for your agents and tasks.&lt;/li&gt; 
 &lt;li&gt;Add your environment variables into the &lt;code&gt;.env&lt;/code&gt; file.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Example of a simple crew with a sequential process:&lt;/h4&gt; 
&lt;p&gt;Instantiate your crew:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;crewai create crew latest-ai-development
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Modify the files as needed to fit your use case:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;agents.yaml&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-yaml&quot;&gt;# src/my_project/config/agents.yaml
researcher:
  role: &amp;gt;
    {topic} Senior Data Researcher
  goal: &amp;gt;
    Uncover cutting-edge developments in {topic}
  backstory: &amp;gt;
    You&#39;re a seasoned researcher with a knack for uncovering the latest
    developments in {topic}. Known for your ability to find the most relevant
    information and present it in a clear and concise manner.

reporting_analyst:
  role: &amp;gt;
    {topic} Reporting Analyst
  goal: &amp;gt;
    Create detailed reports based on {topic} data analysis and research findings
  backstory: &amp;gt;
    You&#39;re a meticulous analyst with a keen eye for detail. You&#39;re known for
    your ability to turn complex data into clear and concise reports, making
    it easy for others to understand and act on the information you provide.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;tasks.yaml&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-yaml&quot;&gt;# src/my_project/config/tasks.yaml
research_task:
  description: &amp;gt;
    Conduct a thorough research about {topic}
    Make sure you find any interesting and relevant information given
    the current year is 2025.
  expected_output: &amp;gt;
    A list with 10 bullet points of the most relevant information about {topic}
  agent: researcher

reporting_task:
  description: &amp;gt;
    Review the context you got and expand each topic into a full section for a report.
    Make sure the report is detailed and contains any and all relevant information.
  expected_output: &amp;gt;
    A fully fledge reports with the mains topics, each with a full section of information.
    Formatted as markdown without &#39;```&#39;
  agent: reporting_analyst
  output_file: report.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;crew.py&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# src/my_project/crew.py
from crewai import Agent, Crew, Process, Task
from crewai.project import CrewBase, agent, crew, task
from crewai_tools import SerperDevTool

@CrewBase
class LatestAiDevelopmentCrew():
	&quot;&quot;&quot;LatestAiDevelopment crew&quot;&quot;&quot;

	@agent
	def researcher(self) -&amp;gt; Agent:
		return Agent(
			config=self.agents_config[&#39;researcher&#39;],
			verbose=True,
			tools=[SerperDevTool()]
		)

	@agent
	def reporting_analyst(self) -&amp;gt; Agent:
		return Agent(
			config=self.agents_config[&#39;reporting_analyst&#39;],
			verbose=True
		)

	@task
	def research_task(self) -&amp;gt; Task:
		return Task(
			config=self.tasks_config[&#39;research_task&#39;],
		)

	@task
	def reporting_task(self) -&amp;gt; Task:
		return Task(
			config=self.tasks_config[&#39;reporting_task&#39;],
			output_file=&#39;report.md&#39;
		)

	@crew
	def crew(self) -&amp;gt; Crew:
		&quot;&quot;&quot;Creates the LatestAiDevelopment crew&quot;&quot;&quot;
		return Crew(
			agents=self.agents, # Automatically created by the @agent decorator
			tasks=self.tasks, # Automatically created by the @task decorator
			process=Process.sequential,
			verbose=True,
		)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;main.py&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;#!/usr/bin/env python
# src/my_project/main.py
import sys
from latest_ai_development.crew import LatestAiDevelopmentCrew

def run():
    &quot;&quot;&quot;
    Run the crew.
    &quot;&quot;&quot;
    inputs = {
        &#39;topic&#39;: &#39;AI Agents&#39;
    }
    LatestAiDevelopmentCrew().crew().kickoff(inputs=inputs)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;3. Running Your Crew&lt;/h3&gt; 
&lt;p&gt;Before running your crew, make sure you have the following keys set as environment variables in your &lt;code&gt;.env&lt;/code&gt; file:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;An &lt;a href=&quot;https://platform.openai.com/account/api-keys&quot;&gt;OpenAI API key&lt;/a&gt; (or other LLM API key): &lt;code&gt;OPENAI_API_KEY=sk-...&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;A &lt;a href=&quot;https://serper.dev/&quot;&gt;Serper.dev&lt;/a&gt; API key: &lt;code&gt;SERPER_API_KEY=YOUR_KEY_HERE&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Lock the dependencies and install them by using the CLI command but first, navigate to your project directory:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;cd my_project
crewai install (Optional)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To run your crew, execute the following command in the root of your project:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;crewai run
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python src/my_project/main.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If an error happens due to the usage of poetry, please run the following command to update your crewai package:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;crewai update
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You should see the output in the console and the &lt;code&gt;report.md&lt;/code&gt; file should be created in the root of your project with the full final report.&lt;/p&gt; 
&lt;p&gt;In addition to the sequential process, you can use the hierarchical process, which automatically assigns a manager to the defined crew to properly coordinate the planning and execution of tasks through delegation and validation of results. &lt;a href=&quot;https://docs.crewai.com/core-concepts/Processes/&quot;&gt;See more about the processes here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;p&gt;CrewAI stands apart as a lean, standalone, high-performance framework delivering simplicity, flexibility, and precise control—free from the complexity and limitations found in other agent frameworks.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Standalone &amp;amp; Lean&lt;/strong&gt;: Completely independent from other frameworks like LangChain, offering faster execution and lighter resource demands.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Flexible &amp;amp; Precise&lt;/strong&gt;: Easily orchestrate autonomous agents through intuitive &lt;a href=&quot;https://docs.crewai.com/concepts/crews&quot;&gt;Crews&lt;/a&gt; or precise &lt;a href=&quot;https://docs.crewai.com/concepts/flows&quot;&gt;Flows&lt;/a&gt;, achieving perfect balance for your needs.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Seamless Integration&lt;/strong&gt;: Effortlessly combine Crews (autonomy) and Flows (precision) to create complex, real-world automations.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Deep Customization&lt;/strong&gt;: Tailor every aspect—from high-level workflows down to low-level internal prompts and agent behaviors.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Reliable Performance&lt;/strong&gt;: Consistent results across simple tasks and complex, enterprise-level automations.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Thriving Community&lt;/strong&gt;: Backed by robust documentation and over 100,000 certified developers, providing exceptional support and guidance.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Choose CrewAI to easily build powerful, adaptable, and production-ready AI automations.&lt;/p&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;p&gt;You can test different real life examples of AI crews in the &lt;a href=&quot;https://github.com/crewAIInc/crewAI-examples?tab=readme-ov-file&quot;&gt;CrewAI-examples repo&lt;/a&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/crewAIInc/crewAI-examples/tree/main/landing_page_generator&quot;&gt;Landing Page Generator&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.crewai.com/how-to/Human-Input-on-Execution&quot;&gt;Having Human input on the execution&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/crewAIInc/crewAI-examples/tree/main/trip_planner&quot;&gt;Trip Planner&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/crewAIInc/crewAI-examples/tree/main/stock_analysis&quot;&gt;Stock Analysis&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Quick Tutorial&lt;/h3&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=tnejrr-0a94&quot; title=&quot;CrewAI Tutorial&quot;&gt;&lt;img src=&quot;https://img.youtube.com/vi/tnejrr-0a94/maxresdefault.jpg&quot; alt=&quot;CrewAI Tutorial&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Write Job Descriptions&lt;/h3&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/crewAIInc/crewAI-examples/tree/main/job-posting&quot;&gt;Check out code for this example&lt;/a&gt; or watch a video below:&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=u98wEMz-9to&quot; title=&quot;Jobs postings&quot;&gt;&lt;img src=&quot;https://img.youtube.com/vi/u98wEMz-9to/maxresdefault.jpg&quot; alt=&quot;Jobs postings&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Trip Planner&lt;/h3&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/crewAIInc/crewAI-examples/tree/main/trip_planner&quot;&gt;Check out code for this example&lt;/a&gt; or watch a video below:&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=xis7rWp-hjs&quot; title=&quot;Trip Planner&quot;&gt;&lt;img src=&quot;https://img.youtube.com/vi/xis7rWp-hjs/maxresdefault.jpg&quot; alt=&quot;Trip Planner&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Stock Analysis&lt;/h3&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/crewAIInc/crewAI-examples/tree/main/stock_analysis&quot;&gt;Check out code for this example&lt;/a&gt; or watch a video below:&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=e0Uj4yWdaAg&quot; title=&quot;Stock Analysis&quot;&gt;&lt;img src=&quot;https://img.youtube.com/vi/e0Uj4yWdaAg/maxresdefault.jpg&quot; alt=&quot;Stock Analysis&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Using Crews and Flows Together&lt;/h3&gt; 
&lt;p&gt;CrewAI&#39;s power truly shines when combining Crews with Flows to create sophisticated automation pipelines. Here&#39;s how you can orchestrate multiple Crews within a Flow:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from crewai.flow.flow import Flow, listen, start, router
from crewai import Crew, Agent, Task
from pydantic import BaseModel

# Define structured state for precise control
class MarketState(BaseModel):
    sentiment: str = &quot;neutral&quot;
    confidence: float = 0.0
    recommendations: list = []

class AdvancedAnalysisFlow(Flow[MarketState]):
    @start()
    def fetch_market_data(self):
        # Demonstrate low-level control with structured state
        self.state.sentiment = &quot;analyzing&quot;
        return {&quot;sector&quot;: &quot;tech&quot;, &quot;timeframe&quot;: &quot;1W&quot;}  # These parameters match the task description template

    @listen(fetch_market_data)
    def analyze_with_crew(self, market_data):
        # Show crew agency through specialized roles
        analyst = Agent(
            role=&quot;Senior Market Analyst&quot;,
            goal=&quot;Conduct deep market analysis with expert insight&quot;,
            backstory=&quot;You&#39;re a veteran analyst known for identifying subtle market patterns&quot;
        )
        researcher = Agent(
            role=&quot;Data Researcher&quot;,
            goal=&quot;Gather and validate supporting market data&quot;,
            backstory=&quot;You excel at finding and correlating multiple data sources&quot;
        )

        analysis_task = Task(
            description=&quot;Analyze {sector} sector data for the past {timeframe}&quot;,
            expected_output=&quot;Detailed market analysis with confidence score&quot;,
            agent=analyst
        )
        research_task = Task(
            description=&quot;Find supporting data to validate the analysis&quot;,
            expected_output=&quot;Corroborating evidence and potential contradictions&quot;,
            agent=researcher
        )

        # Demonstrate crew autonomy
        analysis_crew = Crew(
            agents=[analyst, researcher],
            tasks=[analysis_task, research_task],
            process=Process.sequential,
            verbose=True
        )
        return analysis_crew.kickoff(inputs=market_data)  # Pass market_data as named inputs

    @router(analyze_with_crew)
    def determine_next_steps(self):
        # Show flow control with conditional routing
        if self.state.confidence &amp;gt; 0.8:
            return &quot;high_confidence&quot;
        elif self.state.confidence &amp;gt; 0.5:
            return &quot;medium_confidence&quot;
        return &quot;low_confidence&quot;

    @listen(&quot;high_confidence&quot;)
    def execute_strategy(self):
        # Demonstrate complex decision making
        strategy_crew = Crew(
            agents=[
                Agent(role=&quot;Strategy Expert&quot;,
                      goal=&quot;Develop optimal market strategy&quot;)
            ],
            tasks=[
                Task(description=&quot;Create detailed strategy based on analysis&quot;,
                     expected_output=&quot;Step-by-step action plan&quot;)
            ]
        )
        return strategy_crew.kickoff()

    @listen(&quot;medium_confidence&quot;, &quot;low_confidence&quot;)
    def request_additional_analysis(self):
        self.state.recommendations.append(&quot;Gather more data&quot;)
        return &quot;Additional analysis required&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This example demonstrates how to:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Use Python code for basic data operations&lt;/li&gt; 
 &lt;li&gt;Create and execute Crews as steps in your workflow&lt;/li&gt; 
 &lt;li&gt;Use Flow decorators to manage the sequence of operations&lt;/li&gt; 
 &lt;li&gt;Implement conditional branching based on Crew results&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Connecting Your Crew to a Model&lt;/h2&gt; 
&lt;p&gt;CrewAI supports using various LLMs through a variety of connection options. By default your agents will use the OpenAI API when querying the model. However, there are several other ways to allow your agents to connect to models. For example, you can configure your agents to use a local model via the Ollama tool.&lt;/p&gt; 
&lt;p&gt;Please refer to the &lt;a href=&quot;https://docs.crewai.com/how-to/LLM-Connections/&quot;&gt;Connect CrewAI to LLMs&lt;/a&gt; page for details on configuring you agents&#39; connections to models.&lt;/p&gt; 
&lt;h2&gt;How CrewAI Compares&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;CrewAI&#39;s Advantage&lt;/strong&gt;: CrewAI combines autonomous agent intelligence with precise workflow control through its unique Crews and Flows architecture. The framework excels at both high-level orchestration and low-level customization, enabling complex, production-grade systems with granular control.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;LangGraph&lt;/strong&gt;: While LangGraph provides a foundation for building agent workflows, its approach requires significant boilerplate code and complex state management patterns. The framework&#39;s tight coupling with LangChain can limit flexibility when implementing custom agent behaviors or integrating with external systems.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;em&gt;P.S. CrewAI demonstrates significant performance advantages over LangGraph, executing 5.76x faster in certain cases like this QA task example (&lt;a href=&quot;https://github.com/crewAIInc/crewAI-examples/tree/main/Notebooks/CrewAI%20Flows%20%26%20Langgraph/QA%20Agent&quot;&gt;see comparison&lt;/a&gt;) while achieving higher evaluation scores with faster completion times in certain coding tasks, like in this example (&lt;a href=&quot;https://github.com/crewAIInc/crewAI-examples/raw/main/Notebooks/CrewAI%20Flows%20%26%20Langgraph/Coding%20Assistant/coding_assistant_eval.ipynb&quot;&gt;detailed analysis&lt;/a&gt;).&lt;/em&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Autogen&lt;/strong&gt;: While Autogen excels at creating conversational agents capable of working together, it lacks an inherent concept of process. In Autogen, orchestrating agents&#39; interactions requires additional programming, which can become complex and cumbersome as the scale of tasks grows.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ChatDev&lt;/strong&gt;: ChatDev introduced the idea of processes into the realm of AI agents, but its implementation is quite rigid. Customizations in ChatDev are limited and not geared towards production environments, which can hinder scalability and flexibility in real-world applications.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contribution&lt;/h2&gt; 
&lt;p&gt;CrewAI is open-source and we welcome contributions. If you&#39;re looking to contribute, please:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Fork the repository.&lt;/li&gt; 
 &lt;li&gt;Create a new branch for your feature.&lt;/li&gt; 
 &lt;li&gt;Add your feature or improvement.&lt;/li&gt; 
 &lt;li&gt;Send a pull request.&lt;/li&gt; 
 &lt;li&gt;We appreciate your input!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Installing Dependencies&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;uv lock
uv sync
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Virtual Env&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;uv venv
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Pre-commit hooks&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pre-commit install
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Running Tests&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;uv run pytest .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Running static type checks&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;uvx mypy src
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Packaging&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;uv build
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Installing Locally&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install dist/*.tar.gz
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Telemetry&lt;/h2&gt; 
&lt;p&gt;CrewAI uses anonymous telemetry to collect usage data with the main purpose of helping us improve the library by focusing our efforts on the most used features, integrations and tools.&lt;/p&gt; 
&lt;p&gt;It&#39;s pivotal to understand that &lt;strong&gt;NO data is collected&lt;/strong&gt; concerning prompts, task descriptions, agents&#39; backstories or goals, usage of tools, API calls, responses, any data processed by the agents, or secrets and environment variables, with the exception of the conditions mentioned. When the &lt;code&gt;share_crew&lt;/code&gt; feature is enabled, detailed data including task descriptions, agents&#39; backstories or goals, and other specific attributes are collected to provide deeper insights while respecting user privacy. Users can disable telemetry by setting the environment variable OTEL_SDK_DISABLED to true.&lt;/p&gt; 
&lt;p&gt;Data collected includes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Version of CrewAI 
  &lt;ul&gt; 
   &lt;li&gt;So we can understand how many users are using the latest version&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Version of Python 
  &lt;ul&gt; 
   &lt;li&gt;So we can decide on what versions to better support&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;General OS (e.g. number of CPUs, macOS/Windows/Linux) 
  &lt;ul&gt; 
   &lt;li&gt;So we know what OS we should focus on and if we could build specific OS related features&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Number of agents and tasks in a crew 
  &lt;ul&gt; 
   &lt;li&gt;So we make sure we are testing internally with similar use cases and educate people on the best practices&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Crew Process being used 
  &lt;ul&gt; 
   &lt;li&gt;Understand where we should focus our efforts&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;If Agents are using memory or allowing delegation 
  &lt;ul&gt; 
   &lt;li&gt;Understand if we improved the features or maybe even drop them&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;If Tasks are being executed in parallel or sequentially 
  &lt;ul&gt; 
   &lt;li&gt;Understand if we should focus more on parallel execution&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Language model being used 
  &lt;ul&gt; 
   &lt;li&gt;Improved support on most used languages&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Roles of agents in a crew 
  &lt;ul&gt; 
   &lt;li&gt;Understand high level use cases so we can build better tools, integrations and examples about it&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Tools names available 
  &lt;ul&gt; 
   &lt;li&gt;Understand out of the publicly available tools, which ones are being used the most so we can improve them&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Users can opt-in to Further Telemetry, sharing the complete telemetry data by setting the &lt;code&gt;share_crew&lt;/code&gt; attribute to &lt;code&gt;True&lt;/code&gt; on their Crews. Enabling &lt;code&gt;share_crew&lt;/code&gt; results in the collection of detailed crew and task execution data, including &lt;code&gt;goal&lt;/code&gt;, &lt;code&gt;backstory&lt;/code&gt;, &lt;code&gt;context&lt;/code&gt;, and &lt;code&gt;output&lt;/code&gt; of tasks. This enables a deeper insight into usage patterns while respecting the user&#39;s choice to share.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;CrewAI is released under the &lt;a href=&quot;https://github.com/crewAIInc/crewAI/raw/main/LICENSE&quot;&gt;MIT License&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Frequently Asked Questions (FAQ)&lt;/h2&gt; 
&lt;h3&gt;General&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-what-exactly-is-crewai&quot;&gt;What exactly is CrewAI?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-how-do-i-install-crewai&quot;&gt;How do I install CrewAI?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-does-crewai-depend-on-langchain&quot;&gt;Does CrewAI depend on LangChain?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-is-crewai-open-source&quot;&gt;Is CrewAI open-source?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-does-crewai-collect-data-from-users&quot;&gt;Does CrewAI collect data from users?&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Features and Capabilities&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-can-crewai-handle-complex-use-cases&quot;&gt;Can CrewAI handle complex use cases?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-can-i-use-crewai-with-local-ai-models&quot;&gt;Can I use CrewAI with local AI models?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-what-makes-crews-different-from-flows&quot;&gt;What makes Crews different from Flows?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-how-is-crewai-better-than-langchain&quot;&gt;How is CrewAI better than LangChain?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-does-crewai-support-fine-tuning-or-training-custom-models&quot;&gt;Does CrewAI support fine-tuning or training custom models?&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Resources and Community&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-where-can-i-find-real-world-crewai-examples&quot;&gt;Where can I find real-world CrewAI examples?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-how-can-i-contribute-to-crewai&quot;&gt;How can I contribute to CrewAI?&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Enterprise Features&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-what-additional-features-does-crewai-enterprise-offer&quot;&gt;What additional features does CrewAI Enterprise offer?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-is-crewai-enterprise-available-for-cloud-and-on-premise-deployments&quot;&gt;Is CrewAI Enterprise available for cloud and on-premise deployments?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-can-i-try-crewai-enterprise-for-free&quot;&gt;Can I try CrewAI Enterprise for free?&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Q: What exactly is CrewAI?&lt;/h3&gt; 
&lt;p&gt;A: CrewAI is a standalone, lean, and fast Python framework built specifically for orchestrating autonomous AI agents. Unlike frameworks like LangChain, CrewAI does not rely on external dependencies, making it leaner, faster, and simpler.&lt;/p&gt; 
&lt;h3&gt;Q: How do I install CrewAI?&lt;/h3&gt; 
&lt;p&gt;A: Install CrewAI using pip:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;pip install crewai
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For additional tools, use:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;pip install &#39;crewai[tools]&#39;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Q: Does CrewAI depend on LangChain?&lt;/h3&gt; 
&lt;p&gt;A: No. CrewAI is built entirely from the ground up, with no dependencies on LangChain or other agent frameworks. This ensures a lean, fast, and flexible experience.&lt;/p&gt; 
&lt;h3&gt;Q: Can CrewAI handle complex use cases?&lt;/h3&gt; 
&lt;p&gt;A: Yes. CrewAI excels at both simple and highly complex real-world scenarios, offering deep customization options at both high and low levels, from internal prompts to sophisticated workflow orchestration.&lt;/p&gt; 
&lt;h3&gt;Q: Can I use CrewAI with local AI models?&lt;/h3&gt; 
&lt;p&gt;A: Absolutely! CrewAI supports various language models, including local ones. Tools like Ollama and LM Studio allow seamless integration. Check the &lt;a href=&quot;https://docs.crewai.com/how-to/LLM-Connections/&quot;&gt;LLM Connections documentation&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h3&gt;Q: What makes Crews different from Flows?&lt;/h3&gt; 
&lt;p&gt;A: Crews provide autonomous agent collaboration, ideal for tasks requiring flexible decision-making and dynamic interaction. Flows offer precise, event-driven control, ideal for managing detailed execution paths and secure state management. You can seamlessly combine both for maximum effectiveness.&lt;/p&gt; 
&lt;h3&gt;Q: How is CrewAI better than LangChain?&lt;/h3&gt; 
&lt;p&gt;A: CrewAI provides simpler, more intuitive APIs, faster execution speeds, more reliable and consistent results, robust documentation, and an active community—addressing common criticisms and limitations associated with LangChain.&lt;/p&gt; 
&lt;h3&gt;Q: Is CrewAI open-source?&lt;/h3&gt; 
&lt;p&gt;A: Yes, CrewAI is open-source and actively encourages community contributions and collaboration.&lt;/p&gt; 
&lt;h3&gt;Q: Does CrewAI collect data from users?&lt;/h3&gt; 
&lt;p&gt;A: CrewAI collects anonymous telemetry data strictly for improvement purposes. Sensitive data such as prompts, tasks, or API responses are never collected unless explicitly enabled by the user.&lt;/p&gt; 
&lt;h3&gt;Q: Where can I find real-world CrewAI examples?&lt;/h3&gt; 
&lt;p&gt;A: Check out practical examples in the &lt;a href=&quot;https://github.com/crewAIInc/crewAI-examples&quot;&gt;CrewAI-examples repository&lt;/a&gt;, covering use cases like trip planners, stock analysis, and job postings.&lt;/p&gt; 
&lt;h3&gt;Q: How can I contribute to CrewAI?&lt;/h3&gt; 
&lt;p&gt;A: Contributions are warmly welcomed! Fork the repository, create your branch, implement your changes, and submit a pull request. See the Contribution section of the README for detailed guidelines.&lt;/p&gt; 
&lt;h3&gt;Q: What additional features does CrewAI Enterprise offer?&lt;/h3&gt; 
&lt;p&gt;A: CrewAI Enterprise provides advanced features such as a unified control plane, real-time observability, secure integrations, advanced security, actionable insights, and dedicated 24/7 enterprise support.&lt;/p&gt; 
&lt;h3&gt;Q: Is CrewAI Enterprise available for cloud and on-premise deployments?&lt;/h3&gt; 
&lt;p&gt;A: Yes, CrewAI Enterprise supports both cloud-based and on-premise deployment options, allowing enterprises to meet their specific security and compliance requirements.&lt;/p&gt; 
&lt;h3&gt;Q: Can I try CrewAI Enterprise for free?&lt;/h3&gt; 
&lt;p&gt;A: Yes, you can explore part of the CrewAI Enterprise Suite by accessing the &lt;a href=&quot;https://app.crewai.com&quot;&gt;Crew Control Plane&lt;/a&gt; for free.&lt;/p&gt; 
&lt;h3&gt;Q: Does CrewAI support fine-tuning or training custom models?&lt;/h3&gt; 
&lt;p&gt;A: Yes, CrewAI can integrate with custom-trained or fine-tuned models, allowing you to enhance your agents with domain-specific knowledge and accuracy.&lt;/p&gt; 
&lt;h3&gt;Q: Can CrewAI agents interact with external tools and APIs?&lt;/h3&gt; 
&lt;p&gt;A: Absolutely! CrewAI agents can easily integrate with external tools, APIs, and databases, empowering them to leverage real-world data and resources.&lt;/p&gt; 
&lt;h3&gt;Q: Is CrewAI suitable for production environments?&lt;/h3&gt; 
&lt;p&gt;A: Yes, CrewAI is explicitly designed with production-grade standards, ensuring reliability, stability, and scalability for enterprise deployments.&lt;/p&gt; 
&lt;h3&gt;Q: How scalable is CrewAI?&lt;/h3&gt; 
&lt;p&gt;A: CrewAI is highly scalable, supporting simple automations and large-scale enterprise workflows involving numerous agents and complex tasks simultaneously.&lt;/p&gt; 
&lt;h3&gt;Q: Does CrewAI offer debugging and monitoring tools?&lt;/h3&gt; 
&lt;p&gt;A: Yes, CrewAI Enterprise includes advanced debugging, tracing, and real-time observability features, simplifying the management and troubleshooting of your automations.&lt;/p&gt; 
&lt;h3&gt;Q: What programming languages does CrewAI support?&lt;/h3&gt; 
&lt;p&gt;A: CrewAI is primarily Python-based but easily integrates with services and APIs written in any programming language through its flexible API integration capabilities.&lt;/p&gt; 
&lt;h3&gt;Q: Does CrewAI offer educational resources for beginners?&lt;/h3&gt; 
&lt;p&gt;A: Yes, CrewAI provides extensive beginner-friendly tutorials, courses, and documentation through learn.crewai.com, supporting developers at all skill levels.&lt;/p&gt; 
&lt;h3&gt;Q: Can CrewAI automate human-in-the-loop workflows?&lt;/h3&gt; 
&lt;p&gt;A: Yes, CrewAI fully supports human-in-the-loop workflows, allowing seamless collaboration between human experts and AI agents for enhanced decision-making.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>langchain-ai/ollama-deep-researcher</title>
      <link>https://github.com/langchain-ai/ollama-deep-researcher</link>
      <description>&lt;p&gt;Fully local web research and report writing assistant&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Ollama Deep Researcher&lt;/h1&gt; 
&lt;p&gt;Ollama Deep Researcher is a fully local web research assistant that uses any LLM hosted by &lt;a href=&quot;https://ollama.com/search&quot;&gt;Ollama&lt;/a&gt;. Give it a topic and it will generate a web search query, gather web search results (via &lt;a href=&quot;https://www.tavily.com/&quot;&gt;Tavily&lt;/a&gt; by default), summarize the results of web search, reflect on the summary to examine knowledge gaps, generate a new search query to address the gaps, search, and improve the summary for a user-defined number of cycles. It will provide the user a final markdown summary with all sources used.&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/4308ee9c-abf3-4abb-9d1e-83e7c2c3f187&quot; alt=&quot;research-rabbit&quot;&gt;&lt;/p&gt; 
&lt;p&gt;Short summary: 
 &lt;video src=&quot;https://github.com/user-attachments/assets/02084902-f067-4658-9683-ff312cab7944&quot; controls&gt;&lt;/video&gt;&lt;/p&gt; 
&lt;h2&gt;📺 Video Tutorials&lt;/h2&gt; 
&lt;p&gt;See it in action or build it yourself? Check out these helpful video tutorials:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=sGUjmyfof4Q&quot;&gt;Overview of Ollama Deep Researcher with R1&lt;/a&gt; - Load and test &lt;a href=&quot;https://api-docs.deepseek.com/news/news250120&quot;&gt;DeepSeek R1&lt;/a&gt; &lt;a href=&quot;https://ollama.com/library/deepseek-r1&quot;&gt;distilled models&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=XGuTzHoqlj8&quot;&gt;Building Ollama Deep Researcher from Scratch&lt;/a&gt; - Overview of how this is built.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🚀 Quickstart&lt;/h2&gt; 
&lt;h3&gt;Mac&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Download the Ollama app for Mac &lt;a href=&quot;https://ollama.com/download&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Pull a local LLM from &lt;a href=&quot;https://ollama.com/search&quot;&gt;Ollama&lt;/a&gt;. As an &lt;a href=&quot;https://ollama.com/library/deepseek-r1:8b&quot;&gt;example&lt;/a&gt;:&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;ollama pull deepseek-r1:8b
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start=&quot;3&quot;&gt; 
 &lt;li&gt;Clone the repository:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;git clone https://github.com/langchain-ai/ollama-deep-researcher.git
cd ollama-deep-researcher
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start=&quot;4&quot;&gt; 
 &lt;li&gt;Select a web search tool:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;By default, it will use &lt;a href=&quot;https://duckduckgo.com/&quot;&gt;DuckDuckGo&lt;/a&gt; for web search, which does not require an API key. But you can also use &lt;a href=&quot;https://tavily.com/&quot;&gt;Tavily&lt;/a&gt; or &lt;a href=&quot;https://www.perplexity.ai/hub/blog/introducing-the-sonar-pro-api&quot;&gt;Perplexity&lt;/a&gt; by adding their API keys to the environment file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;cp .env.example .env
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The following environment variables are supported:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;OLLAMA_BASE_URL&lt;/code&gt; - the endpoint of the Ollama service, defaults to &lt;code&gt;http://localhost:11434&lt;/code&gt; if not set&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;OLLAMA_MODEL&lt;/code&gt; - the model to use, defaults to &lt;code&gt;llama3.2&lt;/code&gt; if not set&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;SEARCH_API&lt;/code&gt; - the search API to use, either &lt;code&gt;duckduckgo&lt;/code&gt; (default) or &lt;code&gt;tavily&lt;/code&gt; or &lt;code&gt;perplexity&lt;/code&gt;. You need to set the corresponding API key if tavily or perplexity is used.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;TAVILY_API_KEY&lt;/code&gt; - the tavily API key to use&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;PERPLEXITY_API_KEY&lt;/code&gt; - the perplexity API key to use&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;MAX_WEB_RESEARCH_LOOPS&lt;/code&gt; - the maximum number of research loop steps, defaults to &lt;code&gt;3&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;FETCH_FULL_PAGE&lt;/code&gt; - fetch the full page content if using &lt;code&gt;duckduckgo&lt;/code&gt; for the search API, defaults to &lt;code&gt;false&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;ol start=&quot;5&quot;&gt; 
 &lt;li&gt;(Recommended) Create a virtual environment:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python -m venv .venv
source .venv/bin/activate
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start=&quot;6&quot;&gt; 
 &lt;li&gt;Launch the assistant with the LangGraph server:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# Install uv package manager
curl -LsSf https://astral.sh/uv/install.sh | sh
uvx --refresh --from &quot;langgraph-cli[inmem]&quot; --with-editable . --python 3.11 langgraph dev
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Windows&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Download the Ollama app for Windows &lt;a href=&quot;https://ollama.com/download&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Pull a local LLM from &lt;a href=&quot;https://ollama.com/search&quot;&gt;Ollama&lt;/a&gt;. As an &lt;a href=&quot;https://ollama.com/library/deepseek-r1:8b&quot;&gt;example&lt;/a&gt;:&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-powershell&quot;&gt;ollama pull deepseek-r1:8b
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start=&quot;3&quot;&gt; 
 &lt;li&gt;Clone the repository:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;git clone https://github.com/langchain-ai/ollama-deep-researcher.git
cd ollama-deep-researcher
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start=&quot;4&quot;&gt; 
 &lt;li&gt; &lt;p&gt;Select a web search tool, as above.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;(Recommended) Create a virtual environment: Install &lt;code&gt;Python 3.11&lt;/code&gt; (and add to PATH during installation). Restart your terminal to ensure Python is available, then create and activate a virtual environment:&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-powershell&quot;&gt;python -m venv .venv
.venv\Scripts\Activate.ps1
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start=&quot;6&quot;&gt; 
 &lt;li&gt;Launch the assistant with the LangGraph server:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-powershell&quot;&gt;# Install dependencies
pip install -e .
pip install -U &quot;langgraph-cli[inmem]&quot;            

# Start the LangGraph server
langgraph dev
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Using the LangGraph Studio UI&lt;/h3&gt; 
&lt;p&gt;When you launch LangGraph server, you should see the following output and Studio will open in your browser:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Ready!&lt;/p&gt; 
 &lt;p&gt;API: &lt;a href=&quot;http://127.0.0.1:2024&quot;&gt;http://127.0.0.1:2024&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;Docs: &lt;a href=&quot;http://127.0.0.1:2024/docs&quot;&gt;http://127.0.0.1:2024/docs&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;LangGraph Studio Web UI: &lt;a href=&quot;https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024&quot;&gt;https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Open &lt;code&gt;LangGraph Studio Web UI&lt;/code&gt; via the URL in the output above.&lt;/p&gt; 
&lt;p&gt;In the &lt;code&gt;configuration&lt;/code&gt; tab:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Pick your web search tool (DuckDuckGo, Tavily, or Perplexity) (it will by default be &lt;code&gt;DuckDuckGo&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Set the name of your local LLM to use with Ollama (it will by default be &lt;code&gt;llama3.2&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;You can set the depth of the research iterations (it will by default be &lt;code&gt;3&lt;/code&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;img width=&quot;1621&quot; alt=&quot;Screenshot 2025-01-24 at 10 08 31 PM&quot; src=&quot;https://github.com/user-attachments/assets/7cfd0e04-28fd-4cfa-aee5-9a556d74ab21&quot;&gt; 
&lt;p&gt;Give the assistant a topic for research, and you can visualize its process!&lt;/p&gt; 
&lt;img width=&quot;1621&quot; alt=&quot;Screenshot 2025-01-24 at 10 08 22 PM&quot; src=&quot;https://github.com/user-attachments/assets/4de6bd89-4f3b-424c-a9cb-70ebd3d45c5f&quot;&gt; 
&lt;h3&gt;Model Compatibility Note&lt;/h3&gt; 
&lt;p&gt;When selecting a local LLM, note that this application relies on the model&#39;s ability to produce structured JSON output. Some models may have difficulty with this requirement:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Working well&lt;/strong&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://ollama.com/library/llama3.2&quot;&gt;Llama2 3.2&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://ollama.com/library/deepseek-r1:8b&quot;&gt;DeepSeek R1 (8B)&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Known issues&lt;/strong&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://ollama.com/library/deepseek-llm:7b&quot;&gt;DeepSeek R1 (7B)&lt;/a&gt; - Currently has difficulty producing required JSON output&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If you &lt;a href=&quot;https://github.com/langchain-ai/ollama-deep-researcher/issues/18&quot;&gt;encounter JSON-related errors&lt;/a&gt; (e.g., &lt;code&gt;KeyError: &#39;query&#39;&lt;/code&gt;), try switching to one of the confirmed working models.&lt;/p&gt; 
&lt;h3&gt;Browser Compatibility Note&lt;/h3&gt; 
&lt;p&gt;When accessing the LangGraph Studio UI:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Firefox is recommended for the best experience&lt;/li&gt; 
 &lt;li&gt;Safari users may encounter security warnings due to mixed content (HTTPS/HTTP)&lt;/li&gt; 
 &lt;li&gt;If you encounter issues, try: 
  &lt;ol&gt; 
   &lt;li&gt;Using Firefox or another browser&lt;/li&gt; 
   &lt;li&gt;Disabling ad-blocking extensions&lt;/li&gt; 
   &lt;li&gt;Checking browser console for specific error messages&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;How it works&lt;/h2&gt; 
&lt;p&gt;Ollama Deep Researcher is inspired by &lt;a href=&quot;https://arxiv.org/html/2410.04343v1#:~:text=To%20tackle%20this%20issue%2C%20we,used%20to%20generate%20intermediate%20answers.&quot;&gt;IterDRAG&lt;/a&gt;. This approach will decompose a query into sub-queries, retrieve documents for each one, answer the sub-query, and then build on the answer by retrieving docs for the second sub-query. Here, we do similar:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Given a user-provided topic, use a local LLM (via &lt;a href=&quot;https://ollama.com/search&quot;&gt;Ollama&lt;/a&gt;) to generate a web search query&lt;/li&gt; 
 &lt;li&gt;Uses a search engine (configured for &lt;a href=&quot;https://duckduckgo.com/&quot;&gt;DuckDuckGo&lt;/a&gt;, &lt;a href=&quot;https://www.tavily.com/&quot;&gt;Tavily&lt;/a&gt;, or &lt;a href=&quot;https://www.perplexity.ai/hub/blog/introducing-the-sonar-pro-api&quot;&gt;Perplexity&lt;/a&gt;) to find relevant sources&lt;/li&gt; 
 &lt;li&gt;Uses LLM to summarize the findings from web search related to the user-provided research topic&lt;/li&gt; 
 &lt;li&gt;Then, it uses the LLM to reflect on the summary, identifying knowledge gaps&lt;/li&gt; 
 &lt;li&gt;It generates a new search query to address the knowledge gaps&lt;/li&gt; 
 &lt;li&gt;The process repeats, with the summary being iteratively updated with new information from web search&lt;/li&gt; 
 &lt;li&gt;It will repeat down the research rabbit hole&lt;/li&gt; 
 &lt;li&gt;Runs for a configurable number of iterations (see &lt;code&gt;configuration&lt;/code&gt; tab)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Outputs&lt;/h2&gt; 
&lt;p&gt;The output of the graph is a markdown file containing the research summary, with citations to the sources used.&lt;/p&gt; 
&lt;p&gt;All sources gathered during research are saved to the graph state.&lt;/p&gt; 
&lt;p&gt;You can visualize them in the graph state, which is visible in LangGraph Studio:&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/e8ac1c0b-9acb-4a75-8c15-4e677e92f6cb&quot; alt=&quot;Screenshot 2024-12-05 at 4 08 59 PM&quot;&gt;&lt;/p&gt; 
&lt;p&gt;The final summary is saved to the graph state as well:&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/f6d997d5-9de5-495f-8556-7d3891f6bc96&quot; alt=&quot;Screenshot 2024-12-05 at 4 10 11 PM&quot;&gt;&lt;/p&gt; 
&lt;h2&gt;Deployment Options&lt;/h2&gt; 
&lt;p&gt;There are &lt;a href=&quot;https://langchain-ai.github.io/langgraph/concepts/#deployment-options&quot;&gt;various ways&lt;/a&gt; to deploy this graph.&lt;/p&gt; 
&lt;p&gt;See &lt;a href=&quot;https://github.com/langchain-ai/langchain-academy/tree/main/module-6&quot;&gt;Module 6&lt;/a&gt; of LangChain Academy for a detailed walkthrough of deployment options with LangGraph.&lt;/p&gt; 
&lt;h2&gt;TypeScript Implementation&lt;/h2&gt; 
&lt;p&gt;A TypeScript port of this project (without Perplexity search) is available at: &lt;a href=&quot;https://github.com/PacoVK/ollama-deep-researcher-ts&quot;&gt;https://github.com/PacoVK/ollama-deep-researcher-ts&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Running as a Docker container&lt;/h2&gt; 
&lt;p&gt;The included &lt;code&gt;Dockerfile&lt;/code&gt; only runs LangChain Studio with ollama-deep-researcher as a service, but does not include Ollama as a dependant service. You must run Ollama separately and configure the &lt;code&gt;OLLAMA_BASE_URL&lt;/code&gt; environment variable. Optionally you can also specify the Ollama model to use by providing the &lt;code&gt;OLLAMA_MODEL&lt;/code&gt; environment variable.&lt;/p&gt; 
&lt;p&gt;Clone the repo and build an image:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ docker build -t ollama-deep-researcher .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Run the container:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ docker run --rm -it -p 2024:2024 \
  -e SEARCH_API=&quot;tavily&quot; \ 
  -e TAVILY_API_KEY=&quot;tvly-***YOUR_KEY_HERE***&quot; \
  -e OLLAMA_BASE_URL=&quot;http://host.docker.internal:11434/&quot; \
  -e OLLAMA_MODEL=&quot;llama3.2&quot; \  
  ollama-deep-researcher
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;NOTE: You will see log message:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;2025-02-10T13:45:04.784915Z [info     ] 🎨 Opening Studio in your browser... [browser_opener] api_variant=local_dev message=🎨 Opening Studio in your browser...
URL: https://smith.langchain.com/studio/?baseUrl=http://0.0.0.0:2024
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;...but the browser will not launch from the container.&lt;/p&gt; 
&lt;p&gt;Instead, visit this link with the correct baseUrl IP address: &lt;a href=&quot;https://smith.langchain.com/studio/thread?baseUrl=http://127.0.0.1:2024&quot;&gt;&lt;code&gt;https://smith.langchain.com/studio/thread?baseUrl=http://127.0.0.1:2024&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>openai/openai-python</title>
      <link>https://github.com/openai/openai-python</link>
      <description>&lt;p&gt;The official Python library for the OpenAI API&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;OpenAI Python API library&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://pypi.org/project/openai/&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/v/openai.svg?sanitize=true&quot; alt=&quot;PyPI version&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;The OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.8+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by &lt;a href=&quot;https://github.com/encode/httpx&quot;&gt;httpx&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;It is generated from our &lt;a href=&quot;https://github.com/openai/openai-openapi&quot;&gt;OpenAPI specification&lt;/a&gt; with &lt;a href=&quot;https://stainlessapi.com/&quot;&gt;Stainless&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;The REST API documentation can be found on &lt;a href=&quot;https://platform.openai.com/docs/api-reference&quot;&gt;platform.openai.com&lt;/a&gt;. The full API of this library can be found in &lt;a href=&quot;https://raw.githubusercontent.com/openai/openai-python/main/api.md&quot;&gt;api.md&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;# install from PyPI
pip install openai
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;p&gt;The full API of this library can be found in &lt;a href=&quot;https://raw.githubusercontent.com/openai/openai-python/main/api.md&quot;&gt;api.md&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The primary API for interacting with OpenAI models is the &lt;a href=&quot;https://platform.openai.com/docs/api-reference/responses&quot;&gt;Responses API&lt;/a&gt;. You can generate text from the model with the code below.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import os
from openai import OpenAI

client = OpenAI(
    # This is the default and can be omitted
    api_key=os.environ.get(&quot;OPENAI_API_KEY&quot;),
)

response = client.responses.create(
    model=&quot;gpt-4o&quot;,
    instructions=&quot;You are a coding assistant that talks like a pirate.&quot;,
    input=&quot;How do I check if a Python object is an instance of a class?&quot;,
)

print(response.output_text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The previous standard (supported indefinitely) for generating text is the &lt;a href=&quot;https://platform.openai.com/docs/api-reference/chat&quot;&gt;Chat Completions API&lt;/a&gt;. You can use that API to generate text from the model with the code below.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from openai import OpenAI

client = OpenAI()

completion = client.chat.completions.create(
    model=&quot;gpt-4o&quot;,
    messages=[
        {&quot;role&quot;: &quot;developer&quot;, &quot;content&quot;: &quot;Talk like a pirate.&quot;},
        {
            &quot;role&quot;: &quot;user&quot;,
            &quot;content&quot;: &quot;How do I check if a Python object is an instance of a class?&quot;,
        },
    ],
)

print(completion.choices[0].message.content)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;While you can provide an &lt;code&gt;api_key&lt;/code&gt; keyword argument, we recommend using &lt;a href=&quot;https://pypi.org/project/python-dotenv/&quot;&gt;python-dotenv&lt;/a&gt; to add &lt;code&gt;OPENAI_API_KEY=&quot;My API Key&quot;&lt;/code&gt; to your &lt;code&gt;.env&lt;/code&gt; file so that your API key is not stored in source control. &lt;a href=&quot;https://platform.openai.com/settings/organization/api-keys&quot;&gt;Get an API key here&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Vision&lt;/h3&gt; 
&lt;p&gt;With an image URL:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;prompt = &quot;What is in this image?&quot;
img_url = &quot;https://upload.wikimedia.org/wikipedia/commons/thumb/d/d5/2023_06_08_Raccoon1.jpg/1599px-2023_06_08_Raccoon1.jpg&quot;

response = client.responses.create(
    model=&quot;gpt-4o-mini&quot;,
    input=[
        {
            &quot;role&quot;: &quot;user&quot;,
            &quot;content&quot;: [
                {&quot;type&quot;: &quot;input_text&quot;, &quot;text&quot;: prompt},
                {&quot;type&quot;: &quot;input_image&quot;, &quot;image_url&quot;: f&quot;{img_url}&quot;},
            ],
        }
    ],
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;With the image as a base64 encoded string:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import base64
from openai import OpenAI

client = OpenAI()

prompt = &quot;What is in this image?&quot;
with open(&quot;path/to/image.png&quot;, &quot;rb&quot;) as image_file:
    b64_image = base64.b64encode(image_file.read()).decode(&quot;utf-8&quot;)

response = client.responses.create(
    model=&quot;gpt-4o-mini&quot;,
    input=[
        {
            &quot;role&quot;: &quot;user&quot;,
            &quot;content&quot;: [
                {&quot;type&quot;: &quot;input_text&quot;, &quot;text&quot;: prompt},
                {&quot;type&quot;: &quot;input_image&quot;, &quot;image_url&quot;: f&quot;data:image/png;base64,{b64_image}&quot;},
            ],
        }
    ],
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Async usage&lt;/h2&gt; 
&lt;p&gt;Simply import &lt;code&gt;AsyncOpenAI&lt;/code&gt; instead of &lt;code&gt;OpenAI&lt;/code&gt; and use &lt;code&gt;await&lt;/code&gt; with each API call:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import os
import asyncio
from openai import AsyncOpenAI

client = AsyncOpenAI(
    # This is the default and can be omitted
    api_key=os.environ.get(&quot;OPENAI_API_KEY&quot;),
)


async def main() -&amp;gt; None:
    response = await client.responses.create(
        model=&quot;gpt-4o&quot;, input=&quot;Explain disestablishmentarianism to a smart five year old.&quot;
    )
    print(response.output_text)


asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Functionality between the synchronous and asynchronous clients is otherwise identical.&lt;/p&gt; 
&lt;h2&gt;Streaming responses&lt;/h2&gt; 
&lt;p&gt;We provide support for streaming responses using Server Side Events (SSE).&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from openai import OpenAI

client = OpenAI()

stream = client.responses.create(
    model=&quot;gpt-4o&quot;,
    input=&quot;Write a one-sentence bedtime story about a unicorn.&quot;,
    stream=True,
)

for event in stream:
    print(event)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The async client uses the exact same interface.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import asyncio
from openai import AsyncOpenAI

client = AsyncOpenAI()


async def main():
    stream = client.responses.create(
        model=&quot;gpt-4o&quot;,
        input=&quot;Write a one-sentence bedtime story about a unicorn.&quot;,
        stream=True,
    )

    for event in stream:
        print(event)


asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Realtime API beta&lt;/h2&gt; 
&lt;p&gt;The Realtime API enables you to build low-latency, multi-modal conversational experiences. It currently supports text and audio as both input and output, as well as &lt;a href=&quot;https://platform.openai.com/docs/guides/function-calling&quot;&gt;function calling&lt;/a&gt; through a WebSocket connection.&lt;/p&gt; 
&lt;p&gt;Under the hood the SDK uses the &lt;a href=&quot;https://websockets.readthedocs.io/en/stable/&quot;&gt;&lt;code&gt;websockets&lt;/code&gt;&lt;/a&gt; library to manage connections.&lt;/p&gt; 
&lt;p&gt;The Realtime API works through a combination of client-sent events and server-sent events. Clients can send events to do things like update session configuration or send text and audio inputs. Server events confirm when audio responses have completed, or when a text response from the model has been received. A full event reference can be found &lt;a href=&quot;https://platform.openai.com/docs/api-reference/realtime-client-events&quot;&gt;here&lt;/a&gt; and a guide can be found &lt;a href=&quot;https://platform.openai.com/docs/guides/realtime&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Basic text based example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-py&quot;&gt;import asyncio
from openai import AsyncOpenAI

async def main():
    client = AsyncOpenAI()

    async with client.beta.realtime.connect(model=&quot;gpt-4o-realtime-preview&quot;) as connection:
        await connection.session.update(session={&#39;modalities&#39;: [&#39;text&#39;]})

        await connection.conversation.item.create(
            item={
                &quot;type&quot;: &quot;message&quot;,
                &quot;role&quot;: &quot;user&quot;,
                &quot;content&quot;: [{&quot;type&quot;: &quot;input_text&quot;, &quot;text&quot;: &quot;Say hello!&quot;}],
            }
        )
        await connection.response.create()

        async for event in connection:
            if event.type == &#39;response.text.delta&#39;:
                print(event.delta, flush=True, end=&quot;&quot;)

            elif event.type == &#39;response.text.done&#39;:
                print()

            elif event.type == &quot;response.done&quot;:
                break

asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;However the real magic of the Realtime API is handling audio inputs / outputs, see this example &lt;a href=&quot;https://github.com/openai/openai-python/raw/main/examples/realtime/push_to_talk_app.py&quot;&gt;TUI script&lt;/a&gt; for a fully fledged example.&lt;/p&gt; 
&lt;h3&gt;Realtime error handling&lt;/h3&gt; 
&lt;p&gt;Whenever an error occurs, the Realtime API will send an &lt;a href=&quot;https://platform.openai.com/docs/guides/realtime-model-capabilities#error-handling&quot;&gt;&lt;code&gt;error&lt;/code&gt; event&lt;/a&gt; and the connection will stay open and remain usable. This means you need to handle it yourself, as &lt;em&gt;no errors are raised directly&lt;/em&gt; by the SDK when an &lt;code&gt;error&lt;/code&gt; event comes in.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-py&quot;&gt;client = AsyncOpenAI()

async with client.beta.realtime.connect(model=&quot;gpt-4o-realtime-preview&quot;) as connection:
    ...
    async for event in connection:
        if event.type == &#39;error&#39;:
            print(event.error.type)
            print(event.error.code)
            print(event.error.event_id)
            print(event.error.message)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Using types&lt;/h2&gt; 
&lt;p&gt;Nested request parameters are &lt;a href=&quot;https://docs.python.org/3/library/typing.html#typing.TypedDict&quot;&gt;TypedDicts&lt;/a&gt;. Responses are &lt;a href=&quot;https://docs.pydantic.dev&quot;&gt;Pydantic models&lt;/a&gt; which also provide helper methods for things like:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Serializing back into JSON, &lt;code&gt;model.to_json()&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Converting to a dictionary, &lt;code&gt;model.to_dict()&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Typed requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set &lt;code&gt;python.analysis.typeCheckingMode&lt;/code&gt; to &lt;code&gt;basic&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Pagination&lt;/h2&gt; 
&lt;p&gt;List methods in the OpenAI API are paginated.&lt;/p&gt; 
&lt;p&gt;This library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from openai import OpenAI

client = OpenAI()

all_jobs = []
# Automatically fetches more pages as needed.
for job in client.fine_tuning.jobs.list(
    limit=20,
):
    # Do something with job here
    all_jobs.append(job)
print(all_jobs)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or, asynchronously:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import asyncio
from openai import AsyncOpenAI

client = AsyncOpenAI()


async def main() -&amp;gt; None:
    all_jobs = []
    # Iterate through items across all pages, issuing requests as needed.
    async for job in client.fine_tuning.jobs.list(
        limit=20,
    ):
        all_jobs.append(job)
    print(all_jobs)


asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, you can use the &lt;code&gt;.has_next_page()&lt;/code&gt;, &lt;code&gt;.next_page_info()&lt;/code&gt;, or &lt;code&gt;.get_next_page()&lt;/code&gt; methods for more granular control working with pages:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;first_page = await client.fine_tuning.jobs.list(
    limit=20,
)
if first_page.has_next_page():
    print(f&quot;will fetch next page using these details: {first_page.next_page_info()}&quot;)
    next_page = await first_page.get_next_page()
    print(f&quot;number of items we just fetched: {len(next_page.data)}&quot;)

# Remove `await` for non-async usage.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or just work directly with the returned data:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;first_page = await client.fine_tuning.jobs.list(
    limit=20,
)

print(f&quot;next page cursor: {first_page.after}&quot;)  # =&amp;gt; &quot;next page cursor: ...&quot;
for job in first_page.data:
    print(job.id)

# Remove `await` for non-async usage.
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Nested params&lt;/h2&gt; 
&lt;p&gt;Nested parameters are dictionaries, typed using &lt;code&gt;TypedDict&lt;/code&gt;, for example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from openai import OpenAI

client = OpenAI()

response = client.chat.responses.create(
    input=[
        {
            &quot;role&quot;: &quot;user&quot;,
            &quot;content&quot;: &quot;How much ?&quot;,
        }
    ],
    model=&quot;gpt-4o&quot;,
    response_format={&quot;type&quot;: &quot;json_object&quot;},
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;File uploads&lt;/h2&gt; 
&lt;p&gt;Request parameters that correspond to file uploads can be passed as &lt;code&gt;bytes&lt;/code&gt;, a &lt;a href=&quot;https://docs.python.org/3/library/os.html#os.PathLike&quot;&gt;&lt;code&gt;PathLike&lt;/code&gt;&lt;/a&gt; instance or a tuple of &lt;code&gt;(filename, contents, media type)&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from pathlib import Path
from openai import OpenAI

client = OpenAI()

client.files.create(
    file=Path(&quot;input.jsonl&quot;),
    purpose=&quot;fine-tune&quot;,
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The async client uses the exact same interface. If you pass a &lt;a href=&quot;https://docs.python.org/3/library/os.html#os.PathLike&quot;&gt;&lt;code&gt;PathLike&lt;/code&gt;&lt;/a&gt; instance, the file contents will be read asynchronously automatically.&lt;/p&gt; 
&lt;h2&gt;Handling errors&lt;/h2&gt; 
&lt;p&gt;When the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of &lt;code&gt;openai.APIConnectionError&lt;/code&gt; is raised.&lt;/p&gt; 
&lt;p&gt;When the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of &lt;code&gt;openai.APIStatusError&lt;/code&gt; is raised, containing &lt;code&gt;status_code&lt;/code&gt; and &lt;code&gt;response&lt;/code&gt; properties.&lt;/p&gt; 
&lt;p&gt;All errors inherit from &lt;code&gt;openai.APIError&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import openai
from openai import OpenAI

client = OpenAI()

try:
    client.fine_tuning.jobs.create(
        model=&quot;gpt-4o&quot;,
        training_file=&quot;file-abc123&quot;,
    )
except openai.APIConnectionError as e:
    print(&quot;The server could not be reached&quot;)
    print(e.__cause__)  # an underlying Exception, likely raised within httpx.
except openai.RateLimitError as e:
    print(&quot;A 429 status code was received; we should back off a bit.&quot;)
except openai.APIStatusError as e:
    print(&quot;Another non-200-range status code was received&quot;)
    print(e.status_code)
    print(e.response)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Error codes are as follows:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Status Code&lt;/th&gt; 
   &lt;th&gt;Error Type&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;400&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;BadRequestError&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;401&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;AuthenticationError&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;403&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;PermissionDeniedError&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;404&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;NotFoundError&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;422&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;UnprocessableEntityError&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;429&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;RateLimitError&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&amp;gt;=500&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;InternalServerError&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;APIConnectionError&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Request IDs&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;For more information on debugging requests, see &lt;a href=&quot;https://platform.openai.com/docs/api-reference/debugging-requests&quot;&gt;these docs&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;All object responses in the SDK provide a &lt;code&gt;_request_id&lt;/code&gt; property which is added from the &lt;code&gt;x-request-id&lt;/code&gt; response header so that you can quickly log failing requests and report them back to OpenAI.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;response = await client.responses.create(
    model=&quot;gpt-4o-mini&quot;,
    input=&quot;Say &#39;this is a test&#39;.&quot;,
)
print(response._request_id)  # req_123
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note that unlike other properties that use an &lt;code&gt;_&lt;/code&gt; prefix, the &lt;code&gt;_request_id&lt;/code&gt; property &lt;em&gt;is&lt;/em&gt; public. Unless documented otherwise, &lt;em&gt;all&lt;/em&gt; other &lt;code&gt;_&lt;/code&gt; prefix properties, methods and modules are &lt;em&gt;private&lt;/em&gt;.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT]&lt;br&gt; If you need to access request IDs for failed requests you must catch the &lt;code&gt;APIStatusError&lt;/code&gt; exception&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import openai

try:
    completion = await client.chat.completions.create(
        messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Say this is a test&quot;}], model=&quot;gpt-4&quot;
    )
except openai.APIStatusError as exc:
    print(exc.request_id)  # req_123
    raise exc
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Retries&lt;/h2&gt; 
&lt;p&gt;Certain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and &amp;gt;=500 Internal errors are all retried by default.&lt;/p&gt; 
&lt;p&gt;You can use the &lt;code&gt;max_retries&lt;/code&gt; option to configure or disable retry settings:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 2
    max_retries=0,
)

# Or, configure per-request:
client.with_options(max_retries=5).chat.completions.create(
    messages=[
        {
            &quot;role&quot;: &quot;user&quot;,
            &quot;content&quot;: &quot;How can I get the name of the current day in JavaScript?&quot;,
        }
    ],
    model=&quot;gpt-4o&quot;,
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Timeouts&lt;/h2&gt; 
&lt;p&gt;By default requests time out after 10 minutes. You can configure this with a &lt;code&gt;timeout&lt;/code&gt; option, which accepts a float or an &lt;a href=&quot;https://www.python-httpx.org/advanced/timeouts/#fine-tuning-the-configuration&quot;&gt;&lt;code&gt;httpx.Timeout&lt;/code&gt;&lt;/a&gt; object:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # 20 seconds (default is 10 minutes)
    timeout=20.0,
)

# More granular control:
client = OpenAI(
    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),
)

# Override per-request:
client.with_options(timeout=5.0).chat.completions.create(
    messages=[
        {
            &quot;role&quot;: &quot;user&quot;,
            &quot;content&quot;: &quot;How can I list all files in a directory using Python?&quot;,
        }
    ],
    model=&quot;gpt-4o&quot;,
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;On timeout, an &lt;code&gt;APITimeoutError&lt;/code&gt; is thrown.&lt;/p&gt; 
&lt;p&gt;Note that requests that time out are &lt;a href=&quot;https://raw.githubusercontent.com/openai/openai-python/main/#retries&quot;&gt;retried twice by default&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Advanced&lt;/h2&gt; 
&lt;h3&gt;Logging&lt;/h3&gt; 
&lt;p&gt;We use the standard library &lt;a href=&quot;https://docs.python.org/3/library/logging.html&quot;&gt;&lt;code&gt;logging&lt;/code&gt;&lt;/a&gt; module.&lt;/p&gt; 
&lt;p&gt;You can enable logging by setting the environment variable &lt;code&gt;OPENAI_LOG&lt;/code&gt; to &lt;code&gt;info&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;$ export OPENAI_LOG=info
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or to &lt;code&gt;debug&lt;/code&gt; for more verbose logging.&lt;/p&gt; 
&lt;h3&gt;How to tell whether &lt;code&gt;None&lt;/code&gt; means &lt;code&gt;null&lt;/code&gt; or missing&lt;/h3&gt; 
&lt;p&gt;In an API response, a field may be explicitly &lt;code&gt;null&lt;/code&gt;, or missing entirely; in either case, its value is &lt;code&gt;None&lt;/code&gt; in this library. You can differentiate the two cases with &lt;code&gt;.model_fields_set&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-py&quot;&gt;if response.my_field is None:
  if &#39;my_field&#39; not in response.model_fields_set:
    print(&#39;Got json like {}, without a &quot;my_field&quot; key present at all.&#39;)
  else:
    print(&#39;Got json like {&quot;my_field&quot;: null}.&#39;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Accessing raw response data (e.g. headers)&lt;/h3&gt; 
&lt;p&gt;The &quot;raw&quot; Response object can be accessed by prefixing &lt;code&gt;.with_raw_response.&lt;/code&gt; to any HTTP method call, e.g.,&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-py&quot;&gt;from openai import OpenAI

client = OpenAI()
response = client.chat.completions.with_raw_response.create(
    messages=[{
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: &quot;Say this is a test&quot;,
    }],
    model=&quot;gpt-4o&quot;,
)
print(response.headers.get(&#39;X-My-Header&#39;))

completion = response.parse()  # get the object that `chat.completions.create()` would have returned
print(completion)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;These methods return a &lt;a href=&quot;https://github.com/openai/openai-python/tree/main/src/openai/_legacy_response.py&quot;&gt;&lt;code&gt;LegacyAPIResponse&lt;/code&gt;&lt;/a&gt; object. This is a legacy class as we&#39;re changing it slightly in the next major version.&lt;/p&gt; 
&lt;p&gt;For the sync client this will mostly be the same with the exception of &lt;code&gt;content&lt;/code&gt; &amp;amp; &lt;code&gt;text&lt;/code&gt; will be methods instead of properties. In the async client, all methods will be async.&lt;/p&gt; 
&lt;p&gt;A migration script will be provided &amp;amp; the migration in general should be smooth.&lt;/p&gt; 
&lt;h4&gt;&lt;code&gt;.with_streaming_response&lt;/code&gt;&lt;/h4&gt; 
&lt;p&gt;The above interface eagerly reads the full response body when you make the request, which may not always be what you want.&lt;/p&gt; 
&lt;p&gt;To stream the response body, use &lt;code&gt;.with_streaming_response&lt;/code&gt; instead, which requires a context manager and only reads the response body once you call &lt;code&gt;.read()&lt;/code&gt;, &lt;code&gt;.text()&lt;/code&gt;, &lt;code&gt;.json()&lt;/code&gt;, &lt;code&gt;.iter_bytes()&lt;/code&gt;, &lt;code&gt;.iter_text()&lt;/code&gt;, &lt;code&gt;.iter_lines()&lt;/code&gt; or &lt;code&gt;.parse()&lt;/code&gt;. In the async client, these are async methods.&lt;/p&gt; 
&lt;p&gt;As such, &lt;code&gt;.with_streaming_response&lt;/code&gt; methods return a different &lt;a href=&quot;https://github.com/openai/openai-python/tree/main/src/openai/_response.py&quot;&gt;&lt;code&gt;APIResponse&lt;/code&gt;&lt;/a&gt; object, and the async client returns an &lt;a href=&quot;https://github.com/openai/openai-python/tree/main/src/openai/_response.py&quot;&gt;&lt;code&gt;AsyncAPIResponse&lt;/code&gt;&lt;/a&gt; object.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;with client.chat.completions.with_streaming_response.create(
    messages=[
        {
            &quot;role&quot;: &quot;user&quot;,
            &quot;content&quot;: &quot;Say this is a test&quot;,
        }
    ],
    model=&quot;gpt-4o&quot;,
) as response:
    print(response.headers.get(&quot;X-My-Header&quot;))

    for line in response.iter_lines():
        print(line)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The context manager is required so that the response will reliably be closed.&lt;/p&gt; 
&lt;h3&gt;Making custom/undocumented requests&lt;/h3&gt; 
&lt;p&gt;This library is typed for convenient access to the documented API.&lt;/p&gt; 
&lt;p&gt;If you need to access undocumented endpoints, params, or response properties, the library can still be used.&lt;/p&gt; 
&lt;h4&gt;Undocumented endpoints&lt;/h4&gt; 
&lt;p&gt;To make requests to undocumented endpoints, you can make requests using &lt;code&gt;client.get&lt;/code&gt;, &lt;code&gt;client.post&lt;/code&gt;, and other http verbs. Options on the client will be respected (such as retries) when making this request.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-py&quot;&gt;import httpx

response = client.post(
    &quot;/foo&quot;,
    cast_to=httpx.Response,
    body={&quot;my_param&quot;: True},
)

print(response.headers.get(&quot;x-foo&quot;))
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Undocumented request params&lt;/h4&gt; 
&lt;p&gt;If you want to explicitly send an extra param, you can do so with the &lt;code&gt;extra_query&lt;/code&gt;, &lt;code&gt;extra_body&lt;/code&gt;, and &lt;code&gt;extra_headers&lt;/code&gt; request options.&lt;/p&gt; 
&lt;h4&gt;Undocumented response properties&lt;/h4&gt; 
&lt;p&gt;To access undocumented response properties, you can access the extra fields like &lt;code&gt;response.unknown_prop&lt;/code&gt;. You can also get all the extra fields on the Pydantic model as a dict with &lt;a href=&quot;https://docs.pydantic.dev/latest/api/base_model/#pydantic.BaseModel.model_extra&quot;&gt;&lt;code&gt;response.model_extra&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Configuring the HTTP client&lt;/h3&gt; 
&lt;p&gt;You can directly override the &lt;a href=&quot;https://www.python-httpx.org/api/#client&quot;&gt;httpx client&lt;/a&gt; to customize it for your use case, including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Support for &lt;a href=&quot;https://www.python-httpx.org/advanced/proxies/&quot;&gt;proxies&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Custom &lt;a href=&quot;https://www.python-httpx.org/advanced/transports/&quot;&gt;transports&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Additional &lt;a href=&quot;https://www.python-httpx.org/advanced/clients/&quot;&gt;advanced&lt;/a&gt; functionality&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import httpx
from openai import OpenAI, DefaultHttpxClient

client = OpenAI(
    # Or use the `OPENAI_BASE_URL` env var
    base_url=&quot;http://my.test.server.example.com:8083/v1&quot;,
    http_client=DefaultHttpxClient(
        proxy=&quot;http://my.test.proxy.example.com&quot;,
        transport=httpx.HTTPTransport(local_address=&quot;0.0.0.0&quot;),
    ),
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also customize the client on a per-request basis by using &lt;code&gt;with_options()&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;client.with_options(http_client=DefaultHttpxClient(...))
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Managing HTTP resources&lt;/h3&gt; 
&lt;p&gt;By default the library closes underlying HTTP connections whenever the client is &lt;a href=&quot;https://docs.python.org/3/reference/datamodel.html#object.__del__&quot;&gt;garbage collected&lt;/a&gt;. You can manually close the client using the &lt;code&gt;.close()&lt;/code&gt; method if desired, or with a context manager that closes when exiting.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-py&quot;&gt;from openai import OpenAI

with OpenAI() as client:
  # make requests here
  ...

# HTTP client is now closed
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Microsoft Azure OpenAI&lt;/h2&gt; 
&lt;p&gt;To use this library with &lt;a href=&quot;https://learn.microsoft.com/azure/ai-services/openai/overview&quot;&gt;Azure OpenAI&lt;/a&gt;, use the &lt;code&gt;AzureOpenAI&lt;/code&gt; class instead of the &lt;code&gt;OpenAI&lt;/code&gt; class.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won&#39;t always be correct.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class=&quot;language-py&quot;&gt;from openai import AzureOpenAI

# gets the API Key from environment variable AZURE_OPENAI_API_KEY
client = AzureOpenAI(
    # https://learn.microsoft.com/azure/ai-services/openai/reference#rest-api-versioning
    api_version=&quot;2023-07-01-preview&quot;,
    # https://learn.microsoft.com/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource
    azure_endpoint=&quot;https://example-endpoint.openai.azure.com&quot;,
)

completion = client.chat.completions.create(
    model=&quot;deployment-name&quot;,  # e.g. gpt-35-instant
    messages=[
        {
            &quot;role&quot;: &quot;user&quot;,
            &quot;content&quot;: &quot;How do I output all files in a directory using Python?&quot;,
        },
    ],
)
print(completion.to_json())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;In addition to the options provided in the base &lt;code&gt;OpenAI&lt;/code&gt; client, the following options are provided:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;azure_endpoint&lt;/code&gt; (or the &lt;code&gt;AZURE_OPENAI_ENDPOINT&lt;/code&gt; environment variable)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;azure_deployment&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;api_version&lt;/code&gt; (or the &lt;code&gt;OPENAI_API_VERSION&lt;/code&gt; environment variable)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;azure_ad_token&lt;/code&gt; (or the &lt;code&gt;AZURE_OPENAI_AD_TOKEN&lt;/code&gt; environment variable)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;azure_ad_token_provider&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;An example of using the client with Microsoft Entra ID (formerly known as Azure Active Directory) can be found &lt;a href=&quot;https://github.com/openai/openai-python/raw/main/examples/azure_ad.py&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Versioning&lt;/h2&gt; 
&lt;p&gt;This package generally follows &lt;a href=&quot;https://semver.org/spec/v2.0.0.html&quot;&gt;SemVer&lt;/a&gt; conventions, though certain backwards-incompatible changes may be released as minor versions:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Changes that only affect static types, without breaking runtime behavior.&lt;/li&gt; 
 &lt;li&gt;Changes to library internals which are technically public but not intended or documented for external use. &lt;em&gt;(Please open a GitHub issue to let us know if you are relying on such internals.)&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;Changes that we do not expect to impact the vast majority of users in practice.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;We take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.&lt;/p&gt; 
&lt;p&gt;We are keen for your feedback; please open an &lt;a href=&quot;https://www.github.com/openai/openai-python/issues&quot;&gt;issue&lt;/a&gt; with questions, bugs, or suggestions.&lt;/p&gt; 
&lt;h3&gt;Determining the installed version&lt;/h3&gt; 
&lt;p&gt;If you&#39;ve upgraded to the latest version but aren&#39;t seeing any new features you were expecting then your python environment is likely still using an older version.&lt;/p&gt; 
&lt;p&gt;You can determine the version that is being used at runtime with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-py&quot;&gt;import openai
print(openai.__version__)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;p&gt;Python 3.8 or higher.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;See &lt;a href=&quot;https://raw.githubusercontent.com/openai/openai-python/main/CONTRIBUTING.md&quot;&gt;the contributing documentation&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>AgentOps-AI/agentops</title>
      <link>https://github.com/AgentOps-AI/agentops</link>
      <description>&lt;p&gt;Python SDK for AI agent monitoring, LLM cost tracking, benchmarking, and more. Integrates with most LLMs and agent frameworks including OpenAI Agents SDK, CrewAI, Langchain, Autogen, AG2, and CamelAI&lt;/p&gt;&lt;hr&gt;&lt;div align=&quot;center&quot;&gt; 
 &lt;a href=&quot;https://agentops.ai?ref=gh&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/AgentOps-AI/agentops/main/docs/images/external/logo/github-banner.png&quot; alt=&quot;Logo&quot;&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;em&gt;Observability and DevTool platform for AI Agents&lt;/em&gt; 
&lt;/div&gt; 
&lt;br&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;a href=&quot;https://pepy.tech/project/agentops&quot;&gt; &lt;img src=&quot;https://static.pepy.tech/badge/agentops/month&quot; alt=&quot;Downloads&quot;&gt; &lt;/a&gt; 
 &lt;a href=&quot;https://github.com/agentops-ai/agentops/issues&quot;&gt; &lt;img src=&quot;https://img.shields.io/github/commit-activity/m/agentops-ai/agentops&quot; alt=&quot;git commit activity&quot;&gt; &lt;/a&gt; 
 &lt;img src=&quot;https://img.shields.io/pypi/v/agentops?&amp;amp;color=3670A0&quot; alt=&quot;PyPI - Version&quot;&gt; 
 &lt;a href=&quot;https://opensource.org/licenses/MIT&quot;&gt; &lt;img src=&quot;https://img.shields.io/badge/License-MIT-yellow.svg?&amp;amp;color=3670A0&quot; alt=&quot;License: MIT&quot;&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;a href=&quot;https://twitter.com/agentopsai/&quot;&gt; &lt;img src=&quot;https://img.shields.io/twitter/follow/agentopsai?style=social&quot; alt=&quot;Twitter&quot; style=&quot;height: 20px;&quot;&gt; &lt;/a&gt; &lt;a href=&quot;https://discord.gg/FagdcwwXRR&quot;&gt; &lt;img src=&quot;https://img.shields.io/badge/discord-7289da.svg?style=flat-square&amp;amp;logo=discord&quot; alt=&quot;Discord&quot; style=&quot;height: 20px;&quot;&gt; &lt;/a&gt; &lt;a href=&quot;https://app.agentops.ai/?ref=gh&quot;&gt; &lt;img src=&quot;https://img.shields.io/badge/Dashboard-blue.svg?style=flat-square&quot; alt=&quot;Dashboard&quot; style=&quot;height: 20px;&quot;&gt; &lt;/a&gt; &lt;a href=&quot;https://docs.agentops.ai/introduction&quot;&gt; &lt;img src=&quot;https://img.shields.io/badge/Documentation-orange.svg?style=flat-square&quot; alt=&quot;Documentation&quot; style=&quot;height: 20px;&quot;&gt; &lt;/a&gt; &lt;a href=&quot;https://entelligence.ai/AgentOps-AI&amp;amp;agentops&quot;&gt; &lt;img src=&quot;https://img.shields.io/badge/Chat%20with%20Docs-green.svg?style=flat-square&quot; alt=&quot;Chat with Docs&quot; style=&quot;height: 20px;&quot;&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;video src=&quot;https://github.com/user-attachments/assets/dfb4fa8d-d8c4-4965-9ff6-5b8514c1c22f&quot; width=&quot;650&quot; autoplay loop muted&gt;&lt;/video&gt; 
&lt;/div&gt; 
&lt;br&gt; 
&lt;p&gt;AgentOps helps developers build, evaluate, and monitor AI agents. From prototype to production.&lt;/p&gt; 
&lt;h2&gt;Key Integrations 🔌&lt;/h2&gt; 
&lt;div align=&quot;center&quot; style=&quot;background-color: white; padding: 20px; border-radius: 10px; margin: 0 auto; max-width: 800px;&quot;&gt; 
 &lt;div style=&quot;display: flex; flex-wrap: wrap; justify-content: center; align-items: center; gap: 30px; margin-bottom: 20px;&quot;&gt; 
  &lt;a href=&quot;https://docs.agentops.ai/v1/integrations/openai-agents&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/AgentOps-AI/agentops/main/docs/images/external/openai/agents-sdk.svg?sanitize=true&quot; height=&quot;45&quot; alt=&quot;OpenAI Agents SDK&quot;&gt;&lt;/a&gt; 
  &lt;a href=&quot;https://docs.agentops.ai/v1/integrations/crewai&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/AgentOps-AI/agentops/main/docs/v1/img/docs-icons/crew-banner.png&quot; height=&quot;45&quot; alt=&quot;CrewAI&quot;&gt;&lt;/a&gt; 
  &lt;a href=&quot;https://docs.ag2.ai/docs/ecosystem/agentops&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/AgentOps-AI/agentops/main/docs/images/external/autogen/ag2.svg?sanitize=true&quot; height=&quot;45&quot; alt=&quot;AG2 (AutoGen)&quot;&gt;&lt;/a&gt; 
  &lt;a href=&quot;https://docs.agentops.ai/v1/integrations/microsoft&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/AgentOps-AI/agentops/main/docs/images/external/microsoft/microsoft_logo.svg?sanitize=true&quot; height=&quot;45&quot; alt=&quot;Microsoft&quot;&gt;&lt;/a&gt; 
 &lt;/div&gt; 
 &lt;div style=&quot;display: flex; flex-wrap: wrap; justify-content: center; align-items: center; gap: 30px; margin-bottom: 20px;&quot;&gt; 
  &lt;a href=&quot;https://docs.agentops.ai/v1/integrations/langchain&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/AgentOps-AI/agentops/main/docs/images/external/langchain/langchain-logo.svg?sanitize=true&quot; height=&quot;45&quot; alt=&quot;LangChain&quot;&gt;&lt;/a&gt; 
  &lt;a href=&quot;https://docs.agentops.ai/v1/integrations/camel&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/AgentOps-AI/agentops/main/docs/images/external/camel/camel.png&quot; height=&quot;45&quot; alt=&quot;Camel AI&quot;&gt;&lt;/a&gt; 
  &lt;a href=&quot;https://docs.llamaindex.ai/en/stable/module_guides/observability/?h=agentops#agentops&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/AgentOps-AI/agentops/main/docs/images/external/ollama/ollama-icon.png&quot; height=&quot;45&quot; alt=&quot;LlamaIndex&quot;&gt;&lt;/a&gt; 
  &lt;a href=&quot;https://docs.agentops.ai/v1/integrations/cohere&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/AgentOps-AI/agentops/main/docs/images/external/cohere/cohere-logo.svg?sanitize=true&quot; height=&quot;45&quot; alt=&quot;Cohere&quot;&gt;&lt;/a&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;📊 &lt;strong&gt;Replay Analytics and Debugging&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Step-by-step agent execution graphs&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;💸 &lt;strong&gt;LLM Cost Management&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Track spend with LLM foundation model providers&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;🧪 &lt;strong&gt;Agent Benchmarking&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Test your agents against 1,000+ evals&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;🔐 &lt;strong&gt;Compliance and Security&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Detect common prompt injection and data exfiltration exploits&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;🤝 &lt;strong&gt;Framework Integrations&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Native Integrations with CrewAI, AG2 (AutoGen), Camel AI, &amp;amp; LangChain&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Quick Start ⌨️&lt;/h2&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install agentops
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Session replays in 2 lines of code&lt;/h4&gt; 
&lt;p&gt;Initialize the AgentOps client and automatically get analytics on all your LLM calls.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://app.agentops.ai/settings/projects&quot;&gt;Get an API key&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import agentops

# Beginning of your program (i.e. main.py, __init__.py)
agentops.init( &amp;lt; INSERT YOUR API KEY HERE &amp;gt;)

...

# End of program
agentops.end_session(&#39;Success&#39;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;All your sessions can be viewed on the &lt;a href=&quot;https://app.agentops.ai?ref=gh&quot;&gt;AgentOps dashboard&lt;/a&gt; &lt;br&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Agent Debugging&lt;/summary&gt; 
 &lt;a href=&quot;https://app.agentops.ai?ref=gh&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/AgentOps-AI/agentops/main/docs/images/external/app_screenshots/session-drilldown-metadata.png&quot; style=&quot;width: 90%;&quot; alt=&quot;Agent Metadata&quot;&gt; &lt;/a&gt; 
 &lt;a href=&quot;https://app.agentops.ai?ref=gh&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/AgentOps-AI/agentops/main/docs/images/external/app_screenshots/chat-viewer.png&quot; style=&quot;width: 90%;&quot; alt=&quot;Chat Viewer&quot;&gt; &lt;/a&gt; 
 &lt;a href=&quot;https://app.agentops.ai?ref=gh&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/AgentOps-AI/agentops/main/docs/images/external/app_screenshots/session-drilldown-graphs.png&quot; style=&quot;width: 90%;&quot; alt=&quot;Event Graphs&quot;&gt; &lt;/a&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Session Replays&lt;/summary&gt; 
 &lt;a href=&quot;https://app.agentops.ai?ref=gh&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/AgentOps-AI/agentops/main/docs/images/external/app_screenshots/session-replay.png&quot; style=&quot;width: 90%;&quot; alt=&quot;Session Replays&quot;&gt; &lt;/a&gt; 
&lt;/details&gt; 
&lt;details open&gt; 
 &lt;summary&gt;Summary Analytics&lt;/summary&gt; 
 &lt;a href=&quot;https://app.agentops.ai?ref=gh&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/AgentOps-AI/agentops/main/docs/images/external/app_screenshots/overview.png&quot; style=&quot;width: 90%;&quot; alt=&quot;Summary Analytics&quot;&gt; &lt;/a&gt; 
 &lt;a href=&quot;https://app.agentops.ai?ref=gh&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/AgentOps-AI/agentops/main/docs/images/external/app_screenshots/overview-charts.png&quot; style=&quot;width: 90%;&quot; alt=&quot;Summary Analytics Charts&quot;&gt; &lt;/a&gt; 
&lt;/details&gt; 
&lt;h3&gt;First class Developer Experience&lt;/h3&gt; 
&lt;p&gt;Add powerful observability to your agents, tools, and functions with as little code as possible: one line at a time. &lt;br&gt; Refer to our &lt;a href=&quot;http://docs.agentops.ai&quot;&gt;documentation&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Automatically associate all Events with the agent that originated them
from agentops import track_agent

@track_agent(name=&#39;SomeCustomName&#39;)
class MyAgent:
  ...
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Automatically create ToolEvents for tools that agents will use
from agentops import record_tool

@record_tool(&#39;SampleToolName&#39;)
def sample_tool(...):
  ...
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Automatically create ActionEvents for other functions.
from agentops import record_action

@agentops.record_action(&#39;sample function being record&#39;)
def sample_function(...):
  ...
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Manually record any other Events
from agentops import record, ActionEvent

record(ActionEvent(&quot;received_user_input&quot;))
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Integrations 🦾&lt;/h2&gt; 
&lt;h3&gt;OpenAI Agents SDK 🖇️&lt;/h3&gt; 
&lt;p&gt;Build multi-agent systems with tools, handoffs, and guardrails. AgentOps provides first-class integration with OpenAI Agents.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install agents-sdk
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.agentops.ai/v1/integrations/agentssdk&quot;&gt;AgentOps integration example&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.crewai.com/how-to/AgentOps-Observability&quot;&gt;Official CrewAI documentation&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;CrewAI 🛶&lt;/h3&gt; 
&lt;p&gt;Build Crew agents with observability in just 2 lines of code. Simply set an &lt;code&gt;AGENTOPS_API_KEY&lt;/code&gt; in your environment, and your crews will get automatic monitoring on the AgentOps dashboard.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install &#39;crewai[agentops]&#39;
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.agentops.ai/v1/integrations/crewai&quot;&gt;AgentOps integration example&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.crewai.com/how-to/AgentOps-Observability&quot;&gt;Official CrewAI documentation&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;AG2 🤖&lt;/h3&gt; 
&lt;p&gt;With only two lines of code, add full observability and monitoring to AG2 (formerly AutoGen) agents. Set an &lt;code&gt;AGENTOPS_API_KEY&lt;/code&gt; in your environment and call &lt;code&gt;agentops.init()&lt;/code&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.ag2.ai/notebooks/agentchat_agentops&quot;&gt;AG2 Observability Example&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.ag2.ai/docs/ecosystem/agentops&quot;&gt;AG2 - AgentOps Documentation&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Camel AI 🐪&lt;/h3&gt; 
&lt;p&gt;Track and analyze CAMEL agents with full observability. Set an &lt;code&gt;AGENTOPS_API_KEY&lt;/code&gt; in your environment and initialize AgentOps to get started.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.camel-ai.org/&quot;&gt;Camel AI&lt;/a&gt; - Advanced agent communication framework&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.agentops.ai/v1/integrations/camel&quot;&gt;AgentOps integration example&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.camel-ai.org/cookbooks/agents_tracking.html&quot;&gt;Official Camel AI documentation&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;Installation&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install &quot;camel-ai[all]==0.2.11&quot;
pip install agentops
&lt;/code&gt;&lt;/pre&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import os
import agentops
from camel.agents import ChatAgent
from camel.messages import BaseMessage
from camel.models import ModelFactory
from camel.types import ModelPlatformType, ModelType

# Initialize AgentOps
agentops.init(os.getenv(&quot;AGENTOPS_API_KEY&quot;), tags=[&quot;CAMEL Example&quot;])

# Import toolkits after AgentOps init for tracking
from camel.toolkits import SearchToolkit

# Set up the agent with search tools
sys_msg = BaseMessage.make_assistant_message(
    role_name=&#39;Tools calling operator&#39;,
    content=&#39;You are a helpful assistant&#39;
)

# Configure tools and model
tools = [*SearchToolkit().get_tools()]
model = ModelFactory.create(
    model_platform=ModelPlatformType.OPENAI,
    model_type=ModelType.GPT_4O_MINI,
)

# Create and run the agent
camel_agent = ChatAgent(
    system_message=sys_msg,
    model=model,
    tools=tools,
)

response = camel_agent.step(&quot;What is AgentOps?&quot;)
print(response)

agentops.end_session(&quot;Success&quot;)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Check out our &lt;a href=&quot;https://docs.agentops.ai/v1/integrations/camel&quot;&gt;Camel integration guide&lt;/a&gt; for more examples including multi-agent scenarios.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h3&gt;Langchain 🦜🔗&lt;/h3&gt; 
&lt;p&gt;AgentOps works seamlessly with applications built using Langchain. To use the handler, install Langchain as an optional dependency:&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Installation&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;pip install agentops[langchain]
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;To use the handler, import and set&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import os
from langchain.chat_models import ChatOpenAI
from langchain.agents import initialize_agent, AgentType
from agentops.partners.langchain_callback_handler import LangchainCallbackHandler

AGENTOPS_API_KEY = os.environ[&#39;AGENTOPS_API_KEY&#39;]
handler = LangchainCallbackHandler(api_key=AGENTOPS_API_KEY, tags=[&#39;Langchain Example&#39;])

llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY,
                 callbacks=[handler],
                 model=&#39;gpt-3.5-turbo&#39;)

agent = initialize_agent(tools,
                         llm,
                         agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,
                         verbose=True,
                         callbacks=[handler], # You must pass in a callback handler to record your agent
                         handle_parsing_errors=True)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Check out the &lt;a href=&quot;https://raw.githubusercontent.com/AgentOps-AI/agentops/main/examples/langchain_examples.ipynb&quot;&gt;Langchain Examples Notebook&lt;/a&gt; for more details including Async handlers.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h3&gt;Cohere ⌨️&lt;/h3&gt; 
&lt;p&gt;First class support for Cohere(&amp;gt;=5.4.0). This is a living integration, should you need any added functionality please message us on Discord!&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.agentops.ai/v1/integrations/cohere&quot;&gt;AgentOps integration example&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.cohere.com/reference/about&quot;&gt;Official Cohere documentation&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;Installation&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install cohere
&lt;/code&gt;&lt;/pre&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import cohere
import agentops

# Beginning of program&#39;s code (i.e. main.py, __init__.py)
agentops.init(&amp;lt;INSERT YOUR API KEY HERE&amp;gt;)
co = cohere.Client()

chat = co.chat(
    message=&quot;Is it pronounced ceaux-hear or co-hehray?&quot;
)

print(chat)

agentops.end_session(&#39;Success&#39;)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import cohere
import agentops

# Beginning of program&#39;s code (i.e. main.py, __init__.py)
agentops.init(&amp;lt;INSERT YOUR API KEY HERE&amp;gt;)

co = cohere.Client()

stream = co.chat_stream(
    message=&quot;Write me a haiku about the synergies between Cohere and AgentOps&quot;
)

for event in stream:
    if event.event_type == &quot;text-generation&quot;:
        print(event.text, end=&#39;&#39;)

agentops.end_session(&#39;Success&#39;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;Anthropic ﹨&lt;/h3&gt; 
&lt;p&gt;Track agents built with the Anthropic Python SDK (&amp;gt;=0.32.0).&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.agentops.ai/v1/integrations/anthropic&quot;&gt;AgentOps integration guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.anthropic.com/en/docs/welcome&quot;&gt;Official Anthropic documentation&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;Installation&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install anthropic
&lt;/code&gt;&lt;/pre&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import anthropic
import agentops

# Beginning of program&#39;s code (i.e. main.py, __init__.py)
agentops.init(&amp;lt;INSERT YOUR API KEY HERE&amp;gt;)

client = anthropic.Anthropic(
    # This is the default and can be omitted
    api_key=os.environ.get(&quot;ANTHROPIC_API_KEY&quot;),
)

message = client.messages.create(
        max_tokens=1024,
        messages=[
            {
                &quot;role&quot;: &quot;user&quot;,
                &quot;content&quot;: &quot;Tell me a cool fact about AgentOps&quot;,
            }
        ],
        model=&quot;claude-3-opus-20240229&quot;,
    )
print(message.content)

agentops.end_session(&#39;Success&#39;)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Streaming&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import anthropic
import agentops

# Beginning of program&#39;s code (i.e. main.py, __init__.py)
agentops.init(&amp;lt;INSERT YOUR API KEY HERE&amp;gt;)

client = anthropic.Anthropic(
    # This is the default and can be omitted
    api_key=os.environ.get(&quot;ANTHROPIC_API_KEY&quot;),
)

stream = client.messages.create(
    max_tokens=1024,
    model=&quot;claude-3-opus-20240229&quot;,
    messages=[
        {
            &quot;role&quot;: &quot;user&quot;,
            &quot;content&quot;: &quot;Tell me something cool about streaming agents&quot;,
        }
    ],
    stream=True,
)

response = &quot;&quot;
for event in stream:
    if event.type == &quot;content_block_delta&quot;:
        response += event.delta.text
    elif event.type == &quot;message_stop&quot;:
        print(&quot;\n&quot;)
        print(response)
        print(&quot;\n&quot;)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Async&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import asyncio
from anthropic import AsyncAnthropic

client = AsyncAnthropic(
    # This is the default and can be omitted
    api_key=os.environ.get(&quot;ANTHROPIC_API_KEY&quot;),
)


async def main() -&amp;gt; None:
    message = await client.messages.create(
        max_tokens=1024,
        messages=[
            {
                &quot;role&quot;: &quot;user&quot;,
                &quot;content&quot;: &quot;Tell me something interesting about async agents&quot;,
            }
        ],
        model=&quot;claude-3-opus-20240229&quot;,
    )
    print(message.content)


await main()
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;Mistral 〽️&lt;/h3&gt; 
&lt;p&gt;Track agents built with the Mistral Python SDK (&amp;gt;=0.32.0).&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AgentOps-AI/agentops/main/examples/mistral//mistral_example.ipynb&quot;&gt;AgentOps integration example&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.mistral.ai&quot;&gt;Official Mistral documentation&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;Installation&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install mistralai
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Sync&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from mistralai import Mistral
import agentops

# Beginning of program&#39;s code (i.e. main.py, __init__.py)
agentops.init(&amp;lt;INSERT YOUR API KEY HERE&amp;gt;)

client = Mistral(
    # This is the default and can be omitted
    api_key=os.environ.get(&quot;MISTRAL_API_KEY&quot;),
)

message = client.chat.complete(
        messages=[
            {
                &quot;role&quot;: &quot;user&quot;,
                &quot;content&quot;: &quot;Tell me a cool fact about AgentOps&quot;,
            }
        ],
        model=&quot;open-mistral-nemo&quot;,
    )
print(message.choices[0].message.content)

agentops.end_session(&#39;Success&#39;)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Streaming&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from mistralai import Mistral
import agentops

# Beginning of program&#39;s code (i.e. main.py, __init__.py)
agentops.init(&amp;lt;INSERT YOUR API KEY HERE&amp;gt;)

client = Mistral(
    # This is the default and can be omitted
    api_key=os.environ.get(&quot;MISTRAL_API_KEY&quot;),
)

message = client.chat.stream(
        messages=[
            {
                &quot;role&quot;: &quot;user&quot;,
                &quot;content&quot;: &quot;Tell me something cool about streaming agents&quot;,
            }
        ],
        model=&quot;open-mistral-nemo&quot;,
    )

response = &quot;&quot;
for event in message:
    if event.data.choices[0].finish_reason == &quot;stop&quot;:
        print(&quot;\n&quot;)
        print(response)
        print(&quot;\n&quot;)
    else:
        response += event.text

agentops.end_session(&#39;Success&#39;)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Async&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import asyncio
from mistralai import Mistral

client = Mistral(
    # This is the default and can be omitted
    api_key=os.environ.get(&quot;MISTRAL_API_KEY&quot;),
)


async def main() -&amp;gt; None:
    message = await client.chat.complete_async(
        messages=[
            {
                &quot;role&quot;: &quot;user&quot;,
                &quot;content&quot;: &quot;Tell me something interesting about async agents&quot;,
            }
        ],
        model=&quot;open-mistral-nemo&quot;,
    )
    print(message.choices[0].message.content)


await main()
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Async Streaming&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import asyncio
from mistralai import Mistral

client = Mistral(
    # This is the default and can be omitted
    api_key=os.environ.get(&quot;MISTRAL_API_KEY&quot;),
)


async def main() -&amp;gt; None:
    message = await client.chat.stream_async(
        messages=[
            {
                &quot;role&quot;: &quot;user&quot;,
                &quot;content&quot;: &quot;Tell me something interesting about async streaming agents&quot;,
            }
        ],
        model=&quot;open-mistral-nemo&quot;,
    )

    response = &quot;&quot;
    async for event in message:
        if event.data.choices[0].finish_reason == &quot;stop&quot;:
            print(&quot;\n&quot;)
            print(response)
            print(&quot;\n&quot;)
        else:
            response += event.text


await main()
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;CamelAI ﹨&lt;/h3&gt; 
&lt;p&gt;Track agents built with the CamelAI Python SDK (&amp;gt;=0.32.0).&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.camel-ai.org/cookbooks/agents_tracking.html#&quot;&gt;CamelAI integration guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.camel-ai.org/index.html&quot;&gt;Official CamelAI documentation&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;Installation&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install camel-ai[all]
pip install agentops
&lt;/code&gt;&lt;/pre&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;#Import Dependencies
import agentops
import os
from getpass import getpass
from dotenv import load_dotenv

#Set Keys
load_dotenv()
openai_api_key = os.getenv(&quot;OPENAI_API_KEY&quot;) or &quot;&amp;lt;your openai key here&amp;gt;&quot;
agentops_api_key = os.getenv(&quot;AGENTOPS_API_KEY&quot;) or &quot;&amp;lt;your agentops key here&amp;gt;&quot;



&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AgentOps-AI/agentops/main/examples/camelai_examples/README.md&quot;&gt;You can find usage examples here!&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;LiteLLM 🚅&lt;/h3&gt; 
&lt;p&gt;AgentOps provides support for LiteLLM(&amp;gt;=1.3.1), allowing you to call 100+ LLMs using the same Input/Output Format.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.agentops.ai/v1/integrations/litellm&quot;&gt;AgentOps integration example&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.litellm.ai/docs/providers&quot;&gt;Official LiteLLM documentation&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;Installation&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install litellm
&lt;/code&gt;&lt;/pre&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Do not use LiteLLM like this
# from litellm import completion
# ...
# response = completion(model=&quot;claude-3&quot;, messages=messages)

# Use LiteLLM like this
import litellm
...
response = litellm.completion(model=&quot;claude-3&quot;, messages=messages)
# or
response = await litellm.acompletion(model=&quot;claude-3&quot;, messages=messages)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;LlamaIndex 🦙&lt;/h3&gt; 
&lt;p&gt;AgentOps works seamlessly with applications built using LlamaIndex, a framework for building context-augmented generative AI applications with LLMs.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Installation&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;pip install llama-index-instrumentation-agentops
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;To use the handler, import and set&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from llama_index.core import set_global_handler

# NOTE: Feel free to set your AgentOps environment variables (e.g., &#39;AGENTOPS_API_KEY&#39;)
# as outlined in the AgentOps documentation, or pass the equivalent keyword arguments
# anticipated by AgentOps&#39; AOClient as **eval_params in set_global_handler.

set_global_handler(&quot;agentops&quot;)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Check out the &lt;a href=&quot;https://docs.llamaindex.ai/en/stable/module_guides/observability/?h=agentops#agentops&quot;&gt;LlamaIndex docs&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h3&gt;Llama Stack 🦙🥞&lt;/h3&gt; 
&lt;p&gt;AgentOps provides support for Llama Stack Python Client(&amp;gt;=0.0.53), allowing you to monitor your Agentic applications.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/AgentOps-AI/agentops/pull/530/files/65a5ab4fdcf310326f191d4b870d4f553591e3ea#diff-fdddf65549f3714f8f007ce7dfd1cde720329fe54155d54389dd50fbd81813cb&quot;&gt;AgentOps integration example 1&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/AgentOps-AI/agentops/pull/530/files/65a5ab4fdcf310326f191d4b870d4f553591e3ea#diff-6688ff4fb7ab1ce7b1cc9b8362ca27264a3060c16737fb1d850305787a6e3699&quot;&gt;AgentOps integration example 2&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/meta-llama/llama-stack-client-python&quot;&gt;Official Llama Stack Python Client&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;SwarmZero AI 🐝&lt;/h3&gt; 
&lt;p&gt;Track and analyze SwarmZero agents with full observability. Set an &lt;code&gt;AGENTOPS_API_KEY&lt;/code&gt; in your environment and initialize AgentOps to get started.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://swarmzero.ai&quot;&gt;SwarmZero&lt;/a&gt; - Advanced multi-agent framework&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.agentops.ai/v1/integrations/swarmzero&quot;&gt;AgentOps integration example&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.swarmzero.ai/examples/ai-agents/build-and-monitor-a-web-search-agent&quot;&gt;SwarmZero AI integration example&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.swarmzero.ai/sdk/observability/agentops&quot;&gt;SwarmZero AI - AgentOps documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/swarmzero/swarmzero&quot;&gt;Official SwarmZero Python SDK&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;Installation&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install swarmzero
pip install agentops
&lt;/code&gt;&lt;/pre&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from dotenv import load_dotenv
load_dotenv()

import agentops
agentops.init(&amp;lt;INSERT YOUR API KEY HERE&amp;gt;)

from swarmzero import Agent, Swarm
# ...
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;Time travel debugging 🔮&lt;/h2&gt; 
&lt;div style=&quot;justify-content: center&quot;&gt; 
 &lt;img src=&quot;https://raw.githubusercontent.com/AgentOps-AI/agentops/main/docs/images/external/app_screenshots/time_travel_banner.png&quot; alt=&quot;Time Travel Banner&quot;&gt; 
&lt;/div&gt; 
&lt;br&gt; 
&lt;p&gt;&lt;a href=&quot;https://app.agentops.ai/timetravel&quot;&gt;Try it out!&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Agent Arena 🥊&lt;/h2&gt; 
&lt;p&gt;(coming soon!)&lt;/p&gt; 
&lt;h2&gt;Evaluations Roadmap 🧭&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Platform&lt;/th&gt; 
   &lt;th&gt;Dashboard&lt;/th&gt; 
   &lt;th&gt;Evals&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;✅ Python SDK&lt;/td&gt; 
   &lt;td&gt;✅ Multi-session and Cross-session metrics&lt;/td&gt; 
   &lt;td&gt;✅ Custom eval metrics&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;🚧 Evaluation builder API&lt;/td&gt; 
   &lt;td&gt;✅ Custom event tag tracking&lt;/td&gt; 
   &lt;td&gt;🔜 Agent scorecards&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;✅ &lt;a href=&quot;https://github.com/AgentOps-AI/agentops-node&quot;&gt;Javascript/Typescript SDK&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅ Session replays&lt;/td&gt; 
   &lt;td&gt;🔜 Evaluation playground + leaderboard&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Debugging Roadmap 🧭&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Performance testing&lt;/th&gt; 
   &lt;th&gt;Environments&lt;/th&gt; 
   &lt;th&gt;LLM Testing&lt;/th&gt; 
   &lt;th&gt;Reasoning and execution testing&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;✅ Event latency analysis&lt;/td&gt; 
   &lt;td&gt;🔜 Non-stationary environment testing&lt;/td&gt; 
   &lt;td&gt;🔜 LLM non-deterministic function detection&lt;/td&gt; 
   &lt;td&gt;🚧 Infinite loops and recursive thought detection&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;✅ Agent workflow execution pricing&lt;/td&gt; 
   &lt;td&gt;🔜 Multi-modal environments&lt;/td&gt; 
   &lt;td&gt;🚧 Token limit overflow flags&lt;/td&gt; 
   &lt;td&gt;🔜 Faulty reasoning detection&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;🚧 Success validators (external)&lt;/td&gt; 
   &lt;td&gt;🔜 Execution containers&lt;/td&gt; 
   &lt;td&gt;🔜 Context limit overflow flags&lt;/td&gt; 
   &lt;td&gt;🔜 Generative code validators&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;🔜 Agent controllers/skill tests&lt;/td&gt; 
   &lt;td&gt;✅ Honeypot and prompt injection detection (&lt;a href=&quot;https://promptarmor.com&quot;&gt;PromptArmor&lt;/a&gt;)&lt;/td&gt; 
   &lt;td&gt;🔜 API bill tracking&lt;/td&gt; 
   &lt;td&gt;🔜 Error breakpoint analysis&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;🔜 Information context constraint testing&lt;/td&gt; 
   &lt;td&gt;🔜 Anti-agent roadblocks (i.e. Captchas)&lt;/td&gt; 
   &lt;td&gt;🔜 CI/CD integration checks&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;🔜 Regression testing&lt;/td&gt; 
   &lt;td&gt;🔜 Multi-agent framework visualization&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Why AgentOps? 🤔&lt;/h3&gt; 
&lt;p&gt;Without the right tools, AI agents are slow, expensive, and unreliable. Our mission is to bring your agent from prototype to production. Here&#39;s why AgentOps stands out:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Comprehensive Observability&lt;/strong&gt;: Track your AI agents&#39; performance, user interactions, and API usage.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Real-Time Monitoring&lt;/strong&gt;: Get instant insights with session replays, metrics, and live monitoring tools.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Cost Control&lt;/strong&gt;: Monitor and manage your spend on LLM and API calls.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Failure Detection&lt;/strong&gt;: Quickly identify and respond to agent failures and multi-agent interaction issues.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Tool Usage Statistics&lt;/strong&gt;: Understand how your agents utilize external tools with detailed analytics.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Session-Wide Metrics&lt;/strong&gt;: Gain a holistic view of your agents&#39; sessions with comprehensive statistics.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;AgentOps is designed to make agent observability, testing, and monitoring easy.&lt;/p&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;Check out our growth in the community:&lt;/p&gt; 
&lt;img src=&quot;https://api.star-history.com/svg?repos=AgentOps-AI/agentops&amp;amp;type=Date&quot; style=&quot;max-width: 500px&quot; width=&quot;50%&quot; alt=&quot;Logo&quot;&gt; 
&lt;h2&gt;Popular projects using AgentOps&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align=&quot;left&quot;&gt;Repository&lt;/th&gt; 
   &lt;th align=&quot;right&quot;&gt;Stars&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;img class=&quot;avatar mr-2&quot; src=&quot;https://avatars.githubusercontent.com/u/2707039?s=40&amp;amp;v=4&quot; width=&quot;20&quot; height=&quot;20&quot; alt=&quot;&quot;&gt; &amp;nbsp; &lt;a href=&quot;https://github.com/geekan&quot;&gt;geekan&lt;/a&gt; / &lt;a href=&quot;https://github.com/geekan/MetaGPT&quot;&gt;MetaGPT&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;42787&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;img class=&quot;avatar mr-2&quot; src=&quot;https://avatars.githubusercontent.com/u/130722866?s=40&amp;amp;v=4&quot; width=&quot;20&quot; height=&quot;20&quot; alt=&quot;&quot;&gt; &amp;nbsp; &lt;a href=&quot;https://github.com/run-llama&quot;&gt;run-llama&lt;/a&gt; / &lt;a href=&quot;https://github.com/run-llama/llama_index&quot;&gt;llama_index&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;34446&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;img class=&quot;avatar mr-2&quot; src=&quot;https://avatars.githubusercontent.com/u/170677839?s=40&amp;amp;v=4&quot; width=&quot;20&quot; height=&quot;20&quot; alt=&quot;&quot;&gt; &amp;nbsp; &lt;a href=&quot;https://github.com/crewAIInc&quot;&gt;crewAIInc&lt;/a&gt; / &lt;a href=&quot;https://github.com/crewAIInc/crewAI&quot;&gt;crewAI&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;18287&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;img class=&quot;avatar mr-2&quot; src=&quot;https://avatars.githubusercontent.com/u/134388954?s=40&amp;amp;v=4&quot; width=&quot;20&quot; height=&quot;20&quot; alt=&quot;&quot;&gt; &amp;nbsp; &lt;a href=&quot;https://github.com/camel-ai&quot;&gt;camel-ai&lt;/a&gt; / &lt;a href=&quot;https://github.com/camel-ai/camel&quot;&gt;camel&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;5166&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;img class=&quot;avatar mr-2&quot; src=&quot;https://avatars.githubusercontent.com/u/152537519?s=40&amp;amp;v=4&quot; width=&quot;20&quot; height=&quot;20&quot; alt=&quot;&quot;&gt; &amp;nbsp; &lt;a href=&quot;https://github.com/superagent-ai&quot;&gt;superagent-ai&lt;/a&gt; / &lt;a href=&quot;https://github.com/superagent-ai/superagent&quot;&gt;superagent&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;5050&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;img class=&quot;avatar mr-2&quot; src=&quot;https://avatars.githubusercontent.com/u/30197649?s=40&amp;amp;v=4&quot; width=&quot;20&quot; height=&quot;20&quot; alt=&quot;&quot;&gt; &amp;nbsp; &lt;a href=&quot;https://github.com/iyaja&quot;&gt;iyaja&lt;/a&gt; / &lt;a href=&quot;https://github.com/iyaja/llama-fs&quot;&gt;llama-fs&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;4713&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;img class=&quot;avatar mr-2&quot; src=&quot;https://avatars.githubusercontent.com/u/162546372?s=40&amp;amp;v=4&quot; width=&quot;20&quot; height=&quot;20&quot; alt=&quot;&quot;&gt; &amp;nbsp; &lt;a href=&quot;https://github.com/BasedHardware&quot;&gt;BasedHardware&lt;/a&gt; / &lt;a href=&quot;https://github.com/BasedHardware/Omi&quot;&gt;Omi&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;2723&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;img class=&quot;avatar mr-2&quot; src=&quot;https://avatars.githubusercontent.com/u/454862?s=40&amp;amp;v=4&quot; width=&quot;20&quot; height=&quot;20&quot; alt=&quot;&quot;&gt; &amp;nbsp; &lt;a href=&quot;https://github.com/MervinPraison&quot;&gt;MervinPraison&lt;/a&gt; / &lt;a href=&quot;https://github.com/MervinPraison/PraisonAI&quot;&gt;PraisonAI&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;2007&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;img class=&quot;avatar mr-2&quot; src=&quot;https://avatars.githubusercontent.com/u/140554352?s=40&amp;amp;v=4&quot; width=&quot;20&quot; height=&quot;20&quot; alt=&quot;&quot;&gt; &amp;nbsp; &lt;a href=&quot;https://github.com/AgentOps-AI&quot;&gt;AgentOps-AI&lt;/a&gt; / &lt;a href=&quot;https://github.com/AgentOps-AI/Jaiqu&quot;&gt;Jaiqu&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;272&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;img class=&quot;avatar mr-2&quot; src=&quot;https://avatars.githubusercontent.com/u/173542722?s=48&amp;amp;v=4&quot; width=&quot;20&quot; height=&quot;20&quot; alt=&quot;&quot;&gt; &amp;nbsp; &lt;a href=&quot;https://github.com/swarmzero&quot;&gt;swarmzero&lt;/a&gt; / &lt;a href=&quot;https://github.com/swarmzero/swarmzero&quot;&gt;swarmzero&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;195&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;img class=&quot;avatar mr-2&quot; src=&quot;https://avatars.githubusercontent.com/u/3074263?s=40&amp;amp;v=4&quot; width=&quot;20&quot; height=&quot;20&quot; alt=&quot;&quot;&gt; &amp;nbsp; &lt;a href=&quot;https://github.com/strnad&quot;&gt;strnad&lt;/a&gt; / &lt;a href=&quot;https://github.com/strnad/CrewAI-Studio&quot;&gt;CrewAI-Studio&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;134&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;img class=&quot;avatar mr-2&quot; src=&quot;https://avatars.githubusercontent.com/u/18406448?s=40&amp;amp;v=4&quot; width=&quot;20&quot; height=&quot;20&quot; alt=&quot;&quot;&gt; &amp;nbsp; &lt;a href=&quot;https://github.com/alejandro-ao&quot;&gt;alejandro-ao&lt;/a&gt; / &lt;a href=&quot;https://github.com/alejandro-ao/exa-crewai&quot;&gt;exa-crewai&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;55&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;img class=&quot;avatar mr-2&quot; src=&quot;https://avatars.githubusercontent.com/u/64493665?s=40&amp;amp;v=4&quot; width=&quot;20&quot; height=&quot;20&quot; alt=&quot;&quot;&gt; &amp;nbsp; &lt;a href=&quot;https://github.com/tonykipkemboi&quot;&gt;tonykipkemboi&lt;/a&gt; / &lt;a href=&quot;https://github.com/tonykipkemboi/youtube_yapper_trapper&quot;&gt;youtube_yapper_trapper&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;47&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;img class=&quot;avatar mr-2&quot; src=&quot;https://avatars.githubusercontent.com/u/17598928?s=40&amp;amp;v=4&quot; width=&quot;20&quot; height=&quot;20&quot; alt=&quot;&quot;&gt; &amp;nbsp; &lt;a href=&quot;https://github.com/sethcoast&quot;&gt;sethcoast&lt;/a&gt; / &lt;a href=&quot;https://github.com/sethcoast/cover-letter-builder&quot;&gt;cover-letter-builder&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;27&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;img class=&quot;avatar mr-2&quot; src=&quot;https://avatars.githubusercontent.com/u/109994880?s=40&amp;amp;v=4&quot; width=&quot;20&quot; height=&quot;20&quot; alt=&quot;&quot;&gt; &amp;nbsp; &lt;a href=&quot;https://github.com/bhancockio&quot;&gt;bhancockio&lt;/a&gt; / &lt;a href=&quot;https://github.com/bhancockio/chatgpt4o-analysis&quot;&gt;chatgpt4o-analysis&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;19&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;img class=&quot;avatar mr-2&quot; src=&quot;https://avatars.githubusercontent.com/u/14105911?s=40&amp;amp;v=4&quot; width=&quot;20&quot; height=&quot;20&quot; alt=&quot;&quot;&gt; &amp;nbsp; &lt;a href=&quot;https://github.com/breakstring&quot;&gt;breakstring&lt;/a&gt; / &lt;a href=&quot;https://github.com/breakstring/Agentic_Story_Book_Workflow&quot;&gt;Agentic_Story_Book_Workflow&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;14&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;img class=&quot;avatar mr-2&quot; src=&quot;https://avatars.githubusercontent.com/u/124134656?s=40&amp;amp;v=4&quot; width=&quot;20&quot; height=&quot;20&quot; alt=&quot;&quot;&gt; &amp;nbsp; &lt;a href=&quot;https://github.com/MULTI-ON&quot;&gt;MULTI-ON&lt;/a&gt; / &lt;a href=&quot;https://github.com/MULTI-ON/multion-python&quot;&gt;multion-python&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;13&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;em&gt;Generated using &lt;a href=&quot;https://github.com/nvuillam/github-dependents-info&quot;&gt;github-dependents-info&lt;/a&gt;, by &lt;a href=&quot;https://github.com/nvuillam&quot;&gt;Nicolas Vuillamy&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>simular-ai/Agent-S</title>
      <link>https://github.com/simular-ai/Agent-S</link>
      <description>&lt;p&gt;Agent S: an open agentic framework that uses computers like a human&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&quot;center&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/simular-ai/Agent-S/main/images/agent_s.png&quot; alt=&quot;Logo&quot; style=&quot;vertical-align:middle&quot; width=&quot;60&quot;&gt; Agent S2: &lt;small&gt;An Open, Modular, and Scalable Framework for Computer Use Agents&lt;/small&gt; &lt;/h1&gt; 
&lt;p align=&quot;center&quot;&gt;&amp;nbsp; 🌐 &lt;a href=&quot;https://www.simular.ai/agent-s2&quot;&gt;[S2 blog]&lt;/a&gt;&amp;nbsp; 📄 [S2 Paper] (Coming Soon)&amp;nbsp; 🎥 &lt;a href=&quot;https://www.youtube.com/watch?v=wUGVQl7c0eg&quot;&gt;[S2 Video]&lt;/a&gt; 🗨️ &lt;a href=&quot;https://discord.gg/E2XfsK9fPV&quot;&gt;[Discord]&lt;/a&gt;&amp;nbsp; &lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt;&amp;nbsp; 🌐 &lt;a href=&quot;https://www.simular.ai/agent-s&quot;&gt;[S1 blog]&lt;/a&gt;&amp;nbsp; 📄 &lt;a href=&quot;https://arxiv.org/abs/2410.08164&quot;&gt;[S1 Paper (ICLR 2025)]&lt;/a&gt;&amp;nbsp; 🎥 &lt;a href=&quot;https://www.youtube.com/watch?v=OBDE3Knte0g&quot;&gt;[S1 Video]&lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;🥳 Updates&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;strong&gt;2025/03/12&lt;/strong&gt;: Released Agent S2 along with v0.2.0 of &lt;a href=&quot;https://github.com/simular-ai/Agent-S&quot;&gt;gui-agents&lt;/a&gt;, the new state-of-the-art for computer use, outperforming OpenAI&#39;s CUA/Operator and Anthropic&#39;s Claude 3.7 Sonnet!&lt;/li&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;strong&gt;2025/01/22&lt;/strong&gt;: The &lt;a href=&quot;https://arxiv.org/abs/2410.08164&quot;&gt;Agent S paper&lt;/a&gt; is accepted to ICLR 2025!&lt;/li&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;strong&gt;2025/01/21&lt;/strong&gt;: Released v0.1.2 of &lt;a href=&quot;https://github.com/simular-ai/Agent-S&quot;&gt;gui-agents&lt;/a&gt; library, with support for Linux and Windows!&lt;/li&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;strong&gt;2024/12/05&lt;/strong&gt;: Released v0.1.0 of &lt;a href=&quot;https://github.com/simular-ai/Agent-S&quot;&gt;gui-agents&lt;/a&gt; library, allowing you to use Agent-S for Mac, OSWorld, and WindowsAgentArena with ease!&lt;/li&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; &lt;strong&gt;2024/10/10&lt;/strong&gt;: Released &lt;a href=&quot;https://arxiv.org/abs/2410.08164&quot;&gt;Agent S paper&lt;/a&gt; and codebase!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/simular-ai/Agent-S/main/#-introduction&quot;&gt;💡 Introduction&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/simular-ai/Agent-S/main/#-current-results&quot;&gt;🎯 Current Results&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/simular-ai/Agent-S/main/#%EF%B8%8F-installation--setup&quot;&gt;🛠️ Installation &amp;amp; Setup&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/simular-ai/Agent-S/main/#-usage&quot;&gt;🚀 Usage&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/simular-ai/Agent-S/main/#-acknowledgements&quot;&gt;🤝 Acknowledgements&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/simular-ai/Agent-S/main/#-citation&quot;&gt;💬 Citation&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;💡 Introduction&lt;/h2&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/simular-ai/Agent-S/main/images/agent_s2_teaser.png&quot; width=&quot;800&quot;&gt; &lt;/p&gt; 
&lt;p&gt;Welcome to &lt;strong&gt;Agent S&lt;/strong&gt;, an open-source framework designed to enable autonomous interaction with computers through Agent-Computer Interface. Our mission is to build intelligent GUI agents that can learn from past experiences and perform complex tasks autonomously on your computer.&lt;/p&gt; 
&lt;p&gt;Whether you&#39;re interested in AI, automation, or contributing to cutting-edge agent-based systems, we&#39;re excited to have you here!&lt;/p&gt; 
&lt;h2&gt;🎯 Current Results&lt;/h2&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/simular-ai/Agent-S/main/images/agent_s2_osworld_result.png&quot; width=&quot;600&quot;&gt; &lt;br&gt; Results of Agent S2&#39;s Successful Rate (%) on the OSWorld full test set using Screenshot input only. &lt;/p&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;table border=&quot;0&quot; cellspacing=&quot;0&quot; cellpadding=&quot;5&quot;&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;th&gt;Benchmark&lt;/th&gt; 
    &lt;th&gt;Agent S2&lt;/th&gt; 
    &lt;th&gt;Previous SOTA&lt;/th&gt; 
    &lt;th&gt;Δ improve&lt;/th&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;OSWorld (15 step)&lt;/td&gt; 
    &lt;td&gt;27.0%&lt;/td&gt; 
    &lt;td&gt;22.7% (ByteDance UI-TARS)&lt;/td&gt; 
    &lt;td&gt;+4.3%&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;OSWorld (50 step)&lt;/td&gt; 
    &lt;td&gt;34.5%&lt;/td&gt; 
    &lt;td&gt;32.6% (OpenAI CUA)&lt;/td&gt; 
    &lt;td&gt;+1.9%&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;AndroidWorld&lt;/td&gt; 
    &lt;td&gt;50.0%&lt;/td&gt; 
    &lt;td&gt;46.8% (ByteDance UI-TARS)&lt;/td&gt; 
    &lt;td&gt;+3.2%&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h2&gt;🛠️ Installation &amp;amp; Setup&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;❗&lt;strong&gt;Warning&lt;/strong&gt;❗: If you are on a Linux machine, creating a &lt;code&gt;conda&lt;/code&gt; environment will interfere with &lt;code&gt;pyatspi&lt;/code&gt;. As of now, there&#39;s no clean solution for this issue. Proceed through the installation without using &lt;code&gt;conda&lt;/code&gt; or any virtual environment.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;⚠️&lt;strong&gt;Disclaimer&lt;/strong&gt;⚠️: To leverage the full potential of Agent S2, we utilize &lt;a href=&quot;https://github.com/bytedance/UI-TARS&quot;&gt;UI-TARS&lt;/a&gt; as a grounding model (7B-DPO or 72B-DPO for better performance). They can be hosted locally, or on Hugging Face Inference Endpoints. Our code supports Hugging Face Inference Endpoints. Check out &lt;a href=&quot;https://huggingface.co/learn/cookbook/en/enterprise_dedicated_endpoints&quot;&gt;Hugging Face Inference Endpoints&lt;/a&gt; for more information on how to set up and query this endpoint. However, running Agent S2 does not require this model, and you can use alternative API based models for visual grounding, such as Claude.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Clone the repository:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;git clone https://github.com/simular-ai/Agent-S.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Install the gui-agents package:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install gui-agents
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Set your LLM API Keys and other environment variables. You can do this by adding the following line to your .bashrc (Linux), or .zshrc (MacOS) file.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;export OPENAI_API_KEY=&amp;lt;YOUR_API_KEY&amp;gt;
export ANTHROPIC_API_KEY=&amp;lt;YOUR_ANTHROPIC_API_KEY&amp;gt;
export HF_TOKEN=&amp;lt;YOUR_HF_TOKEN&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, you can set the environment variable in your Python script:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;import os
os.environ[&quot;OPENAI_API_KEY&quot;] = &quot;&amp;lt;YOUR_API_KEY&amp;gt;&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;We also support Azure OpenAI, Anthropic, and vLLM inference. For more information refer to &lt;a href=&quot;https://raw.githubusercontent.com/simular-ai/Agent-S/main/models.md&quot;&gt;models.md&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Setup Retrieval from Web using Perplexica&lt;/h3&gt; 
&lt;p&gt;Agent S works best with web-knowledge retrieval. To enable this feature, you need to setup Perplexica:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Ensure Docker Desktop is installed and running on your system.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Navigate to the directory containing the project files.&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt; cd Perplexica
 git submodule update --init
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Rename the &lt;code&gt;sample.config.toml&lt;/code&gt; file to &lt;code&gt;config.toml&lt;/code&gt;. For Docker setups, you need only fill in the following fields:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;&lt;code&gt;OPENAI&lt;/code&gt;: Your OpenAI API key. &lt;strong&gt;You only need to fill this if you wish to use OpenAI&#39;s models&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;code&gt;OLLAMA&lt;/code&gt;: Your Ollama API URL. You should enter it as &lt;code&gt;http://host.docker.internal:PORT_NUMBER&lt;/code&gt;. If you installed Ollama on port 11434, use &lt;code&gt;http://host.docker.internal:11434&lt;/code&gt;. For other ports, adjust accordingly. &lt;strong&gt;You need to fill this if you wish to use Ollama&#39;s models instead of OpenAI&#39;s&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;code&gt;GROQ&lt;/code&gt;: Your Groq API key. &lt;strong&gt;You only need to fill this if you wish to use Groq&#39;s hosted models&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;code&gt;ANTHROPIC&lt;/code&gt;: Your Anthropic API key. &lt;strong&gt;You only need to fill this if you wish to use Anthropic models&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: You can change these after starting Perplexica from the settings dialog.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;code&gt;SIMILARITY_MEASURE&lt;/code&gt;: The similarity measure to use (This is filled by default; you can leave it as is if you are unsure about it.)&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Ensure you are in the directory containing the &lt;code&gt;docker-compose.yaml&lt;/code&gt; file and execute:&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;docker compose up -d
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Our implementation of Agent S incorporates the Perplexica API to integrate a search engine capability, which allows for a more convenient and responsive user experience. If you want to tailor the API to your settings and specific requirements, you may modify the URL and the message of request parameters in &lt;code&gt;agent_s/query_perplexica.py&lt;/code&gt;. For a comprehensive guide on configuring the Perplexica API, please refer to &lt;a href=&quot;https://github.com/ItzCrazyKns/Perplexica/raw/master/docs/API/SEARCH.md&quot;&gt;Perplexica Search API Documentation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;For a more detailed setup and usage guide, please refer to the &lt;a href=&quot;https://github.com/ItzCrazyKns/Perplexica.git&quot;&gt;Perplexica Repository&lt;/a&gt;.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;❗&lt;strong&gt;Warning&lt;/strong&gt;❗: The agent will directly run python code to control your computer. Please use with care.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;🚀 Usage&lt;/h2&gt; 
&lt;h3&gt;CLI&lt;/h3&gt; 
&lt;p&gt;Run Agent S2 with a specific model (default is &lt;code&gt;gpt-4o&lt;/code&gt;):&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;agent_s --model claude-3-7-sonnet-20250219 --grounding_model claude-3-7-sonnet-20250219
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or use a custom endpoint:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;agent_s --model claude-3-7-sonnet-20250219 --endpoint_provider &quot;huggingface&quot; --endpoint_url &quot;&amp;lt;endpoint_url&amp;gt;/v1/&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Main Model Settings&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;--model&lt;/code&gt;&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Purpose: Specifies the main generation model&lt;/li&gt; 
   &lt;li&gt;Example: &lt;code&gt;gpt-4o&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Default: &lt;code&gt;gpt-4o&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Grounding Configuration Options&lt;/h4&gt; 
&lt;p&gt;You can use either Configuration 1 or Configuration 2:&lt;/p&gt; 
&lt;h5&gt;&lt;strong&gt;Configuration 1: API-Based Models&lt;/strong&gt;&lt;/h5&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;--grounding_model&lt;/code&gt;&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Purpose: Specifies the model for visual understanding&lt;/li&gt; 
   &lt;li&gt;Supports: 
    &lt;ul&gt; 
     &lt;li&gt;Anthropic Claude models (e.g., &lt;code&gt;claude-3-7-sonnet&lt;/code&gt;)&lt;/li&gt; 
     &lt;li&gt;OpenAI GPT models (e.g., &lt;code&gt;gpt-4-vision&lt;/code&gt;)&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Default: None&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h5&gt;&lt;strong&gt;Configuration 2: Custom Endpoint&lt;/strong&gt;&lt;/h5&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;--endpoint_provider&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Purpose: Specifies the endpoint provider&lt;/li&gt; 
   &lt;li&gt;Currently supports: HuggingFace TGI&lt;/li&gt; 
   &lt;li&gt;Default: &lt;code&gt;huggingface&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;--endpoint_url&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Purpose: The URL for your custom endpoint&lt;/li&gt; 
   &lt;li&gt;Default: None&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This will show a user query prompt where you can enter your query and interact with Agent S2. You can use any model from the list of supported models in &lt;a href=&quot;https://raw.githubusercontent.com/simular-ai/Agent-S/main/models.md&quot;&gt;models.md&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;&lt;code&gt;gui_agents&lt;/code&gt; SDK&lt;/h3&gt; 
&lt;p&gt;First, we import the necessary modules. &lt;code&gt;GraphSearchAgent&lt;/code&gt; is the main agent class for Agent S2. &lt;code&gt;OSWorldACI&lt;/code&gt; is our grounding agent that translates agent actions into executable python code.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;import pyautogui
import io
from gui_agents.s2.agents.agent_s import GraphSearchAgent
from gui_agents.s2.agents.grounding import OSWorldACI

# Load in your API keys.
from dotenv import load_dotenv
load_dotenv()

current_platform = &quot;ubuntu&quot;  # &quot;macos&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Next, we define our engine parameters. &lt;code&gt;engine_params&lt;/code&gt; is used for the main agent, and &lt;code&gt;engine_params_for_grounding&lt;/code&gt; is for grounding. For &lt;code&gt;engine_params_for_grounding&lt;/code&gt;, we support the Claude, GPT series, and Hugging Face Inference Endpoints.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;engine_type_for_grounding = &quot;huggingface&quot;

engine_params = {
    &quot;engine_type&quot;: &quot;openai&quot;,
    &quot;model&quot;: &quot;gpt-4o&quot;,
}

if engine_type_for_grounding == &quot;huggingface&quot;:
  engine_params_for_grounding = {
      &quot;engine_type&quot;: &quot;huggingface&quot;,
      &quot;endpoint_url&quot;: &quot;&amp;lt;endpoint_url&amp;gt;/v1/&quot;,
  }
elif engine_type_for_grounding == &quot;claude&quot;:
  engine_params_for_grounding = {
      &quot;engine_type&quot;: &quot;claude&quot;,
      &quot;model&quot;: &quot;claude-3-7-sonnet-20250219&quot;,
  }
elif engine_type_for_grounding == &quot;gpt&quot;:
  engine_params_for_grounding = {
    &quot;engine_type&quot;: &quot;gpt&quot;,
    &quot;model&quot;: &quot;gpt-4o&quot;,
  }
else:
  raise ValueError(&quot;Invalid engine type for grounding&quot;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then, we define our grounding agent and Agent S2.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;grounding_agent = OSWorldACI(
    platform=current_platform,
    engine_params_for_generation=engine_params,
    engine_params_for_grounding=engine_params_for_grounding
)

agent = GraphSearchAgent(
  engine_params,
  grounding_agent,
  platform=current_platform,
  action_space=&quot;pyautogui&quot;,
  observation_type=&quot;mixed&quot;,
  search_engine=&quot;Perplexica&quot;  # Assuming you have set up Perplexica.
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Finally, let&#39;s query the agent!&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;# Get screenshot.
screenshot = pyautogui.screenshot()
buffered = io.BytesIO() 
screenshot.save(buffered, format=&quot;PNG&quot;)
screenshot_bytes = buffered.getvalue()

obs = {
  &quot;screenshot&quot;: screenshot_bytes,
}

instruction = &quot;Close VS Code&quot;
info, action = agent.predict(instruction=instruction, observation=obs)

exec(action[0])
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Refer to &lt;code&gt;gui_agents/s2/cli_app.py&lt;/code&gt; for more details on how the inference loop works.&lt;/p&gt; 
&lt;h3&gt;OSWorld&lt;/h3&gt; 
&lt;p&gt;To deploy Agent S2 in OSWorld, follow the &lt;a href=&quot;https://raw.githubusercontent.com/simular-ai/Agent-S/main/OSWorld.md&quot;&gt;OSWorld Deployment instructions&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;🤝 Acknowledgements&lt;/h2&gt; 
&lt;p&gt;We extend our sincere thanks to Tianbao Xie for developing OSWorld and discussing computer use challenges. We also appreciate the engaging discussions with Yujia Qin and Shihao Liang regarding UI-TARS.&lt;/p&gt; 
&lt;h2&gt;💬 Citations&lt;/h2&gt; 
&lt;p&gt;If you find this codebase useful, please cite&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@inproceedings{agashe2025agents,
    title={{Agent S: An Open Agentic Framework that Uses Computers Like a Human}},
    author={Saaket Agashe and Jiuzhou Han and Shuyu Gan and Jiachen Yang and Ang Li and Xin Eric Wang},
    booktitle={International Conference on Learning Representations (ICLR)},
    year={2025},
    url={https://arxiv.org/abs/2410.08164}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>hpcaitech/Open-Sora</title>
      <link>https://github.com/hpcaitech/Open-Sora</link>
      <description>&lt;p&gt;Open-Sora: Democratizing Efficient Video Production for All&lt;/p&gt;&lt;hr&gt;&lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/raw/main/readme/icon.png&quot; width=&quot;250&quot;&gt; &lt;/p&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;a href=&quot;https://github.com/hpcaitech/Open-Sora/stargazers&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/hpcaitech/Open-Sora?style=social&quot;&gt;&lt;/a&gt; 
 &lt;a href=&quot;https://arxiv.org/abs/2503.09642v1&quot;&gt;&lt;img src=&quot;https://img.shields.io/static/v1?label=Tech%20Report%202.0&amp;amp;message=Arxiv&amp;amp;color=red&quot;&gt;&lt;/a&gt; 
 &lt;a href=&quot;https://arxiv.org/abs/2412.20404&quot;&gt;&lt;img src=&quot;https://img.shields.io/static/v1?label=Tech%20Report%201.2&amp;amp;message=Arxiv&amp;amp;color=red&quot;&gt;&lt;/a&gt; 
 &lt;a href=&quot;https://hpcaitech.github.io/Open-Sora/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Gallery-View-orange?logo=&amp;amp;&quot;&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;a href=&quot;https://discord.gg/kZakZzrSUT&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Discord-join-blueviolet?logo=discord&amp;amp;&quot;&gt;&lt;/a&gt; 
 &lt;a href=&quot;https://join.slack.com/t/colossalaiworkspace/shared_invite/zt-247ipg9fk-KRRYmUl~u2ll2637WRURVA&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Slack-ColossalAI-blueviolet?logo=slack&amp;amp;&quot;&gt;&lt;/a&gt; 
 &lt;a href=&quot;https://x.com/YangYou1991/status/1899973689460044010&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Twitter-Discuss-blue?logo=twitter&amp;amp;&quot;&gt;&lt;/a&gt; 
 &lt;a href=&quot;https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/WeChat.png&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/%E5%BE%AE%E4%BF%A1-%E5%B0%8F%E5%8A%A9%E6%89%8B%E5%8A%A0%E7%BE%A4-green?logo=wechat&amp;amp;&quot;&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;Open-Sora: Democratizing Efficient Video Production for All&lt;/h2&gt; 
&lt;p&gt;We design and implement &lt;strong&gt;Open-Sora&lt;/strong&gt;, an initiative dedicated to &lt;strong&gt;efficiently&lt;/strong&gt; producing high-quality video. We hope to make the model, tools and all details accessible to all. By embracing &lt;strong&gt;open-source&lt;/strong&gt; principles, Open-Sora not only democratizes access to advanced video generation techniques, but also offers a streamlined and user-friendly platform that simplifies the complexities of video generation. With Open-Sora, our goal is to foster innovation, creativity, and inclusivity within the field of content creation.&lt;/p&gt; 
&lt;p&gt;🎬 For a professional AI video-generation product, try &lt;a href=&quot;https://video-ocean.com/&quot;&gt;Video Ocean&lt;/a&gt; — powered by a superior model.&lt;/p&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;a href=&quot;https://video-ocean.com/&quot;&gt; &lt;img src=&quot;https://github.com/hpcaitech/public_assets/raw/main/colossalai/img/3.gif&quot; width=&quot;850&quot;&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;a href=&quot;https://hpc-ai.com/?utm_source=github&amp;amp;utm_medium=social&amp;amp;utm_campaign=promotion-opensora&quot;&gt; &lt;img src=&quot;https://github.com/hpcaitech/public_assets/raw/main/colossalai/img/1.gif&quot; width=&quot;850&quot;&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;!-- [[中文文档](/docs/zh_CN/README.md)] [[潞晨云](https://cloud.luchentech.com/)|[OpenSora镜像](https://cloud.luchentech.com/doc/docs/image/open-sora/)|[视频教程](https://www.bilibili.com/video/BV1ow4m1e7PX/?vd_source=c6b752764cd36ff0e535a768e35d98d2)] --&gt; 
&lt;h2&gt;📰 News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;[2025.03.12]&lt;/strong&gt; 🔥 We released &lt;strong&gt;Open-Sora 2.0&lt;/strong&gt; (11B). 🎬 11B model achieves &lt;a href=&quot;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/#evaluation&quot;&gt;on-par performance&lt;/a&gt; with 11B HunyuanVideo &amp;amp; 30B Step-Video on 📐VBench &amp;amp; 📊Human Preference. 🛠️ Fully open-source: checkpoints and training codes for training with only &lt;strong&gt;$200K&lt;/strong&gt;. &lt;a href=&quot;https://arxiv.org/abs/2503.09642v1&quot;&gt;[report]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025.02.20]&lt;/strong&gt; 🔥 We released &lt;strong&gt;Open-Sora 1.3&lt;/strong&gt; (1B). With the upgraded VAE and Transformer architecture, the quality of our generated videos has been greatly improved 🚀. &lt;a href=&quot;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/#open-sora-13-model-weights&quot;&gt;[checkpoints]&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/report_04.md&quot;&gt;[report]&lt;/a&gt; &lt;a href=&quot;https://huggingface.co/spaces/hpcai-tech/open-sora&quot;&gt;[demo]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2024.12.23]&lt;/strong&gt; The development cost of video generation models has saved by 50%! Open-source solutions are now available with H200 GPU vouchers. &lt;a href=&quot;https://company.hpc-ai.com/blog/the-development-cost-of-video-generation-models-has-saved-by-50-open-source-solutions-are-now-available-with-h200-gpu-vouchers&quot;&gt;[blog]&lt;/a&gt; &lt;a href=&quot;https://github.com/hpcaitech/Open-Sora/raw/main/scripts/train.py&quot;&gt;[code]&lt;/a&gt; &lt;a href=&quot;https://colossalai.org/zh-Hans/docs/get_started/bonus/&quot;&gt;[vouchers]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2024.06.17]&lt;/strong&gt; We released &lt;strong&gt;Open-Sora 1.2&lt;/strong&gt;, which includes &lt;strong&gt;3D-VAE&lt;/strong&gt;, &lt;strong&gt;rectified flow&lt;/strong&gt;, and &lt;strong&gt;score condition&lt;/strong&gt;. The video quality is greatly improved. &lt;a href=&quot;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/#open-sora-12-model-weights&quot;&gt;[checkpoints]&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/report_03.md&quot;&gt;[report]&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2412.20404&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2024.04.25]&lt;/strong&gt; 🤗 We released the &lt;a href=&quot;https://huggingface.co/spaces/hpcai-tech/open-sora&quot;&gt;Gradio demo for Open-Sora&lt;/a&gt; on Hugging Face Spaces.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2024.04.25]&lt;/strong&gt; We released &lt;strong&gt;Open-Sora 1.1&lt;/strong&gt;, which supports &lt;strong&gt;2s~15s, 144p to 720p, any aspect ratio&lt;/strong&gt; text-to-image, &lt;strong&gt;text-to-video, image-to-video, video-to-video, infinite time&lt;/strong&gt; generation. In addition, a full video processing pipeline is released. &lt;a href=&quot;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/#open-sora-11-model-weights&quot;&gt;[checkpoints]&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/report_02.md&quot;&gt;[report]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2024.03.18]&lt;/strong&gt; We released &lt;strong&gt;Open-Sora 1.0&lt;/strong&gt;, a fully open-source project for video generation. Open-Sora 1.0 supports a full pipeline of video data preprocessing, training with &lt;a href=&quot;https://github.com/hpcaitech/ColossalAI&quot;&gt;&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/raw/main/readme/colossal_ai.png&quot; width=&quot;8%&quot;&gt;&lt;/a&gt; acceleration, inference, and more. Our model can produce 2s 512x512 videos with only 3 days training. &lt;a href=&quot;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/#open-sora-10-model-weights&quot;&gt;[checkpoints]&lt;/a&gt; &lt;a href=&quot;https://hpc-ai.com/blog/open-sora-v1.0&quot;&gt;[blog]&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/report_01.md&quot;&gt;[report]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2024.03.04]&lt;/strong&gt; Open-Sora provides training with 46% cost reduction. &lt;a href=&quot;https://hpc-ai.com/blog/open-sora&quot;&gt;[blog]&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;📍 Since Open-Sora is under active development, we remain different branchs for different versions. The latest version is &lt;a href=&quot;https://github.com/hpcaitech/Open-Sora&quot;&gt;main&lt;/a&gt;. Old versions include: &lt;a href=&quot;https://github.com/hpcaitech/Open-Sora/tree/opensora/v1.0&quot;&gt;v1.0&lt;/a&gt;, &lt;a href=&quot;https://github.com/hpcaitech/Open-Sora/tree/opensora/v1.1&quot;&gt;v1.1&lt;/a&gt;, &lt;a href=&quot;https://github.com/hpcaitech/Open-Sora/tree/opensora/v1.2&quot;&gt;v1.2&lt;/a&gt;, &lt;a href=&quot;https://github.com/hpcaitech/Open-Sora/tree/opensora/v1.3&quot;&gt;v1.3&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;🎥 Latest Demo&lt;/h2&gt; 
&lt;p&gt;Demos are presented in compressed GIF format for convenience. For original quality samples and their corresponding prompts, please visit our &lt;a href=&quot;https://hpcaitech.github.io/Open-Sora/&quot;&gt;Gallery&lt;/a&gt;.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;strong&gt;5s 1024×576&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;5s 576×1024&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;5s 576×1024&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://streamable.com/e/8g9y9h?autoplay=1&quot;&gt;&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v2.0/ft_0001_1_1.gif&quot; width=&quot;&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://streamable.com/e/k50mnv?autoplay=1&quot;&gt;&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v2.0/movie_0160.gif&quot; width=&quot;&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://streamable.com/e/bzrn9n?autoplay=1&quot;&gt;&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v2.0/movie_0017.gif&quot; width=&quot;&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://streamable.com/e/dsv8da?autoplay=1&quot;&gt;&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v2.0/ft_0012_1_1.gif&quot; width=&quot;&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://streamable.com/e/3wif07?autoplay=1&quot;&gt;&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v2.0/douyin_0005.gif&quot; width=&quot;&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://streamable.com/e/us2w7h?autoplay=1&quot;&gt;&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v2.0/movie_0037.gif&quot; width=&quot;&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://streamable.com/e/yfwk8i?autoplay=1&quot;&gt;&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v2.0/ft_0055_1_1.gif&quot; width=&quot;&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://streamable.com/e/jgjil0?autoplay=1&quot;&gt;&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v2.0/sora_0019.gif&quot; width=&quot;&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://streamable.com/e/lsoai1?autoplay=1&quot;&gt;&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v2.0/movie_0463.gif&quot; width=&quot;&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;details&gt; 
 &lt;summary&gt;OpenSora 1.3 Demo&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;&lt;strong&gt;5s 720×1280&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;5s 720×1280&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;5s 720×1280&lt;/strong&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href=&quot;https://streamable.com/e/r0imrp?quality=highest&amp;amp;autoplay=1&quot;&gt;&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.3/demo_tomato.gif&quot; width=&quot;&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://streamable.com/e/hfvjkh?quality=highest&amp;amp;autoplay=1&quot;&gt;&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.3/demo_fisherman.gif&quot; width=&quot;&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://streamable.com/e/kutmma?quality=highest&amp;amp;autoplay=1&quot;&gt;&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.3/demo_girl2.gif&quot; width=&quot;&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href=&quot;https://streamable.com/e/osn1la?quality=highest&amp;amp;autoplay=1&quot;&gt;&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.3/demo_grape.gif&quot; width=&quot;&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://streamable.com/e/l1pzws?quality=highest&amp;amp;autoplay=1&quot;&gt;&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.3/demo_mushroom.gif&quot; width=&quot;&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://streamable.com/e/2vqari?quality=highest&amp;amp;autoplay=1&quot;&gt;&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.3/demo_parrot.gif&quot; width=&quot;&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href=&quot;https://streamable.com/e/1in7d6?quality=highest&amp;amp;autoplay=1&quot;&gt;&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.3/demo_trans.gif&quot; width=&quot;&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://streamable.com/e/e9bi4o?quality=highest&amp;amp;autoplay=1&quot;&gt;&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.3/demo_bear.gif&quot; width=&quot;&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://streamable.com/e/09z7xi?quality=highest&amp;amp;autoplay=1&quot;&gt;&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.3/demo_futureflower.gif&quot; width=&quot;&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href=&quot;https://streamable.com/e/16c3hk?quality=highest&amp;amp;autoplay=1&quot;&gt;&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.3/demo_fire.gif&quot; width=&quot;&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://streamable.com/e/wi250w?quality=highest&amp;amp;autoplay=1&quot;&gt;&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.3/demo_man.gif&quot; width=&quot;&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://streamable.com/e/vw5b64?quality=highest&amp;amp;autoplay=1&quot;&gt;&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.3/demo_black.gif&quot; width=&quot;&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;OpenSora 1.2 Demo&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;&lt;strong&gt;4s 720×1280&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;4s 720×1280&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;4s 720×1280&lt;/strong&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/hpcaitech/Open-Sora/assets/99191637/7895aab6-ed23-488c-8486-091480c26327&quot;&gt;&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.2/sample_0013.gif&quot; width=&quot;&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/hpcaitech/Open-Sora/assets/99191637/20f07c7b-182b-4562-bbee-f1df74c86c9a&quot;&gt;&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.2/sample_1718.gif&quot; width=&quot;&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/hpcaitech/Open-Sora/assets/99191637/3d897e0d-dc21-453a-b911-b3bda838acc2&quot;&gt;&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.2/sample_0087.gif&quot; width=&quot;&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/hpcaitech/Open-Sora/assets/99191637/644bf938-96ce-44aa-b797-b3c0b513d64c&quot;&gt;&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.2/sample_0052.gif&quot; width=&quot;&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/hpcaitech/Open-Sora/assets/99191637/272d88ac-4b4a-484d-a665-8d07431671d0&quot;&gt;&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.2/sample_1719.gif&quot; width=&quot;&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/hpcaitech/Open-Sora/assets/99191637/ebbac621-c34e-4bb4-9543-1c34f8989764&quot;&gt;&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.2/sample_0002.gif&quot; width=&quot;&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/hpcaitech/Open-Sora/assets/99191637/a1e3a1a3-4abd-45f5-8df2-6cced69da4ca&quot;&gt;&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.2/sample_0011.gif&quot; width=&quot;&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/hpcaitech/Open-Sora/assets/99191637/d6ce9c13-28e1-4dff-9644-cc01f5f11926&quot;&gt;&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.2/sample_0004.gif&quot; width=&quot;&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/hpcaitech/Open-Sora/assets/99191637/561978f8-f1b0-4f4d-ae7b-45bec9001b4a&quot;&gt;&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.2/sample_0061.gif&quot; width=&quot;&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;OpenSora 1.1 Demo&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;&lt;strong&gt;2s 240×426&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;2s 240×426&lt;/strong&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/hpcaitech/Open-Sora-dev/assets/99191637/c31ebc52-de39-4a4e-9b1e-9211d45e05b2&quot;&gt;&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.1/sample_16x240x426_9.gif&quot; width=&quot;&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/hpcaitech/Open-Sora-dev/assets/99191637/c31ebc52-de39-4a4e-9b1e-9211d45e05b2&quot;&gt;&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.1/sora_16x240x426_26.gif&quot; width=&quot;&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/hpcaitech/Open-Sora-dev/assets/99191637/f7ce4aaa-528f-40a8-be7a-72e61eaacbbd&quot;&gt;&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.1/sora_16x240x426_27.gif&quot; width=&quot;&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/hpcaitech/Open-Sora-dev/assets/99191637/5d58d71e-1fda-4d90-9ad3-5f2f7b75c6a9&quot;&gt;&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.1/sora_16x240x426_40.gif&quot; width=&quot;&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;&lt;strong&gt;2s 426×240&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;4s 480×854&lt;/strong&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/hpcaitech/Open-Sora-dev/assets/99191637/34ecb4a0-4eef-4286-ad4c-8e3a87e5a9fd&quot;&gt;&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.1/sora_16x426x240_24.gif&quot; width=&quot;&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/hpcaitech/Open-Sora-dev/assets/99191637/c1619333-25d7-42ba-a91c-18dbc1870b18&quot;&gt;&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.1/sample_32x480x854_9.gif&quot; width=&quot;&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;&lt;strong&gt;16s 320×320&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;16s 224×448&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;2s 426×240&lt;/strong&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/hpcaitech/Open-Sora/assets/99191637/3cab536e-9b43-4b33-8da8-a0f9cf842ff2&quot;&gt;&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.1/sample_16s_320x320.gif&quot; width=&quot;&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/hpcaitech/Open-Sora/assets/99191637/9fb0b9e0-c6f4-4935-b29e-4cac10b373c4&quot;&gt;&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.1/sample_16s_224x448.gif&quot; width=&quot;&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/hpcaitech/Open-Sora-dev/assets/99191637/3e892ad2-9543-4049-b005-643a4c1bf3bf&quot;&gt;&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.1/sora_16x426x240_3.gif&quot; width=&quot;&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;OpenSora 1.0 Demo&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;&lt;strong&gt;2s 512×512&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;2s 512×512&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;2s 512×512&lt;/strong&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/hpcaitech/Open-Sora/assets/99191637/de1963d3-b43b-4e68-a670-bb821ebb6f80&quot;&gt;&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.0/sample_0.gif&quot; width=&quot;&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/hpcaitech/Open-Sora/assets/99191637/13f8338f-3d42-4b71-8142-d234fbd746cc&quot;&gt;&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.0/sample_1.gif&quot; width=&quot;&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/hpcaitech/Open-Sora/assets/99191637/fa6a65a6-e32a-4d64-9a9e-eabb0ebb8c16&quot;&gt;&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.0/sample_2.gif&quot; width=&quot;&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;A serene night scene in a forested area. [...] The video is a time-lapse, capturing the transition from day to night, with the lake and forest serving as a constant backdrop.&lt;/td&gt; 
    &lt;td&gt;A soaring drone footage captures the majestic beauty of a coastal cliff, [...] The water gently laps at the rock base and the greenery that clings to the top of the cliff.&lt;/td&gt; 
    &lt;td&gt;The majestic beauty of a waterfall cascading down a cliff into a serene lake. [...] The camera angle provides a bird&#39;s eye view of the waterfall.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/hpcaitech/Open-Sora/assets/99191637/64232f84-1b36-4750-a6c0-3e610fa9aa94&quot;&gt;&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.0/sample_3.gif&quot; width=&quot;&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/hpcaitech/Open-Sora/assets/99191637/983a1965-a374-41a7-a76b-c07941a6c1e9&quot;&gt;&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.0/sample_4.gif&quot; width=&quot;&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/hpcaitech/Open-Sora/assets/99191637/ec10c879-9767-4c31-865f-2e8d6cf11e65&quot;&gt;&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.0/sample_5.gif&quot; width=&quot;&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;A bustling city street at night, filled with the glow of car headlights and the ambient light of streetlights. [...]&lt;/td&gt; 
    &lt;td&gt;The vibrant beauty of a sunflower field. The sunflowers are arranged in neat rows, creating a sense of order and symmetry. [...]&lt;/td&gt; 
    &lt;td&gt;A serene underwater scene featuring a sea turtle swimming through a coral reef. The turtle, with its greenish-brown shell [...]&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;p&gt;Videos are downsampled to &lt;code&gt;.gif&lt;/code&gt; for display. Click for original videos. Prompts are trimmed for display, see &lt;a href=&quot;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/assets/texts/t2v_samples.txt&quot;&gt;here&lt;/a&gt; for full prompts.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;🔆 Reports&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://arxiv.org/abs/2503.09642v1&quot;&gt;Tech Report of Open-Sora 2.0&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/train.md&quot;&gt;Step by step to train or finetune your own model&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/ae.md&quot;&gt;Step by step to train and evaluate an video autoencoder&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/hcae.md&quot;&gt;Visit the high compression video autoencoder&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Reports of previous version (better see in according branch): 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/report_04.md&quot;&gt;Open-Sora 1.3&lt;/a&gt;: shift-window attention, unified spatial-temporal VAE, etc.&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/report_03.md&quot;&gt;Open-Sora 1.2&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2412.20404&quot;&gt;Tech Report&lt;/a&gt;: rectified flow, 3d-VAE, score condition, evaluation, etc.&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/report_02.md&quot;&gt;Open-Sora 1.1&lt;/a&gt;: multi-resolution/length/aspect-ratio, image/video conditioning/editing, data preprocessing, etc.&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/report_01.md&quot;&gt;Open-Sora 1.0&lt;/a&gt;: architecture, captioning, etc.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;📍 Since Open-Sora is under active development, we remain different branchs for different versions. The latest version is &lt;a href=&quot;https://github.com/hpcaitech/Open-Sora&quot;&gt;main&lt;/a&gt;. Old versions include: &lt;a href=&quot;https://github.com/hpcaitech/Open-Sora/tree/opensora/v1.0&quot;&gt;v1.0&lt;/a&gt;, &lt;a href=&quot;https://github.com/hpcaitech/Open-Sora/tree/opensora/v1.1&quot;&gt;v1.1&lt;/a&gt;, &lt;a href=&quot;https://github.com/hpcaitech/Open-Sora/tree/opensora/v1.2&quot;&gt;v1.2&lt;/a&gt;, &lt;a href=&quot;https://github.com/hpcaitech/Open-Sora/tree/opensora/v1.3&quot;&gt;v1.3&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Quickstart&lt;/h2&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# create a virtual env and activate (conda as an example)
conda create -n opensora python=3.10
conda activate opensora

# download the repo
git clone https://github.com/hpcaitech/Open-Sora
cd Open-Sora

# Ensure torch &amp;gt;= 2.4.0
pip install -v . # for development mode, `pip install -v -e .`
pip install xformers==0.0.27.post2 --index-url https://download.pytorch.org/whl/cu121 # install xformers according to your cuda version
pip install flash-attn --no-build-isolation
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Optionally, you can install flash attention 3 for faster speed.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;git clone https://github.com/Dao-AILab/flash-attention # 4f0640d5
cd flash-attention/hopper
python setup.py install
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Model Download&lt;/h3&gt; 
&lt;p&gt;Our 11B model supports 256px and 768px resolution. Both T2V and I2V are supported by one model. 🤗 &lt;a href=&quot;https://huggingface.co/hpcai-tech/Open-Sora-v2&quot;&gt;Huggingface&lt;/a&gt; 🤖 &lt;a href=&quot;https://modelscope.cn/models/luchentech/Open-Sora-v2&quot;&gt;ModelScope&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Download from huggingface:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install &quot;huggingface_hub[cli]&quot;
huggingface-cli download hpcai-tech/Open-Sora-v2 --local-dir ./ckpts
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Download from ModelScope:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install modelscope
modelscope download hpcai-tech/Open-Sora-v2 --local_dir ./ckpts
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Text-to-Video Generation&lt;/h3&gt; 
&lt;p&gt;Our model is optimized for image-to-video generation, but it can also be used for text-to-video generation. To generate high quality videos, with the help of flux text-to-image model, we build a text-to-image-to-video pipeline. For 256x256 resolution:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# Generate one given prompt
torchrun --nproc_per_node 1 --standalone scripts/diffusion/inference.py configs/diffusion/inference/t2i2v_256px.py --save-dir samples --prompt &quot;raining, sea&quot;

# Save memory with offloading
torchrun --nproc_per_node 1 --standalone scripts/diffusion/inference.py configs/diffusion/inference/t2i2v_256px.py --save-dir samples --prompt &quot;raining, sea&quot; --offload True

# Generation with csv
torchrun --nproc_per_node 1 --standalone scripts/diffusion/inference.py configs/diffusion/inference/t2i2v_256px.py --save-dir samples --dataset.data-path assets/texts/example.csv
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For 768x768 resolution:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# One GPU
torchrun --nproc_per_node 1 --standalone scripts/diffusion/inference.py configs/diffusion/inference/t2i2v_768px.py --save-dir samples --prompt &quot;raining, sea&quot;

# Multi-GPU with colossalai sp
torchrun --nproc_per_node 8 --standalone scripts/diffusion/inference.py configs/diffusion/inference/t2i2v_768px.py --save-dir samples --prompt &quot;raining, sea&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can adjust the generation aspect ratio by &lt;code&gt;--aspect_ratio&lt;/code&gt; and the generation length by &lt;code&gt;--num_frames&lt;/code&gt;. Candidate values for aspect_ratio includes &lt;code&gt;16:9&lt;/code&gt;, &lt;code&gt;9:16&lt;/code&gt;, &lt;code&gt;1:1&lt;/code&gt;, &lt;code&gt;2.39:1&lt;/code&gt;. Candidate values for num_frames should be &lt;code&gt;4k+1&lt;/code&gt; and less than 129.&lt;/p&gt; 
&lt;p&gt;You can also run direct text-to-video by:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# One GPU for 256px
torchrun --nproc_per_node 1 --standalone scripts/diffusion/inference.py configs/diffusion/inference/256px.py --prompt &quot;raining, sea&quot;
# Multi-GPU for 768px
torchrun --nproc_per_node 8 --standalone scripts/diffusion/inference.py configs/diffusion/inference/768px.py --prompt &quot;raining, sea&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Image-to-Video Generation&lt;/h3&gt; 
&lt;p&gt;Given a prompt and a reference image, you can generate a video with the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# 256px
torchrun --nproc_per_node 1 --standalone scripts/diffusion/inference.py configs/diffusion/inference/256px.py --cond_type i2v_head --prompt &quot;A plump pig wallows in a muddy pond on a rustic farm, its pink snout poking out as it snorts contentedly. The camera captures the pig&#39;s playful splashes, sending ripples through the water under the midday sun. Wooden fences and a red barn stand in the background, framed by rolling green hills. The pig&#39;s muddy coat glistens in the sunlight, showcasing the simple pleasures of its carefree life.&quot; --ref assets/texts/i2v.png

# 256px with csv
torchrun --nproc_per_node 1 --standalone scripts/diffusion/inference.py configs/diffusion/inference/256px.py --cond_type i2v_head --dataset.data-path assets/texts/i2v.csv

# Multi-GPU 768px
torchrun --nproc_per_node 8 --standalone scripts/diffusion/inference.py configs/diffusion/inference/768px.py --cond_type i2v_head --dataset.data-path assets/texts/i2v.csv
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Advanced Usage&lt;/h2&gt; 
&lt;h3&gt;Motion Score&lt;/h3&gt; 
&lt;p&gt;During training, we provide motion score into the text prompt. During inference, you can use the following command to generate videos with motion score (the default score is 4):&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;torchrun --nproc_per_node 1 --standalone scripts/diffusion/inference.py configs/diffusion/inference/t2i2v_256px.py --save-dir samples --prompt &quot;raining, sea&quot; --motion-score 4
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;We also provide a dynamic motion score evaluator. After setting your OpenAI API key, you can use the following command to evaluate the motion score of a video:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;torchrun --nproc_per_node 1 --standalone scripts/diffusion/inference.py configs/diffusion/inference/t2i2v_256px.py --save-dir samples --prompt &quot;raining, sea&quot; --motion-score dynamic
&lt;/code&gt;&lt;/pre&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Score&lt;/th&gt; 
   &lt;th&gt;1&lt;/th&gt; 
   &lt;th&gt;4&lt;/th&gt; 
   &lt;th&gt;7&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v2.0/motion_score_1.gif&quot; width=&quot;&quot;&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v2.0/motion_score_4.gif&quot; width=&quot;&quot;&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v2.0/motion_score_7.gif&quot; width=&quot;&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Prompt Refine&lt;/h3&gt; 
&lt;p&gt;We take advantage of ChatGPT to refine the prompt. You can use the following command to refine the prompt. The function is available for both text-to-video and image-to-video generation.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;export OPENAI_API_KEY=sk-xxxx
torchrun --nproc_per_node 1 --standalone scripts/diffusion/inference.py configs/diffusion/inference/t2i2v_256px.py --save-dir samples --prompt &quot;raining, sea&quot; --refine-prompt True
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Reproductivity&lt;/h3&gt; 
&lt;p&gt;To make the results reproducible, you can set the random seed by:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;torchrun --nproc_per_node 1 --standalone scripts/diffusion/inference.py configs/diffusion/inference/t2i2v_256px.py --save-dir samples --prompt &quot;raining, sea&quot; --sampling_option.seed 42 --seed 42
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Use &lt;code&gt;--num-sample k&lt;/code&gt; to generate &lt;code&gt;k&lt;/code&gt; samples for each prompt.&lt;/p&gt; 
&lt;h2&gt;Computational Efficiency&lt;/h2&gt; 
&lt;p&gt;We test the computational efficiency of text-to-video on H100/H800 GPU. For 256x256, we use colossalai&#39;s tensor parallelism, and &lt;code&gt;--offload True&lt;/code&gt; is used. For 768x768, we use colossalai&#39;s sequence parallelism. All use number of steps 50. The results are presented in the format: $\color{blue}{\text{Total time (s)}}/\color{red}{\text{peak GPU memory (GB)}}$&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Resolution&lt;/th&gt; 
   &lt;th&gt;1x GPU&lt;/th&gt; 
   &lt;th&gt;2x GPUs&lt;/th&gt; 
   &lt;th&gt;4x GPUs&lt;/th&gt; 
   &lt;th&gt;8x GPUs&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;256x256&lt;/td&gt; 
   &lt;td&gt;$\color{blue}{60}/\color{red}{52.5}$&lt;/td&gt; 
   &lt;td&gt;$\color{blue}{40}/\color{red}{44.3}$&lt;/td&gt; 
   &lt;td&gt;$\color{blue}{34}/\color{red}{44.3}$&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;768x768&lt;/td&gt; 
   &lt;td&gt;$\color{blue}{1656}/\color{red}{60.3}$&lt;/td&gt; 
   &lt;td&gt;$\color{blue}{863}/\color{red}{48.3}$&lt;/td&gt; 
   &lt;td&gt;$\color{blue}{466}/\color{red}{44.3}$&lt;/td&gt; 
   &lt;td&gt;$\color{blue}{276}/\color{red}{44.3}$&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Evaluation&lt;/h2&gt; 
&lt;p&gt;On &lt;a href=&quot;https://huggingface.co/spaces/Vchitect/VBench_Leaderboard&quot;&gt;VBench&lt;/a&gt;, Open-Sora 2.0 significantly narrows the gap with OpenAI’s Sora, reducing it from 4.52% → 0.69% compared to Open-Sora 1.2.&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/raw/main/readme/v2_vbench.png&quot; alt=&quot;VBench&quot;&gt;&lt;/p&gt; 
&lt;p&gt;Human preference results show our model is on par with HunyuanVideo 11B and Step-Video 30B.&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/raw/main/readme/v2_winrate.png&quot; alt=&quot;Win Rate&quot;&gt;&lt;/p&gt; 
&lt;p&gt;With strong performance, Open-Sora 2.0 is cost-effective.&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/raw/main/readme/v2_cost.png&quot; alt=&quot;Cost&quot;&gt;&lt;/p&gt; 
&lt;h2&gt;Contribution&lt;/h2&gt; 
&lt;p&gt;Thanks goes to these wonderful contributors:&lt;/p&gt; 
&lt;a href=&quot;https://github.com/hpcaitech/Open-Sora/graphs/contributors&quot;&gt; &lt;img src=&quot;https://contrib.rocks/image?repo=hpcaitech/Open-Sora&quot;&gt; &lt;/a&gt; 
&lt;p&gt;If you wish to contribute to this project, please refer to the &lt;a href=&quot;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/CONTRIBUTING.md&quot;&gt;Contribution Guideline&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Acknowledgement&lt;/h2&gt; 
&lt;p&gt;Here we only list a few of the projects. For other works and datasets, please refer to our report.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/hpcaitech/ColossalAI&quot;&gt;ColossalAI&lt;/a&gt;: A powerful large model parallel acceleration and optimization system.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/facebookresearch/DiT&quot;&gt;DiT&lt;/a&gt;: Scalable Diffusion Models with Transformers.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/NUS-HPC-AI-Lab/OpenDiT&quot;&gt;OpenDiT&lt;/a&gt;: An acceleration for DiT training. We adopt valuable acceleration strategies for training progress from OpenDiT.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/PixArt-alpha/PixArt-alpha&quot;&gt;PixArt&lt;/a&gt;: An open-source DiT-based text-to-image model.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/black-forest-labs/flux&quot;&gt;Flux&lt;/a&gt;: A powerful text-to-image generation model.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Vchitect/Latte&quot;&gt;Latte&lt;/a&gt;: An attempt to efficiently train DiT for video.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Tencent/HunyuanVideo/tree/main?tab=readme-ov-file&quot;&gt;HunyuanVideo&lt;/a&gt;: Open-Source text-to-video model.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://huggingface.co/stabilityai/sd-vae-ft-mse-original&quot;&gt;StabilityAI VAE&lt;/a&gt;: A powerful image VAE model.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/mit-han-lab/efficientvit&quot;&gt;DC-AE&lt;/a&gt;: Deep Compression AutoEncoder for image compression.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/openai/CLIP&quot;&gt;CLIP&lt;/a&gt;: A powerful text-image embedding model.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/google-research/text-to-text-transfer-transformer&quot;&gt;T5&lt;/a&gt;: A powerful text encoder.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/haotian-liu/LLaVA&quot;&gt;LLaVA&lt;/a&gt;: A powerful image captioning model based on &lt;a href=&quot;https://huggingface.co/mistralai/Mistral-7B-v0.1&quot;&gt;Mistral-7B&lt;/a&gt; and &lt;a href=&quot;https://huggingface.co/01-ai/Yi-34B&quot;&gt;Yi-34B&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/magic-research/PLLaVA&quot;&gt;PLLaVA&lt;/a&gt;: A powerful video captioning model.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/mira-space/MiraData&quot;&gt;MiraData&lt;/a&gt;: A large-scale video dataset with long durations and structured caption.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bibtex&quot;&gt;@article{opensora,
  title={Open-sora: Democratizing efficient video production for all},
  author={Zheng, Zangwei and Peng, Xiangyu and Yang, Tianji and Shen, Chenhui and Li, Shenggui and Liu, Hongxin and Zhou, Yukun and Li, Tianyi and You, Yang},
  journal={arXiv preprint arXiv:2412.20404},
  year={2024}
}

@article{opensora2,
    title={Open-Sora 2.0: Training a Commercial-Level Video Generation Model in $200k}, 
    author={Xiangyu Peng and Zangwei Zheng and Chenhui Shen and Tom Young and Xinying Guo and Binluo Wang and Hang Xu and Hongxin Liu and Mingyan Jiang and Wenjun Li and Yuhui Wang and Anbang Ye and Gang Ren and Qianran Ma and Wanying Liang and Xiang Lian and Xiwen Wu and Yuting Zhong and Zhuangyan Li and Chaoyu Gong and Guojun Lei and Leijun Cheng and Limin Zhang and Minghao Li and Ruijie Zhang and Silan Hu and Shijie Huang and Xiaokang Wang and Yuanheng Zhao and Yuqi Wang and Ziang Wei and Yang You},
    year={2025},
    journal={arXiv preprint arXiv:2503.09642},
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://star-history.com/#hpcaitech/Open-Sora&amp;amp;Date&quot;&gt;&lt;img src=&quot;https://api.star-history.com/svg?repos=hpcaitech/Open-Sora&amp;amp;type=Date&quot; alt=&quot;Star History Chart&quot;&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>BerriAI/litellm</title>
      <link>https://github.com/BerriAI/litellm</link>
      <description>&lt;p&gt;Python SDK, Proxy Server (LLM Gateway) to call 100+ LLM APIs in OpenAI format - [Bedrock, Azure, OpenAI, VertexAI, Cohere, Anthropic, Sagemaker, HuggingFace, Replicate, Groq]&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&quot;center&quot;&gt; 🚅 LiteLLM &lt;/h1&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;/p&gt;
&lt;p align=&quot;center&quot;&gt; &lt;a href=&quot;https://render.com/deploy?repo=https://github.com/BerriAI/litellm&quot; target=&quot;_blank&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;https://render.com/images/deploy-to-render-button.svg?sanitize=true&quot; alt=&quot;Deploy to Render&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://railway.app/template/HLP0Ub?referralCode=jch2ME&quot;&gt; &lt;img src=&quot;https://railway.app/button.svg?sanitize=true&quot; alt=&quot;Deploy on Railway&quot;&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt;Call all LLM APIs using the OpenAI format [Bedrock, Huggingface, VertexAI, TogetherAI, Azure, OpenAI, Groq etc.] &lt;br&gt; &lt;/p&gt; 
&lt;h4 align=&quot;center&quot;&gt;&lt;a href=&quot;https://docs.litellm.ai/docs/simple_proxy&quot; target=&quot;_blank&quot;&gt;LiteLLM Proxy Server (LLM Gateway)&lt;/a&gt; | &lt;a href=&quot;https://docs.litellm.ai/docs/hosted&quot; target=&quot;_blank&quot;&gt; Hosted Proxy (Preview)&lt;/a&gt; | &lt;a href=&quot;https://docs.litellm.ai/docs/enterprise&quot; target=&quot;_blank&quot;&gt;Enterprise Tier&lt;/a&gt;&lt;/h4&gt; 
&lt;h4 align=&quot;center&quot;&gt; &lt;a href=&quot;https://pypi.org/project/litellm/&quot; target=&quot;_blank&quot;&gt; &lt;img src=&quot;https://img.shields.io/pypi/v/litellm.svg?sanitize=true&quot; alt=&quot;PyPI Version&quot;&gt; &lt;/a&gt; &lt;a href=&quot;https://dl.circleci.com/status-badge/redirect/gh/BerriAI/litellm/tree/main&quot; target=&quot;_blank&quot;&gt; &lt;img src=&quot;https://dl.circleci.com/status-badge/img/gh/BerriAI/litellm/tree/main.svg?style=svg&quot; alt=&quot;CircleCI&quot;&gt; &lt;/a&gt; &lt;a href=&quot;https://www.ycombinator.com/companies/berriai&quot;&gt; &lt;img src=&quot;https://img.shields.io/badge/Y%20Combinator-W23-orange?style=flat-square&quot; alt=&quot;Y Combinator W23&quot;&gt; &lt;/a&gt; &lt;a href=&quot;https://wa.link/huol9n&quot;&gt; &lt;img src=&quot;https://img.shields.io/static/v1?label=Chat%20on&amp;amp;message=WhatsApp&amp;amp;color=success&amp;amp;logo=WhatsApp&amp;amp;style=flat-square&quot; alt=&quot;Whatsapp&quot;&gt; &lt;/a&gt; &lt;a href=&quot;https://discord.gg/wuPM9dRgDw&quot;&gt; &lt;img src=&quot;https://img.shields.io/static/v1?label=Chat%20on&amp;amp;message=Discord&amp;amp;color=blue&amp;amp;logo=Discord&amp;amp;style=flat-square&quot; alt=&quot;Discord&quot;&gt; &lt;/a&gt; &lt;/h4&gt; 
&lt;p&gt;LiteLLM manages:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Translate inputs to provider&#39;s &lt;code&gt;completion&lt;/code&gt;, &lt;code&gt;embedding&lt;/code&gt;, and &lt;code&gt;image_generation&lt;/code&gt; endpoints&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.litellm.ai/docs/completion/output&quot;&gt;Consistent output&lt;/a&gt;, text responses will always be available at &lt;code&gt;[&#39;choices&#39;][0][&#39;message&#39;][&#39;content&#39;]&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Retry/fallback logic across multiple deployments (e.g. Azure/OpenAI) - &lt;a href=&quot;https://docs.litellm.ai/docs/routing&quot;&gt;Router&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Set Budgets &amp;amp; Rate limits per project, api key, model &lt;a href=&quot;https://docs.litellm.ai/docs/simple_proxy&quot;&gt;LiteLLM Proxy Server (LLM Gateway)&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/BerriAI/litellm?tab=readme-ov-file#openai-proxy---docs&quot;&gt;&lt;strong&gt;Jump to LiteLLM Proxy (LLM Gateway) Docs&lt;/strong&gt;&lt;/a&gt; &lt;br&gt; &lt;a href=&quot;https://github.com/BerriAI/litellm?tab=readme-ov-file#supported-providers-docs&quot;&gt;&lt;strong&gt;Jump to Supported LLM Providers&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;🚨 &lt;strong&gt;Stable Release:&lt;/strong&gt; Use docker images with the &lt;code&gt;-stable&lt;/code&gt; tag. These have undergone 12 hour load tests, before being published. &lt;a href=&quot;https://docs.litellm.ai/docs/proxy/release_cycle&quot;&gt;More information about the release cycle here&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Support for more providers. Missing a provider or LLM Platform, raise a &lt;a href=&quot;https://github.com/BerriAI/litellm/issues/new?assignees=&amp;amp;labels=enhancement&amp;amp;projects=&amp;amp;template=feature_request.yml&amp;amp;title=%5BFeature%5D%3A+&quot;&gt;feature request&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Usage (&lt;a href=&quot;https://docs.litellm.ai/docs/&quot;&gt;&lt;strong&gt;Docs&lt;/strong&gt;&lt;/a&gt;)&lt;/h1&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] LiteLLM v1.0.0 now requires &lt;code&gt;openai&amp;gt;=1.0.0&lt;/code&gt;. Migration guide &lt;a href=&quot;https://docs.litellm.ai/docs/migration&quot;&gt;here&lt;/a&gt;&lt;br&gt; LiteLLM v1.40.14+ now requires &lt;code&gt;pydantic&amp;gt;=2.0.0&lt;/code&gt;. No changes required.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;a target=&quot;_blank&quot; href=&quot;https://colab.research.google.com/github/BerriAI/litellm/blob/main/cookbook/liteLLM_Getting_Started.ipynb&quot;&gt; &lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot;&gt; &lt;/a&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;pip install litellm
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from litellm import completion
import os

## set ENV variables
os.environ[&quot;OPENAI_API_KEY&quot;] = &quot;your-openai-key&quot;
os.environ[&quot;ANTHROPIC_API_KEY&quot;] = &quot;your-anthropic-key&quot;

messages = [{ &quot;content&quot;: &quot;Hello, how are you?&quot;,&quot;role&quot;: &quot;user&quot;}]

# openai call
response = completion(model=&quot;openai/gpt-4o&quot;, messages=messages)

# anthropic call
response = completion(model=&quot;anthropic/claude-3-sonnet-20240229&quot;, messages=messages)
print(response)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Response (OpenAI Format)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-json&quot;&gt;{
    &quot;id&quot;: &quot;chatcmpl-565d891b-a42e-4c39-8d14-82a1f5208885&quot;,
    &quot;created&quot;: 1734366691,
    &quot;model&quot;: &quot;claude-3-sonnet-20240229&quot;,
    &quot;object&quot;: &quot;chat.completion&quot;,
    &quot;system_fingerprint&quot;: null,
    &quot;choices&quot;: [
        {
            &quot;finish_reason&quot;: &quot;stop&quot;,
            &quot;index&quot;: 0,
            &quot;message&quot;: {
                &quot;content&quot;: &quot;Hello! As an AI language model, I don&#39;t have feelings, but I&#39;m operating properly and ready to assist you with any questions or tasks you may have. How can I help you today?&quot;,
                &quot;role&quot;: &quot;assistant&quot;,
                &quot;tool_calls&quot;: null,
                &quot;function_call&quot;: null
            }
        }
    ],
    &quot;usage&quot;: {
        &quot;completion_tokens&quot;: 43,
        &quot;prompt_tokens&quot;: 13,
        &quot;total_tokens&quot;: 56,
        &quot;completion_tokens_details&quot;: null,
        &quot;prompt_tokens_details&quot;: {
            &quot;audio_tokens&quot;: null,
            &quot;cached_tokens&quot;: 0
        },
        &quot;cache_creation_input_tokens&quot;: 0,
        &quot;cache_read_input_tokens&quot;: 0
    }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Call any model supported by a provider, with &lt;code&gt;model=&amp;lt;provider_name&amp;gt;/&amp;lt;model_name&amp;gt;&lt;/code&gt;. There might be provider-specific details here, so refer to &lt;a href=&quot;https://docs.litellm.ai/docs/providers&quot;&gt;provider docs for more information&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Async (&lt;a href=&quot;https://docs.litellm.ai/docs/completion/stream#async-completion&quot;&gt;Docs&lt;/a&gt;)&lt;/h2&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from litellm import acompletion
import asyncio

async def test_get_response():
    user_message = &quot;Hello, how are you?&quot;
    messages = [{&quot;content&quot;: user_message, &quot;role&quot;: &quot;user&quot;}]
    response = await acompletion(model=&quot;openai/gpt-4o&quot;, messages=messages)
    return response

response = asyncio.run(test_get_response())
print(response)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Streaming (&lt;a href=&quot;https://docs.litellm.ai/docs/completion/stream&quot;&gt;Docs&lt;/a&gt;)&lt;/h2&gt; 
&lt;p&gt;liteLLM supports streaming the model response back, pass &lt;code&gt;stream=True&lt;/code&gt; to get a streaming iterator in response.&lt;br&gt; Streaming is supported for all models (Bedrock, Huggingface, TogetherAI, Azure, OpenAI, etc.)&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from litellm import completion
response = completion(model=&quot;openai/gpt-4o&quot;, messages=messages, stream=True)
for part in response:
    print(part.choices[0].delta.content or &quot;&quot;)

# claude 2
response = completion(&#39;anthropic/claude-3-sonnet-20240229&#39;, messages, stream=True)
for part in response:
    print(part)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Response chunk (OpenAI Format)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-json&quot;&gt;{
    &quot;id&quot;: &quot;chatcmpl-2be06597-eb60-4c70-9ec5-8cd2ab1b4697&quot;,
    &quot;created&quot;: 1734366925,
    &quot;model&quot;: &quot;claude-3-sonnet-20240229&quot;,
    &quot;object&quot;: &quot;chat.completion.chunk&quot;,
    &quot;system_fingerprint&quot;: null,
    &quot;choices&quot;: [
        {
            &quot;finish_reason&quot;: null,
            &quot;index&quot;: 0,
            &quot;delta&quot;: {
                &quot;content&quot;: &quot;Hello&quot;,
                &quot;role&quot;: &quot;assistant&quot;,
                &quot;function_call&quot;: null,
                &quot;tool_calls&quot;: null,
                &quot;audio&quot;: null
            },
            &quot;logprobs&quot;: null
        }
    ]
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Logging Observability (&lt;a href=&quot;https://docs.litellm.ai/docs/observability/callbacks&quot;&gt;Docs&lt;/a&gt;)&lt;/h2&gt; 
&lt;p&gt;LiteLLM exposes pre defined callbacks to send data to Lunary, MLflow, Langfuse, DynamoDB, s3 Buckets, Helicone, Promptlayer, Traceloop, Athina, Slack&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from litellm import completion

## set env variables for logging tools (when using MLflow, no API key set up is required)
os.environ[&quot;LUNARY_PUBLIC_KEY&quot;] = &quot;your-lunary-public-key&quot;
os.environ[&quot;HELICONE_API_KEY&quot;] = &quot;your-helicone-auth-key&quot;
os.environ[&quot;LANGFUSE_PUBLIC_KEY&quot;] = &quot;&quot;
os.environ[&quot;LANGFUSE_SECRET_KEY&quot;] = &quot;&quot;
os.environ[&quot;ATHINA_API_KEY&quot;] = &quot;your-athina-api-key&quot;

os.environ[&quot;OPENAI_API_KEY&quot;] = &quot;your-openai-key&quot;

# set callbacks
litellm.success_callback = [&quot;lunary&quot;, &quot;mlflow&quot;, &quot;langfuse&quot;, &quot;athina&quot;, &quot;helicone&quot;] # log input/output to lunary, langfuse, supabase, athina, helicone etc

#openai call
response = completion(model=&quot;openai/gpt-4o&quot;, messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Hi 👋 - i&#39;m openai&quot;}])
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;LiteLLM Proxy Server (LLM Gateway) - (&lt;a href=&quot;https://docs.litellm.ai/docs/simple_proxy&quot;&gt;Docs&lt;/a&gt;)&lt;/h1&gt; 
&lt;p&gt;Track spend + Load Balance across multiple projects&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://docs.litellm.ai/docs/hosted&quot;&gt;Hosted Proxy (Preview)&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;The proxy provides:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.litellm.ai/docs/proxy/virtual_keys#custom-auth&quot;&gt;Hooks for auth&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.litellm.ai/docs/proxy/logging#step-1---create-your-custom-litellm-callback-class&quot;&gt;Hooks for logging&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.litellm.ai/docs/proxy/virtual_keys#tracking-spend&quot;&gt;Cost tracking&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.litellm.ai/docs/proxy/users#set-rate-limits&quot;&gt;Rate Limiting&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;📖 Proxy Endpoints - &lt;a href=&quot;https://litellm-api.up.railway.app/&quot;&gt;Swagger Docs&lt;/a&gt;&lt;/h2&gt; 
&lt;h2&gt;Quick Start Proxy - CLI&lt;/h2&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;pip install &#39;litellm[proxy]&#39;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Step 1: Start litellm proxy&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;$ litellm --model huggingface/bigcode/starcoder

#INFO: Proxy running on http://0.0.0.0:4000
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Step 2: Make ChatCompletions Request to Proxy&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] 💡 &lt;a href=&quot;https://docs.litellm.ai/docs/proxy/user_keys&quot;&gt;Use LiteLLM Proxy with Langchain (Python, JS), OpenAI SDK (Python, JS) Anthropic SDK, Mistral SDK, LlamaIndex, Instructor, Curl&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import openai # openai v1.0.0+
client = openai.OpenAI(api_key=&quot;anything&quot;,base_url=&quot;http://0.0.0.0:4000&quot;) # set proxy to base_url
# request sent to model set on litellm proxy, `litellm --model`
response = client.chat.completions.create(model=&quot;gpt-3.5-turbo&quot;, messages = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: &quot;this is a test request, write a short poem&quot;
    }
])

print(response)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Proxy Key Management (&lt;a href=&quot;https://docs.litellm.ai/docs/proxy/virtual_keys&quot;&gt;Docs&lt;/a&gt;)&lt;/h2&gt; 
&lt;p&gt;Connect the proxy with a Postgres DB to create proxy keys&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# Get the code
git clone https://github.com/BerriAI/litellm

# Go to folder
cd litellm

# Add the master key - you can change this after setup
echo &#39;LITELLM_MASTER_KEY=&quot;sk-1234&quot;&#39; &amp;gt; .env

# Add the litellm salt key - you cannot change this after adding a model
# It is used to encrypt / decrypt your LLM API Key credentials
# We recommend - https://1password.com/password-generator/ 
# password generator to get a random hash for litellm salt key
echo &#39;LITELLM_SALT_KEY=&quot;sk-1234&quot;&#39; &amp;gt; .env

source .env

# Start
docker-compose up
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;UI on &lt;code&gt;/ui&lt;/code&gt; on your proxy server &lt;img src=&quot;https://github.com/BerriAI/litellm/assets/29436595/47c97d5e-b9be-4839-b28c-43d7f4f10033&quot; alt=&quot;ui_3&quot;&gt;&lt;/p&gt; 
&lt;p&gt;Set budgets and rate limits across multiple projects &lt;code&gt;POST /key/generate&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Request&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;curl &#39;http://0.0.0.0:4000/key/generate&#39; \
--header &#39;Authorization: Bearer sk-1234&#39; \
--header &#39;Content-Type: application/json&#39; \
--data-raw &#39;{&quot;models&quot;: [&quot;gpt-3.5-turbo&quot;, &quot;gpt-4&quot;, &quot;claude-2&quot;], &quot;duration&quot;: &quot;20m&quot;,&quot;metadata&quot;: {&quot;user&quot;: &quot;ishaan@berri.ai&quot;, &quot;team&quot;: &quot;core-infra&quot;}}&#39;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Expected Response&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;{
    &quot;key&quot;: &quot;sk-kdEXbIqZRwEeEiHwdg7sFA&quot;, # Bearer token
    &quot;expires&quot;: &quot;2023-11-19T01:38:25.838000+00:00&quot; # datetime object
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Supported Providers (&lt;a href=&quot;https://docs.litellm.ai/docs/providers&quot;&gt;Docs&lt;/a&gt;)&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Provider&lt;/th&gt; 
   &lt;th&gt;&lt;a href=&quot;https://docs.litellm.ai/docs/#basic-usage&quot;&gt;Completion&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href=&quot;https://docs.litellm.ai/docs/completion/stream#streaming-responses&quot;&gt;Streaming&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href=&quot;https://docs.litellm.ai/docs/completion/stream#async-completion&quot;&gt;Async Completion&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href=&quot;https://docs.litellm.ai/docs/completion/stream#async-streaming&quot;&gt;Async Streaming&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href=&quot;https://docs.litellm.ai/docs/embedding/supported_embedding&quot;&gt;Async Embedding&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href=&quot;https://docs.litellm.ai/docs/image_generation&quot;&gt;Async Image Generation&lt;/a&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://docs.litellm.ai/docs/providers/openai&quot;&gt;openai&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://docs.litellm.ai/docs/providers/azure&quot;&gt;azure&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://docs.litellm.ai/docs/providers/aiml&quot;&gt;AI/ML API&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://docs.litellm.ai/docs/providers/aws_sagemaker&quot;&gt;aws - sagemaker&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://docs.litellm.ai/docs/providers/bedrock&quot;&gt;aws - bedrock&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://docs.litellm.ai/docs/providers/vertex&quot;&gt;google - vertex_ai&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://docs.litellm.ai/docs/providers/palm&quot;&gt;google - palm&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://docs.litellm.ai/docs/providers/gemini&quot;&gt;google AI Studio - gemini&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://docs.litellm.ai/docs/providers/mistral&quot;&gt;mistral ai api&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://docs.litellm.ai/docs/providers/cloudflare_workers&quot;&gt;cloudflare AI Workers&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://docs.litellm.ai/docs/providers/cohere&quot;&gt;cohere&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://docs.litellm.ai/docs/providers/anthropic&quot;&gt;anthropic&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://docs.litellm.ai/docs/providers/empower&quot;&gt;empower&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://docs.litellm.ai/docs/providers/huggingface&quot;&gt;huggingface&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://docs.litellm.ai/docs/providers/replicate&quot;&gt;replicate&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://docs.litellm.ai/docs/providers/togetherai&quot;&gt;together_ai&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://docs.litellm.ai/docs/providers/openrouter&quot;&gt;openrouter&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://docs.litellm.ai/docs/providers/ai21&quot;&gt;ai21&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://docs.litellm.ai/docs/providers/baseten&quot;&gt;baseten&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://docs.litellm.ai/docs/providers/vllm&quot;&gt;vllm&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://docs.litellm.ai/docs/providers/nlp_cloud&quot;&gt;nlp_cloud&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://docs.litellm.ai/docs/providers/aleph_alpha&quot;&gt;aleph alpha&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://docs.litellm.ai/docs/providers/petals&quot;&gt;petals&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://docs.litellm.ai/docs/providers/ollama&quot;&gt;ollama&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://docs.litellm.ai/docs/providers/deepinfra&quot;&gt;deepinfra&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://docs.litellm.ai/docs/providers/perplexity&quot;&gt;perplexity-ai&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://docs.litellm.ai/docs/providers/groq&quot;&gt;Groq AI&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://docs.litellm.ai/docs/providers/deepseek&quot;&gt;Deepseek&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://docs.litellm.ai/docs/providers/anyscale&quot;&gt;anyscale&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://docs.litellm.ai/docs/providers/watsonx&quot;&gt;IBM - watsonx.ai&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://docs.litellm.ai/docs/providers/voyage&quot;&gt;voyage ai&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://docs.litellm.ai/docs/providers/xinference&quot;&gt;xinference [Xorbits Inference]&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://docs.litellm.ai/docs/providers/friendliai&quot;&gt;FriendliAI&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://docs.litellm.ai/docs/providers/galadriel&quot;&gt;Galadriel&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;a href=&quot;https://docs.litellm.ai/docs/&quot;&gt;&lt;strong&gt;Read the Docs&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Interested in contributing? Contributions to LiteLLM Python SDK, Proxy Server, and contributing LLM integrations are both accepted and highly encouraged! &lt;a href=&quot;https://docs.litellm.ai/docs/extras/contributing_code&quot;&gt;See our Contribution Guide for more details&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Enterprise&lt;/h1&gt; 
&lt;p&gt;For companies that need better security, user management and professional support&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://calendly.com/d/4mp-gd3-k5k/litellm-1-1-onboarding-chat&quot;&gt;Talk to founders&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;This covers:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;✅ &lt;strong&gt;Features under the &lt;a href=&quot;https://docs.litellm.ai/docs/proxy/enterprise&quot;&gt;LiteLLM Commercial License&lt;/a&gt;:&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;✅ &lt;strong&gt;Feature Prioritization&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;✅ &lt;strong&gt;Custom Integrations&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;✅ &lt;strong&gt;Professional Support - Dedicated discord + slack&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;✅ &lt;strong&gt;Custom SLAs&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;✅ &lt;strong&gt;Secure access with Single Sign-On&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Code Quality / Linting&lt;/h1&gt; 
&lt;p&gt;LiteLLM follows the &lt;a href=&quot;https://google.github.io/styleguide/pyguide.html&quot;&gt;Google Python Style Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;We run:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Ruff for &lt;a href=&quot;https://github.com/BerriAI/litellm/raw/e19bb55e3b4c6a858b6e364302ebbf6633a51de5/.circleci/config.yml#L320&quot;&gt;formatting and linting checks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Mypy + Pyright for typing &lt;a href=&quot;https://github.com/BerriAI/litellm/raw/e19bb55e3b4c6a858b6e364302ebbf6633a51de5/.circleci/config.yml#L90&quot;&gt;1&lt;/a&gt;, &lt;a href=&quot;https://github.com/BerriAI/litellm/raw/e19bb55e3b4c6a858b6e364302ebbf6633a51de5/.pre-commit-config.yaml#L4&quot;&gt;2&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Black for &lt;a href=&quot;https://github.com/BerriAI/litellm/raw/e19bb55e3b4c6a858b6e364302ebbf6633a51de5/.circleci/config.yml#L79&quot;&gt;formatting&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;isort for &lt;a href=&quot;https://github.com/BerriAI/litellm/raw/e19bb55e3b4c6a858b6e364302ebbf6633a51de5/.pre-commit-config.yaml#L10&quot;&gt;import sorting&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If you have suggestions on how to improve the code quality feel free to open an issue or a PR.&lt;/p&gt; 
&lt;h1&gt;Support / talk with founders&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://calendly.com/d/4mp-gd3-k5k/berriai-1-1-onboarding-litellm-hosted-version&quot;&gt;Schedule Demo 👋&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://discord.gg/wuPM9dRgDw&quot;&gt;Community Discord 💭&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Our numbers 📞 +1 (770) 8783-106 / ‭+1 (412) 618-6238‬&lt;/li&gt; 
 &lt;li&gt;Our emails ✉️ &lt;a href=&quot;mailto:ishaan@berri.ai&quot;&gt;ishaan@berri.ai&lt;/a&gt; / &lt;a href=&quot;mailto:krrish@berri.ai&quot;&gt;krrish@berri.ai&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Why did we build this&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Need for simplicity&lt;/strong&gt;: Our code started to get extremely complicated managing &amp;amp; translating calls between Azure, OpenAI and Cohere.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Contributors&lt;/h1&gt; 
&lt;!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section --&gt; 
&lt;!-- prettier-ignore-start --&gt; 
&lt;!-- markdownlint-disable --&gt; 
&lt;!-- markdownlint-restore --&gt; 
&lt;!-- prettier-ignore-end --&gt; 
&lt;!-- ALL-CONTRIBUTORS-LIST:END --&gt; 
&lt;a href=&quot;https://github.com/BerriAI/litellm/graphs/contributors&quot;&gt; &lt;img src=&quot;https://contrib.rocks/image?repo=BerriAI/litellm&quot;&gt; &lt;/a&gt; 
&lt;h2&gt;Run in Developer mode&lt;/h2&gt; 
&lt;h3&gt;Services&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Setup .env file in root&lt;/li&gt; 
 &lt;li&gt;Run dependant services &lt;code&gt;docker-compose up db prometheus&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Backend&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;(In root) create virtual environment &lt;code&gt;python -m venv .venv&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Activate virtual environment &lt;code&gt;source .venv/bin/activate&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Install dependencies &lt;code&gt;pip install -e &quot;.[all]&quot;&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Start proxy backend &lt;code&gt;uvicorn litellm.proxy.proxy_server:app --host localhost --port 4000 --reload&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Frontend&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Navigate to &lt;code&gt;ui/litellm-dashboard&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Install dependencies &lt;code&gt;npm install&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Run &lt;code&gt;npm run dev&lt;/code&gt; to start the dashboard&lt;/li&gt; 
&lt;/ol&gt;</description>
    </item>
    
  </channel>
</rss>
