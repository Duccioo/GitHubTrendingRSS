[
  {
    "name": "Stream29/ProxyAsLocalModel",
    "url": "https://github.com/Stream29/ProxyAsLocalModel",
    "description": "Proxy remote LLM API as Ollama and LM Studio, for using them in JetBrains AI Assistant",
    "stars": 82,
    "forks": 6,
    "language": "Kotlin",
    "topics": [],
    "license": "No license",
    "created_at": "2025-04-30T06:55:57Z",
    "updated_at": "2025-05-04T23:14:33Z",
    "owner": {
      "name": "Stream29",
      "avatar_url": "https://avatars.githubusercontent.com/u/36751053?v=4"
    },
    "telegraph_url": null
  }
]