<?xml version="1.0" encoding="UTF-8" ?>
<rss xmlns:ns0="http://www.w3.org/2005/Atom" version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>GitHub Trending - Ruby (Weekly)</title><link>https://duccioo.github.io/GitHubTrendingRSS/ruby_weekly.xml</link><ns0:link href="https://duccioo.github.io/GitHubTrendingRSS/ruby_weekly.xml" rel="self" type="application/rss+xml" /><description>Repository più popolari su GitHub in Ruby nel periodo weekly</description><language>it-it</language><lastBuildDate>Sat, 11 Oct 2025 02:51:48 +0000</lastBuildDate><item><title>📝 docusealco/rllama (62 ⭐)</title><link>https://github.com/docusealco/rllama</link><guid isPermaLink="true">https://github.com/docusealco/rllama</guid><description>
        &lt;![CDATA[
        &lt;p&gt;👤 &lt;strong&gt;Owner:&lt;/strong&gt; docusealco&lt;/p&gt;
        &lt;p&gt;📝 &lt;strong&gt;Repository:&lt;/strong&gt; &lt;a href="https://github.com/docusealco/rllama"&gt;docusealco/rllama&lt;/a&gt;&lt;/p&gt;
        &lt;p&gt;📄 &lt;strong&gt;Descrizione:&lt;/strong&gt; Ruby FFI bindings for llama.cpp to run open-source LLMs such as GPT-OSS, Qwen 3, Gemma 3, and Llama 3 locally with Ruby code.&lt;/p&gt;
        &lt;p&gt;⭐ &lt;strong&gt;Stelle:&lt;/strong&gt; 62&lt;/p&gt;
        &lt;p&gt;🍴 &lt;strong&gt;Forks:&lt;/strong&gt; 3&lt;/p&gt;
        &lt;p&gt;💻 &lt;strong&gt;Linguaggio:&lt;/strong&gt; Ruby&lt;/p&gt;
        &lt;p&gt;&lt;strong&gt;🏷️ Topics:&lt;/strong&gt; &lt;span style="background-color: #f0f0f0; padding: 2px 5px; border-radius: 3px; margin-right: 3px; font-size: 0.9em;"&gt;ai&lt;/span&gt; &lt;span style="background-color: #f0f0f0; padding: 2px 5px; border-radius: 3px; margin-right: 3px; font-size: 0.9em;"&gt;embeddings&lt;/span&gt; &lt;span style="background-color: #f0f0f0; padding: 2px 5px; border-radius: 3px; margin-right: 3px; font-size: 0.9em;"&gt;ffi&lt;/span&gt; &lt;span style="background-color: #f0f0f0; padding: 2px 5px; border-radius: 3px; margin-right: 3px; font-size: 0.9em;"&gt;gguf&lt;/span&gt; &lt;span style="background-color: #f0f0f0; padding: 2px 5px; border-radius: 3px; margin-right: 3px; font-size: 0.9em;"&gt;inference&lt;/span&gt; &lt;span style="background-color: #f0f0f0; padding: 2px 5px; border-radius: 3px; margin-right: 3px; font-size: 0.9em;"&gt;llamacpp&lt;/span&gt; &lt;span style="background-color: #f0f0f0; padding: 2px 5px; border-radius: 3px; margin-right: 3px; font-size: 0.9em;"&gt;llm&lt;/span&gt; &lt;span style="background-color: #f0f0f0; padding: 2px 5px; border-radius: 3px; margin-right: 3px; font-size: 0.9em;"&gt;ruby&lt;/span&gt;&lt;/p&gt;
        &lt;p&gt;📜 &lt;strong&gt;Licenza:&lt;/strong&gt; No license&lt;/p&gt;
        &lt;p&gt;⏰ &lt;strong&gt;Creato il:&lt;/strong&gt; 2025-10-05T08:02:20Z&lt;/p&gt;
        &lt;p&gt;🔄 &lt;strong&gt;Ultimo Aggiornamento:&lt;/strong&gt; 2025-10-10T21:53:03Z&lt;/p&gt;
        
        &lt;hr&gt;
        &lt;p&gt;&lt;a href="https://github.com/docusealco/rllama"&gt;Visita il Repository su GitHub&lt;/a&gt;&lt;/p&gt;
        ]]&gt;
        </description><pubDate>Fri, 10 Oct 2025 21:53:03 +0000</pubDate><category>Ruby</category><category>ai</category><category>embeddings</category><category>ffi</category><category>gguf</category><category>inference</category><category>llamacpp</category><category>llm</category><category>ruby</category></item></channel></rss>