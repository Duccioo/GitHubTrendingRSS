<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>GitHub Python Monthly Trending</title>
    <description>Monthly Trending of Python in GitHub</description>
    <pubDate>Sun, 16 Mar 2025 02:07:33 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>huggingface/lerobot</title>
      <link>https://github.com/huggingface/lerobot</link>
      <description>&lt;p&gt;🤗 LeRobot: Making AI for Robotics more accessible with end-to-end learning&lt;/p&gt;&lt;hr&gt;&lt;p align=&quot;center&quot;&gt; 
 &lt;picture&gt; 
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;media/lerobot-logo-thumbnail.png&quot;&gt; 
  &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;media/lerobot-logo-thumbnail.png&quot;&gt; 
  &lt;img alt=&quot;LeRobot, Hugging Face Robotics Library&quot; src=&quot;https://raw.githubusercontent.com/huggingface/lerobot/main/media/lerobot-logo-thumbnail.png&quot; style=&quot;max-width: 100%;&quot;&gt; 
 &lt;/picture&gt; &lt;br&gt; &lt;br&gt; &lt;/p&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;p&gt;&lt;a href=&quot;https://github.com/huggingface/lerobot/actions/workflows/nightly-tests.yml?query=branch%3Amain&quot;&gt;&lt;img src=&quot;https://github.com/huggingface/lerobot/actions/workflows/nightly-tests.yml/badge.svg?branch=main&quot; alt=&quot;Tests&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://codecov.io/gh/huggingface/lerobot&quot;&gt;&lt;img src=&quot;https://codecov.io/gh/huggingface/lerobot/branch/main/graph/badge.svg?token=TODO&quot; alt=&quot;Coverage&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://www.python.org/downloads/&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/pyversions/lerobot&quot; alt=&quot;Python versions&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/huggingface/lerobot/raw/main/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/License-Apache%202.0-blue.svg?sanitize=true&quot; alt=&quot;License&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://pypi.org/project/lerobot/&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/status/lerobot&quot; alt=&quot;Status&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://pypi.org/project/lerobot/&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/v/lerobot&quot; alt=&quot;Version&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/huggingface/lerobot/tree/main/examples&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Examples-green.svg?sanitize=true&quot; alt=&quot;Examples&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/huggingface/lerobot/raw/main/CODE_OF_CONDUCT.md&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Contributor%20Covenant-v2.1%20adopted-ff69b4.svg?sanitize=true&quot; alt=&quot;Contributor Covenant&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://discord.gg/s3KuuzsPFb&quot;&gt;&lt;img src=&quot;https://dcbadge.vercel.app/api/server/C5P34WJ68S?style=flat&quot; alt=&quot;Discord&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2 align=&quot;center&quot;&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/huggingface/lerobot/raw/main/examples/10_use_so100.md&quot;&gt; Build Your Own SO-100 Robot!&lt;/a&gt;&lt;/p&gt; &lt;/h2&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;img src=&quot;https://raw.githubusercontent.com/huggingface/lerobot/main/media/so100/leader_follower.webp?raw=true&quot; alt=&quot;SO-100 leader and follower arms&quot; title=&quot;SO-100 leader and follower arms&quot; width=&quot;50%&quot;&gt; 
 &lt;p&gt;&lt;strong&gt;Meet the SO-100 – Just $110 per arm!&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;Train it in minutes with a few simple moves on your laptop.&lt;/p&gt; 
 &lt;p&gt;Then sit back and watch your creation act autonomously! 🤯&lt;/p&gt; 
 &lt;p&gt;&lt;a href=&quot;https://github.com/huggingface/lerobot/raw/main/examples/10_use_so100.md&quot;&gt; Get the full SO-100 tutorial here.&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;Want to take it to the next level? Make your SO-100 mobile by building LeKiwi!&lt;/p&gt; 
 &lt;p&gt;Check out the &lt;a href=&quot;https://github.com/huggingface/lerobot/raw/main/examples/11_use_lekiwi.md&quot;&gt;LeKiwi tutorial&lt;/a&gt; and bring your robot to life on wheels.&lt;/p&gt; 
 &lt;img src=&quot;https://raw.githubusercontent.com/huggingface/lerobot/main/media/lekiwi/kiwi.webp?raw=true&quot; alt=&quot;LeKiwi mobile robot&quot; title=&quot;LeKiwi mobile robot&quot; width=&quot;50%&quot;&gt; 
&lt;/div&gt; 
&lt;br&gt; 
&lt;h3 align=&quot;center&quot;&gt; &lt;p&gt;LeRobot: State-of-the-art AI for real-world robotics&lt;/p&gt; &lt;/h3&gt; 
&lt;hr&gt; 
&lt;p&gt;🤗 LeRobot aims to provide models, datasets, and tools for real-world robotics in PyTorch. The goal is to lower the barrier to entry to robotics so that everyone can contribute and benefit from sharing datasets and pretrained models.&lt;/p&gt; 
&lt;p&gt;🤗 LeRobot contains state-of-the-art approaches that have been shown to transfer to the real-world with a focus on imitation learning and reinforcement learning.&lt;/p&gt; 
&lt;p&gt;🤗 LeRobot already provides a set of pretrained models, datasets with human collected demonstrations, and simulation environments to get started without assembling a robot. In the coming weeks, the plan is to add more and more support for real-world robotics on the most affordable and capable robots out there.&lt;/p&gt; 
&lt;p&gt;🤗 LeRobot hosts pretrained models and datasets on this Hugging Face community page: &lt;a href=&quot;https://huggingface.co/lerobot&quot;&gt;huggingface.co/lerobot&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;Examples of pretrained models on simulation environments&lt;/h4&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src=&quot;https://raw.githubusercontent.com/huggingface/lerobot/main/media/gym/aloha_act.gif&quot; width=&quot;100%&quot; alt=&quot;ACT policy on ALOHA env&quot;&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src=&quot;https://raw.githubusercontent.com/huggingface/lerobot/main/media/gym/simxarm_tdmpc.gif&quot; width=&quot;100%&quot; alt=&quot;TDMPC policy on SimXArm env&quot;&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src=&quot;https://raw.githubusercontent.com/huggingface/lerobot/main/media/gym/pusht_diffusion.gif&quot; width=&quot;100%&quot; alt=&quot;Diffusion policy on PushT env&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;ACT policy on ALOHA env&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;TDMPC policy on SimXArm env&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Diffusion policy on PushT env&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h3&gt;Acknowledgment&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Thanks to Tony Zhao, Zipeng Fu and colleagues for open sourcing ACT policy, ALOHA environments and datasets. Ours are adapted from &lt;a href=&quot;https://tonyzhaozh.github.io/aloha&quot;&gt;ALOHA&lt;/a&gt; and &lt;a href=&quot;https://mobile-aloha.github.io&quot;&gt;Mobile ALOHA&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Thanks to Cheng Chi, Zhenjia Xu and colleagues for open sourcing Diffusion policy, Pusht environment and datasets, as well as UMI datasets. Ours are adapted from &lt;a href=&quot;https://diffusion-policy.cs.columbia.edu&quot;&gt;Diffusion Policy&lt;/a&gt; and &lt;a href=&quot;https://umi-gripper.github.io&quot;&gt;UMI Gripper&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Thanks to Nicklas Hansen, Yunhai Feng and colleagues for open sourcing TDMPC policy, Simxarm environments and datasets. Ours are adapted from &lt;a href=&quot;https://github.com/nicklashansen/tdmpc&quot;&gt;TDMPC&lt;/a&gt; and &lt;a href=&quot;https://www.yunhaifeng.com/FOWM&quot;&gt;FOWM&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Thanks to Antonio Loquercio and Ashish Kumar for their early support.&lt;/li&gt; 
 &lt;li&gt;Thanks to &lt;a href=&quot;https://sjlee.cc/&quot;&gt;Seungjae (Jay) Lee&lt;/a&gt;, &lt;a href=&quot;https://mahis.life/&quot;&gt;Mahi Shafiullah&lt;/a&gt; and colleagues for open sourcing &lt;a href=&quot;https://sjlee.cc/vq-bet/&quot;&gt;VQ-BeT&lt;/a&gt; policy and helping us adapt the codebase to our repository. The policy is adapted from &lt;a href=&quot;https://github.com/jayLEE0301/vq_bet_official&quot;&gt;VQ-BeT repo&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;Download our source code:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;git clone https://github.com/huggingface/lerobot.git
cd lerobot
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Create a virtual environment with Python 3.10 and activate it, e.g. with &lt;a href=&quot;https://docs.anaconda.com/free/miniconda/index.html&quot;&gt;&lt;code&gt;miniconda&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;conda create -y -n lerobot python=3.10
conda activate lerobot
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Install 🤗 LeRobot:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; Depending on your platform, If you encounter any build errors during this step you may need to install &lt;code&gt;cmake&lt;/code&gt; and &lt;code&gt;build-essential&lt;/code&gt; for building some of our dependencies. On linux: &lt;code&gt;sudo apt-get install cmake build-essential&lt;/code&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;For simulations, 🤗 LeRobot comes with gymnasium environments that can be installed as extras:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/huggingface/gym-aloha&quot;&gt;aloha&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/huggingface/gym-xarm&quot;&gt;xarm&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/huggingface/gym-pusht&quot;&gt;pusht&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For instance, to install 🤗 LeRobot with aloha and pusht, use:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install -e &quot;.[aloha, pusht]&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To use &lt;a href=&quot;https://docs.wandb.ai/quickstart&quot;&gt;Weights and Biases&lt;/a&gt; for experiment tracking, log in with&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;wandb login
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;(note: you will also need to enable WandB in the configuration. See below.)&lt;/p&gt; 
&lt;h2&gt;Walkthrough&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;.
├── examples             # contains demonstration examples, start here to learn about LeRobot
|   └── advanced         # contains even more examples for those who have mastered the basics
├── lerobot
|   ├── configs          # contains config classes with all options that you can override in the command line
|   ├── common           # contains classes and utilities
|   |   ├── datasets       # various datasets of human demonstrations: aloha, pusht, xarm
|   |   ├── envs           # various sim environments: aloha, pusht, xarm
|   |   ├── policies       # various policies: act, diffusion, tdmpc
|   |   ├── robot_devices  # various real devices: dynamixel motors, opencv cameras, koch robots
|   |   └── utils          # various utilities
|   └── scripts          # contains functions to execute via command line
|       ├── eval.py                 # load policy and evaluate it on an environment
|       ├── train.py                # train a policy via imitation learning and/or reinforcement learning
|       ├── control_robot.py        # teleoperate a real robot, record data, run a policy
|       ├── push_dataset_to_hub.py  # convert your dataset into LeRobot dataset format and upload it to the Hugging Face hub
|       └── visualize_dataset.py    # load a dataset and render its demonstrations
├── outputs               # contains results of scripts execution: logs, videos, model checkpoints
└── tests                 # contains pytest utilities for continuous integration
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Visualize datasets&lt;/h3&gt; 
&lt;p&gt;Check out &lt;a href=&quot;https://raw.githubusercontent.com/huggingface/lerobot/main/examples/1_load_lerobot_dataset.py&quot;&gt;example 1&lt;/a&gt; that illustrates how to use our dataset class which automatically downloads data from the Hugging Face hub.&lt;/p&gt; 
&lt;p&gt;You can also locally visualize episodes from a dataset on the hub by executing our script from the command line:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python lerobot/scripts/visualize_dataset.py \
    --repo-id lerobot/pusht \
    --episode-index 0
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or from a dataset in a local folder with the &lt;code&gt;root&lt;/code&gt; option and the &lt;code&gt;--local-files-only&lt;/code&gt; (in the following case the dataset will be searched for in &lt;code&gt;./my_local_data_dir/lerobot/pusht&lt;/code&gt;)&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python lerobot/scripts/visualize_dataset.py \
    --repo-id lerobot/pusht \
    --root ./my_local_data_dir \
    --local-files-only 1 \
    --episode-index 0
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;It will open &lt;code&gt;rerun.io&lt;/code&gt; and display the camera streams, robot states and actions, like this:&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github-production-user-asset-6210df.s3.amazonaws.com/4681518/328035972-fd46b787-b532-47e2-bb6f-fd536a55a7ed.mov?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;amp;X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240505%2Fus-east-1%2Fs3%2Faws4_request&amp;amp;X-Amz-Date=20240505T172924Z&amp;amp;X-Amz-Expires=300&amp;amp;X-Amz-Signature=d680b26c532eeaf80740f08af3320d22ad0b8a4e4da1bcc4f33142c15b509eda&amp;amp;X-Amz-SignedHeaders=host&amp;amp;actor_id=24889239&amp;amp;key_id=0&amp;amp;repo_id=748713144&quot;&gt;https://github-production-user-asset-6210df.s3.amazonaws.com/4681518/328035972-fd46b787-b532-47e2-bb6f-fd536a55a7ed.mov?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;amp;X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240505%2Fus-east-1%2Fs3%2Faws4_request&amp;amp;X-Amz-Date=20240505T172924Z&amp;amp;X-Amz-Expires=300&amp;amp;X-Amz-Signature=d680b26c532eeaf80740f08af3320d22ad0b8a4e4da1bcc4f33142c15b509eda&amp;amp;X-Amz-SignedHeaders=host&amp;amp;actor_id=24889239&amp;amp;key_id=0&amp;amp;repo_id=748713144&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Our script can also visualize datasets stored on a distant server. See &lt;code&gt;python lerobot/scripts/visualize_dataset.py --help&lt;/code&gt; for more instructions.&lt;/p&gt; 
&lt;h3&gt;The &lt;code&gt;LeRobotDataset&lt;/code&gt; format&lt;/h3&gt; 
&lt;p&gt;A dataset in &lt;code&gt;LeRobotDataset&lt;/code&gt; format is very simple to use. It can be loaded from a repository on the Hugging Face hub or a local folder simply with e.g. &lt;code&gt;dataset = LeRobotDataset(&quot;lerobot/aloha_static_coffee&quot;)&lt;/code&gt; and can be indexed into like any Hugging Face and PyTorch dataset. For instance &lt;code&gt;dataset[0]&lt;/code&gt; will retrieve a single temporal frame from the dataset containing observation(s) and an action as PyTorch tensors ready to be fed to a model.&lt;/p&gt; 
&lt;p&gt;A specificity of &lt;code&gt;LeRobotDataset&lt;/code&gt; is that, rather than retrieving a single frame by its index, we can retrieve several frames based on their temporal relationship with the indexed frame, by setting &lt;code&gt;delta_timestamps&lt;/code&gt; to a list of relative times with respect to the indexed frame. For example, with &lt;code&gt;delta_timestamps = {&quot;observation.image&quot;: [-1, -0.5, -0.2, 0]}&lt;/code&gt; one can retrieve, for a given index, 4 frames: 3 &quot;previous&quot; frames 1 second, 0.5 seconds, and 0.2 seconds before the indexed frame, and the indexed frame itself (corresponding to the 0 entry). See example &lt;a href=&quot;https://raw.githubusercontent.com/huggingface/lerobot/main/examples/1_load_lerobot_dataset.py&quot;&gt;1_load_lerobot_dataset.py&lt;/a&gt; for more details on &lt;code&gt;delta_timestamps&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Under the hood, the &lt;code&gt;LeRobotDataset&lt;/code&gt; format makes use of several ways to serialize data which can be useful to understand if you plan to work more closely with this format. We tried to make a flexible yet simple dataset format that would cover most type of features and specificities present in reinforcement learning and robotics, in simulation and in real-world, with a focus on cameras and robot states but easily extended to other types of sensory inputs as long as they can be represented by a tensor.&lt;/p&gt; 
&lt;p&gt;Here are the important details and internal structure organization of a typical &lt;code&gt;LeRobotDataset&lt;/code&gt; instantiated with &lt;code&gt;dataset = LeRobotDataset(&quot;lerobot/aloha_static_coffee&quot;)&lt;/code&gt;. The exact features will change from dataset to dataset but not the main aspects:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;dataset attributes:
  ├ hf_dataset: a Hugging Face dataset (backed by Arrow/parquet). Typical features example:
  │  ├ observation.images.cam_high (VideoFrame):
  │  │   VideoFrame = {&#39;path&#39;: path to a mp4 video, &#39;timestamp&#39; (float32): timestamp in the video}
  │  ├ observation.state (list of float32): position of an arm joints (for instance)
  │  ... (more observations)
  │  ├ action (list of float32): goal position of an arm joints (for instance)
  │  ├ episode_index (int64): index of the episode for this sample
  │  ├ frame_index (int64): index of the frame for this sample in the episode ; starts at 0 for each episode
  │  ├ timestamp (float32): timestamp in the episode
  │  ├ next.done (bool): indicates the end of en episode ; True for the last frame in each episode
  │  └ index (int64): general index in the whole dataset
  ├ episode_data_index: contains 2 tensors with the start and end indices of each episode
  │  ├ from (1D int64 tensor): first frame index for each episode — shape (num episodes,) starts with 0
  │  └ to: (1D int64 tensor): last frame index for each episode — shape (num episodes,)
  ├ stats: a dictionary of statistics (max, mean, min, std) for each feature in the dataset, for instance
  │  ├ observation.images.cam_high: {&#39;max&#39;: tensor with same number of dimensions (e.g. `(c, 1, 1)` for images, `(c,)` for states), etc.}
  │  ...
  ├ info: a dictionary of metadata on the dataset
  │  ├ codebase_version (str): this is to keep track of the codebase version the dataset was created with
  │  ├ fps (float): frame per second the dataset is recorded/synchronized to
  │  ├ video (bool): indicates if frames are encoded in mp4 video files to save space or stored as png files
  │  └ encoding (dict): if video, this documents the main options that were used with ffmpeg to encode the videos
  ├ videos_dir (Path): where the mp4 videos or png images are stored/accessed
  └ camera_keys (list of string): the keys to access camera features in the item returned by the dataset (e.g. `[&quot;observation.images.cam_high&quot;, ...]`)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;A &lt;code&gt;LeRobotDataset&lt;/code&gt; is serialised using several widespread file formats for each of its parts, namely:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;hf_dataset stored using Hugging Face datasets library serialization to parquet&lt;/li&gt; 
 &lt;li&gt;videos are stored in mp4 format to save space&lt;/li&gt; 
 &lt;li&gt;metadata are stored in plain json/jsonl files&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Dataset can be uploaded/downloaded from the HuggingFace hub seamlessly. To work on a local dataset, you can specify its location with the &lt;code&gt;root&lt;/code&gt; argument if it&#39;s not in the default &lt;code&gt;~/.cache/huggingface/lerobot&lt;/code&gt; location.&lt;/p&gt; 
&lt;h3&gt;Evaluate a pretrained policy&lt;/h3&gt; 
&lt;p&gt;Check out &lt;a href=&quot;https://raw.githubusercontent.com/huggingface/lerobot/main/examples/2_evaluate_pretrained_policy.py&quot;&gt;example 2&lt;/a&gt; that illustrates how to download a pretrained policy from Hugging Face hub, and run an evaluation on its corresponding environment.&lt;/p&gt; 
&lt;p&gt;We also provide a more capable script to parallelize the evaluation over multiple environments during the same rollout. Here is an example with a pretrained model hosted on &lt;a href=&quot;https://huggingface.co/lerobot/diffusion_pusht&quot;&gt;lerobot/diffusion_pusht&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python lerobot/scripts/eval.py \
    --policy.path=lerobot/diffusion_pusht \
    --env.type=pusht \
    --eval.batch_size=10 \
    --eval.n_episodes=10 \
    --policy.use_amp=false \
    --policy.device=cuda
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note: After training your own policy, you can re-evaluate the checkpoints with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python lerobot/scripts/eval.py --policy.path={OUTPUT_DIR}/checkpoints/last/pretrained_model
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;code&gt;python lerobot/scripts/eval.py --help&lt;/code&gt; for more instructions.&lt;/p&gt; 
&lt;h3&gt;Train your own policy&lt;/h3&gt; 
&lt;p&gt;Check out &lt;a href=&quot;https://raw.githubusercontent.com/huggingface/lerobot/main/examples/3_train_policy.py&quot;&gt;example 3&lt;/a&gt; that illustrate how to train a model using our core library in python, and &lt;a href=&quot;https://raw.githubusercontent.com/huggingface/lerobot/main/examples/4_train_policy_with_script.md&quot;&gt;example 4&lt;/a&gt; that shows how to use our training script from command line.&lt;/p&gt; 
&lt;p&gt;To use wandb for logging training and evaluation curves, make sure you&#39;ve run &lt;code&gt;wandb login&lt;/code&gt; as a one-time setup step. Then, when running the training command above, enable WandB in the configuration by adding &lt;code&gt;--wandb.enable=true&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;A link to the wandb logs for the run will also show up in yellow in your terminal. Here is an example of what they look like in your browser. Please also check &lt;a href=&quot;https://raw.githubusercontent.com/huggingface/lerobot/main/examples/4_train_policy_with_script.md#typical-logs-and-metrics&quot;&gt;here&lt;/a&gt; for the explanation of some commonly used metrics in logs.&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/huggingface/lerobot/main/media/wandb.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt; 
&lt;p&gt;Note: For efficiency, during training every checkpoint is evaluated on a low number of episodes. You may use &lt;code&gt;--eval.n_episodes=500&lt;/code&gt; to evaluate on more episodes than the default. Or, after training, you may want to re-evaluate your best checkpoints on more episodes or change the evaluation settings. See &lt;code&gt;python lerobot/scripts/eval.py --help&lt;/code&gt; for more instructions.&lt;/p&gt; 
&lt;h4&gt;Reproduce state-of-the-art (SOTA)&lt;/h4&gt; 
&lt;p&gt;We provide some pretrained policies on our &lt;a href=&quot;https://huggingface.co/lerobot&quot;&gt;hub page&lt;/a&gt; that can achieve state-of-the-art performances. You can reproduce their training by loading the config from their run. Simply running:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python lerobot/scripts/train.py --config_path=lerobot/diffusion_pusht
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;reproduces SOTA results for Diffusion Policy on the PushT task.&lt;/p&gt; 
&lt;h2&gt;Contribute&lt;/h2&gt; 
&lt;p&gt;If you would like to contribute to 🤗 LeRobot, please check out our &lt;a href=&quot;https://github.com/huggingface/lerobot/raw/main/CONTRIBUTING.md&quot;&gt;contribution guide&lt;/a&gt;.&lt;/p&gt; 
&lt;!-- ### Add a new dataset

To add a dataset to the hub, you need to login using a write-access token, which can be generated from the [Hugging Face settings](https://huggingface.co/settings/tokens):
```bash
huggingface-cli login --token ${HUGGINGFACE_TOKEN} --add-to-git-credential
```

Then point to your raw dataset folder (e.g. `data/aloha_static_pingpong_test_raw`), and push your dataset to the hub with:
```bash
python lerobot/scripts/push_dataset_to_hub.py \
--raw-dir data/aloha_static_pingpong_test_raw \
--out-dir data \
--repo-id lerobot/aloha_static_pingpong_test \
--raw-format aloha_hdf5
```

See `python lerobot/scripts/push_dataset_to_hub.py --help` for more instructions.

If your dataset format is not supported, implement your own in `lerobot/common/datasets/push_dataset_to_hub/${raw_format}_format.py` by copying examples like [pusht_zarr](https://github.com/huggingface/lerobot/blob/main/lerobot/common/datasets/push_dataset_to_hub/pusht_zarr_format.py), [umi_zarr](https://github.com/huggingface/lerobot/blob/main/lerobot/common/datasets/push_dataset_to_hub/umi_zarr_format.py), [aloha_hdf5](https://github.com/huggingface/lerobot/blob/main/lerobot/common/datasets/push_dataset_to_hub/aloha_hdf5_format.py), or [xarm_pkl](https://github.com/huggingface/lerobot/blob/main/lerobot/common/datasets/push_dataset_to_hub/xarm_pkl_format.py). --&gt; 
&lt;h3&gt;Add a pretrained policy&lt;/h3&gt; 
&lt;p&gt;Once you have trained a policy you may upload it to the Hugging Face hub using a hub id that looks like &lt;code&gt;${hf_user}/${repo_name}&lt;/code&gt; (e.g. &lt;a href=&quot;https://huggingface.co/lerobot/diffusion_pusht&quot;&gt;lerobot/diffusion_pusht&lt;/a&gt;).&lt;/p&gt; 
&lt;p&gt;You first need to find the checkpoint folder located inside your experiment directory (e.g. &lt;code&gt;outputs/train/2024-05-05/20-21-12_aloha_act_default/checkpoints/002500&lt;/code&gt;). Within that there is a &lt;code&gt;pretrained_model&lt;/code&gt; directory which should contain:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;config.json&lt;/code&gt;: A serialized version of the policy configuration (following the policy&#39;s dataclass config).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;model.safetensors&lt;/code&gt;: A set of &lt;code&gt;torch.nn.Module&lt;/code&gt; parameters, saved in &lt;a href=&quot;https://huggingface.co/docs/safetensors/index&quot;&gt;Hugging Face Safetensors&lt;/a&gt; format.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;train_config.json&lt;/code&gt;: A consolidated configuration containing all parameter userd for training. The policy configuration should match &lt;code&gt;config.json&lt;/code&gt; exactly. Thisis useful for anyone who wants to evaluate your policy or for reproducibility.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;To upload these to the hub, run the following:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;huggingface-cli upload ${hf_user}/${repo_name} path/to/pretrained_model
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;a href=&quot;https://github.com/huggingface/lerobot/raw/main/lerobot/scripts/eval.py&quot;&gt;eval.py&lt;/a&gt; for an example of how other people may use your policy.&lt;/p&gt; 
&lt;h3&gt;Improve your code with profiling&lt;/h3&gt; 
&lt;p&gt;An example of a code snippet to profile the evaluation of a policy:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from torch.profiler import profile, record_function, ProfilerActivity

def trace_handler(prof):
    prof.export_chrome_trace(f&quot;tmp/trace_schedule_{prof.step_num}.json&quot;)

with profile(
    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],
    schedule=torch.profiler.schedule(
        wait=2,
        warmup=2,
        active=3,
    ),
    on_trace_ready=trace_handler
) as prof:
    with record_function(&quot;eval_policy&quot;):
        for i in range(num_episodes):
            prof.step()
            # insert code to profile, potentially whole body of eval_policy function
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you want, you can cite this work with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bibtex&quot;&gt;@misc{cadene2024lerobot,
    author = {Cadene, Remi and Alibert, Simon and Soare, Alexander and Gallouedec, Quentin and Zouitine, Adil and Wolf, Thomas},
    title = {LeRobot: State-of-the-art Machine Learning for Real-World Robotics in Pytorch},
    howpublished = &quot;\url{https://github.com/huggingface/lerobot}&quot;,
    year = {2024}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Additionally, if you are using any of the particular policy architecture, pretrained models, or datasets, it is recommended to cite the original authors of the work as they appear below:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://diffusion-policy.cs.columbia.edu&quot;&gt;Diffusion Policy&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bibtex&quot;&gt;@article{chi2024diffusionpolicy,
	author = {Cheng Chi and Zhenjia Xu and Siyuan Feng and Eric Cousineau and Yilun Du and Benjamin Burchfiel and Russ Tedrake and Shuran Song},
	title ={Diffusion Policy: Visuomotor Policy Learning via Action Diffusion},
	journal = {The International Journal of Robotics Research},
	year = {2024},
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://tonyzhaozh.github.io/aloha&quot;&gt;ACT or ALOHA&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bibtex&quot;&gt;@article{zhao2023learning,
  title={Learning fine-grained bimanual manipulation with low-cost hardware},
  author={Zhao, Tony Z and Kumar, Vikash and Levine, Sergey and Finn, Chelsea},
  journal={arXiv preprint arXiv:2304.13705},
  year={2023}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.nicklashansen.com/td-mpc/&quot;&gt;TDMPC&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bibtex&quot;&gt;@inproceedings{Hansen2022tdmpc,
	title={Temporal Difference Learning for Model Predictive Control},
	author={Nicklas Hansen and Xiaolong Wang and Hao Su},
	booktitle={ICML},
	year={2022}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://sjlee.cc/vq-bet/&quot;&gt;VQ-BeT&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bibtex&quot;&gt;@article{lee2024behavior,
  title={Behavior generation with latent actions},
  author={Lee, Seungjae and Wang, Yibin and Etukuru, Haritheja and Kim, H Jin and Shafiullah, Nur Muhammad Mahi and Pinto, Lerrel},
  journal={arXiv preprint arXiv:2403.03181},
  year={2024}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://star-history.com/#huggingface/lerobot&amp;amp;Timeline&quot;&gt;&lt;img src=&quot;https://api.star-history.com/svg?repos=huggingface/lerobot&amp;amp;type=Timeline&quot; alt=&quot;Star History Chart&quot;&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>browser-use/browser-use</title>
      <link>https://github.com/browser-use/browser-use</link>
      <description>&lt;p&gt;Make websites accessible for AI agents&lt;/p&gt;&lt;hr&gt;&lt;picture&gt; 
 &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;./static/browser-use-dark.png&quot;&gt; 
 &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;./static/browser-use.png&quot;&gt; 
 &lt;img alt=&quot;Shows a black Browser Use Logo in light color mode and a white one in dark color mode.&quot; src=&quot;https://raw.githubusercontent.com/browser-use/browser-use/main/static/browser-use.png&quot; width=&quot;full&quot;&gt; 
&lt;/picture&gt; 
&lt;h1 align=&quot;center&quot;&gt;Enable AI to control your browser 🤖&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/gregpr07/browser-use/stargazers&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/gregpr07/browser-use?style=social&quot; alt=&quot;GitHub stars&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://link.browser-use.com/discord&quot;&gt;&lt;img src=&quot;https://img.shields.io/discord/1303749220842340412?color=7289DA&amp;amp;label=Discord&amp;amp;logo=discord&amp;amp;logoColor=white&quot; alt=&quot;Discord&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://cloud.browser-use.com&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Cloud-%E2%98%81%EF%B8%8F-blue&quot; alt=&quot;Cloud&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://docs.browser-use.com&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Documentation-%F0%9F%93%95-blue&quot; alt=&quot;Documentation&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://x.com/gregpr07&quot;&gt;&lt;img src=&quot;https://img.shields.io/twitter/follow/Gregor?style=social&quot; alt=&quot;Twitter Follow&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://x.com/mamagnus00&quot;&gt;&lt;img src=&quot;https://img.shields.io/twitter/follow/Magnus?style=social&quot; alt=&quot;Twitter Follow&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://app.workweave.ai/reports/repository/org_T5Pvn3UBswTHIsN1dWS3voPg/881458615&quot;&gt;&lt;img src=&quot;https://img.shields.io/endpoint?url=https%3A%2F%2Fapp.workweave.ai%2Fapi%2Frepository%2Fbadge%2Forg_T5Pvn3UBswTHIsN1dWS3voPg%2F881458615&amp;amp;labelColor=#EC6341&quot; alt=&quot;Weave Badge&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;🌐 Browser-use is the easiest way to connect your AI agents with the browser.&lt;/p&gt; 
&lt;p&gt;💡 See what others are building and share your projects in our &lt;a href=&quot;https://link.browser-use.com/discord&quot;&gt;Discord&lt;/a&gt;! Want Swag? Check out our &lt;a href=&quot;https://browsermerch.com&quot;&gt;Merch store&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;🌤️ Skip the setup - try our &lt;b&gt;hosted version&lt;/b&gt; for instant browser automation! &lt;b&gt;&lt;a href=&quot;https://cloud.browser-use.com&quot;&gt;Try the cloud ☁︎&lt;/a&gt;&lt;/b&gt;.&lt;/p&gt; 
&lt;h1&gt;Quick start&lt;/h1&gt; 
&lt;p&gt;With pip (Python&amp;gt;=3.11):&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install browser-use
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;install playwright:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;playwright install
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Spin up your agent:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from langchain_openai import ChatOpenAI
from browser_use import Agent
import asyncio
from dotenv import load_dotenv
load_dotenv()

async def main():
    agent = Agent(
        task=&quot;Compare the price of gpt-4o and DeepSeek-V3&quot;,
        llm=ChatOpenAI(model=&quot;gpt-4o&quot;),
    )
    await agent.run()

asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Add your API keys for the provider you want to use to your &lt;code&gt;.env&lt;/code&gt; file.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;OPENAI_API_KEY=
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For other settings, models, and more, check out the &lt;a href=&quot;https://docs.browser-use.com&quot;&gt;documentation 📕&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Test with UI&lt;/h3&gt; 
&lt;p&gt;You can test &lt;a href=&quot;https://github.com/browser-use/web-ui&quot;&gt;browser-use with a UI repository&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Or simply run the gradio example:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;uv pip install gradio
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python examples/ui/gradio_demo.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;Demos&lt;/h1&gt; 
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/browser-use/browser-use/raw/main/examples/use-cases/shopping.py&quot;&gt;Task&lt;/a&gt;: Add grocery items to cart, and checkout.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=L2Ya9PYNns8&quot;&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/d9359085-bde6-41d4-aa4e-6520d0221872&quot; alt=&quot;AI Did My Groceries&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; 
&lt;p&gt;Prompt: Add my latest LinkedIn follower to my leads in Salesforce.&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/1440affc-a552-442e-b702-d0d3b277b0ae&quot; alt=&quot;LinkedIn to Salesforce&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/browser-use/browser-use/raw/main/examples/use-cases/find_and_apply_to_jobs.py&quot;&gt;Prompt&lt;/a&gt;: Read my CV &amp;amp; find ML jobs, save them to a file, and then start applying for them in new tabs, if you need help, ask me.&#39;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/user-attachments/assets/171fb4d6-0355-46f2-863e-edb04a828d04&quot;&gt;https://github.com/user-attachments/assets/171fb4d6-0355-46f2-863e-edb04a828d04&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/browser-use/browser-use/raw/main/examples/browser/real_browser.py&quot;&gt;Prompt&lt;/a&gt;: Write a letter in Google Docs to my Papa, thanking him for everything, and save the document as a PDF.&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/242ade3e-15bc-41c2-988f-cbc5415a66aa&quot; alt=&quot;Letter to Papa&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/browser-use/browser-use/raw/main/examples/custom-functions/save_to_file_hugging_face.py&quot;&gt;Prompt&lt;/a&gt;: Look up models with a license of cc-by-sa-4.0 and sort by most likes on Hugging face, save top 5 to file.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/user-attachments/assets/de73ee39-432c-4b97-b4e8-939fd7f323b3&quot;&gt;https://github.com/user-attachments/assets/de73ee39-432c-4b97-b4e8-939fd7f323b3&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; 
&lt;h2&gt;More examples&lt;/h2&gt; 
&lt;p&gt;For more examples see the &lt;a href=&quot;https://raw.githubusercontent.com/browser-use/browser-use/main/examples&quot;&gt;examples&lt;/a&gt; folder or join the &lt;a href=&quot;https://link.browser-use.com/discord&quot;&gt;Discord&lt;/a&gt; and show off your project.&lt;/p&gt; 
&lt;h1&gt;Vision&lt;/h1&gt; 
&lt;p&gt;Tell your computer what to do, and it gets it done.&lt;/p&gt; 
&lt;h2&gt;Roadmap&lt;/h2&gt; 
&lt;h3&gt;Agent&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; disabled&gt; Improve agent memory (summarize, compress, RAG, etc.)&lt;/li&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; disabled&gt; Enhance planning capabilities (load website specific context)&lt;/li&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; disabled&gt; Reduce token consumption (system prompt, DOM state)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;DOM Extraction&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; disabled&gt; Improve extraction for datepickers, dropdowns, special elements&lt;/li&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; disabled&gt; Improve state representation for UI elements&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Rerunning tasks&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; disabled&gt; LLM as fallback&lt;/li&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; disabled&gt; Make it easy to define workfows templates where LLM fills in the details&lt;/li&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; disabled&gt; Return playwright script from the agent&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Datasets&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; disabled&gt; Create datasets for complex tasks&lt;/li&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; disabled&gt; Benchmark various models against each other&lt;/li&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; disabled&gt; Fine-tuning models for specific tasks&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;User Experience&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; disabled&gt; Human-in-the-loop execution&lt;/li&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; disabled&gt; Improve the generated GIF quality&lt;/li&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; disabled&gt; Create various demos for tutorial execution, job application, QA testing, social media, etc.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We love contributions! Feel free to open issues for bugs or feature requests. To contribute to the docs, check out the &lt;code&gt;/docs&lt;/code&gt; folder.&lt;/p&gt; 
&lt;h2&gt;Local Setup&lt;/h2&gt; 
&lt;p&gt;To learn more about the library, check out the &lt;a href=&quot;https://docs.browser-use.com/development/local-setup&quot;&gt;local setup 📕&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Cooperations&lt;/h2&gt; 
&lt;p&gt;We are forming a commission to define best practices for UI/UX design for browser agents. Together, we&#39;re exploring how software redesign improves the performance of AI agents and gives these companies a competitive advantage by designing their existing software to be at the forefront of the agent age.&lt;/p&gt; 
&lt;p&gt;Email &lt;a href=&quot;mailto:tbiddle@loop11.com?subject=I%20want%20to%20join%20the%20UI/UX%20commission%20for%20AI%20agents&amp;amp;body=Hi%20Toby%2C%0A%0AI%20found%20you%20in%20the%20browser-use%20GitHub%20README.%0A%0A&quot;&gt;Toby&lt;/a&gt; to apply for a seat on the committee.&lt;/p&gt; 
&lt;h2&gt;Swag&lt;/h2&gt; 
&lt;p&gt;Want to show off your Browser-use swag? Check out our &lt;a href=&quot;https://browsermerch.com&quot;&gt;Merch store&lt;/a&gt;. Good contributors will receive swag for free 👀.&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you use Browser Use in your research or project, please cite:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bibtex&quot;&gt;@software{browser_use2024,
  author = {Müller, Magnus and Žunič, Gregor},
  title = {Browser Use: Enable AI to control your browser},
  year = {2024},
  publisher = {GitHub},
  url = {https://github.com/browser-use/browser-use}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;img src=&quot;https://github.com/user-attachments/assets/402b2129-b6ac-44d3-a217-01aea3277dce&quot; width=&quot;400&quot;&gt; 
 &lt;p&gt;&lt;a href=&quot;https://x.com/gregpr07&quot;&gt;&lt;img src=&quot;https://img.shields.io/twitter/follow/Gregor?style=social&quot; alt=&quot;Twitter Follow&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://x.com/mamagnus00&quot;&gt;&lt;img src=&quot;https://img.shields.io/twitter/follow/Magnus?style=social&quot; alt=&quot;Twitter Follow&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align=&quot;center&quot;&gt;
  Made with ❤️ in Zurich and San Francisco 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>hummingbot/hummingbot</title>
      <link>https://github.com/hummingbot/hummingbot</link>
      <description>&lt;p&gt;Open source software that helps you create and deploy high-frequency crypto trading bots&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&quot;https://i.ibb.co/X5zNkKw/blacklogo-with-text.png&quot; alt=&quot;Hummingbot&quot;&gt;&lt;/p&gt; 
&lt;hr&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/hummingbot/hummingbot/raw/master/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/License-Apache%202.0-informational.svg?sanitize=true&quot; alt=&quot;License&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://twitter.com/_hummingbot&quot;&gt;&lt;img src=&quot;https://img.shields.io/twitter/url?url=https://twitter.com/_hummingbot?style=social&amp;amp;label=_hummingbot&quot; alt=&quot;Twitter&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/@hummingbot&quot;&gt;&lt;img src=&quot;https://img.shields.io/youtube/channel/subscribers/UCxzzdEnDRbylLMWmaMjywOA&quot; alt=&quot;Youtube&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://discord.gg/hummingbot&quot;&gt;&lt;img src=&quot;https://img.shields.io/discord/530578568154054663?logo=discord&amp;amp;logoColor=white&amp;amp;style=flat-square&quot; alt=&quot;Discord&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Hummingbot is an open-source framework that helps you design and deploy automated trading strategies, or &lt;strong&gt;bots&lt;/strong&gt;, that can run on many centralized or decentralized exchanges. Over the past year, Hummingbot users have generated over $34 billion in trading volume across 140+ unique trading venues.&lt;/p&gt; 
&lt;p&gt;The Hummingbot codebase is free and publicly available under the Apache 2.0 open-source license. Our mission is to &lt;strong&gt;democratize high-frequency trading&lt;/strong&gt; by creating a global community of algorithmic traders and developers that share knowledge and contribute to the codebase.&lt;/p&gt; 
&lt;h2&gt;Quick Links&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://hummingbot.org&quot;&gt;Website and Docs&lt;/a&gt;: Official Hummingbot website and documentation&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://hummingbot.org/installation/docker/&quot;&gt;Installation&lt;/a&gt;: Install Hummingbot on various platforms&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://discord.gg/hummingbot&quot;&gt;Discord&lt;/a&gt;: The main gathering spot for the global Hummingbot community&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/c/hummingbot&quot;&gt;YouTube&lt;/a&gt;: Videos that teach you how to get the most of of Hummingbot&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://twitter.com/_hummingbot&quot;&gt;Twitter&lt;/a&gt;: Get the latest announcements about Hummingbot&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://p.datadoghq.com/sb/a96a744f5-a15479d77992ccba0d23aecfd4c87a52&quot;&gt;Reported Volumes&lt;/a&gt;: Reported trading volumes across all Hummingbot instances&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://hummingbot.substack.com&quot;&gt;Newsletter&lt;/a&gt;: Get our newsletter whenever we ship a new release&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Exchange Connectors&lt;/h2&gt; 
&lt;p&gt;Hummingbot connectors standardize REST and WebSocket API interfaces to different types of exchanges, enabling you to build sophisticated trading strategies that can be deployed across many exchanges with minimal changes. We classify exchanges into the following categories:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;CEX&lt;/strong&gt;: Centralized exchanges that take custody of your funds. Use API keys to connect with Hummingbot.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;DEX&lt;/strong&gt;: Decentralized, non-custodial exchanges that operate on a blockchain. Use wallet keys to connect with Hummingbot.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;In addition, connectors differ based on the type of market supported:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;CLOB Spot&lt;/strong&gt;: Connectors to spot markets on central limit order book (CLOB) exchanges&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;CLOB Perp&lt;/strong&gt;: Connectors to perpetual futures markets on CLOB exchanges&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;AMM&lt;/strong&gt;: Connectors to spot markets on Automatic Market Maker (AMM) decentralized exchanges&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Exchange Sponsors&lt;/h3&gt; 
&lt;p&gt;We are grateful for the following exchanges that support the development and maintenance of Hummingbot via broker partnerships and sponsorships.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Connector ID&lt;/th&gt; 
   &lt;th&gt;Exchange&lt;/th&gt; 
   &lt;th&gt;CEX/DEX&lt;/th&gt; 
   &lt;th&gt;Market Type&lt;/th&gt; 
   &lt;th&gt;Docs&lt;/th&gt; 
   &lt;th&gt;Discount&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;binance&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://accounts.binance.com/register?ref=CBWO4LU6&quot;&gt;Binance&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;CEX&lt;/td&gt; 
   &lt;td&gt;CLOB Spot&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://hummingbot.org/exchanges/binance/&quot;&gt;Docs&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://accounts.binance.com/register?ref=CBWO4LU6&quot;&gt;&lt;img src=&quot;https://img.shields.io/static/v1?label=Fee&amp;amp;message=%2d10%25&amp;amp;color=orange&quot; alt=&quot;Sign up for Binance using Hummingbot&#39;s referral link for a 10% discount!&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;binance_perpetual&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://accounts.binance.com/register?ref=CBWO4LU6&quot;&gt;Binance&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;CEX&lt;/td&gt; 
   &lt;td&gt;CLOB Perp&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://hummingbot.org/exchanges/binance/&quot;&gt;Docs&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://accounts.binance.com/register?ref=CBWO4LU6&quot;&gt;&lt;img src=&quot;https://img.shields.io/static/v1?label=Fee&amp;amp;message=%2d10%25&amp;amp;color=orange&quot; alt=&quot;Sign up for Binance using Hummingbot&#39;s referral link for a 10% discount!&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;gate_io&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.gate.io/referral/invite/HBOTGATE_0_103&quot;&gt;Gate.io&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;CEX&lt;/td&gt; 
   &lt;td&gt;CLOB Spot&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://hummingbot.org/exchanges/gate-io/&quot;&gt;Docs&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.gate.io/referral/invite/HBOTGATE_0_103&quot;&gt;&lt;img src=&quot;https://img.shields.io/static/v1?label=Fee&amp;amp;message=%2d20%25&amp;amp;color=orange&quot; alt=&quot;Sign up for Gate.io using Hummingbot&#39;s referral link for a 10% discount!&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;gate_io_perpetual&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.gate.io/referral/invite/HBOTGATE_0_103&quot;&gt;Gate.io&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;CEX&lt;/td&gt; 
   &lt;td&gt;CLOB Perp&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://hummingbot.org/exchanges/gate-io/&quot;&gt;Docs&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.gate.io/referral/invite/HBOTGATE_0_103&quot;&gt;&lt;img src=&quot;https://img.shields.io/static/v1?label=Fee&amp;amp;message=%2d20%25&amp;amp;color=orange&quot; alt=&quot;Sign up for Gate.io using Hummingbot&#39;s referral link for a 20% discount!&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;htx&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.htx.com.pk/invite/en-us/1h?invite_code=re4w9223&quot;&gt;HTX (Huobi)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;CEX&lt;/td&gt; 
   &lt;td&gt;CLOB Spot&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://hummingbot.org/exchanges/huobi/&quot;&gt;Docs&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.htx.com.pk/invite/en-us/1h?invite_code=re4w9223&quot;&gt;&lt;img src=&quot;https://img.shields.io/static/v1?label=Fee&amp;amp;message=%2d20%25&amp;amp;color=orange&quot; alt=&quot;Sign up for HTX using Hummingbot&#39;s referral link for a 20% discount!&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;kucoin&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.kucoin.com/r/af/hummingbot&quot;&gt;KuCoin&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;CEX&lt;/td&gt; 
   &lt;td&gt;CLOB Spot&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://hummingbot.org/exchanges/kucoin/&quot;&gt;Docs&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.kucoin.com/r/af/hummingbot&quot;&gt;&lt;img src=&quot;https://img.shields.io/static/v1?label=Fee&amp;amp;message=%2d20%25&amp;amp;color=orange&quot; alt=&quot;Sign up for Kucoin using Hummingbot&#39;s referral link for a 20% discount!&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;kucoin_perpetual&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.kucoin.com/r/af/hummingbot&quot;&gt;KuCoin&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;CEX&lt;/td&gt; 
   &lt;td&gt;CLOB Perp&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://hummingbot.org/exchanges/kucoin/&quot;&gt;Docs&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.kucoin.com/r/af/hummingbot&quot;&gt;&lt;img src=&quot;https://img.shields.io/static/v1?label=Fee&amp;amp;message=%2d20%25&amp;amp;color=orange&quot; alt=&quot;Sign up for Kucoin using Hummingbot&#39;s referral link for a 20% discount!&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;okx&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.okx.com/join/1931920269&quot;&gt;OKX&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;CEX&lt;/td&gt; 
   &lt;td&gt;CLOB Spot&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://hummingbot.org/exchanges/okx/okx/&quot;&gt;Docs&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.okx.com/join/1931920269&quot;&gt;&lt;img src=&quot;https://img.shields.io/static/v1?label=Fee&amp;amp;message=%2d20%25&amp;amp;color=orange&quot; alt=&quot;Sign up for Kucoin using Hummingbot&#39;s referral link for a 20% discount!&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;okx_perpetual&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.okx.com/join/1931920269&quot;&gt;OKX&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;CEX&lt;/td&gt; 
   &lt;td&gt;CLOB Perp&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://hummingbot.org/exchanges/okx/okx/&quot;&gt;Docs&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.okx.com/join/1931920269&quot;&gt;&lt;img src=&quot;https://img.shields.io/static/v1?label=Fee&amp;amp;message=%2d20%25&amp;amp;color=orange&quot; alt=&quot;Sign up for Kucoin using Hummingbot&#39;s referral link for a 20% discount!&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;dydx_v4_perpetual&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.dydx.exchange/&quot;&gt;dYdX&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;DEX&lt;/td&gt; 
   &lt;td&gt;CLOB Perp&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://hummingbot.org/exchanges/dydx/&quot;&gt;Docs&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;hyperliquid_perpetual&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://hyperliquid.io/&quot;&gt;Hyperliquid&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;DEX&lt;/td&gt; 
   &lt;td&gt;CLOB Perp&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://hummingbot.org/exchanges/hyperliquid/&quot;&gt;Docs&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;xrpl&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://xrpl.org/&quot;&gt;XRP Ledger&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;DEX&lt;/td&gt; 
   &lt;td&gt;CLOB Spot&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://hummingbot.org/exchanges/xrpl/&quot;&gt;Docs&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Other Exchange Connectors&lt;/h3&gt; 
&lt;p&gt;Currently, the master branch of Hummingbot also includes the following exchange connectors, which are maintained and updated through the Hummingbot Foundation governance process. See &lt;a href=&quot;https://hummingbot.org/governance/&quot;&gt;Governance&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Connector ID&lt;/th&gt; 
   &lt;th&gt;Exchange&lt;/th&gt; 
   &lt;th&gt;CEX/DEX&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Docs&lt;/th&gt; 
   &lt;th&gt;Discount&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ascend_ex&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;AscendEx&lt;/td&gt; 
   &lt;td&gt;CEX&lt;/td&gt; 
   &lt;td&gt;CLOB Spot&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://hummingbot.org/exchanges/ascendex/&quot;&gt;Docs&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;balancer&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Balancer&lt;/td&gt; 
   &lt;td&gt;DEX&lt;/td&gt; 
   &lt;td&gt;AMM&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://hummingbot.org/exchanges/balancer/&quot;&gt;Docs&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;bitget_perpetual&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Bitget&lt;/td&gt; 
   &lt;td&gt;CEX&lt;/td&gt; 
   &lt;td&gt;CLOB Perp&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://hummingbot.org/exchanges/bitget-perpetual/&quot;&gt;Docs&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;bitmart&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;BitMart&lt;/td&gt; 
   &lt;td&gt;CEX&lt;/td&gt; 
   &lt;td&gt;CLOB Spot&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://hummingbot.org/exchanges/bitmart/&quot;&gt;Docs&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;bitrue&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Bitrue&lt;/td&gt; 
   &lt;td&gt;CEX&lt;/td&gt; 
   &lt;td&gt;CLOB Spot&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://hummingbot.org/exchanges/bitrue/&quot;&gt;Docs&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;bitstamp&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Bitstamp&lt;/td&gt; 
   &lt;td&gt;CEX&lt;/td&gt; 
   &lt;td&gt;CLOB Spot&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://hummingbot.org/exchanges/bitstamp/&quot;&gt;Docs&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;btc_markets&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;BTC Markets&lt;/td&gt; 
   &lt;td&gt;CEX&lt;/td&gt; 
   &lt;td&gt;CLOB Spot&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://hummingbot.org/exchanges/btc-markets/&quot;&gt;Docs&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;bybit&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Bybit&lt;/td&gt; 
   &lt;td&gt;CEX&lt;/td&gt; 
   &lt;td&gt;CLOB Spot&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://hummingbot.org/exchanges/bybit/&quot;&gt;Docs&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;bybit_perpetual&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Bybit&lt;/td&gt; 
   &lt;td&gt;CEX&lt;/td&gt; 
   &lt;td&gt;CLOB Perp&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://hummingbot.org/exchanges/bybit/&quot;&gt;Docs&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;carbon&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Carbon&lt;/td&gt; 
   &lt;td&gt;DEX&lt;/td&gt; 
   &lt;td&gt;AMM&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://hummingbot.org/exchanges/carbon/&quot;&gt;Docs&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;coinbase_advanced_trade&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Coinbase&lt;/td&gt; 
   &lt;td&gt;CEX&lt;/td&gt; 
   &lt;td&gt;CLOB Spot&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://hummingbot.org/exchanges/coinbase/&quot;&gt;Docs&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;cube&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Cube&lt;/td&gt; 
   &lt;td&gt;CEX&lt;/td&gt; 
   &lt;td&gt;CLOB Spot&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://hummingbot.org/exchanges/cube/&quot;&gt;Docs&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;curve&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Curve&lt;/td&gt; 
   &lt;td&gt;DEX&lt;/td&gt; 
   &lt;td&gt;AMM&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://hummingbot.org/exchanges/curve/&quot;&gt;Docs&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;dexalot&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Dexalot&lt;/td&gt; 
   &lt;td&gt;DEX&lt;/td&gt; 
   &lt;td&gt;CLOB Spot&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://hummingbot.org/exchanges/dexalot/&quot;&gt;Docs&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;hashkey&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;HashKey&lt;/td&gt; 
   &lt;td&gt;CEX&lt;/td&gt; 
   &lt;td&gt;CLOB Spot&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://hummingbot.org/exchanges/hashkey/&quot;&gt;Docs&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;hashkey_perpetual&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;HashKey&lt;/td&gt; 
   &lt;td&gt;CEX&lt;/td&gt; 
   &lt;td&gt;CLOB Perp&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://hummingbot.org/exchanges/hashkey/&quot;&gt;Docs&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;injective_v2&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Injective Helix&lt;/td&gt; 
   &lt;td&gt;DEX&lt;/td&gt; 
   &lt;td&gt;CLOB Spot&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://hummingbot.org/exchanges/injective/&quot;&gt;Docs&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;injective_v2_perpetual&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Injective Helix&lt;/td&gt; 
   &lt;td&gt;DEX&lt;/td&gt; 
   &lt;td&gt;CLOB Perp&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://hummingbot.org/exchanges/injective/&quot;&gt;Docs&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;kraken&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Kraken&lt;/td&gt; 
   &lt;td&gt;CEX&lt;/td&gt; 
   &lt;td&gt;CLOB Spot&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://hummingbot.org/exchanges/kraken/&quot;&gt;Docs&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;mad_meerkat&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Mad Meerkat&lt;/td&gt; 
   &lt;td&gt;DEX&lt;/td&gt; 
   &lt;td&gt;AMM&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://hummingbot.org/exchanges/mad-meerkat/&quot;&gt;Docs&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;mexc&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;MEXC&lt;/td&gt; 
   &lt;td&gt;CEX&lt;/td&gt; 
   &lt;td&gt;CLOB Spot&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://hummingbot.org/exchanges/mexc/&quot;&gt;Docs&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;openocean&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;OpenOcean&lt;/td&gt; 
   &lt;td&gt;DEX&lt;/td&gt; 
   &lt;td&gt;AMM&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://hummingbot.org/exchanges/openocean/&quot;&gt;Docs&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;pancakeswap&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;PancakeSwap&lt;/td&gt; 
   &lt;td&gt;DEX&lt;/td&gt; 
   &lt;td&gt;AMM&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://hummingbot.org/exchanges/pancakeswap/&quot;&gt;Docs&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;pangolin&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Pangolin&lt;/td&gt; 
   &lt;td&gt;CEX&lt;/td&gt; 
   &lt;td&gt;DEX&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://hummingbot.org/exchanges/pangolin/&quot;&gt;Docs&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;quickswap&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;QuickSwap&lt;/td&gt; 
   &lt;td&gt;DEX&lt;/td&gt; 
   &lt;td&gt;AMM&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://hummingbot.org/exchanges/quickswap/&quot;&gt;Docs&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;sushiswap&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;SushiSwap&lt;/td&gt; 
   &lt;td&gt;DEX&lt;/td&gt; 
   &lt;td&gt;AMM&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://hummingbot.org/exchanges/sushiswap/&quot;&gt;Docs&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;tinyman&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Tinyman&lt;/td&gt; 
   &lt;td&gt;DEX&lt;/td&gt; 
   &lt;td&gt;AMM&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://hummingbot.org/exchanges/tinyman/&quot;&gt;Docs&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;traderjoe&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Trader Joe&lt;/td&gt; 
   &lt;td&gt;DEX&lt;/td&gt; 
   &lt;td&gt;AMM&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://hummingbot.org/exchanges/traderjoe/&quot;&gt;Docs&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;uniswap&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Uniswap&lt;/td&gt; 
   &lt;td&gt;DEX&lt;/td&gt; 
   &lt;td&gt;AMM&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://hummingbot.org/exchanges/uniswap/&quot;&gt;Docs&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;vertex&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Vertex&lt;/td&gt; 
   &lt;td&gt;DEX&lt;/td&gt; 
   &lt;td&gt;CLOB Spot&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://hummingbot.org/exchanges/vertex/&quot;&gt;Docs&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;vvs&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;VVS&lt;/td&gt; 
   &lt;td&gt;DEX&lt;/td&gt; 
   &lt;td&gt;AMM&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://hummingbot.org/exchanges/vvs/&quot;&gt;Docs&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;xsswap&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;XSSwap&lt;/td&gt; 
   &lt;td&gt;DEX&lt;/td&gt; 
   &lt;td&gt;AMM&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://hummingbot.org/exchanges/xswap/&quot;&gt;Docs&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Other Hummingbot Repos&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/hummingbot/deploy&quot;&gt;Deploy&lt;/a&gt;: Deploy Hummingbot in various configurations with Docker&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/hummingbot/dashboard&quot;&gt;Dashboard&lt;/a&gt;: Web app that help you create, backtest, deploy, and manage Hummingbot instances&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/hummingbot/quants-lab&quot;&gt;Quants Lab&lt;/a&gt;: Juypter notebooks that enable you to fetch data and perform research using Hummingbot&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/hummingbot/gateway&quot;&gt;Gateway&lt;/a&gt;: Typescript based API client for DEX connectors&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/hummingbot/hummingbot-site&quot;&gt;Hummingbot Site&lt;/a&gt;: Official documentation for Hummingbot - we welcome contributions here too!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributions&lt;/h2&gt; 
&lt;p&gt;The Hummingbot architecture features modular components that can be maintained and extended by individual community members.&lt;/p&gt; 
&lt;p&gt;We welcome contributions from the community! Please review these &lt;a href=&quot;https://raw.githubusercontent.com/hummingbot/hummingbot/master/CONTRIBUTING.md&quot;&gt;guidelines&lt;/a&gt; before submitting a pull request.&lt;/p&gt; 
&lt;p&gt;To have your exchange connector or other pull request merged into the codebase, please submit a New Connector Proposal or Pull Request Proposal, following these &lt;a href=&quot;https://hummingbot.org/governance/proposals/&quot;&gt;guidelines&lt;/a&gt;. Note that you will need some amount of &lt;a href=&quot;https://etherscan.io/token/0xe5097d9baeafb89f9bcb78c9290d545db5f9e9cb&quot;&gt;HBOT tokens&lt;/a&gt; in your Ethereum wallet to submit a proposal.&lt;/p&gt; 
&lt;h2&gt;Legal&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;License&lt;/strong&gt;: Hummingbot is open source and licensed under &lt;a href=&quot;https://raw.githubusercontent.com/hummingbot/hummingbot/master/LICENSE&quot;&gt;Apache 2.0&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Data collection&lt;/strong&gt;: See &lt;a href=&quot;https://hummingbot.org/reporting/&quot;&gt;Reporting&lt;/a&gt; for information on anonymous data collection and reporting in Hummingbot.&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>sgl-project/sglang</title>
      <link>https://github.com/sgl-project/sglang</link>
      <description>&lt;p&gt;SGLang is a fast serving framework for large language models and vision language models.&lt;/p&gt;&lt;hr&gt;&lt;div align=&quot;center&quot; id=&quot;sglangtop&quot;&gt; 
 &lt;img src=&quot;https://raw.githubusercontent.com/sgl-project/sglang/main/assets/logo.png&quot; alt=&quot;logo&quot; width=&quot;400&quot; margin=&quot;10px&quot;&gt; 
 &lt;p&gt;&lt;a href=&quot;https://pypi.org/project/sglang&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/v/sglang&quot; alt=&quot;PyPI&quot;&gt;&lt;/a&gt; &lt;img src=&quot;https://img.shields.io/pypi/dm/sglang&quot; alt=&quot;PyPI - Downloads&quot;&gt; &lt;a href=&quot;https://github.com/sgl-project/sglang/tree/main/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/license/sgl-project/sglang.svg?sanitize=true&quot; alt=&quot;license&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/sgl-project/sglang/issues&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/issues-closed-raw/sgl-project/sglang&quot; alt=&quot;issue resolution&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/sgl-project/sglang/issues&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/issues-raw/sgl-project/sglang&quot; alt=&quot;open issues&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://gurubase.io/g/sglang&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Gurubase-(experimental)-006BFF&quot; alt=&quot;&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr&gt; 
&lt;p&gt;| &lt;a href=&quot;https://lmsys.org/blog/2024-07-25-sglang-llama3/&quot;&gt;&lt;strong&gt;Blog&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&quot;https://docs.sglang.ai/&quot;&gt;&lt;strong&gt;Documentation&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&quot;https://slack.sglang.ai/&quot;&gt;&lt;strong&gt;Join Slack&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&quot;https://meeting.sglang.ai/&quot;&gt;&lt;strong&gt;Join Bi-Weekly Development Meeting&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&quot;https://github.com/sgl-project/sglang/issues/4042&quot;&gt;&lt;strong&gt;Roadmap&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&quot;https://github.com/sgl-project/sgl-learning-materials?tab=readme-ov-file#slides&quot;&gt;&lt;strong&gt;Slides&lt;/strong&gt;&lt;/a&gt; |&lt;/p&gt; 
&lt;h2&gt;News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;[2025/01] 🔥 SGLang provides day one support for DeepSeek V3/R1 models on NVIDIA and AMD GPUs with DeepSeek-specific optimizations. (&lt;a href=&quot;https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3&quot;&gt;instructions&lt;/a&gt;, &lt;a href=&quot;https://www.amd.com/en/developer/resources/technical-articles/amd-instinct-gpus-power-deepseek-v3-revolutionizing-ai-development-with-sglang.html&quot;&gt;AMD blog&lt;/a&gt;, &lt;a href=&quot;https://x.com/lmsysorg/status/1887262321636221412&quot;&gt;10+ other companies&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;[2024/12] 🔥 v0.4 Release: Zero-Overhead Batch Scheduler, Cache-Aware Load Balancer, Faster Structured Outputs (&lt;a href=&quot;https://lmsys.org/blog/2024-12-04-sglang-v0-4/&quot;&gt;blog&lt;/a&gt;).&lt;/li&gt; 
 &lt;li&gt;[2024/09] v0.3 Release: 7x Faster DeepSeek MLA, 1.5x Faster torch.compile, Multi-Image/Video LLaVA-OneVision (&lt;a href=&quot;https://lmsys.org/blog/2024-09-04-sglang-v0-3/&quot;&gt;blog&lt;/a&gt;).&lt;/li&gt; 
 &lt;li&gt;[2024/07] v0.2 Release: Faster Llama3 Serving with SGLang Runtime (vs. TensorRT-LLM, vLLM) (&lt;a href=&quot;https://lmsys.org/blog/2024-07-25-sglang-llama3/&quot;&gt;blog&lt;/a&gt;).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;More&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;[2024/10] The First SGLang Online Meetup (&lt;a href=&quot;https://github.com/sgl-project/sgl-learning-materials?tab=readme-ov-file#the-first-sglang-online-meetup&quot;&gt;slides&lt;/a&gt;).&lt;/li&gt; 
  &lt;li&gt;[2024/02] SGLang enables &lt;strong&gt;3x faster JSON decoding&lt;/strong&gt; with compressed finite state machine (&lt;a href=&quot;https://lmsys.org/blog/2024-02-05-compressed-fsm/&quot;&gt;blog&lt;/a&gt;).&lt;/li&gt; 
  &lt;li&gt;[2024/01] SGLang provides up to &lt;strong&gt;5x faster inference&lt;/strong&gt; with RadixAttention (&lt;a href=&quot;https://lmsys.org/blog/2024-01-17-sglang/&quot;&gt;blog&lt;/a&gt;).&lt;/li&gt; 
  &lt;li&gt;[2024/01] SGLang powers the serving of the official &lt;strong&gt;LLaVA v1.6&lt;/strong&gt; release demo (&lt;a href=&quot;https://github.com/haotian-liu/LLaVA?tab=readme-ov-file#demo&quot;&gt;usage&lt;/a&gt;).&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;About&lt;/h2&gt; 
&lt;p&gt;SGLang is a fast serving framework for large language models and vision language models. It makes your interaction with models faster and more controllable by co-designing the backend runtime and frontend language. The core features include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Fast Backend Runtime&lt;/strong&gt;: Provides efficient serving with RadixAttention for prefix caching, jump-forward constrained decoding, overhead-free CPU scheduler, continuous batching, token attention (paged attention), tensor parallelism, FlashInfer kernels, chunked prefill, and quantization (FP8/INT4/AWQ/GPTQ).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Flexible Frontend Language&lt;/strong&gt;: Offers an intuitive interface for programming LLM applications, including chained generation calls, advanced prompting, control flow, multi-modal inputs, parallelism, and external interactions.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Extensive Model Support&lt;/strong&gt;: Supports a wide range of generative models (Llama, Gemma, Mistral, QWen, DeepSeek, LLaVA, etc.), embedding models (e5-mistral, gte, mcdse) and reward models (Skywork), with easy extensibility for integrating new models.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Active Community&lt;/strong&gt;: SGLang is open-source and backed by an active community with industry adoption.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.sglang.ai/start/install.html&quot;&gt;Install SGLang&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.sglang.ai/backend/send_request.html&quot;&gt;Quick Start&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.sglang.ai/backend/openai_api_completions.html&quot;&gt;Backend Tutorial&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.sglang.ai/frontend/frontend.html&quot;&gt;Frontend Tutorial&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.sglang.ai/references/contribution_guide.html&quot;&gt;Contribution Guide&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Benchmark and Performance&lt;/h2&gt; 
&lt;p&gt;Learn more in the release blogs: &lt;a href=&quot;https://lmsys.org/blog/2024-07-25-sglang-llama3/&quot;&gt;v0.2 blog&lt;/a&gt;, &lt;a href=&quot;https://lmsys.org/blog/2024-09-04-sglang-v0-3/&quot;&gt;v0.3 blog&lt;/a&gt;, &lt;a href=&quot;https://lmsys.org/blog/2024-12-04-sglang-v0-4/&quot;&gt;v0.4 blog&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Roadmap&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/sgl-project/sglang/issues/4042&quot;&gt;Development Roadmap (2025 H1)&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Adoption and Sponsorship&lt;/h2&gt; 
&lt;p&gt;The project has been deployed to large-scale production, generating trillions of tokens every day. It is supported by the following institutions: AMD, Atlas Cloud, Baseten, Cursor, DataCrunch, Etched, Hyperbolic, Iflytek, Jam &amp;amp; Tea Studios, LinkedIn, LMSYS, Meituan, Nebius, Novita AI, NVIDIA, RunPod, Stanford, UC Berkeley, UCLA, xAI, and 01.AI.&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/sgl-project/sgl-learning-materials/main/slides/adoption.png&quot; alt=&quot;logo&quot; width=&quot;800&quot; margin=&quot;10px&quot;&gt;&lt;/p&gt; 
&lt;h2&gt;Contact Us&lt;/h2&gt; 
&lt;p&gt;For enterprises interested in adopting or deploying SGLang at scale, including technical consulting, sponsorship opportunities, or partnership inquiries, please contact us at &lt;a href=&quot;mailto:contact@sglang.ai&quot;&gt;contact@sglang.ai&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Acknowledgment and Citation&lt;/h2&gt; 
&lt;p&gt;We learned the design and reused code from the following projects: &lt;a href=&quot;https://github.com/guidance-ai/guidance&quot;&gt;Guidance&lt;/a&gt;, &lt;a href=&quot;https://github.com/vllm-project/vllm&quot;&gt;vLLM&lt;/a&gt;, &lt;a href=&quot;https://github.com/ModelTC/lightllm&quot;&gt;LightLLM&lt;/a&gt;, &lt;a href=&quot;https://github.com/flashinfer-ai/flashinfer&quot;&gt;FlashInfer&lt;/a&gt;, &lt;a href=&quot;https://github.com/outlines-dev/outlines&quot;&gt;Outlines&lt;/a&gt;, and &lt;a href=&quot;https://github.com/eth-sri/lmql&quot;&gt;LMQL&lt;/a&gt;. Please cite the paper, &lt;a href=&quot;https://arxiv.org/abs/2312.07104&quot;&gt;SGLang: Efficient Execution of Structured Language Model Programs&lt;/a&gt;, if you find the project useful.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>sinaptik-ai/pandas-ai</title>
      <link>https://github.com/sinaptik-ai/pandas-ai</link>
      <description>&lt;p&gt;Chat with your database or your datalake (SQL, CSV, parquet). PandasAI makes data analysis conversational using LLMs and RAG.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;img src=&quot;https://raw.githubusercontent.com/sinaptik-ai/pandas-ai/main/assets/logo.png&quot; alt=&quot;PandaAI&quot;&gt;&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://pypi.org/project/pandasai/&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/v/pandasai?label=Release&amp;amp;style=flat-square&quot; alt=&quot;Release&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/sinaptik-ai/pandas-ai/actions/workflows/ci-core.yml/badge.svg&quot;&gt;&lt;img src=&quot;https://github.com/sinaptik-ai/pandas-ai/actions/workflows/ci-core.yml/badge.svg?sanitize=true&quot; alt=&quot;CI&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/sinaptik-ai/pandas-ai/actions/workflows/cd.yml/badge.svg&quot;&gt;&lt;img src=&quot;https://github.com/sinaptik-ai/pandas-ai/actions/workflows/cd.yml/badge.svg?sanitize=true&quot; alt=&quot;CD&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://codecov.io/gh/sinaptik-ai/pandas-ai&quot;&gt;&lt;img src=&quot;https://codecov.io/gh/sinaptik-ai/pandas-ai/branch/main/graph/badge.svg?sanitize=true&quot; alt=&quot;Coverage&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://discord.gg/KYKj9F2FRH&quot;&gt;&lt;img src=&quot;https://dcbadge.vercel.app/api/server/kF7FqH2FwS?style=flat&amp;amp;compact=true&quot; alt=&quot;Discord&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://pepy.tech/project/pandasai&quot;&gt;&lt;img src=&quot;https://static.pepy.tech/badge/pandasai&quot; alt=&quot;Downloads&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://opensource.org/licenses/MIT&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/License-MIT-yellow.svg?sanitize=true&quot; alt=&quot;License: MIT&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/drive/1ZnO-njhL7TBOYPZaqvMvGtsjckZKrv2E?usp=sharing&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open in Colab&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;PandaAI is a Python platform that makes it easy to ask questions to your data in natural language. It helps non-technical users to interact with their data in a more natural way, and it helps technical users to save time, and effort when working with data.&lt;/p&gt; 
&lt;h1&gt;🔧 Getting started&lt;/h1&gt; 
&lt;p&gt;You can find the full documentation for PandaAI &lt;a href=&quot;https://pandas-ai.readthedocs.io/en/latest/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You can either decide to use PandaAI in your Jupyter notebooks, Streamlit apps, or use the client and server architecture from the repo.&lt;/p&gt; 
&lt;h2&gt;☁️ Using the platform&lt;/h2&gt; 
&lt;p&gt;The library can be used alongside our powerful data platform, making end-to-end conversational data analytics possible with as little as a few lines of code.&lt;/p&gt; 
&lt;p&gt;Load your data, save them as a dataframe, and push them to the platform&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import pandasai as pai

pai.api_key.set(&quot;your-pai-api-key&quot;)

file = pai.read_csv(&quot;./filepath.csv&quot;)

dataset = pai.create(path=&quot;your-organization/dataset-name&quot;,
    df=file,
    name=&quot;dataset-name&quot;,
    description=&quot;dataset-description&quot;)

dataset.push()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Your team can now access and query this data using natural language through the platform.&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/sinaptik-ai/pandas-ai/main/assets/demo.gif&quot; alt=&quot;PandaAI&quot;&gt;&lt;/p&gt; 
&lt;h2&gt;📚 Using the library&lt;/h2&gt; 
&lt;h3&gt;Python Requirements&lt;/h3&gt; 
&lt;p&gt;Python version &lt;code&gt;3.8+ &amp;lt;3.12&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;📦 Installation&lt;/h3&gt; 
&lt;p&gt;You can install the PandaAI library using pip or poetry.&lt;/p&gt; 
&lt;p&gt;With pip:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install &quot;pandasai&amp;gt;=3.0.0b2&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;With poetry:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;poetry add &quot;pandasai&amp;gt;=3.0.0b2&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;💻 Usage&lt;/h3&gt; 
&lt;h4&gt;Ask questions&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import pandasai as pai

# Sample DataFrame
df = pai.DataFrame({
    &quot;country&quot;: [&quot;United States&quot;, &quot;United Kingdom&quot;, &quot;France&quot;, &quot;Germany&quot;, &quot;Italy&quot;, &quot;Spain&quot;, &quot;Canada&quot;, &quot;Australia&quot;, &quot;Japan&quot;, &quot;China&quot;],
    &quot;revenue&quot;: [5000, 3200, 2900, 4100, 2300, 2100, 2500, 2600, 4500, 7000]
})

# By default, unless you choose a different LLM, it will use BambooLLM.
# You can get your free API key signing up at https://app.pandabi.ai (you can also configure it in your .env file)
pai.api_key.set(&quot;your-pai-api-key&quot;)

df.chat(&#39;Which are the top 5 countries by sales?&#39;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code&gt;China, United States, Japan, Germany, Australia
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr&gt; 
&lt;p&gt;Or you can ask more complex questions:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;df.chat(
    &quot;What is the total sales for the top 3 countries by sales?&quot;
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code&gt;The total sales for the top 3 countries by sales is 16500.
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Visualize charts&lt;/h4&gt; 
&lt;p&gt;You can also ask PandaAI to generate charts for you:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;df.chat(
    &quot;Plot the histogram of countries showing for each one the gd. Use different colors for each bar&quot;,
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/sinaptik-ai/pandas-ai/main/assets/histogram-chart.png?raw=true&quot; alt=&quot;Chart&quot;&gt;&lt;/p&gt; 
&lt;h4&gt;Multiple DataFrames&lt;/h4&gt; 
&lt;p&gt;You can also pass in multiple dataframes to PandaAI and ask questions relating them.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import pandasai as pai

employees_data = {
    &#39;EmployeeID&#39;: [1, 2, 3, 4, 5],
    &#39;Name&#39;: [&#39;John&#39;, &#39;Emma&#39;, &#39;Liam&#39;, &#39;Olivia&#39;, &#39;William&#39;],
    &#39;Department&#39;: [&#39;HR&#39;, &#39;Sales&#39;, &#39;IT&#39;, &#39;Marketing&#39;, &#39;Finance&#39;]
}

salaries_data = {
    &#39;EmployeeID&#39;: [1, 2, 3, 4, 5],
    &#39;Salary&#39;: [5000, 6000, 4500, 7000, 5500]
}

employees_df = pai.DataFrame(employees_data)
salaries_df = pai.DataFrame(salaries_data)

# By default, unless you choose a different LLM, it will use BambooLLM.
# You can get your free API key signing up at https://app.pandabi.ai (you can also configure it in your .env file)
pai.api_key.set(&quot;your-pai-api-key&quot;)

pai.chat(&quot;Who gets paid the most?&quot;, employees_df, salaries_df)
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code&gt;Olivia gets paid the most.
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Docker Sandbox&lt;/h4&gt; 
&lt;p&gt;You can run PandaAI in a Docker sandbox, providing a secure, isolated environment to execute code safely and mitigate the risk of malicious attacks.&lt;/p&gt; 
&lt;h5&gt;Python Requirements&lt;/h5&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install &quot;pandasai-docker&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;Usage&lt;/h5&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import pandasai as pai
from pandasai_docker import DockerSandbox

# Initialize the sandbox
sandbox = DockerSandbox()
sandbox.start()

employees_data = {
    &#39;EmployeeID&#39;: [1, 2, 3, 4, 5],
    &#39;Name&#39;: [&#39;John&#39;, &#39;Emma&#39;, &#39;Liam&#39;, &#39;Olivia&#39;, &#39;William&#39;],
    &#39;Department&#39;: [&#39;HR&#39;, &#39;Sales&#39;, &#39;IT&#39;, &#39;Marketing&#39;, &#39;Finance&#39;]
}

salaries_data = {
    &#39;EmployeeID&#39;: [1, 2, 3, 4, 5],
    &#39;Salary&#39;: [5000, 6000, 4500, 7000, 5500]
}

employees_df = pai.DataFrame(employees_data)
salaries_df = pai.DataFrame(salaries_data)

# By default, unless you choose a different LLM, it will use BambooLLM.
# You can get your free API key signing up at https://app.pandabi.ai (you can also configure it in your .env file)
pai.api_key.set(&quot;your-pai-api-key&quot;)

pai.chat(&quot;Who gets paid the most?&quot;, employees_df, salaries_df, sandbox=sandbox)

# Don&#39;t forget to stop the sandbox when done
sandbox.stop()
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code&gt;Olivia gets paid the most.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can find more examples in the &lt;a href=&quot;https://raw.githubusercontent.com/sinaptik-ai/pandas-ai/main/examples&quot;&gt;examples&lt;/a&gt; directory.&lt;/p&gt; 
&lt;h2&gt;📜 License&lt;/h2&gt; 
&lt;p&gt;PandaAI is available under the MIT expat license, except for the &lt;code&gt;pandasai/ee&lt;/code&gt; directory of this repository, which has its &lt;a href=&quot;https://github.com/sinaptik-ai/pandas-ai/raw/main/ee/LICENSE&quot;&gt;license here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you are interested in managed PandaAI Cloud or self-hosted Enterprise Offering, &lt;a href=&quot;https://getpanda.ai/pricing&quot;&gt;contact us&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Resources&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Beta Notice&lt;/strong&gt;&lt;br&gt; Release v3 is currently in beta. The following documentation and examples reflect the features and functionality in progress and may change before the final release.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://pandas-ai.readthedocs.io/en/latest/&quot;&gt;Docs&lt;/a&gt; for comprehensive documentation&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/sinaptik-ai/pandas-ai/main/examples&quot;&gt;Examples&lt;/a&gt; for example notebooks&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://discord.gg/KYKj9F2FRH&quot;&gt;Discord&lt;/a&gt; for discussion with the community and PandaAI team&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🤝 Contributing&lt;/h2&gt; 
&lt;p&gt;Contributions are welcome! Please check the outstanding issues and feel free to open a pull request. For more information, please check out the &lt;a href=&quot;https://raw.githubusercontent.com/sinaptik-ai/pandas-ai/main/CONTRIBUTING.md&quot;&gt;contributing guidelines&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Thank you!&lt;/h3&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/sinaptik-ai/pandas-ai/graphs/contributors&quot;&gt;&lt;img src=&quot;https://contrib.rocks/image?repo=sinaptik-ai/pandas-ai&quot; alt=&quot;Contributors&quot;&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>hiyouga/LLaMA-Factory</title>
      <link>https://github.com/hiyouga/LLaMA-Factory</link>
      <description>&lt;p&gt;Unified Efficient Fine-Tuning of 100+ LLMs &amp; VLMs (ACL 2024)&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/assets/logo.png&quot; alt=&quot;# LLaMA Factory&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/hiyouga/LLaMA-Factory/stargazers&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/hiyouga/LLaMA-Factory?style=social&quot; alt=&quot;GitHub Repo stars&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/hiyouga/LLaMA-Factory/commits/main&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/last-commit/hiyouga/LLaMA-Factory&quot; alt=&quot;GitHub last commit&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/hiyouga/LLaMA-Factory/graphs/contributors&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/contributors/hiyouga/LLaMA-Factory?color=orange&quot; alt=&quot;GitHub contributors&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/hiyouga/LLaMA-Factory/actions/workflows/tests.yml&quot;&gt;&lt;img src=&quot;https://github.com/hiyouga/LLaMA-Factory/actions/workflows/tests.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub workflow&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://pypi.org/project/llamafactory/&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/v/llamafactory&quot; alt=&quot;PyPI&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://scholar.google.com/scholar?cites=12620864006390196564&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/citation-349-green&quot; alt=&quot;Citation&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/hiyouga/LLaMA-Factory/pulls&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/PRs-welcome-blue&quot; alt=&quot;GitHub pull request&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://twitter.com/llamafactory_ai&quot;&gt;&lt;img src=&quot;https://img.shields.io/twitter/follow/llamafactory_ai&quot; alt=&quot;Twitter&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://discord.gg/rKfvV9r9FK&quot;&gt;&lt;img src=&quot;https://dcbadge.vercel.app/api/server/rKfvV9r9FK?compact=true&amp;amp;style=flat&quot; alt=&quot;Discord&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://gitcode.com/zhengyaowei/LLaMA-Factory&quot;&gt;&lt;img src=&quot;https://gitcode.com/zhengyaowei/LLaMA-Factory/star/badge.svg?sanitize=true&quot; alt=&quot;GitCode&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open in Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory&quot;&gt;&lt;img src=&quot;https://gallery.pai-ml.com/assets/open-in-dsw.svg?sanitize=true&quot; alt=&quot;Open in DSW&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://huggingface.co/spaces/hiyouga/LLaMA-Board&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/%F0%9F%A4%97-Open%20in%20Spaces-blue&quot; alt=&quot;Spaces&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://modelscope.cn/studios/hiyouga/LLaMA-Board&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/ModelScope-Open%20in%20Studios-blue&quot; alt=&quot;Studios&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://aws.amazon.com/cn/blogs/china/a-one-stop-code-free-model-fine-tuning-deployment-platform-based-on-sagemaker-and-llama-factory/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/SageMaker-Open%20in%20AWS-blue&quot; alt=&quot;SageMaker&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3 align=&quot;center&quot;&gt; Easily fine-tune 100+ large language models with zero-code &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#quickstart&quot;&gt;CLI&lt;/a&gt; and &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#fine-tuning-with-llama-board-gui-powered-by-gradio&quot;&gt;Web UI&lt;/a&gt; &lt;/h3&gt; 
&lt;p align=&quot;center&quot;&gt; 
 &lt;picture&gt; 
  &lt;img alt=&quot;Github trend&quot; src=&quot;https://trendshift.io/api/badge/repositories/4535&quot;&gt; 
 &lt;/picture&gt; &lt;/p&gt; 
&lt;p&gt;👋 Join our &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/assets/wechat.jpg&quot;&gt;WeChat&lt;/a&gt; or &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/assets/wechat_npu.jpg&quot;&gt;NPU user group&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;[ English | &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/README_zh.md&quot;&gt;中文&lt;/a&gt; ]&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Fine-tuning a large language model can be easy as...&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/user-attachments/assets/319abcde-7b31-4874-8969-b30b0f64c96a&quot;&gt;https://github.com/user-attachments/assets/319abcde-7b31-4874-8969-b30b0f64c96a&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Choose your path:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Documentation&lt;/strong&gt;: &lt;a href=&quot;https://llamafactory.readthedocs.io/en/latest/&quot;&gt;https://llamafactory.readthedocs.io/en/latest/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Colab (free)&lt;/strong&gt;: &lt;a href=&quot;https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing&quot;&gt;https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Local machine&lt;/strong&gt;: Please refer to &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#getting-started&quot;&gt;usage&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;PAI-DSW (free trial)&lt;/strong&gt;: &lt;a href=&quot;https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory&quot;&gt;Llama3 Example&lt;/a&gt; | &lt;a href=&quot;https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory_qwen2vl&quot;&gt;Qwen2-VL Example&lt;/a&gt; | &lt;a href=&quot;https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory_deepseek_r1_distill_7b&quot;&gt;DeepSeek-R1-Distill Example&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Amazon SageMaker&lt;/strong&gt;: &lt;a href=&quot;https://aws.amazon.com/cn/blogs/china/a-one-stop-code-free-model-fine-tuning-deployment-platform-based-on-sagemaker-and-llama-factory/&quot;&gt;Blog&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Except for the above links, all other websites are unauthorized third-party websites. Please carefully use them.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#features&quot;&gt;Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#benchmark&quot;&gt;Benchmark&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#changelog&quot;&gt;Changelog&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#supported-models&quot;&gt;Supported Models&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#supported-training-approaches&quot;&gt;Supported Training Approaches&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#provided-datasets&quot;&gt;Provided Datasets&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#requirement&quot;&gt;Requirement&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#getting-started&quot;&gt;Getting Started&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#installation&quot;&gt;Installation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#data-preparation&quot;&gt;Data Preparation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#quickstart&quot;&gt;Quickstart&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#fine-tuning-with-llama-board-gui-powered-by-gradio&quot;&gt;Fine-Tuning with LLaMA Board GUI&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#build-docker&quot;&gt;Build Docker&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#deploy-with-openai-style-api-and-vllm&quot;&gt;Deploy with OpenAI-style API and vLLM&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#download-from-modelscope-hub&quot;&gt;Download from ModelScope Hub&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#download-from-modelers-hub&quot;&gt;Download from Modelers Hub&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#use-wb-logger&quot;&gt;Use W&amp;amp;B Logger&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#use-swanlab-logger&quot;&gt;Use SwanLab Logger&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#projects-using-llama-factory&quot;&gt;Projects using LLaMA Factory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#license&quot;&gt;License&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#citation&quot;&gt;Citation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#acknowledgement&quot;&gt;Acknowledgement&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Various models&lt;/strong&gt;: LLaMA, LLaVA, Mistral, Mixtral-MoE, Qwen, Qwen2-VL, DeepSeek, Yi, Gemma, ChatGLM, Phi, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Integrated methods&lt;/strong&gt;: (Continuous) pre-training, (multimodal) supervised fine-tuning, reward modeling, PPO, DPO, KTO, ORPO, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Scalable resources&lt;/strong&gt;: 16-bit full-tuning, freeze-tuning, LoRA and 2/3/4/5/6/8-bit QLoRA via AQLM/AWQ/GPTQ/LLM.int8/HQQ/EETQ.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Advanced algorithms&lt;/strong&gt;: &lt;a href=&quot;https://github.com/jiaweizzhao/GaLore&quot;&gt;GaLore&lt;/a&gt;, &lt;a href=&quot;https://github.com/Ledzy/BAdam&quot;&gt;BAdam&lt;/a&gt;, &lt;a href=&quot;https://github.com/zhuhanqing/APOLLO&quot;&gt;APOLLO&lt;/a&gt;, &lt;a href=&quot;https://github.com/zyushun/Adam-mini&quot;&gt;Adam-mini&lt;/a&gt;, DoRA, LongLoRA, LLaMA Pro, Mixture-of-Depths, LoRA+, LoftQ and PiSSA.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Practical tricks&lt;/strong&gt;: &lt;a href=&quot;https://github.com/Dao-AILab/flash-attention&quot;&gt;FlashAttention-2&lt;/a&gt;, &lt;a href=&quot;https://github.com/unslothai/unsloth&quot;&gt;Unsloth&lt;/a&gt;, &lt;a href=&quot;https://github.com/linkedin/Liger-Kernel&quot;&gt;Liger Kernel&lt;/a&gt;, RoPE scaling, NEFTune and rsLoRA.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Wide tasks&lt;/strong&gt;: Multi-turn dialogue, tool using, image understanding, visual grounding, video recognition, audio understanding, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Experiment monitors&lt;/strong&gt;: LlamaBoard, TensorBoard, Wandb, MLflow, &lt;a href=&quot;https://github.com/SwanHubX/SwanLab&quot;&gt;SwanLab&lt;/a&gt;, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Faster inference&lt;/strong&gt;: OpenAI-style API, Gradio UI and CLI with &lt;a href=&quot;https://github.com/vllm-project/vllm&quot;&gt;vLLM worker&lt;/a&gt; or &lt;a href=&quot;https://github.com/sgl-project/sglang&quot;&gt;SGLang worker&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Day-N Support for Fine-Tuning Cutting-Edge Models&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Support Date&lt;/th&gt; 
   &lt;th&gt;Model Name&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Day 0&lt;/td&gt; 
   &lt;td&gt;Qwen2.5 / Qwen2.5-VL / Gemma 3 / InternLM 3 / MiniCPM-o-2.6&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Day 1&lt;/td&gt; 
   &lt;td&gt;Llama 3 / GLM-4 / Mistral Small / PaliGemma2&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Benchmark&lt;/h2&gt; 
&lt;p&gt;Compared to ChatGLM&#39;s &lt;a href=&quot;https://github.com/THUDM/ChatGLM2-6B/tree/main/ptuning&quot;&gt;P-Tuning&lt;/a&gt;, LLaMA Factory&#39;s LoRA tuning offers up to &lt;strong&gt;3.7 times faster&lt;/strong&gt; training speed with a better Rouge score on the advertising text generation task. By leveraging 4-bit quantization technique, LLaMA Factory&#39;s QLoRA further improves the efficiency regarding the GPU memory.&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/assets/benchmark.svg?sanitize=true&quot; alt=&quot;benchmark&quot;&gt;&lt;/p&gt; 
&lt;details&gt;
 &lt;summary&gt;Definitions&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Training Speed&lt;/strong&gt;: the number of training samples processed per second during the training. (bs=4, cutoff_len=1024)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Rouge Score&lt;/strong&gt;: Rouge-2 score on the development set of the &lt;a href=&quot;https://aclanthology.org/D19-1321.pdf&quot;&gt;advertising text generation&lt;/a&gt; task. (bs=4, cutoff_len=1024)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;GPU Memory&lt;/strong&gt;: Peak GPU memory usage in 4-bit quantized training. (bs=1, cutoff_len=1024)&lt;/li&gt; 
  &lt;li&gt;We adopt &lt;code&gt;pre_seq_len=128&lt;/code&gt; for ChatGLM&#39;s P-Tuning and &lt;code&gt;lora_rank=32&lt;/code&gt; for LLaMA Factory&#39;s LoRA tuning.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;Changelog&lt;/h2&gt; 
&lt;p&gt;[25/03/15] We supported &lt;strong&gt;&lt;a href=&quot;https://github.com/sgl-project/sglang&quot;&gt;SGLang&lt;/a&gt;&lt;/strong&gt; as inference backend. Try &lt;code&gt;infer_backend: sglang&lt;/code&gt; to accelerate inference.&lt;/p&gt; 
&lt;p&gt;[25/03/12] We supported fine-tuning the &lt;strong&gt;&lt;a href=&quot;https://huggingface.co/blog/gemma3&quot;&gt;Gemma-3&lt;/a&gt;&lt;/strong&gt; model.&lt;/p&gt; 
&lt;p&gt;[25/02/24] Announcing &lt;strong&gt;&lt;a href=&quot;https://github.com/hiyouga/EasyR1&quot;&gt;EasyR1&lt;/a&gt;&lt;/strong&gt;, an efficient, scalable and multi-modality RL training framework for efficient GRPO training.&lt;/p&gt; 
&lt;p&gt;[25/02/11] We supported saving the &lt;strong&gt;&lt;a href=&quot;https://github.com/ollama/ollama&quot;&gt;Ollama&lt;/a&gt;&lt;/strong&gt; modelfile when exporting the model checkpoints. See &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&quot;&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
&lt;p&gt;[25/02/05] We supported fine-tuning the &lt;strong&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/Qwen/Qwen2-Audio-7B-Instruct&quot;&gt;Qwen2-Audio&lt;/a&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;a href=&quot;https://huggingface.co/openbmb/MiniCPM-o-2_6&quot;&gt;MiniCPM-o-2.6&lt;/a&gt;&lt;/strong&gt; on audio understanding tasks.&lt;/p&gt; 
&lt;p&gt;[25/01/31] We supported fine-tuning the &lt;strong&gt;&lt;a href=&quot;https://huggingface.co/deepseek-ai/DeepSeek-R1&quot;&gt;DeepSeek-R1&lt;/a&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;a href=&quot;https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct&quot;&gt;Qwen2.5-VL&lt;/a&gt;&lt;/strong&gt; model.&lt;/p&gt; 
&lt;details&gt;
 &lt;summary&gt;Full Changelog&lt;/summary&gt; 
 &lt;p&gt;[25/01/15] We supported &lt;strong&gt;&lt;a href=&quot;https://arxiv.org/abs/2412.05270&quot;&gt;APOLLO&lt;/a&gt;&lt;/strong&gt; optimizer. See &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&quot;&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[25/01/14] We supported fine-tuning the &lt;strong&gt;&lt;a href=&quot;https://huggingface.co/openbmb/MiniCPM-o-2_6&quot;&gt;MiniCPM-o-2.6&lt;/a&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;a href=&quot;https://huggingface.co/openbmb/MiniCPM-V-2_6&quot;&gt;MiniCPM-V-2.6&lt;/a&gt;&lt;/strong&gt; models. Thank &lt;a href=&quot;https://github.com/BUAADreamer&quot;&gt;@BUAADreamer&lt;/a&gt;&#39;s PR.&lt;/p&gt; 
 &lt;p&gt;[25/01/14] We supported fine-tuning the &lt;strong&gt;&lt;a href=&quot;https://huggingface.co/collections/internlm/&quot;&gt;InternLM 3&lt;/a&gt;&lt;/strong&gt; models. Thank &lt;a href=&quot;https://github.com/hhaAndroid&quot;&gt;@hhaAndroid&lt;/a&gt;&#39;s PR.&lt;/p&gt; 
 &lt;p&gt;[25/01/10] We supported fine-tuning the &lt;strong&gt;&lt;a href=&quot;https://huggingface.co/microsoft/phi-4&quot;&gt;Phi-4&lt;/a&gt;&lt;/strong&gt; model.&lt;/p&gt; 
 &lt;p&gt;[24/12/21] We supported using &lt;strong&gt;&lt;a href=&quot;https://github.com/SwanHubX/SwanLab&quot;&gt;SwanLab&lt;/a&gt;&lt;/strong&gt; for experiment tracking and visualization. See &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#use-swanlab-logger&quot;&gt;this section&lt;/a&gt; for details.&lt;/p&gt; 
 &lt;p&gt;[24/11/27] We supported fine-tuning the &lt;strong&gt;&lt;a href=&quot;https://huggingface.co/Skywork/Skywork-o1-Open-Llama-3.1-8B&quot;&gt;Skywork-o1&lt;/a&gt;&lt;/strong&gt; model and the &lt;strong&gt;&lt;a href=&quot;https://huggingface.co/datasets/O1-OPEN/OpenO1-SFT&quot;&gt;OpenO1&lt;/a&gt;&lt;/strong&gt; dataset.&lt;/p&gt; 
 &lt;p&gt;[24/10/09] We supported downloading pre-trained models and datasets from the &lt;strong&gt;&lt;a href=&quot;https://modelers.cn/models&quot;&gt;Modelers Hub&lt;/a&gt;&lt;/strong&gt;. See &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#download-from-modelers-hub&quot;&gt;this tutorial&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[24/09/19] We supported fine-tuning the &lt;strong&gt;&lt;a href=&quot;https://qwenlm.github.io/blog/qwen2.5/&quot;&gt;Qwen2.5&lt;/a&gt;&lt;/strong&gt; models.&lt;/p&gt; 
 &lt;p&gt;[24/08/30] We supported fine-tuning the &lt;strong&gt;&lt;a href=&quot;https://qwenlm.github.io/blog/qwen2-vl/&quot;&gt;Qwen2-VL&lt;/a&gt;&lt;/strong&gt; models. Thank &lt;a href=&quot;https://github.com/simonJJJ&quot;&gt;@simonJJJ&lt;/a&gt;&#39;s PR.&lt;/p&gt; 
 &lt;p&gt;[24/08/27] We supported &lt;strong&gt;&lt;a href=&quot;https://github.com/linkedin/Liger-Kernel&quot;&gt;Liger Kernel&lt;/a&gt;&lt;/strong&gt;. Try &lt;code&gt;enable_liger_kernel: true&lt;/code&gt; for efficient training.&lt;/p&gt; 
 &lt;p&gt;[24/08/09] We supported &lt;strong&gt;&lt;a href=&quot;https://github.com/zyushun/Adam-mini&quot;&gt;Adam-mini&lt;/a&gt;&lt;/strong&gt; optimizer. See &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&quot;&gt;examples&lt;/a&gt; for usage. Thank &lt;a href=&quot;https://github.com/relic-yuexi&quot;&gt;@relic-yuexi&lt;/a&gt;&#39;s PR.&lt;/p&gt; 
 &lt;p&gt;[24/07/04] We supported &lt;a href=&quot;https://github.com/MeetKai/functionary/tree/main/functionary/train/packing&quot;&gt;contamination-free packed training&lt;/a&gt;. Use &lt;code&gt;neat_packing: true&lt;/code&gt; to activate it. Thank &lt;a href=&quot;https://github.com/chuan298&quot;&gt;@chuan298&lt;/a&gt;&#39;s PR.&lt;/p&gt; 
 &lt;p&gt;[24/06/16] We supported &lt;strong&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.02948&quot;&gt;PiSSA&lt;/a&gt;&lt;/strong&gt; algorithm. See &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&quot;&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[24/06/07] We supported fine-tuning the &lt;strong&gt;&lt;a href=&quot;https://qwenlm.github.io/blog/qwen2/&quot;&gt;Qwen2&lt;/a&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;a href=&quot;https://github.com/THUDM/GLM-4&quot;&gt;GLM-4&lt;/a&gt;&lt;/strong&gt; models.&lt;/p&gt; 
 &lt;p&gt;[24/05/26] We supported &lt;strong&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.14734&quot;&gt;SimPO&lt;/a&gt;&lt;/strong&gt; algorithm for preference learning. See &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&quot;&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[24/05/20] We supported fine-tuning the &lt;strong&gt;PaliGemma&lt;/strong&gt; series models. Note that the PaliGemma models are pre-trained models, you need to fine-tune them with &lt;code&gt;paligemma&lt;/code&gt; template for chat completion.&lt;/p&gt; 
 &lt;p&gt;[24/05/18] We supported &lt;strong&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.01306&quot;&gt;KTO&lt;/a&gt;&lt;/strong&gt; algorithm for preference learning. See &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&quot;&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[24/05/14] We supported training and inference on the Ascend NPU devices. Check &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#installation&quot;&gt;installation&lt;/a&gt; section for details.&lt;/p&gt; 
 &lt;p&gt;[24/04/26] We supported fine-tuning the &lt;strong&gt;LLaVA-1.5&lt;/strong&gt; multimodal LLMs. See &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&quot;&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[24/04/22] We provided a &lt;strong&gt;&lt;a href=&quot;https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing&quot;&gt;Colab notebook&lt;/a&gt;&lt;/strong&gt; for fine-tuning the Llama-3 model on a free T4 GPU. Two Llama-3-derived models fine-tuned using LLaMA Factory are available at Hugging Face, check &lt;a href=&quot;https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat&quot;&gt;Llama3-8B-Chinese-Chat&lt;/a&gt; and &lt;a href=&quot;https://huggingface.co/zhichen/Llama3-Chinese&quot;&gt;Llama3-Chinese&lt;/a&gt; for details.&lt;/p&gt; 
 &lt;p&gt;[24/04/21] We supported &lt;strong&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.02258&quot;&gt;Mixture-of-Depths&lt;/a&gt;&lt;/strong&gt; according to &lt;a href=&quot;https://github.com/astramind-ai/Mixture-of-depths&quot;&gt;AstraMindAI&#39;s implementation&lt;/a&gt;. See &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&quot;&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[24/04/16] We supported &lt;strong&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.02827&quot;&gt;BAdam&lt;/a&gt;&lt;/strong&gt; optimizer. See &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&quot;&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[24/04/16] We supported &lt;strong&gt;&lt;a href=&quot;https://github.com/unslothai/unsloth&quot;&gt;unsloth&lt;/a&gt;&lt;/strong&gt;&#39;s long-sequence training (Llama-2-7B-56k within 24GB). It achieves &lt;strong&gt;117%&lt;/strong&gt; speed and &lt;strong&gt;50%&lt;/strong&gt; memory compared with FlashAttention-2, more benchmarks can be found in &lt;a href=&quot;https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-comparison&quot;&gt;this page&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;[24/03/31] We supported &lt;strong&gt;&lt;a href=&quot;https://arxiv.org/abs/2403.07691&quot;&gt;ORPO&lt;/a&gt;&lt;/strong&gt;. See &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&quot;&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[24/03/21] Our paper &quot;&lt;a href=&quot;https://arxiv.org/abs/2403.13372&quot;&gt;LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models&lt;/a&gt;&quot; is available at arXiv!&lt;/p&gt; 
 &lt;p&gt;[24/03/20] We supported &lt;strong&gt;FSDP+QLoRA&lt;/strong&gt; that fine-tunes a 70B model on 2x24GB GPUs. See &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&quot;&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[24/03/13] We supported &lt;strong&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.12354&quot;&gt;LoRA+&lt;/a&gt;&lt;/strong&gt;. See &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&quot;&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[24/03/07] We supported &lt;strong&gt;&lt;a href=&quot;https://arxiv.org/abs/2403.03507&quot;&gt;GaLore&lt;/a&gt;&lt;/strong&gt; optimizer. See &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&quot;&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[24/03/07] We integrated &lt;strong&gt;&lt;a href=&quot;https://github.com/vllm-project/vllm&quot;&gt;vLLM&lt;/a&gt;&lt;/strong&gt; for faster and concurrent inference. Try &lt;code&gt;infer_backend: vllm&lt;/code&gt; to enjoy &lt;strong&gt;270%&lt;/strong&gt; inference speed.&lt;/p&gt; 
 &lt;p&gt;[24/02/28] We supported weight-decomposed LoRA (&lt;strong&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.09353&quot;&gt;DoRA&lt;/a&gt;&lt;/strong&gt;). Try &lt;code&gt;use_dora: true&lt;/code&gt; to activate DoRA training.&lt;/p&gt; 
 &lt;p&gt;[24/02/15] We supported &lt;strong&gt;block expansion&lt;/strong&gt; proposed by &lt;a href=&quot;https://github.com/TencentARC/LLaMA-Pro&quot;&gt;LLaMA Pro&lt;/a&gt;. See &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&quot;&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[24/02/05] Qwen1.5 (Qwen2 beta version) series models are supported in LLaMA-Factory. Check this &lt;a href=&quot;https://qwenlm.github.io/blog/qwen1.5/&quot;&gt;blog post&lt;/a&gt; for details.&lt;/p&gt; 
 &lt;p&gt;[24/01/18] We supported &lt;strong&gt;agent tuning&lt;/strong&gt; for most models, equipping model with tool using abilities by fine-tuning with &lt;code&gt;dataset: glaive_toolcall_en&lt;/code&gt;.&lt;/p&gt; 
 &lt;p&gt;[23/12/23] We supported &lt;strong&gt;&lt;a href=&quot;https://github.com/unslothai/unsloth&quot;&gt;unsloth&lt;/a&gt;&lt;/strong&gt;&#39;s implementation to boost LoRA tuning for the LLaMA, Mistral and Yi models. Try &lt;code&gt;use_unsloth: true&lt;/code&gt; argument to activate unsloth patch. It achieves &lt;strong&gt;170%&lt;/strong&gt; speed in our benchmark, check &lt;a href=&quot;https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-comparison&quot;&gt;this page&lt;/a&gt; for details.&lt;/p&gt; 
 &lt;p&gt;[23/12/12] We supported fine-tuning the latest MoE model &lt;strong&gt;&lt;a href=&quot;https://huggingface.co/mistralai/Mixtral-8x7B-v0.1&quot;&gt;Mixtral 8x7B&lt;/a&gt;&lt;/strong&gt; in our framework. See hardware requirement &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#hardware-requirement&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;[23/12/01] We supported downloading pre-trained models and datasets from the &lt;strong&gt;&lt;a href=&quot;https://modelscope.cn/models&quot;&gt;ModelScope Hub&lt;/a&gt;&lt;/strong&gt;. See &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#download-from-modelscope-hub&quot;&gt;this tutorial&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[23/10/21] We supported &lt;strong&gt;&lt;a href=&quot;https://arxiv.org/abs/2310.05914&quot;&gt;NEFTune&lt;/a&gt;&lt;/strong&gt; trick for fine-tuning. Try &lt;code&gt;neftune_noise_alpha: 5&lt;/code&gt; argument to activate NEFTune.&lt;/p&gt; 
 &lt;p&gt;[23/09/27] We supported &lt;strong&gt;$S^2$-Attn&lt;/strong&gt; proposed by &lt;a href=&quot;https://github.com/dvlab-research/LongLoRA&quot;&gt;LongLoRA&lt;/a&gt; for the LLaMA models. Try &lt;code&gt;shift_attn: true&lt;/code&gt; argument to enable shift short attention.&lt;/p&gt; 
 &lt;p&gt;[23/09/23] We integrated MMLU, C-Eval and CMMLU benchmarks in this repo. See &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&quot;&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[23/09/10] We supported &lt;strong&gt;&lt;a href=&quot;https://github.com/Dao-AILab/flash-attention&quot;&gt;FlashAttention-2&lt;/a&gt;&lt;/strong&gt;. Try &lt;code&gt;flash_attn: fa2&lt;/code&gt; argument to enable FlashAttention-2 if you are using RTX4090, A100 or H100 GPUs.&lt;/p&gt; 
 &lt;p&gt;[23/08/12] We supported &lt;strong&gt;RoPE scaling&lt;/strong&gt; to extend the context length of the LLaMA models. Try &lt;code&gt;rope_scaling: linear&lt;/code&gt; argument in training and &lt;code&gt;rope_scaling: dynamic&lt;/code&gt; argument at inference to extrapolate the position embeddings.&lt;/p&gt; 
 &lt;p&gt;[23/08/11] We supported &lt;strong&gt;&lt;a href=&quot;https://arxiv.org/abs/2305.18290&quot;&gt;DPO training&lt;/a&gt;&lt;/strong&gt; for instruction-tuned models. See &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&quot;&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[23/07/31] We supported &lt;strong&gt;dataset streaming&lt;/strong&gt;. Try &lt;code&gt;streaming: true&lt;/code&gt; and &lt;code&gt;max_steps: 10000&lt;/code&gt; arguments to load your dataset in streaming mode.&lt;/p&gt; 
 &lt;p&gt;[23/07/29] We released two instruction-tuned 13B models at Hugging Face. See these Hugging Face Repos (&lt;a href=&quot;https://huggingface.co/hiyouga/Llama-2-Chinese-13b-chat&quot;&gt;LLaMA-2&lt;/a&gt; / &lt;a href=&quot;https://huggingface.co/hiyouga/Baichuan-13B-sft&quot;&gt;Baichuan&lt;/a&gt;) for details.&lt;/p&gt; 
 &lt;p&gt;[23/07/18] We developed an &lt;strong&gt;all-in-one Web UI&lt;/strong&gt; for training, evaluation and inference. Try &lt;code&gt;train_web.py&lt;/code&gt; to fine-tune models in your Web browser. Thank &lt;a href=&quot;https://github.com/KanadeSiina&quot;&gt;@KanadeSiina&lt;/a&gt; and &lt;a href=&quot;https://github.com/codemayq&quot;&gt;@codemayq&lt;/a&gt; for their efforts in the development.&lt;/p&gt; 
 &lt;p&gt;[23/07/09] We released &lt;strong&gt;&lt;a href=&quot;https://github.com/hiyouga/FastEdit&quot;&gt;FastEdit&lt;/a&gt;&lt;/strong&gt; ⚡🩹, an easy-to-use package for editing the factual knowledge of large language models efficiently. Please follow &lt;a href=&quot;https://github.com/hiyouga/FastEdit&quot;&gt;FastEdit&lt;/a&gt; if you are interested.&lt;/p&gt; 
 &lt;p&gt;[23/06/29] We provided a &lt;strong&gt;reproducible example&lt;/strong&gt; of training a chat model using instruction-following datasets, see &lt;a href=&quot;https://huggingface.co/hiyouga/Baichuan-7B-sft&quot;&gt;Baichuan-7B-sft&lt;/a&gt; for details.&lt;/p&gt; 
 &lt;p&gt;[23/06/22] We aligned the &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/src/api_demo.py&quot;&gt;demo API&lt;/a&gt; with the &lt;a href=&quot;https://platform.openai.com/docs/api-reference/chat&quot;&gt;OpenAI&#39;s&lt;/a&gt; format where you can insert the fine-tuned model in &lt;strong&gt;arbitrary ChatGPT-based applications&lt;/strong&gt;.&lt;/p&gt; 
 &lt;p&gt;[23/06/03] We supported quantized training and inference (aka &lt;strong&gt;&lt;a href=&quot;https://github.com/artidoro/qlora&quot;&gt;QLoRA&lt;/a&gt;&lt;/strong&gt;). See &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&quot;&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;Supported Models&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Model size&lt;/th&gt; 
   &lt;th&gt;Template&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/baichuan-inc&quot;&gt;Baichuan 2&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B/13B&lt;/td&gt; 
   &lt;td&gt;baichuan2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/bigscience&quot;&gt;BLOOM/BLOOMZ&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;560M/1.1B/1.7B/3B/7.1B/176B&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/THUDM&quot;&gt;ChatGLM3&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;6B&lt;/td&gt; 
   &lt;td&gt;chatglm3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/CohereForAI&quot;&gt;Command R&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;35B/104B&lt;/td&gt; 
   &lt;td&gt;cohere&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/deepseek-ai&quot;&gt;DeepSeek (Code/MoE)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B/16B/67B/236B&lt;/td&gt; 
   &lt;td&gt;deepseek&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/deepseek-ai&quot;&gt;DeepSeek 2.5/3&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;236B/671B&lt;/td&gt; 
   &lt;td&gt;deepseek3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/deepseek-ai&quot;&gt;DeepSeek R1 (Distill)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1.5B/7B/8B/14B/32B/70B/671B&lt;/td&gt; 
   &lt;td&gt;deepseek3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/tiiuae&quot;&gt;Falcon&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B/11B/40B/180B&lt;/td&gt; 
   &lt;td&gt;falcon&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/google&quot;&gt;Gemma/Gemma 2/CodeGemma&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2B/7B/9B/27B&lt;/td&gt; 
   &lt;td&gt;gemma&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/google&quot;&gt;Gemma 3&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1B/4B/12B/27B&lt;/td&gt; 
   &lt;td&gt;gemma3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/THUDM&quot;&gt;GLM-4&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;9B&lt;/td&gt; 
   &lt;td&gt;glm4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/openai-community&quot;&gt;GPT-2&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;0.1B/0.4B/0.8B/1.5B&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/ibm-granite&quot;&gt;Granite 3.0-3.1&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1B/2B/3B/8B&lt;/td&gt; 
   &lt;td&gt;granite3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/IndexTeam&quot;&gt;Index&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1.9B&lt;/td&gt; 
   &lt;td&gt;index&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/tencent/&quot;&gt;Hunyuan&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B&lt;/td&gt; 
   &lt;td&gt;hunyuan&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/internlm&quot;&gt;InternLM 2-3&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B/8B/20B&lt;/td&gt; 
   &lt;td&gt;intern2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/facebookresearch/llama&quot;&gt;Llama&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B/13B/33B/65B&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/meta-llama&quot;&gt;Llama 2&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B/13B/70B&lt;/td&gt; 
   &lt;td&gt;llama2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/meta-llama&quot;&gt;Llama 3-3.3&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1B/3B/8B/70B&lt;/td&gt; 
   &lt;td&gt;llama3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/meta-llama&quot;&gt;Llama 3.2 Vision&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;11B/90B&lt;/td&gt; 
   &lt;td&gt;mllama&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/llava-hf&quot;&gt;LLaVA-1.5&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B/13B&lt;/td&gt; 
   &lt;td&gt;llava&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/llava-hf&quot;&gt;LLaVA-NeXT&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B/8B/13B/34B/72B/110B&lt;/td&gt; 
   &lt;td&gt;llava_next&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/llava-hf&quot;&gt;LLaVA-NeXT-Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B/34B&lt;/td&gt; 
   &lt;td&gt;llava_next_video&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/openbmb&quot;&gt;MiniCPM&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1B/2B/4B&lt;/td&gt; 
   &lt;td&gt;cpm/cpm3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/openbmb&quot;&gt;MiniCPM-o-2.6/MiniCPM-V-2.6&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;8B&lt;/td&gt; 
   &lt;td&gt;minicpm_o/minicpm_v&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/mistralai&quot;&gt;Ministral/Mistral-Nemo&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;8B/12B&lt;/td&gt; 
   &lt;td&gt;ministral&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/mistralai&quot;&gt;Mistral/Mixtral&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B/8x7B/8x22B&lt;/td&gt; 
   &lt;td&gt;mistral&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/mistralai&quot;&gt;Mistral Small&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;24B&lt;/td&gt; 
   &lt;td&gt;mistral_small&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/allenai&quot;&gt;OLMo&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1B/7B&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/google&quot;&gt;PaliGemma/PaliGemma2&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;3B/10B/28B&lt;/td&gt; 
   &lt;td&gt;paligemma&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/microsoft&quot;&gt;Phi-1.5/Phi-2&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1.3B/2.7B&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/microsoft&quot;&gt;Phi-3/Phi-3.5&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;4B/14B&lt;/td&gt; 
   &lt;td&gt;phi&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/microsoft&quot;&gt;Phi-3-small&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B&lt;/td&gt; 
   &lt;td&gt;phi_small&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/microsoft&quot;&gt;Phi-4&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;14B&lt;/td&gt; 
   &lt;td&gt;phi4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/mistralai&quot;&gt;Pixtral&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;12B&lt;/td&gt; 
   &lt;td&gt;pixtral&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/Qwen&quot;&gt;Qwen/QwQ (1-2.5) (Code/Math/MoE)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;0.5B/1.5B/3B/7B/14B/32B/72B/110B&lt;/td&gt; 
   &lt;td&gt;qwen&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/Qwen&quot;&gt;Qwen2-Audio&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B&lt;/td&gt; 
   &lt;td&gt;qwen2_audio&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/Qwen&quot;&gt;Qwen2-VL/Qwen2.5-VL/QVQ&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2B/3B/7B/72B&lt;/td&gt; 
   &lt;td&gt;qwen2_vl&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/Skywork&quot;&gt;Skywork o1&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;8B&lt;/td&gt; 
   &lt;td&gt;skywork_o1&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/bigcode&quot;&gt;StarCoder 2&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;3B/7B/15B&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/Tele-AI&quot;&gt;TeleChat2&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;3B/7B/35B/115B&lt;/td&gt; 
   &lt;td&gt;telechat2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/xverse&quot;&gt;XVERSE&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B/13B/65B&lt;/td&gt; 
   &lt;td&gt;xverse&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/01-ai&quot;&gt;Yi/Yi-1.5 (Code)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1.5B/6B/9B/34B&lt;/td&gt; 
   &lt;td&gt;yi&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/01-ai&quot;&gt;Yi-VL&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;6B/34B&lt;/td&gt; 
   &lt;td&gt;yi_vl&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/IEITYuan&quot;&gt;Yuan 2&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2B/51B/102B&lt;/td&gt; 
   &lt;td&gt;yuan&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] For the &quot;base&quot; models, the &lt;code&gt;template&lt;/code&gt; argument can be chosen from &lt;code&gt;default&lt;/code&gt;, &lt;code&gt;alpaca&lt;/code&gt;, &lt;code&gt;vicuna&lt;/code&gt; etc. But make sure to use the &lt;strong&gt;corresponding template&lt;/strong&gt; for the &quot;instruct/chat&quot; models.&lt;/p&gt; 
 &lt;p&gt;Remember to use the &lt;strong&gt;SAME&lt;/strong&gt; template in training and inference.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Please refer to &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/src/llamafactory/extras/constants.py&quot;&gt;constants.py&lt;/a&gt; for a full list of models we supported.&lt;/p&gt; 
&lt;p&gt;You also can add a custom chat template to &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/src/llamafactory/data/template.py&quot;&gt;template.py&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Supported Training Approaches&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Approach&lt;/th&gt; 
   &lt;th&gt;Full-tuning&lt;/th&gt; 
   &lt;th&gt;Freeze-tuning&lt;/th&gt; 
   &lt;th&gt;LoRA&lt;/th&gt; 
   &lt;th&gt;QLoRA&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Pre-Training&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Supervised Fine-Tuning&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Reward Modeling&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;PPO Training&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;DPO Training&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;KTO Training&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ORPO Training&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;SimPO Training&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] The implementation details of PPO can be found in &lt;a href=&quot;https://newfacade.github.io/notes-on-reinforcement-learning/17-ppo-trl.html&quot;&gt;this blog&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Provided Datasets&lt;/h2&gt; 
&lt;details&gt;
 &lt;summary&gt;Pre-training datasets&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/data/wiki_demo.txt&quot;&gt;Wiki Demo (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/tiiuae/falcon-refinedweb&quot;&gt;RefinedWeb (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/togethercomputer/RedPajama-Data-V2&quot;&gt;RedPajama V2 (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/olm/olm-wikipedia-20221220&quot;&gt;Wikipedia (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/pleisto/wikipedia-cn-20230720-filtered&quot;&gt;Wikipedia (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/EleutherAI/pile&quot;&gt;Pile (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/Skywork/SkyPile-150B&quot;&gt;SkyPile (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/HuggingFaceFW/fineweb&quot;&gt;FineWeb (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu&quot;&gt;FineWeb-Edu (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/bigcode/the-stack&quot;&gt;The Stack (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/bigcode/starcoderdata&quot;&gt;StarCoder (en)&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;Supervised fine-tuning datasets&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/data/identity.json&quot;&gt;Identity (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/tatsu-lab/stanford_alpaca&quot;&gt;Stanford Alpaca (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/ymcui/Chinese-LLaMA-Alpaca-3&quot;&gt;Stanford Alpaca (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM&quot;&gt;Alpaca GPT4 (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/glaiveai/glaive-function-calling-v2&quot;&gt;Glaive Function Calling V2 (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/GAIR/lima&quot;&gt;LIMA (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/JosephusCheung/GuanacoDataset&quot;&gt;Guanaco Dataset (multilingual)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/BelleGroup/train_2M_CN&quot;&gt;BELLE 2M (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/BelleGroup/train_1M_CN&quot;&gt;BELLE 1M (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/BelleGroup/train_0.5M_CN&quot;&gt;BELLE 0.5M (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/BelleGroup/generated_chat_0.4M&quot;&gt;BELLE Dialogue 0.4M (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/BelleGroup/school_math_0.25M&quot;&gt;BELLE School Math 0.25M (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/BelleGroup/multiturn_chat_0.8M&quot;&gt;BELLE Multiturn Chat 0.8M (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/thunlp/UltraChat&quot;&gt;UltraChat (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/garage-bAInd/Open-Platypus&quot;&gt;OpenPlatypus (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k&quot;&gt;CodeAlpaca 20k (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/QingyiSi/Alpaca-CoT&quot;&gt;Alpaca CoT (multilingual)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/Open-Orca/OpenOrca&quot;&gt;OpenOrca (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/Open-Orca/SlimOrca&quot;&gt;SlimOrca (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/TIGER-Lab/MathInstruct&quot;&gt;MathInstruct (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/YeungNLP/firefly-train-1.1M&quot;&gt;Firefly 1.1M (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/wiki_qa&quot;&gt;Wiki QA (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/suolyer/webqa&quot;&gt;Web QA (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/zxbsmk/webnovel_cn&quot;&gt;WebNovel (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/berkeley-nest/Nectar&quot;&gt;Nectar (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://www.modelscope.cn/datasets/deepctrl/deepctrl-sft-data&quot;&gt;deepctrl (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/HasturOfficial/adgen&quot;&gt;Advertise Generating (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/totally-not-an-llm/sharegpt-hyperfiltered-3k&quot;&gt;ShareGPT Hyperfiltered (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/shibing624/sharegpt_gpt4&quot;&gt;ShareGPT4 (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k&quot;&gt;UltraChat 200k (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/THUDM/AgentInstruct&quot;&gt;AgentInstruct (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/lmsys/lmsys-chat-1m&quot;&gt;LMSYS Chat 1M (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/WizardLM/WizardLM_evol_instruct_V2_196k&quot;&gt;Evol Instruct V2 (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/HuggingFaceTB/cosmopedia&quot;&gt;Cosmopedia (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/hfl/stem_zh_instruction&quot;&gt;STEM (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/hfl/ruozhiba_gpt4_turbo&quot;&gt;Ruozhiba (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/m-a-p/neo_sft_phase2&quot;&gt;Neo-sft (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/Magpie-Align/Magpie-Pro-300K-Filtered&quot;&gt;Magpie-Pro-300K-Filtered (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/argilla/magpie-ultra-v0.1&quot;&gt;Magpie-ultra-v0.1 (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/TIGER-Lab/WebInstructSub&quot;&gt;WebInstructSub (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/O1-OPEN/OpenO1-SFT&quot;&gt;OpenO1-SFT (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k&quot;&gt;Open-Thoughts (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/open-r1/OpenR1-Math-220k&quot;&gt;Open-R1-Math (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/Congliu/Chinese-DeepSeek-R1-Distill-data-110k-SFT&quot;&gt;Chinese-DeepSeek-R1-Distill (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/BUAADreamer/llava-en-zh-300k&quot;&gt;LLaVA mixed (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/jugg1024/pokemon-gpt4o-captions&quot;&gt;Pokemon-gpt4o-captions (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/mayflowergmbh/oasst_de&quot;&gt;Open Assistant (de)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/mayflowergmbh/dolly-15k_de&quot;&gt;Dolly 15k (de)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/mayflowergmbh/alpaca-gpt4_de&quot;&gt;Alpaca GPT4 (de)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/mayflowergmbh/openschnabeltier_de&quot;&gt;OpenSchnabeltier (de)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/mayflowergmbh/evol-instruct_de&quot;&gt;Evol Instruct (de)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/mayflowergmbh/dolphin_de&quot;&gt;Dolphin (de)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/mayflowergmbh/booksum_de&quot;&gt;Booksum (de)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/mayflowergmbh/airoboros-3.0_de&quot;&gt;Airoboros (de)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/mayflowergmbh/ultra-chat_de&quot;&gt;Ultrachat (de)&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;Preference datasets&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/hiyouga/DPO-En-Zh-20k&quot;&gt;DPO mixed (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized&quot;&gt;UltraFeedback (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/openbmb/RLHF-V-Dataset&quot;&gt;RLHF-V (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/Zhihui/VLFeedback&quot;&gt;VLFeedback (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/Intel/orca_dpo_pairs&quot;&gt;Orca DPO Pairs (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/Anthropic/hh-rlhf&quot;&gt;HH-RLHF (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/berkeley-nest/Nectar&quot;&gt;Nectar (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/mayflowergmbh/intel_orca_dpo_pairs_de&quot;&gt;Orca DPO (de)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/argilla/kto-mix-15k&quot;&gt;KTO mixed (en)&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;p&gt;Some datasets require confirmation before using them, so we recommend logging in with your Hugging Face account using these commands.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install --upgrade huggingface_hub
huggingface-cli login
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Requirement&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Mandatory&lt;/th&gt; 
   &lt;th&gt;Minimum&lt;/th&gt; 
   &lt;th&gt;Recommend&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;python&lt;/td&gt; 
   &lt;td&gt;3.9&lt;/td&gt; 
   &lt;td&gt;3.10&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;torch&lt;/td&gt; 
   &lt;td&gt;1.13.1&lt;/td&gt; 
   &lt;td&gt;2.5.1&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;transformers&lt;/td&gt; 
   &lt;td&gt;4.41.2&lt;/td&gt; 
   &lt;td&gt;4.49.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;datasets&lt;/td&gt; 
   &lt;td&gt;2.16.0&lt;/td&gt; 
   &lt;td&gt;3.2.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;accelerate&lt;/td&gt; 
   &lt;td&gt;0.34.0&lt;/td&gt; 
   &lt;td&gt;1.2.1&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;peft&lt;/td&gt; 
   &lt;td&gt;0.11.1&lt;/td&gt; 
   &lt;td&gt;0.12.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;trl&lt;/td&gt; 
   &lt;td&gt;0.8.6&lt;/td&gt; 
   &lt;td&gt;0.9.6&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Optional&lt;/th&gt; 
   &lt;th&gt;Minimum&lt;/th&gt; 
   &lt;th&gt;Recommend&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;CUDA&lt;/td&gt; 
   &lt;td&gt;11.6&lt;/td&gt; 
   &lt;td&gt;12.2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;deepspeed&lt;/td&gt; 
   &lt;td&gt;0.10.0&lt;/td&gt; 
   &lt;td&gt;0.16.4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;bitsandbytes&lt;/td&gt; 
   &lt;td&gt;0.39.0&lt;/td&gt; 
   &lt;td&gt;0.43.1&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;vllm&lt;/td&gt; 
   &lt;td&gt;0.4.3&lt;/td&gt; 
   &lt;td&gt;0.7.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;flash-attn&lt;/td&gt; 
   &lt;td&gt;2.3.0&lt;/td&gt; 
   &lt;td&gt;2.7.2&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Hardware Requirement&lt;/h3&gt; 
&lt;p&gt;* &lt;em&gt;estimated&lt;/em&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Method&lt;/th&gt; 
   &lt;th&gt;Bits&lt;/th&gt; 
   &lt;th&gt;7B&lt;/th&gt; 
   &lt;th&gt;14B&lt;/th&gt; 
   &lt;th&gt;30B&lt;/th&gt; 
   &lt;th&gt;70B&lt;/th&gt; 
   &lt;th&gt;&lt;code&gt;x&lt;/code&gt;B&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Full (&lt;code&gt;bf16&lt;/code&gt; or &lt;code&gt;fp16&lt;/code&gt;)&lt;/td&gt; 
   &lt;td&gt;32&lt;/td&gt; 
   &lt;td&gt;120GB&lt;/td&gt; 
   &lt;td&gt;240GB&lt;/td&gt; 
   &lt;td&gt;600GB&lt;/td&gt; 
   &lt;td&gt;1200GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;18x&lt;/code&gt;GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Full (&lt;code&gt;pure_bf16&lt;/code&gt;)&lt;/td&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;60GB&lt;/td&gt; 
   &lt;td&gt;120GB&lt;/td&gt; 
   &lt;td&gt;300GB&lt;/td&gt; 
   &lt;td&gt;600GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;8x&lt;/code&gt;GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Freeze/LoRA/GaLore/APOLLO/BAdam&lt;/td&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;16GB&lt;/td&gt; 
   &lt;td&gt;32GB&lt;/td&gt; 
   &lt;td&gt;64GB&lt;/td&gt; 
   &lt;td&gt;160GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;2x&lt;/code&gt;GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;QLoRA&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;10GB&lt;/td&gt; 
   &lt;td&gt;20GB&lt;/td&gt; 
   &lt;td&gt;40GB&lt;/td&gt; 
   &lt;td&gt;80GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;x&lt;/code&gt;GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;QLoRA&lt;/td&gt; 
   &lt;td&gt;4&lt;/td&gt; 
   &lt;td&gt;6GB&lt;/td&gt; 
   &lt;td&gt;12GB&lt;/td&gt; 
   &lt;td&gt;24GB&lt;/td&gt; 
   &lt;td&gt;48GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;x/2&lt;/code&gt;GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;QLoRA&lt;/td&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;4GB&lt;/td&gt; 
   &lt;td&gt;8GB&lt;/td&gt; 
   &lt;td&gt;16GB&lt;/td&gt; 
   &lt;td&gt;24GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;x/4&lt;/code&gt;GB&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] Installation is mandatory.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
pip install -e &quot;.[torch,metrics]&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Extra dependencies available: torch, torch-npu, metrics, deepspeed, liger-kernel, bitsandbytes, hqq, eetq, gptq, awq, aqlm, vllm, sglang, galore, apollo, badam, adam-mini, qwen, minicpm_v, modelscope, openmind, swanlab, quality&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] Use &lt;code&gt;pip install --no-deps -e .&lt;/code&gt; to resolve package conflicts.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;details&gt;
 &lt;summary&gt;Setting up a virtual environment with &lt;b&gt;uv&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;Create an isolated Python environment with &lt;a href=&quot;https://github.com/astral-sh/uv&quot;&gt;uv&lt;/a&gt;:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;uv sync --extra torch --extra metrics --prerelease=allow
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Run LLaMA-Factory in the isolated environment:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;uv run --prerelease=allow llamafactory-cli train examples/train_lora/llama3_lora_pretrain.yaml
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;For Windows users&lt;/summary&gt; 
 &lt;h4&gt;Install BitsAndBytes&lt;/h4&gt; 
 &lt;p&gt;If you want to enable the quantized LoRA (QLoRA) on the Windows platform, you need to install a pre-built version of &lt;code&gt;bitsandbytes&lt;/code&gt; library, which supports CUDA 11.1 to 12.2, please select the appropriate &lt;a href=&quot;https://github.com/jllllll/bitsandbytes-windows-webui/releases/tag/wheels&quot;&gt;release version&lt;/a&gt; based on your CUDA version.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.2.post2-py3-none-win_amd64.whl
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Install Flash Attention-2&lt;/h4&gt; 
 &lt;p&gt;To enable FlashAttention-2 on the Windows platform, please use the script from &lt;a href=&quot;https://huggingface.co/lldacing/flash-attention-windows-wheel&quot;&gt;flash-attention-windows-wheel&lt;/a&gt; to compile and install it by yourself.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;For Ascend NPU users&lt;/summary&gt; 
 &lt;p&gt;To install LLaMA Factory on Ascend NPU devices, please upgrade Python to version 3.10 or higher and specify extra dependencies: &lt;code&gt;pip install -e &quot;.[torch-npu,metrics]&quot;&lt;/code&gt;. Additionally, you need to install the &lt;strong&gt;&lt;a href=&quot;https://www.hiascend.com/developer/download/community/result?module=cann&quot;&gt;Ascend CANN Toolkit and Kernels&lt;/a&gt;&lt;/strong&gt;. Please follow the &lt;a href=&quot;https://www.hiascend.com/document/detail/en/CANNCommunityEdition/600alphaX/softwareinstall/instg/atlasdeploy_03_0031.html&quot;&gt;installation tutorial&lt;/a&gt; or use the following commands:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# replace the url according to your CANN version and devices
# install CANN Toolkit
wget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C20SPC702/Ascend-cann-toolkit_8.0.0.alpha002_linux-&quot;$(uname -i)&quot;.run
bash Ascend-cann-toolkit_8.0.0.alpha002_linux-&quot;$(uname -i)&quot;.run --install

# install CANN Kernels
wget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C20SPC702/Ascend-cann-kernels-910b_8.0.0.alpha002_linux-&quot;$(uname -i)&quot;.run
bash Ascend-cann-kernels-910b_8.0.0.alpha002_linux-&quot;$(uname -i)&quot;.run --install

# set env variables
source /usr/local/Ascend/ascend-toolkit/set_env.sh
&lt;/code&gt;&lt;/pre&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Requirement&lt;/th&gt; 
    &lt;th&gt;Minimum&lt;/th&gt; 
    &lt;th&gt;Recommend&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CANN&lt;/td&gt; 
    &lt;td&gt;8.0.RC1&lt;/td&gt; 
    &lt;td&gt;8.0.0.alpha002&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;torch&lt;/td&gt; 
    &lt;td&gt;2.1.0&lt;/td&gt; 
    &lt;td&gt;2.4.0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;torch-npu&lt;/td&gt; 
    &lt;td&gt;2.1.0&lt;/td&gt; 
    &lt;td&gt;2.4.0.post2&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;deepspeed&lt;/td&gt; 
    &lt;td&gt;0.13.2&lt;/td&gt; 
    &lt;td&gt;0.13.2&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;p&gt;Remember to use &lt;code&gt;ASCEND_RT_VISIBLE_DEVICES&lt;/code&gt; instead of &lt;code&gt;CUDA_VISIBLE_DEVICES&lt;/code&gt; to specify the device to use.&lt;/p&gt; 
 &lt;p&gt;If you cannot infer model on NPU devices, try setting &lt;code&gt;do_sample: false&lt;/code&gt; in the configurations.&lt;/p&gt; 
 &lt;p&gt;Download the pre-built Docker images: &lt;a href=&quot;http://mirrors.cn-central-221.ovaijisuan.com/detail/130.html&quot;&gt;32GB&lt;/a&gt; | &lt;a href=&quot;http://mirrors.cn-central-221.ovaijisuan.com/detail/131.html&quot;&gt;64GB&lt;/a&gt;&lt;/p&gt; 
 &lt;h4&gt;Install BitsAndBytes&lt;/h4&gt; 
 &lt;p&gt;To use QLoRA based on bitsandbytes on Ascend NPU, please follow these 3 steps:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Manually compile bitsandbytes: Refer to &lt;a href=&quot;https://huggingface.co/docs/bitsandbytes/installation?backend=Ascend+NPU&amp;amp;platform=Ascend+NPU&quot;&gt;the installation documentation&lt;/a&gt; for the NPU version of bitsandbytes to complete the compilation and installation. The compilation requires a cmake version of at least 3.22.1 and a g++ version of at least 12.x.&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# Install bitsandbytes from source
# Clone bitsandbytes repo, Ascend NPU backend is currently enabled on multi-backend-refactor branch
git clone -b multi-backend-refactor https://github.com/bitsandbytes-foundation/bitsandbytes.git
cd bitsandbytes/

# Install dependencies
pip install -r requirements-dev.txt

# Install the dependencies for the compilation tools. Note that the commands for this step may vary depending on the operating system. The following are provided for reference
apt-get install -y build-essential cmake

# Compile &amp;amp; install  
cmake -DCOMPUTE_BACKEND=npu -S .
make
pip install .
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start=&quot;2&quot;&gt; 
  &lt;li&gt;Install transformers from the main branch.&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;git clone -b main https://github.com/huggingface/transformers.git
cd transformers
pip install .
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start=&quot;3&quot;&gt; 
  &lt;li&gt;Set &lt;code&gt;double_quantization: false&lt;/code&gt; in the configuration. You can refer to the &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/train_qlora/llama3_lora_sft_bnb_npu.yaml&quot;&gt;example&lt;/a&gt;.&lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/details&gt; 
&lt;h3&gt;Data Preparation&lt;/h3&gt; 
&lt;p&gt;Please refer to &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/data/README.md&quot;&gt;data/README.md&lt;/a&gt; for checking the details about the format of dataset files. You can either use datasets on HuggingFace / ModelScope / Modelers hub or load the dataset in local disk.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Please update &lt;code&gt;data/dataset_info.json&lt;/code&gt; to use your custom dataset.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Quickstart&lt;/h3&gt; 
&lt;p&gt;Use the following 3 commands to run LoRA &lt;strong&gt;fine-tuning&lt;/strong&gt;, &lt;strong&gt;inference&lt;/strong&gt; and &lt;strong&gt;merging&lt;/strong&gt; of the Llama3-8B-Instruct model, respectively.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;llamafactory-cli train examples/train_lora/llama3_lora_sft.yaml
llamafactory-cli chat examples/inference/llama3_lora_sft.yaml
llamafactory-cli export examples/merge_lora/llama3_lora_sft.yaml
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&quot;&gt;examples/README.md&lt;/a&gt; for advanced usage (including distributed training).&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] Use &lt;code&gt;llamafactory-cli help&lt;/code&gt; to show help information.&lt;/p&gt; 
 &lt;p&gt;Read &lt;a href=&quot;https://github.com/hiyouga/LLaMA-Factory/issues/4614&quot;&gt;FAQs&lt;/a&gt; first if you encounter any problems.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Fine-Tuning with LLaMA Board GUI (powered by &lt;a href=&quot;https://github.com/gradio-app/gradio&quot;&gt;Gradio&lt;/a&gt;)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;llamafactory-cli webui
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Build Docker&lt;/h3&gt; 
&lt;p&gt;For CUDA users:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;cd docker/docker-cuda/
docker compose up -d
docker compose exec llamafactory bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For Ascend NPU users:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;cd docker/docker-npu/
docker compose up -d
docker compose exec llamafactory bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For AMD ROCm users:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;cd docker/docker-rocm/
docker compose up -d
docker compose exec llamafactory bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt;
 &lt;summary&gt;Build without Docker Compose&lt;/summary&gt; 
 &lt;p&gt;For CUDA users:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;docker build -f ./docker/docker-cuda/Dockerfile \
    --build-arg INSTALL_BNB=false \
    --build-arg INSTALL_VLLM=false \
    --build-arg INSTALL_DEEPSPEED=false \
    --build-arg INSTALL_FLASHATTN=false \
    --build-arg PIP_INDEX=https://pypi.org/simple \
    -t llamafactory:latest .

docker run -dit --gpus=all \
    -v ./hf_cache:/root/.cache/huggingface \
    -v ./ms_cache:/root/.cache/modelscope \
    -v ./om_cache:/root/.cache/openmind \
    -v ./data:/app/data \
    -v ./output:/app/output \
    -p 7860:7860 \
    -p 8000:8000 \
    --shm-size 16G \
    --name llamafactory \
    llamafactory:latest

docker exec -it llamafactory bash
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;For Ascend NPU users:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# Choose docker image upon your environment
docker build -f ./docker/docker-npu/Dockerfile \
    --build-arg INSTALL_DEEPSPEED=false \
    --build-arg PIP_INDEX=https://pypi.org/simple \
    -t llamafactory:latest .

# Change `device` upon your resources
docker run -dit \
    -v ./hf_cache:/root/.cache/huggingface \
    -v ./ms_cache:/root/.cache/modelscope \
    -v ./om_cache:/root/.cache/openmind \
    -v ./data:/app/data \
    -v ./output:/app/output \
    -v /usr/local/dcmi:/usr/local/dcmi \
    -v /usr/local/bin/npu-smi:/usr/local/bin/npu-smi \
    -v /usr/local/Ascend/driver:/usr/local/Ascend/driver \
    -v /etc/ascend_install.info:/etc/ascend_install.info \
    -p 7860:7860 \
    -p 8000:8000 \
    --device /dev/davinci0 \
    --device /dev/davinci_manager \
    --device /dev/devmm_svm \
    --device /dev/hisi_hdc \
    --shm-size 16G \
    --name llamafactory \
    llamafactory:latest

docker exec -it llamafactory bash
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;For AMD ROCm users:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;docker build -f ./docker/docker-rocm/Dockerfile \
    --build-arg INSTALL_BNB=false \
    --build-arg INSTALL_VLLM=false \
    --build-arg INSTALL_DEEPSPEED=false \
    --build-arg INSTALL_FLASHATTN=false \
    --build-arg PIP_INDEX=https://pypi.org/simple \
    -t llamafactory:latest .

docker run -dit \
    -v ./hf_cache:/root/.cache/huggingface \
    -v ./ms_cache:/root/.cache/modelscope \
    -v ./om_cache:/root/.cache/openmind \
    -v ./data:/app/data \
    -v ./output:/app/output \
    -v ./saves:/app/saves \
    -p 7860:7860 \
    -p 8000:8000 \
    --device /dev/kfd \
    --device /dev/dri \
    --shm-size 16G \
    --name llamafactory \
    llamafactory:latest

docker exec -it llamafactory bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;Details about volume&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;hf_cache&lt;/code&gt;: Utilize Hugging Face cache on the host machine. Reassignable if a cache already exists in a different directory.&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;ms_cache&lt;/code&gt;: Similar to Hugging Face cache but for ModelScope users.&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;om_cache&lt;/code&gt;: Similar to Hugging Face cache but for Modelers users.&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;data&lt;/code&gt;: Place datasets on this dir of the host machine so that they can be selected on LLaMA Board GUI.&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;output&lt;/code&gt;: Set export dir to this location so that the merged result can be accessed directly on the host machine.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;Deploy with OpenAI-style API and vLLM&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;API_PORT=8000 llamafactory-cli api examples/inference/llama3_vllm.yaml
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] Visit &lt;a href=&quot;https://platform.openai.com/docs/api-reference/chat/create&quot;&gt;this page&lt;/a&gt; for API document.&lt;/p&gt; 
 &lt;p&gt;Examples: &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/scripts/api_example/test_image.py&quot;&gt;Image understanding&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/scripts/api_example/test_toolcall.py&quot;&gt;Function calling&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Download from ModelScope Hub&lt;/h3&gt; 
&lt;p&gt;If you have trouble with downloading models and datasets from Hugging Face, you can use ModelScope.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;export USE_MODELSCOPE_HUB=1 # `set USE_MODELSCOPE_HUB=1` for Windows
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Train the model by specifying a model ID of the ModelScope Hub as the &lt;code&gt;model_name_or_path&lt;/code&gt;. You can find a full list of model IDs at &lt;a href=&quot;https://modelscope.cn/models&quot;&gt;ModelScope Hub&lt;/a&gt;, e.g., &lt;code&gt;LLM-Research/Meta-Llama-3-8B-Instruct&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Download from Modelers Hub&lt;/h3&gt; 
&lt;p&gt;You can also use Modelers Hub to download models and datasets.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;export USE_OPENMIND_HUB=1 # `set USE_OPENMIND_HUB=1` for Windows
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Train the model by specifying a model ID of the Modelers Hub as the &lt;code&gt;model_name_or_path&lt;/code&gt;. You can find a full list of model IDs at &lt;a href=&quot;https://modelers.cn/models&quot;&gt;Modelers Hub&lt;/a&gt;, e.g., &lt;code&gt;TeleAI/TeleChat-7B-pt&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Use W&amp;amp;B Logger&lt;/h3&gt; 
&lt;p&gt;To use &lt;a href=&quot;https://wandb.ai&quot;&gt;Weights &amp;amp; Biases&lt;/a&gt; for logging experimental results, you need to add the following arguments to yaml files.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-yaml&quot;&gt;report_to: wandb
run_name: test_run # optional
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Set &lt;code&gt;WANDB_API_KEY&lt;/code&gt; to &lt;a href=&quot;https://wandb.ai/authorize&quot;&gt;your key&lt;/a&gt; when launching training tasks to log in with your W&amp;amp;B account.&lt;/p&gt; 
&lt;h3&gt;Use SwanLab Logger&lt;/h3&gt; 
&lt;p&gt;To use &lt;a href=&quot;https://github.com/SwanHubX/SwanLab&quot;&gt;SwanLab&lt;/a&gt; for logging experimental results, you need to add the following arguments to yaml files.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-yaml&quot;&gt;use_swanlab: true
swanlab_run_name: test_run # optional
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;When launching training tasks, you can log in to SwanLab in three ways:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Add &lt;code&gt;swanlab_api_key=&amp;lt;your_api_key&amp;gt;&lt;/code&gt; to the yaml file, and set it to your &lt;a href=&quot;https://swanlab.cn/settings&quot;&gt;API key&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Set the environment variable &lt;code&gt;SWANLAB_API_KEY&lt;/code&gt; to your &lt;a href=&quot;https://swanlab.cn/settings&quot;&gt;API key&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Use the &lt;code&gt;swanlab login&lt;/code&gt; command to complete the login.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Projects using LLaMA Factory&lt;/h2&gt; 
&lt;p&gt;If you have a project that should be incorporated, please contact via email or create a pull request.&lt;/p&gt; 
&lt;details&gt;
 &lt;summary&gt;Click to show&lt;/summary&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Wang et al. ESRL: Efficient Sampling-based Reinforcement Learning for Sequence Generation. 2023. &lt;a href=&quot;https://arxiv.org/abs/2308.02223&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Yu et al. Open, Closed, or Small Language Models for Text Classification? 2023. &lt;a href=&quot;https://arxiv.org/abs/2308.10092&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Wang et al. UbiPhysio: Support Daily Functioning, Fitness, and Rehabilitation with Action Understanding and Feedback in Natural Language. 2023. &lt;a href=&quot;https://arxiv.org/abs/2308.10526&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Luceri et al. Leveraging Large Language Models to Detect Influence Campaigns in Social Media. 2023. &lt;a href=&quot;https://arxiv.org/abs/2311.07816&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zhang et al. Alleviating Hallucinations of Large Language Models through Induced Hallucinations. 2023. &lt;a href=&quot;https://arxiv.org/abs/2312.15710&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Wang et al. Know Your Needs Better: Towards Structured Understanding of Marketer Demands with Analogical Reasoning Augmented LLMs. KDD 2024. &lt;a href=&quot;https://arxiv.org/abs/2401.04319&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Wang et al. CANDLE: Iterative Conceptualization and Instantiation Distillation from Large Language Models for Commonsense Reasoning. ACL 2024. &lt;a href=&quot;https://arxiv.org/abs/2401.07286&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Choi et al. FACT-GPT: Fact-Checking Augmentation via Claim Matching with LLMs. 2024. &lt;a href=&quot;https://arxiv.org/abs/2402.05904&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zhang et al. AutoMathText: Autonomous Data Selection with Language Models for Mathematical Texts. 2024. &lt;a href=&quot;https://arxiv.org/abs/2402.07625&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Lyu et al. KnowTuning: Knowledge-aware Fine-tuning for Large Language Models. 2024. &lt;a href=&quot;https://arxiv.org/abs/2402.11176&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Yang et al. LaCo: Large Language Model Pruning via Layer Collaps. 2024. &lt;a href=&quot;https://arxiv.org/abs/2402.11187&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Bhardwaj et al. Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic. 2024. &lt;a href=&quot;https://arxiv.org/abs/2402.11746&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Yang et al. Enhancing Empathetic Response Generation by Augmenting LLMs with Small-scale Empathetic Models. 2024. &lt;a href=&quot;https://arxiv.org/abs/2402.11801&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Yi et al. Generation Meets Verification: Accelerating Large Language Model Inference with Smart Parallel Auto-Correct Decoding. ACL 2024 Findings. &lt;a href=&quot;https://arxiv.org/abs/2402.11809&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Cao et al. Head-wise Shareable Attention for Large Language Models. 2024. &lt;a href=&quot;https://arxiv.org/abs/2402.11819&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zhang et al. Enhancing Multilingual Capabilities of Large Language Models through Self-Distillation from Resource-Rich Languages. 2024. &lt;a href=&quot;https://arxiv.org/abs/2402.12204&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Kim et al. Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models. 2024. &lt;a href=&quot;https://arxiv.org/abs/2402.14714&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Yu et al. KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models. ACL 2024. &lt;a href=&quot;https://arxiv.org/abs/2402.15043&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Huang et al. Key-Point-Driven Data Synthesis with its Enhancement on Mathematical Reasoning. 2024. &lt;a href=&quot;https://arxiv.org/abs/2403.02333&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Duan et al. Negating Negatives: Alignment without Human Positive Samples via Distributional Dispreference Optimization. 2024. &lt;a href=&quot;https://arxiv.org/abs/2403.03419&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Xie and Schwertfeger. Empowering Robotics with Large Language Models: osmAG Map Comprehension with LLMs. 2024. &lt;a href=&quot;https://arxiv.org/abs/2403.08228&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Wu et al. Large Language Models are Parallel Multilingual Learners. 2024. &lt;a href=&quot;https://arxiv.org/abs/2403.09073&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zhang et al. EDT: Improving Large Language Models&#39; Generation by Entropy-based Dynamic Temperature Sampling. 2024. &lt;a href=&quot;https://arxiv.org/abs/2403.14541&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Weller et al. FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions. 2024. &lt;a href=&quot;https://arxiv.org/abs/2403.15246&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Hongbin Na. CBT-LLM: A Chinese Large Language Model for Cognitive Behavioral Therapy-based Mental Health Question Answering. COLING 2024. &lt;a href=&quot;https://arxiv.org/abs/2403.16008&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zan et al. CodeS: Natural Language to Code Repository via Multi-Layer Sketch. 2024. &lt;a href=&quot;https://arxiv.org/abs/2403.16443&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Liu et al. Extensive Self-Contrast Enables Feedback-Free Language Model Alignment. 2024. &lt;a href=&quot;https://arxiv.org/abs/2404.00604&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Luo et al. BAdam: A Memory Efficient Full Parameter Training Method for Large Language Models. 2024. &lt;a href=&quot;https://arxiv.org/abs/2404.02827&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Du et al. Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model. 2024. &lt;a href=&quot;https://arxiv.org/abs/2404.04167&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Ma et al. Parameter Efficient Quasi-Orthogonal Fine-Tuning via Givens Rotation. ICML 2024. &lt;a href=&quot;https://arxiv.org/abs/2404.04316&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Liu et al. Dynamic Generation of Personalities with Large Language Models. 2024. &lt;a href=&quot;https://arxiv.org/abs/2404.07084&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Shang et al. How Far Have We Gone in Stripped Binary Code Understanding Using Large Language Models. 2024. &lt;a href=&quot;https://arxiv.org/abs/2404.09836&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Huang et al. LLMTune: Accelerate Database Knob Tuning with Large Language Models. 2024. &lt;a href=&quot;https://arxiv.org/abs/2404.11581&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Deng et al. Text-Tuple-Table: Towards Information Integration in Text-to-Table Generation via Global Tuple Extraction. 2024. &lt;a href=&quot;https://arxiv.org/abs/2404.14215&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Acikgoz et al. Hippocrates: An Open-Source Framework for Advancing Large Language Models in Healthcare. 2024. &lt;a href=&quot;https://arxiv.org/abs/2404.16621&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zhang et al. Small Language Models Need Strong Verifiers to Self-Correct Reasoning. ACL 2024 Findings. &lt;a href=&quot;https://arxiv.org/abs/2404.17140&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zhou et al. FREB-TQA: A Fine-Grained Robustness Evaluation Benchmark for Table Question Answering. NAACL 2024. &lt;a href=&quot;https://arxiv.org/abs/2404.18585&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Xu et al. Large Language Models for Cyber Security: A Systematic Literature Review. 2024. &lt;a href=&quot;https://arxiv.org/abs/2405.04760&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Dammu et al. &quot;They are uncultured&quot;: Unveiling Covert Harms and Social Threats in LLM Generated Conversations. 2024. &lt;a href=&quot;https://arxiv.org/abs/2405.05378&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Yi et al. A safety realignment framework via subspace-oriented model fusion for large language models. 2024. &lt;a href=&quot;https://arxiv.org/abs/2405.09055&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Lou et al. SPO: Multi-Dimensional Preference Sequential Alignment With Implicit Reward Modeling. 2024. &lt;a href=&quot;https://arxiv.org/abs/2405.12739&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zhang et al. Getting More from Less: Large Language Models are Good Spontaneous Multilingual Learners. 2024. &lt;a href=&quot;https://arxiv.org/abs/2405.13816&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zhang et al. TS-Align: A Teacher-Student Collaborative Framework for Scalable Iterative Finetuning of Large Language Models. 2024. &lt;a href=&quot;https://arxiv.org/abs/2405.20215&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zihong Chen. Sentence Segmentation and Sentence Punctuation Based on XunziALLM. 2024. &lt;a href=&quot;https://aclanthology.org/2024.lt4hala-1.30&quot;&gt;[paper]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Gao et al. The Best of Both Worlds: Toward an Honest and Helpful Large Language Model. 2024. &lt;a href=&quot;https://arxiv.org/abs/2406.00380&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Wang and Song. MARS: Benchmarking the Metaphysical Reasoning Abilities of Language Models with a Multi-task Evaluation Dataset. 2024. &lt;a href=&quot;https://arxiv.org/abs/2406.02106&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Hu et al. Computational Limits of Low-Rank Adaptation (LoRA) for Transformer-Based Models. 2024. &lt;a href=&quot;https://arxiv.org/abs/2406.03136&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Ge et al. Time Sensitive Knowledge Editing through Efficient Finetuning. ACL 2024. &lt;a href=&quot;https://arxiv.org/abs/2406.04496&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Tan et al. Peer Review as A Multi-Turn and Long-Context Dialogue with Role-Based Interactions. 2024. &lt;a href=&quot;https://arxiv.org/abs/2406.05688&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Song et al. Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters. 2024. &lt;a href=&quot;https://arxiv.org/abs/2406.05955&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Gu et al. RWKV-CLIP: A Robust Vision-Language Representation Learner. 2024. &lt;a href=&quot;https://arxiv.org/abs/2406.06973&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Chen et al. Advancing Tool-Augmented Large Language Models: Integrating Insights from Errors in Inference Trees. 2024. &lt;a href=&quot;https://arxiv.org/abs/2406.07115&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zhu et al. Are Large Language Models Good Statisticians?. 2024. &lt;a href=&quot;https://arxiv.org/abs/2406.07815&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Li et al. Know the Unknown: An Uncertainty-Sensitive Method for LLM Instruction Tuning. 2024. &lt;a href=&quot;https://arxiv.org/abs/2406.10099&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Ding et al. IntentionQA: A Benchmark for Evaluating Purchase Intention Comprehension Abilities of Language Models in E-commerce. 2024. &lt;a href=&quot;https://arxiv.org/abs/2406.10173&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;He et al. COMMUNITY-CROSS-INSTRUCT: Unsupervised Instruction Generation for Aligning Large Language Models to Online Communities. 2024. &lt;a href=&quot;https://arxiv.org/abs/2406.12074&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Lin et al. FVEL: Interactive Formal Verification Environment with Large Language Models via Theorem Proving. 2024. &lt;a href=&quot;https://arxiv.org/abs/2406.14408&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Treutlein et al. Connecting the Dots: LLMs can Infer and Verbalize Latent Structure from Disparate Training Data. 2024. &lt;a href=&quot;https://arxiv.org/abs/2406.14546&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Feng et al. SS-Bench: A Benchmark for Social Story Generation and Evaluation. 2024. &lt;a href=&quot;https://arxiv.org/abs/2406.15695&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Feng et al. Self-Constructed Context Decompilation with Fined-grained Alignment Enhancement. 2024. &lt;a href=&quot;https://arxiv.org/abs/2406.17233&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Liu et al. Large Language Models for Cuffless Blood Pressure Measurement From Wearable Biosignals. 2024. &lt;a href=&quot;https://arxiv.org/abs/2406.18069&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Iyer et al. Exploring Very Low-Resource Translation with LLMs: The University of Edinburgh&#39;s Submission to AmericasNLP 2024 Translation Task. AmericasNLP 2024. &lt;a href=&quot;https://aclanthology.org/2024.americasnlp-1.25&quot;&gt;[paper]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Li et al. Calibrating LLMs with Preference Optimization on Thought Trees for Generating Rationale in Science Question Scoring. 2024. &lt;a href=&quot;https://arxiv.org/abs/2406.19949&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Yang et al. Financial Knowledge Large Language Model. 2024. &lt;a href=&quot;https://arxiv.org/abs/2407.00365&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Lin et al. DogeRM: Equipping Reward Models with Domain Knowledge through Model Merging. 2024. &lt;a href=&quot;https://arxiv.org/abs/2407.01470&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Bako et al. Evaluating the Semantic Profiling Abilities of LLMs for Natural Language Utterances in Data Visualization. 2024. &lt;a href=&quot;https://arxiv.org/abs/2407.06129&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Huang et al. RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation Quantization. 2024. &lt;a href=&quot;https://arxiv.org/abs/2407.08044&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Jiang et al. LLM-Collaboration on Automatic Science Journalism for the General Audience. 2024. &lt;a href=&quot;https://arxiv.org/abs/2407.09756&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Inouye et al. Applied Auto-tuning on LoRA Hyperparameters. 2024. &lt;a href=&quot;https://scholarcommons.scu.edu/cseng_senior/272/&quot;&gt;[paper]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Qi et al. Research on Tibetan Tourism Viewpoints information generation system based on LLM. 2024. &lt;a href=&quot;https://arxiv.org/abs/2407.13561&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Xu et al. Course-Correction: Safety Alignment Using Synthetic Preferences. 2024. &lt;a href=&quot;https://arxiv.org/abs/2407.16637&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Sun et al. LAMBDA: A Large Model Based Data Agent. 2024. &lt;a href=&quot;https://arxiv.org/abs/2407.17535&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zhu et al. CollectiveSFT: Scaling Large Language Models for Chinese Medical Benchmark with Collective Instructions in Healthcare. 2024. &lt;a href=&quot;https://arxiv.org/abs/2407.19705&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Yu et al. Correcting Negative Bias in Large Language Models through Negative Attention Score Alignment. 2024. &lt;a href=&quot;https://arxiv.org/abs/2408.00137&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Xie et al. The Power of Personalized Datasets: Advancing Chinese Composition Writing for Elementary School through Targeted Model Fine-Tuning. IALP 2024. &lt;a href=&quot;https://www.asianlp.sg/conferences/ialp2024/proceedings/papers/IALP2024_P055.pdf&quot;&gt;[paper]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Liu et al. Instruct-Code-Llama: Improving Capabilities of Language Model in Competition Level Code Generation by Online Judge Feedback. ICIC 2024. &lt;a href=&quot;https://link.springer.com/chapter/10.1007/978-981-97-5669-8_11&quot;&gt;[paper]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Wang et al. Cybernetic Sentinels: Unveiling the Impact of Safety Data Selection on Model Security in Supervised Fine-Tuning. ICIC 2024. &lt;a href=&quot;https://link.springer.com/chapter/10.1007/978-981-97-5669-8_23&quot;&gt;[paper]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Xia et al. Understanding the Performance and Estimating the Cost of LLM Fine-Tuning. 2024. &lt;a href=&quot;https://arxiv.org/abs/2408.04693&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zeng et al. Perceive, Reflect, and Plan: Designing LLM Agent for Goal-Directed City Navigation without Instructions. 2024. &lt;a href=&quot;https://arxiv.org/abs/2408.04168&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Xia et al. Using Pre-trained Language Model for Accurate ESG Prediction. FinNLP 2024. &lt;a href=&quot;https://aclanthology.org/2024.finnlp-2.1/&quot;&gt;[paper]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Liang et al. I-SHEEP: Self-Alignment of LLM from Scratch through an Iterative Self-Enhancement Paradigm. 2024. &lt;a href=&quot;https://arxiv.org/abs/2408.08072&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Bai et al. Aligning Large Language Model with Direct Multi-Preference Optimization for Recommendation. CIKM 2024. &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3627673.3679611&quot;&gt;[paper]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/Yu-Yang-Li/StarWhisper&quot;&gt;StarWhisper&lt;/a&gt;&lt;/strong&gt;: A large language model for Astronomy, based on ChatGLM2-6B and Qwen-14B.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/FudanDISC/DISC-LawLLM&quot;&gt;DISC-LawLLM&lt;/a&gt;&lt;/strong&gt;: A large language model specialized in Chinese legal domain, based on Baichuan-13B, is capable of retrieving and reasoning on legal knowledge.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/X-D-Lab/Sunsimiao&quot;&gt;Sunsimiao&lt;/a&gt;&lt;/strong&gt;: A large language model specialized in Chinese medical domain, based on Baichuan-7B and ChatGLM-6B.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/WangRongsheng/CareGPT&quot;&gt;CareGPT&lt;/a&gt;&lt;/strong&gt;: A series of large language models for Chinese medical domain, based on LLaMA2-7B and Baichuan-13B.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/PKU-YuanGroup/Machine-Mindset/&quot;&gt;MachineMindset&lt;/a&gt;&lt;/strong&gt;: A series of MBTI Personality large language models, capable of giving any LLM 16 different personality types based on different datasets and training methods.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://huggingface.co/Nekochu/Luminia-13B-v3&quot;&gt;Luminia-13B-v3&lt;/a&gt;&lt;/strong&gt;: A large language model specialized in generate metadata for stable diffusion. &lt;a href=&quot;https://huggingface.co/spaces/Nekochu/Luminia-13B_SD_Prompt&quot;&gt;[demo]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/BUAADreamer/Chinese-LLaVA-Med&quot;&gt;Chinese-LLaVA-Med&lt;/a&gt;&lt;/strong&gt;: A multimodal large language model specialized in Chinese medical domain, based on LLaVA-1.5-7B.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/THUDM/AutoRE&quot;&gt;AutoRE&lt;/a&gt;&lt;/strong&gt;: A document-level relation extraction system based on large language models.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NVIDIA/RTX-AI-Toolkit&quot;&gt;NVIDIA RTX AI Toolkit&lt;/a&gt;&lt;/strong&gt;: SDKs for fine-tuning LLMs on Windows PC for NVIDIA RTX.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/LazyAGI/LazyLLM&quot;&gt;LazyLLM&lt;/a&gt;&lt;/strong&gt;: An easy and lazy way for building multi-agent LLMs applications and supports model fine-tuning via LLaMA Factory.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NLPJCL/RAG-Retrieval&quot;&gt;RAG-Retrieval&lt;/a&gt;&lt;/strong&gt;: A full pipeline for RAG retrieval model fine-tuning, inference, and distillation. &lt;a href=&quot;https://zhuanlan.zhihu.com/p/987727357&quot;&gt;[blog]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/Qihoo360/360-LLaMA-Factory&quot;&gt;360-LLaMA-Factory&lt;/a&gt;&lt;/strong&gt;: A modified library that supports long sequence SFT &amp;amp; DPO using ring attention.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://novasky-ai.github.io/posts/sky-t1/&quot;&gt;Sky-T1&lt;/a&gt;&lt;/strong&gt;: An o1-like model fine-tuned by NovaSky AI with very small cost.&lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/details&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This repository is licensed under the &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/LICENSE&quot;&gt;Apache-2.0 License&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Please follow the model licenses to use the corresponding model weights: &lt;a href=&quot;https://huggingface.co/baichuan-inc/Baichuan2-7B-Base/blob/main/Community%20License%20for%20Baichuan%202%20Model.pdf&quot;&gt;Baichuan 2&lt;/a&gt; / &lt;a href=&quot;https://huggingface.co/spaces/bigscience/license&quot;&gt;BLOOM&lt;/a&gt; / &lt;a href=&quot;https://github.com/THUDM/ChatGLM3/raw/main/MODEL_LICENSE&quot;&gt;ChatGLM3&lt;/a&gt; / &lt;a href=&quot;https://cohere.com/c4ai-cc-by-nc-license&quot;&gt;Command R&lt;/a&gt; / &lt;a href=&quot;https://github.com/deepseek-ai/DeepSeek-LLM/raw/main/LICENSE-MODEL&quot;&gt;DeepSeek&lt;/a&gt; / &lt;a href=&quot;https://huggingface.co/tiiuae/falcon-180B/blob/main/LICENSE.txt&quot;&gt;Falcon&lt;/a&gt; / &lt;a href=&quot;https://ai.google.dev/gemma/terms&quot;&gt;Gemma&lt;/a&gt; / &lt;a href=&quot;https://huggingface.co/THUDM/glm-4-9b/blob/main/LICENSE&quot;&gt;GLM-4&lt;/a&gt; / &lt;a href=&quot;https://github.com/openai/gpt-2/raw/master/LICENSE&quot;&gt;GPT-2&lt;/a&gt; / &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/LICENSE&quot;&gt;Granite&lt;/a&gt; / &lt;a href=&quot;https://huggingface.co/IndexTeam/Index-1.9B/blob/main/LICENSE&quot;&gt;Index&lt;/a&gt; / &lt;a href=&quot;https://github.com/InternLM/InternLM#license&quot;&gt;InternLM&lt;/a&gt; / &lt;a href=&quot;https://github.com/facebookresearch/llama/raw/main/MODEL_CARD.md&quot;&gt;Llama&lt;/a&gt; / &lt;a href=&quot;https://ai.meta.com/llama/license/&quot;&gt;Llama 2 (LLaVA-1.5)&lt;/a&gt; / &lt;a href=&quot;https://llama.meta.com/llama3/license/&quot;&gt;Llama 3&lt;/a&gt; / &lt;a href=&quot;https://github.com/OpenBMB/MiniCPM/raw/main/MiniCPM%20Model%20License.md&quot;&gt;MiniCPM&lt;/a&gt; / &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/LICENSE&quot;&gt;Mistral/Mixtral/Pixtral&lt;/a&gt; / &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/LICENSE&quot;&gt;OLMo&lt;/a&gt; / &lt;a href=&quot;https://huggingface.co/microsoft/phi-1_5/resolve/main/Research%20License.docx&quot;&gt;Phi-1.5/Phi-2&lt;/a&gt; / &lt;a href=&quot;https://huggingface.co/microsoft/Phi-3-mini-4k-instruct/blob/main/LICENSE&quot;&gt;Phi-3/Phi-4&lt;/a&gt; / &lt;a href=&quot;https://github.com/QwenLM/Qwen/raw/main/Tongyi%20Qianwen%20LICENSE%20AGREEMENT&quot;&gt;Qwen&lt;/a&gt; / &lt;a href=&quot;https://huggingface.co/Skywork/Skywork-13B-base/blob/main/Skywork%20Community%20License.pdf&quot;&gt;Skywork&lt;/a&gt; / &lt;a href=&quot;https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement&quot;&gt;StarCoder 2&lt;/a&gt; / &lt;a href=&quot;https://huggingface.co/Tele-AI/telechat-7B/blob/main/TeleChat%E6%A8%A1%E5%9E%8B%E7%A4%BE%E5%8C%BA%E8%AE%B8%E5%8F%AF%E5%8D%8F%E8%AE%AE.pdf&quot;&gt;TeleChat2&lt;/a&gt; / &lt;a href=&quot;https://github.com/xverse-ai/XVERSE-13B/raw/main/MODEL_LICENSE.pdf&quot;&gt;XVERSE&lt;/a&gt; / &lt;a href=&quot;https://huggingface.co/01-ai/Yi-6B/blob/main/LICENSE&quot;&gt;Yi&lt;/a&gt; / &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/LICENSE&quot;&gt;Yi-1.5&lt;/a&gt; / &lt;a href=&quot;https://github.com/IEIT-Yuan/Yuan-2.0/raw/main/LICENSE-Yuan&quot;&gt;Yuan 2&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If this work is helpful, please kindly cite as:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bibtex&quot;&gt;@inproceedings{zheng2024llamafactory,
  title={LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models},
  author={Yaowei Zheng and Richong Zhang and Junhao Zhang and Yanhan Ye and Zheyan Luo and Zhangchi Feng and Yongqiang Ma},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)},
  address={Bangkok, Thailand},
  publisher={Association for Computational Linguistics},
  year={2024},
  url={http://arxiv.org/abs/2403.13372}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Acknowledgement&lt;/h2&gt; 
&lt;p&gt;This repo benefits from &lt;a href=&quot;https://github.com/huggingface/peft&quot;&gt;PEFT&lt;/a&gt;, &lt;a href=&quot;https://github.com/huggingface/trl&quot;&gt;TRL&lt;/a&gt;, &lt;a href=&quot;https://github.com/artidoro/qlora&quot;&gt;QLoRA&lt;/a&gt; and &lt;a href=&quot;https://github.com/lm-sys/FastChat&quot;&gt;FastChat&lt;/a&gt;. Thanks for their wonderful works.&lt;/p&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;img src=&quot;https://api.star-history.com/svg?repos=hiyouga/LLaMA-Factory&amp;amp;type=Date&quot; alt=&quot;Star History Chart&quot;&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>RockChinQ/LangBot</title>
      <link>https://github.com/RockChinQ/LangBot</link>
      <description>&lt;p&gt;😎丰富生态、🧩支持扩展、🦄多模态 - 大模型原生即时通信机器人平台 | 适配 QQ / 微信（企业微信、个人微信）/ 飞书 / 钉钉 / Discord / Telegram 等消息平台 | 支持 ChatGPT、DeepSeek、Dify、Claude、Gemini、xAI Grok、Ollama、LM Studio、阿里云百炼、火山方舟、SiliconFlow、Qwen、Moonshot、ChatGLM、SillyTraven 等 LLM 的机器人 / Agent | LLM-based instant messaging bots platform, supports Discord, Telegram, WeChat, Lark, DingTalk, QQ&lt;/p&gt;&lt;hr&gt;&lt;p align=&quot;center&quot;&gt; &lt;a href=&quot;https://langbot.app&quot;&gt; &lt;img src=&quot;https://docs.langbot.app/social.png&quot; alt=&quot;LangBot&quot;&gt; &lt;/a&gt; &lt;/p&gt;
&lt;div align=&quot;center&quot;&gt; 
 &lt;p&gt;&lt;a href=&quot;https://trendshift.io/repositories/12901&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/12901&quot; alt=&quot;RockChinQ%2FLangBot | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href=&quot;https://docs.langbot.app&quot;&gt;项目主页&lt;/a&gt; ｜ &lt;a href=&quot;https://docs.langbot.app/insight/intro.html&quot;&gt;功能介绍&lt;/a&gt; ｜ &lt;a href=&quot;https://docs.langbot.app/insight/guide.html&quot;&gt;部署文档&lt;/a&gt; ｜ &lt;a href=&quot;https://docs.langbot.app/usage/faq.html&quot;&gt;常见问题&lt;/a&gt; ｜ &lt;a href=&quot;https://docs.langbot.app/plugin/plugin-intro.html&quot;&gt;插件介绍&lt;/a&gt; ｜ &lt;a href=&quot;https://github.com/RockChinQ/LangBot/issues/new?assignees=&amp;amp;labels=%E7%8B%AC%E7%AB%8B%E6%8F%92%E4%BB%B6&amp;amp;projects=&amp;amp;template=submit-plugin.yml&amp;amp;title=%5BPlugin%5D%3A+%E8%AF%B7%E6%B1%82%E7%99%BB%E8%AE%B0%E6%96%B0%E6%8F%92%E4%BB%B6&quot;&gt;提交插件&lt;/a&gt;&lt;/p&gt; 
 &lt;div align=&quot;center&quot;&gt;
   😎高稳定、🧩支持扩展、🦄多模态 - 大模型原生即时通信机器人平台🤖 
 &lt;/div&gt; 
 &lt;br&gt; 
 &lt;p&gt;&lt;a href=&quot;https://discord.gg/wdNEHETs87&quot;&gt;&lt;img src=&quot;https://img.shields.io/discord/1335141740050649118?logo=discord&amp;amp;labelColor=%20%235462eb&amp;amp;logoColor=%20%23f5f5f5&amp;amp;color=%20%235462eb&quot; alt=&quot;Discord&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://qm.qq.com/q/JLi38whHum&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/%E7%A4%BE%E5%8C%BAQQ%E7%BE%A4-966235608-blue&quot; alt=&quot;QQ Group&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/RockChinQ/LangBot/releases/latest&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/v/release/RockChinQ/LangBot&quot; alt=&quot;GitHub release (latest by date)&quot;&gt;&lt;/a&gt; &lt;img src=&quot;https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.qchatgpt.rockchin.top%2Fapi%2Fv2%2Fview%2Frealtime%2Fcount_query%3Fminute%3D10080&amp;amp;query=%24.data.count&amp;amp;label=%E4%BD%BF%E7%94%A8%E9%87%8F%EF%BC%887%E6%97%A5%EF%BC%89&quot; alt=&quot;Dynamic JSON Badge&quot;&gt; &lt;img src=&quot;https://img.shields.io/badge/python-3.10%20%7C%203.11%20%7C%203.12-blue.svg?sanitize=true&quot; alt=&quot;python&quot;&gt; &lt;a href=&quot;https://gitcode.com/RockChinQ/LangBot&quot;&gt;&lt;img src=&quot;https://gitcode.com/RockChinQ/LangBot/star/badge.svg?sanitize=true&quot; alt=&quot;star&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/RockChinQ/LangBot/master/README.md&quot;&gt;简体中文&lt;/a&gt; / &lt;a href=&quot;https://raw.githubusercontent.com/RockChinQ/LangBot/master/README_EN.md&quot;&gt;English&lt;/a&gt; / &lt;a href=&quot;https://raw.githubusercontent.com/RockChinQ/LangBot/master/README_JP.md&quot;&gt;日本語&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;/p&gt; 
&lt;h2&gt;✨ 特性&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;💬 大模型对话、Agent：支持多种大模型，适配群聊和私聊；具有多轮对话、工具调用、多模态能力，并深度适配 &lt;a href=&quot;https://dify.ai&quot;&gt;Dify&lt;/a&gt;。目前支持 QQ、QQ频道、企业微信、个人微信、飞书、Discord、Telegram 等平台。&lt;/li&gt; 
 &lt;li&gt;🛠️ 高稳定性、功能完备：原生支持访问控制、限速、敏感词过滤等机制；配置简单，支持多种部署方式。&lt;/li&gt; 
 &lt;li&gt;🧩 插件扩展、活跃社区：支持事件驱动、组件扩展等插件机制；丰富生态，目前已有数十个&lt;a href=&quot;https://docs.langbot.app/plugin/plugin-intro.html&quot;&gt;插件&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;😻 [New] Web 管理面板：支持通过浏览器管理 LangBot 实例，具体支持功能，查看&lt;a href=&quot;https://docs.langbot.app/webui/intro.html&quot;&gt;文档&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;📦 开始使用&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT]&lt;/p&gt; 
 &lt;p&gt;在您开始任何方式部署之前，请务必阅读&lt;a href=&quot;https://docs.langbot.app/insight/guide.html&quot;&gt;新手指引&lt;/a&gt;。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Docker Compose 部署&lt;/h4&gt; 
&lt;p&gt;适合熟悉 Docker 的用户，查看文档&lt;a href=&quot;https://docs.langbot.app/deploy/langbot/docker.html&quot;&gt;Docker 部署&lt;/a&gt;。&lt;/p&gt; 
&lt;h4&gt;宝塔面板部署&lt;/h4&gt; 
&lt;p&gt;已上架宝塔面板，若您已安装宝塔面板，可以根据&lt;a href=&quot;https://docs.langbot.app/deploy/langbot/one-click/bt.html&quot;&gt;文档&lt;/a&gt;使用。&lt;/p&gt; 
&lt;h4&gt;Zeabur 云部署&lt;/h4&gt; 
&lt;p&gt;社区贡献的 Zeabur 模板。&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://zeabur.com/zh-CN/templates/ZKTBDH&quot;&gt;&lt;img src=&quot;https://zeabur.com/button.svg?sanitize=true&quot; alt=&quot;Deploy on Zeabur&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;Railway 云部署&lt;/h4&gt; 
&lt;p&gt;&lt;a href=&quot;https://railway.app/template/yRrAyL?referralCode=vogKPF&quot;&gt;&lt;img src=&quot;https://railway.com/button.svg?sanitize=true&quot; alt=&quot;Deploy on Railway&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;手动部署&lt;/h4&gt; 
&lt;p&gt;直接使用发行版运行，查看文档&lt;a href=&quot;https://docs.langbot.app/deploy/langbot/manual.html&quot;&gt;手动部署&lt;/a&gt;。&lt;/p&gt; 
&lt;h2&gt;📸 效果展示&lt;/h2&gt; 
&lt;img alt=&quot;回复效果（带有联网插件）&quot; src=&quot;https://docs.langbot.app/QChatGPT-0516.png&quot; width=&quot;500px&quot;&gt; 
&lt;ul&gt; 
 &lt;li&gt;WebUI Demo: &lt;a href=&quot;https://demo.langbot.dev/&quot;&gt;https://demo.langbot.dev/&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;登录信息：邮箱：&lt;code&gt;demo@langbot.app&lt;/code&gt; 密码：&lt;code&gt;langbot123456&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;注意：仅展示webui效果，公开环境，请不要在其中填入您的任何敏感信息。&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🔌 组件兼容性&lt;/h2&gt; 
&lt;h3&gt;消息平台&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;平台&lt;/th&gt; 
   &lt;th&gt;状态&lt;/th&gt; 
   &lt;th&gt;备注&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;QQ 个人号&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;QQ 个人号私聊、群聊&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;QQ 官方机器人&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;QQ 官方机器人，支持频道、私聊、群聊&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;企业微信&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;个人微信&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;使用 &lt;a href=&quot;https://github.com/Devo919/Gewechat&quot;&gt;Gewechat&lt;/a&gt; 接入&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;微信公众号&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;飞书&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;钉钉&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Discord&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Telegram&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;WhatsApp&lt;/td&gt; 
   &lt;td&gt;🚧&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;🚧: 正在开发中&lt;/p&gt; 
&lt;h3&gt;大模型&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;模型&lt;/th&gt; 
   &lt;th&gt;状态&lt;/th&gt; 
   &lt;th&gt;备注&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://platform.openai.com/&quot;&gt;OpenAI&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;可接入任何 OpenAI 接口格式模型&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.deepseek.com/&quot;&gt;DeepSeek&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.moonshot.cn/&quot;&gt;Moonshot&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.anthropic.com/&quot;&gt;Anthropic&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://x.ai/&quot;&gt;xAI&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://open.bigmodel.cn/&quot;&gt;智谱AI&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://dify.ai&quot;&gt;Dify&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;LLMOps 平台&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://ollama.com/&quot;&gt;Ollama&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;本地大模型运行平台&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://lmstudio.ai/&quot;&gt;LMStudio&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;本地大模型运行平台&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://ai.gitee.com/&quot;&gt;GiteeAI&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;大模型接口聚合平台&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://siliconflow.cn/&quot;&gt;SiliconFlow&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;大模型聚合平台&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://bailian.console.aliyun.com/&quot;&gt;阿里云百炼&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;大模型聚合平台, LLMOps 平台&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://console.volcengine.com/ark/region:ark+cn-beijing/model?vendor=Bytedance&amp;amp;view=LIST_VIEW&quot;&gt;火山方舟&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;大模型聚合平台, LLMOps 平台&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;TTS&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;平台/模型&lt;/th&gt; 
   &lt;th&gt;备注&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://fish.audio/zh-CN/discovery/&quot;&gt;FishAudio&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/the-lazy-me/NewChatVoice&quot;&gt;插件&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.ttson.cn/?source=thelazy&quot;&gt;海豚 AI&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/the-lazy-me/NewChatVoice&quot;&gt;插件&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://portal.azure.com/&quot;&gt;AzureTTS&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/Ingnaryk/LangBot_AzureTTS&quot;&gt;插件&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;文生图&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;平台/模型&lt;/th&gt; 
   &lt;th&gt;备注&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;阿里云百炼&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/Thetail001/LangBot_BailianTextToImagePlugin&quot;&gt;插件&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;😘 社区贡献&lt;/h2&gt; 
&lt;p&gt;LangBot 离不开以下贡献者和社区内所有人的贡献，我们欢迎任何形式的贡献和反馈。&lt;/p&gt; 
&lt;a href=&quot;https://github.com/RockChinQ/LangBot/graphs/contributors&quot;&gt; &lt;img src=&quot;https://contrib.rocks/image?repo=RockChinQ/LangBot&quot;&gt; &lt;/a&gt;</description>
    </item>
    
    <item>
      <title>X-PLUG/MobileAgent</title>
      <link>https://github.com/X-PLUG/MobileAgent</link>
      <description>&lt;p&gt;Mobile-Agent: The Powerful Mobile Device Operation Assistant Family&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/X-PLUG/MobileAgent/main/assets/logo.png?v=1&amp;amp;type=image&quot; alt=&quot;&quot;&gt;&lt;/p&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;h3&gt;Mobile-Agent: The Powerful Mobile Device Operation Assistant Family&lt;/h3&gt;
 &lt;h3&gt; 
  &lt;div align=&quot;center&quot;&gt; 
   &lt;a href=&quot;https://huggingface.co/spaces/junyangwang0410/Mobile-Agent&quot;&gt;&lt;img src=&quot;https://huggingface.co/datasets/huggingface/badges/raw/main/open-in-hf-spaces-sm-dark.svg?sanitize=true&quot; alt=&quot;Open in Spaces&quot;&gt;&lt;/a&gt; 
   &lt;a href=&quot;https://modelscope.cn/studios/wangjunyang/Mobile-Agent-v2&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/X-PLUG/MobileAgent/main/assets/Demo-ModelScope-brightgreen.svg?sanitize=true&quot; alt=&quot;Demo ModelScope&quot;&gt;&lt;/a&gt; 
   &lt;a href=&quot;https://arxiv.org/abs/2502.14282%20&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Arxiv-2502.14282-b31b1b.svg?logo=arXiv&quot; alt=&quot;&quot;&gt;&lt;/a&gt; 
   &lt;a href=&quot;https://arxiv.org/abs/2501.11733&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Arxiv-2501.11733-b31b1b.svg?logo=arXiv&quot; alt=&quot;&quot;&gt;&lt;/a&gt; 
   &lt;a href=&quot;https://arxiv.org/abs/2406.01014%20&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Arxiv-2406.01014-b31b1b.svg?logo=arXiv&quot; alt=&quot;&quot;&gt;&lt;/a&gt; 
   &lt;a href=&quot;https://arxiv.org/abs/2401.16158&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Arxiv-2401.16158-b31b1b.svg?logo=arXiv&quot; alt=&quot;&quot;&gt;&lt;/a&gt; 
  &lt;/div&gt; &lt;p align=&quot;center&quot;&gt; &lt;a href=&quot;https://trendshift.io/repositories/7423&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/7423&quot; alt=&quot;MobileAgent | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;&gt;&lt;/a&gt; &lt;/p&gt; &lt;/h3&gt;
&lt;/div&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;a href=&quot;https://raw.githubusercontent.com/X-PLUG/MobileAgent/main/README.md&quot;&gt;English&lt;/a&gt; | 
 &lt;a href=&quot;https://raw.githubusercontent.com/X-PLUG/MobileAgent/main/README_zh.md&quot;&gt;简体中文&lt;/a&gt; | 
 &lt;a href=&quot;https://raw.githubusercontent.com/X-PLUG/MobileAgent/main/README_ja.md&quot;&gt;日本語&lt;/a&gt; 
 &lt;hr&gt; 
&lt;/div&gt; 
&lt;h2&gt;📺Demo&lt;/h2&gt; 
&lt;h3&gt;Newest PC-Agent&lt;/h3&gt; 
&lt;p&gt;See &lt;a href=&quot;https://arxiv.org/abs/2502.14282&quot;&gt;paper&lt;/a&gt; for details.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/user-attachments/assets/b13bbb14-b39a-4c6b-b4a6-3df97de517dc&quot;&gt;https://github.com/user-attachments/assets/b13bbb14-b39a-4c6b-b4a6-3df97de517dc&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Mobile-Agent-E&lt;/h3&gt; 
&lt;p&gt;See the &lt;a href=&quot;https://x-plug.github.io/MobileAgent&quot;&gt;project page&lt;/a&gt; for video demos.&lt;/p&gt; 
&lt;!-- &lt;div style=&quot;display: flex; justify-content: space-between; gap: 10px; flex-wrap: wrap;&quot;&gt;
  &lt;video width=&quot;30%&quot; controls&gt;
    &lt;source src=&quot;https://raw.githubusercontent.com/X-PLUG/MobileAgent/main/Mobile-Agent-E/static/videos/bouldering_gym.mp4&quot; type=&quot;video/mp4&quot;&gt;
  &lt;/video&gt;
  &lt;video width=&quot;30%&quot; controls&gt;
    &lt;source src=&quot;https://raw.githubusercontent.com/X-PLUG/MobileAgent/main/Mobile-Agent-E/static/videos/shopping.mp4&quot; type=&quot;video/mp4&quot;&gt;
  &lt;/video&gt;
  &lt;video width=&quot;30%&quot; controls&gt;
    &lt;source src=&quot;https://raw.githubusercontent.com/X-PLUG/MobileAgent/main/Mobile-Agent-E/static/videos/survey.mp4&quot; type=&quot;video/mp4&quot;&gt;
  &lt;/video&gt;
&lt;/div&gt; --&gt; 
&lt;h3&gt;Mobile-Agent-v3 (Note: The video is not accelerated)&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;YouTube&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=EMbIpzqJld0&quot;&gt;&lt;img src=&quot;https://img.youtube.com/vi/EMbIpzqJld0/0.jpg&quot; alt=&quot;YouTube&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Bilibili&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.bilibili.com/video/BV1pPvyekEsa/?share_source=copy_web&amp;amp;vd_source=47ffcd57083495a8965c8cdbe1a751ae&quot;&gt;&lt;img src=&quot;https://img.youtube.com/vi/EMbIpzqJld0/0.jpg&quot; alt=&quot;Bilibili&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;PC-Agent&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Chrome and DingTalk&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/user-attachments/assets/b890a08f-8a2f-426d-9458-aa3699185030&quot;&gt;https://github.com/user-attachments/assets/b890a08f-8a2f-426d-9458-aa3699185030&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Word&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/user-attachments/assets/37f0a0a5-3d21-4232-9d1d-0fe845d0f77d&quot;&gt;https://github.com/user-attachments/assets/37f0a0a5-3d21-4232-9d1d-0fe845d0f77d&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Mobile-Agent-v2&lt;/h3&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/X-PLUG/MobileAgent/assets/127390760/d907795d-b5b9-48bf-b1db-70cf3f45d155&quot;&gt;https://github.com/X-PLUG/MobileAgent/assets/127390760/d907795d-b5b9-48bf-b1db-70cf3f45d155&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Mobile-Agent&lt;/h3&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/X-PLUG/MobileAgent/assets/127390760/26c48fb0-67ed-4df6-97b2-aa0c18386d31&quot;&gt;https://github.com/X-PLUG/MobileAgent/assets/127390760/26c48fb0-67ed-4df6-97b2-aa0c18386d31&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;📢News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;🔥🔥[2.21.25] We have released an updated version of PC-Agent. Check the &lt;a href=&quot;https://arxiv.org/abs/2502.14282&quot;&gt;paper&lt;/a&gt; for details. The code will be updated soon.&lt;/li&gt; 
 &lt;li&gt;🔥🔥[1.20.25] We propose &lt;a href=&quot;https://x-plug.github.io/MobileAgent&quot;&gt;Mobile-Agent-E&lt;/a&gt;, a hierarchical multi-agent framework capable of self-evolution through past experience, achieving stronger performance on complex, multi-app tasks.&lt;/li&gt; 
 &lt;li&gt;🔥🔥[9.26] Mobile-Agent-v2 has been accepted by &lt;strong&gt;The Thirty-eighth Annual Conference on Neural Information Processing Systems (NeurIPS 2024)&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;🔥[8.23] We proposed PC-Agent, a &lt;strong&gt;PC&lt;/strong&gt; operation assistant supporting both &lt;strong&gt;Mac and Windows&lt;/strong&gt; platforms.&lt;/li&gt; 
 &lt;li&gt;🔥[7.29] Mobile-Agent won the &lt;strong&gt;best demo award&lt;/strong&gt; at the &lt;em&gt;&lt;strong&gt;The 23rd China National Conference on Computational Linguistics&lt;/strong&gt;&lt;/em&gt; (CCL 2024). On the CCL 2024, we displayed the upcoming Mobile-Agent-v3. It has smaller memory overhead (8 GB), faster reasoning speed (10s-15s per operation), and all uses open source models. Video demo, please see the last section 📺Demo.&lt;/li&gt; 
 &lt;li&gt;[6.27] We proposed Demo that can upload mobile phone screenshots to experience Mobile-Agent-V2 in &lt;a href=&quot;https://huggingface.co/spaces/junyangwang0410/Mobile-Agent&quot;&gt;Hugging Face&lt;/a&gt; and &lt;a href=&quot;https://modelscope.cn/studios/wangjunyang/Mobile-Agent-v2&quot;&gt;ModelScope&lt;/a&gt;. You don’t need to configure models and devices, and you can experience it immediately.&lt;/li&gt; 
 &lt;li&gt;[6. 4] Modelscope-Agent has supported Mobile-Agent-V2, based on Android Adb Env, please check in the &lt;a href=&quot;https://github.com/modelscope/modelscope-agent/tree/master/apps/mobile_agent&quot;&gt;application&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;[6. 4] We proposed Mobile-Agent-v2, a mobile device operation assistant with effective navigation via multi-agent collaboration.&lt;/li&gt; 
 &lt;li&gt;[3.10] Mobile-Agent has been accepted by the &lt;strong&gt;ICLR 2024 Workshop on Large Language Model (LLM) Agents&lt;/strong&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;📱Version&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/X-PLUG/MobileAgent/main/PC-Agent/README.md&quot;&gt;PC-Agent&lt;/a&gt; - A Hierarchical Multi-Agent Collaboration Framework for Complex Task Automation on PC&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/X-PLUG/MobileAgent/main/Mobile-Agent-E/README.md&quot;&gt;Mobile-Agent-E&lt;/a&gt; - Stronger performance on complex, long-horizon, reasoning-intensive tasks, with self-evolution capability&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/X-PLUG/MobileAgent/main/Mobile-Agent-v3/README.md&quot;&gt;Mobile-Agent-v3&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/X-PLUG/MobileAgent/main/Mobile-Agent-v2/README.md&quot;&gt;Mobile-Agent-v2&lt;/a&gt; - Mobile Device Operation Assistant with Effective Navigation via Multi-Agent Collaboration&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/X-PLUG/MobileAgent/main/Mobile-Agent/README.md&quot;&gt;Mobile-Agent&lt;/a&gt; - Autonomous Multi-Modal Mobile Device Agent with Visual Perception&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;⭐Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://star-history.com/#X-PLUG/MobileAgent&amp;amp;Date&quot;&gt;&lt;img src=&quot;https://api.star-history.com/svg?repos=X-PLUG/MobileAgent&amp;amp;type=Date&quot; alt=&quot;Star History Chart&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;📑Citation&lt;/h2&gt; 
&lt;p&gt;If you find Mobile-Agent useful for your research and applications, please cite using this BibTeX:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@article{liu2025pc,
  title={PC-Agent: A Hierarchical Multi-Agent Collaboration Framework for Complex Task Automation on PC},
  author={Liu, Haowei and Zhang, Xi and Xu, Haiyang and Wanyan, Yuyang and Wang, Junyang and Yan, Ming and Zhang, Ji and Yuan, Chunfeng and Xu, Changsheng and Hu, Weiming and Huang, Fei},
  journal={arXiv preprint arXiv:2502.14282},
  year={2025}
}

@article{wang2025mobile,
  title={Mobile-Agent-E: Self-Evolving Mobile Assistant for Complex Tasks},
  author={Wang, Zhenhailong and Xu, Haiyang and Wang, Junyang and Zhang, Xi and Yan, Ming and Zhang, Ji and Huang, Fei and Ji, Heng},
  journal={arXiv preprint arXiv:2501.11733},
  year={2025}
}

@article{wang2024mobile2,
  title={Mobile-Agent-v2: Mobile Device Operation Assistant with Effective Navigation via Multi-Agent Collaboration},
  author={Wang, Junyang and Xu, Haiyang and Jia, Haitao and Zhang, Xi and Yan, Ming and Shen, Weizhou and Zhang, Ji and Huang, Fei and Sang, Jitao},
  journal={arXiv preprint arXiv:2406.01014},
  year={2024}
}

@article{wang2024mobile,
  title={Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception},
  author={Wang, Junyang and Xu, Haiyang and Ye, Jiabo and Yan, Ming and Shen, Weizhou and Zhang, Ji and Huang, Fei and Sang, Jitao},
  journal={arXiv preprint arXiv:2401.16158},
  year={2024}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;📦Related Projects&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/mnotgod96/AppAgent&quot;&gt;AppAgent: Multimodal Agents as Smartphone Users&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/X-PLUG/mPLUG-Owl&quot;&gt;mPLUG-Owl &amp;amp; mPLUG-Owl2: Modularized Multimodal Large Language Model&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen-VL&quot;&gt;Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/IDEA-Research/GroundingDINO&quot;&gt;GroundingDINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/openai/CLIP&quot;&gt;CLIP: Contrastive Language-Image Pretraining&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>jingyaogong/minimind</title>
      <link>https://github.com/jingyaogong/minimind</link>
      <description>&lt;p&gt;🚀🚀 「大模型」2小时完全从0训练26M的小参数GPT！🌏 Train a 26M-parameter GPT from scratch in just 2h!&lt;/p&gt;&lt;hr&gt;&lt;div align=&quot;center&quot;&gt; 
 &lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jingyaogong/minimind/master/images/logo.png&quot; alt=&quot;logo&quot;&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;p&gt;&lt;img src=&quot;https://visitor-badge.laobi.icu/badge?page_id=jingyaogong/minimind&quot; alt=&quot;visitors&quot;&gt; &lt;a href=&quot;https://github.com/jingyaogong/minimind/stargazers&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/jingyaogong/minimind?style=social&quot; alt=&quot;GitHub Repo stars&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/jingyaogong/minimind/master/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/license/jingyaogong/minimind&quot; alt=&quot;GitHub Code License&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/jingyaogong/minimind/commits/master&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/last-commit/jingyaogong/minimind&quot; alt=&quot;GitHub last commit&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/jingyaogong/minimind/pulls&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/PRs-welcome-blue&quot; alt=&quot;GitHub pull request&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://huggingface.co/collections/jingyaogong/minimind-66caf8d999f5c7fa64f399e5&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/%F0%9F%A4%97-MiniMind%20%20Collection-blue&quot; alt=&quot;Collection&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;h3&gt;&quot;大道至简&quot;&lt;/h3&gt; 
&lt;/div&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;p&gt;中文 | &lt;a href=&quot;https://raw.githubusercontent.com/jingyaogong/minimind/master/README_en.md&quot;&gt;English&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;ul&gt; 
 &lt;li&gt;此开源项目旨在完全从0开始，仅用3块钱成本 + 2小时！即可训练出仅为25.8M的超小语言模型&lt;strong&gt;MiniMind&lt;/strong&gt;。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MiniMind&lt;/strong&gt;系列极其轻量，最小版本体积是 GPT-3 的 $\frac{1}{7000}$，力求做到最普通的个人GPU也可快速训练。&lt;/li&gt; 
 &lt;li&gt;项目同时开源了大模型的极简结构-包含拓展共享混合专家(MoE)、数据集清洗、预训练(Pretrain)、监督微调(SFT)、LoRA微调， 直接偏好强化学习(DPO)算法、模型蒸馏算法等全过程代码。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MiniMind&lt;/strong&gt;同时拓展了视觉多模态的VLM: &lt;a href=&quot;https://github.com/jingyaogong/minimind-v&quot;&gt;MiniMind-V&lt;/a&gt;。&lt;/li&gt; 
 &lt;li&gt;项目所有核心算法代码均从0使用PyTorch原生重构！不依赖第三方库提供的抽象接口。&lt;/li&gt; 
 &lt;li&gt;这不仅是大语言模型的全阶段开源复现，也是一个入门LLM的教程。&lt;/li&gt; 
 &lt;li&gt;希望此项目能为所有人提供一个抛砖引玉的示例，一起感受创造的乐趣！推动更广泛AI社区的进步！&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;为防止误解，“2小时” 基于NVIDIA 3090硬件设备（单卡）测试，“3块钱” 指GPU服务器租用成本，具体规格详情见下文。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jingyaogong/minimind/master/images/minimind2.gif&quot; alt=&quot;minimind2&quot;&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href=&quot;https://www.modelscope.cn/studios/gongjy/MiniMind-Reasoning&quot;&gt;🔗🍓推理模型&lt;/a&gt; | &lt;a href=&quot;https://www.modelscope.cn/studios/gongjy/MiniMind&quot;&gt;🔗🤖常规模型&lt;/a&gt; | &lt;a href=&quot;https://www.bilibili.com/video/BV12dHPeqE72/?share_source=copy_web&amp;amp;vd_source=670c2504f88726f8cf4a21ef6147c0e8&quot;&gt;🔗🎞️视频介绍&lt;/a&gt;&lt;/p&gt; 
 &lt;div align=&quot;center&quot;&gt; 
  &lt;table&gt; 
   &lt;tbody&gt;
    &lt;tr&gt; 
     &lt;td align=&quot;center&quot;&gt; &lt;a href=&quot;https://huggingface.co/collections/jingyaogong/minimind-66caf8d999f5c7fa64f399e5&quot; style=&quot;text-decoration: none;&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jingyaogong/minimind/master/images/and_huggingface.png&quot; alt=&quot;Hugging Face Logo&quot; style=&quot;vertical-align: middle; width: auto; max-width: 100%;&quot;&gt; &lt;/a&gt; &lt;/td&gt; 
     &lt;td align=&quot;center&quot;&gt; &lt;a href=&quot;https://www.modelscope.cn/profile/gongjy&quot; style=&quot;text-decoration: none;&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jingyaogong/minimind/master/images/and_modelscope.png&quot; alt=&quot;ModelScope Logo&quot; style=&quot;vertical-align: middle; width: auto; max-width: 100%;&quot;&gt; &lt;/a&gt; &lt;/td&gt; 
    &lt;/tr&gt; 
   &lt;/tbody&gt;
  &lt;/table&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;h1&gt;📌 Introduction&lt;/h1&gt; 
&lt;p&gt;大语言模型（Large Language Model, LLM）的出现引发了全世界对AI的空前关注。 无论是ChatGPT、DeepSeek还是Qwen，都以其惊艳的效果令人叹为观止。 然而，动辄数百亿参数的庞大规模，使得它们对个人设备而言不仅难以训练，甚至连部署都显得遥不可及。 打开大模型的“黑盒子”，探索其内部运作机制，多么令人心潮澎湃！ 遗憾的是，99%的探索只能止步于使用LoRA等技术对现有大模型进行少量微调，学习一些新指令或任务。 这就好比教牛顿如何使用21世纪的智能手机——虽然有趣，却完全偏离了理解物理本质的初衷。 与此同时，第三方的大模型框架和工具库，如transformers+trl，几乎只暴露了高度抽象的接口。 通过短短10行代码，就能完成“加载模型+加载数据集+推理+强化学习”的全流程训练。 这种高效的封装固然便利，但也像一架高速飞船，将我们与底层实现隔离开来，阻碍了深入探究LLM核心代码的机会。 然而，“用乐高拼出一架飞机，远比坐在头等舱里飞行更让人兴奋！”。 更糟糕的是，互联网上充斥着大量付费课程和营销号，以漏洞百出、一知半解的内容推销AI教程。 正因如此，本项目初衷是拉低LLM的学习门槛，让每个人都能从理解每一行代码开始， 从零开始亲手训练一个极小的语言模型。是的，从&lt;strong&gt;零开始训练&lt;/strong&gt;，而不是仅仅进行&lt;strong&gt;推理&lt;/strong&gt;！ 最低只需3块钱不到的服务器成本，就能亲身体验从0到1构建一个语言模型的全过程。 一起感受创造的乐趣吧！&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] （截至2025-02-07）MiniMind系列已完成多个型号模型的预训练，最小仅需25.8M（0.02B），即可具备流畅对话能力！&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt; 
 &lt;summary&gt;Models List&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;模型 (大小)&lt;/th&gt; 
    &lt;th&gt;推理占用 (约)&lt;/th&gt; 
    &lt;th&gt;Release&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;MiniMind2-small (26M)&lt;/td&gt; 
    &lt;td&gt;0.5 GB&lt;/td&gt; 
    &lt;td&gt;2025.02.06&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;MiniMind2-MoE (145M)&lt;/td&gt; 
    &lt;td&gt;1.0 GB&lt;/td&gt; 
    &lt;td&gt;2025.02.06&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;MiniMind2 (104M)&lt;/td&gt; 
    &lt;td&gt;1.0 GB&lt;/td&gt; 
    &lt;td&gt;2025.02.06&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;minimind-v1-small (26M)&lt;/td&gt; 
    &lt;td&gt;0.5 GB&lt;/td&gt; 
    &lt;td&gt;2024.08.28&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;minimind-v1-moe (4×26M)&lt;/td&gt; 
    &lt;td&gt;1.0 GB&lt;/td&gt; 
    &lt;td&gt;2024.09.17&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;minimind-v1 (108M)&lt;/td&gt; 
    &lt;td&gt;1.0 GB&lt;/td&gt; 
    &lt;td&gt;2024.09.01&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;p&gt;&lt;strong&gt;项目包含&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;MiniMind-LLM结构的全部代码（Dense+MoE模型）。&lt;/li&gt; 
 &lt;li&gt;包含Tokenizer分词器详细训练代码。&lt;/li&gt; 
 &lt;li&gt;包含Pretrain、SFT、LoRA、RLHF-DPO、模型蒸馏的全过程训练代码。&lt;/li&gt; 
 &lt;li&gt;收集、蒸馏、整理并清洗去重所有阶段的高质量数据集，且全部开源。&lt;/li&gt; 
 &lt;li&gt;从0实现预训练、指令微调、LoRA、DPO强化学习，白盒模型蒸馏。关键算法几乎不依赖第三方封装的框架，且全部开源。&lt;/li&gt; 
 &lt;li&gt;同时兼容&lt;code&gt;transformers&lt;/code&gt;、&lt;code&gt;trl&lt;/code&gt;、&lt;code&gt;peft&lt;/code&gt;等第三方主流框架。&lt;/li&gt; 
 &lt;li&gt;训练支持单机单卡、单机多卡(DDP、DeepSpeed)训练，支持wandb可视化训练流程。支持动态启停训练。&lt;/li&gt; 
 &lt;li&gt;在第三方测评榜（C-Eval、C-MMLU、OpenBookQA等）进行模型测试。&lt;/li&gt; 
 &lt;li&gt;实现Openai-Api协议的极简服务端，便于集成到第三方ChatUI使用（FastGPT、Open-WebUI等）。&lt;/li&gt; 
 &lt;li&gt;基于streamlit实现最简聊天WebUI前端。&lt;/li&gt; 
 &lt;li&gt;复现(蒸馏/RL)大型推理模型DeepSeek-R1的MiniMind-Reason模型，&lt;strong&gt;数据+模型&lt;/strong&gt;全部开源！&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;希望此开源项目可以帮助LLM初学者快速入门！&lt;/p&gt; 
&lt;h3&gt;👉&lt;strong&gt;更新日志&lt;/strong&gt;&lt;/h3&gt; 
&lt;details close&gt; 
 &lt;summary&gt; &lt;b&gt;2025-02-09 (newest 🎉🎉🎉)&lt;/b&gt; &lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;迎来发布以来重大更新，Release MiniMind2 Series。&lt;/li&gt; 
  &lt;li&gt;代码几乎全部重构，使用更简洁明了的统一结构。 如有旧代码的兼容性需要，可访问&lt;a href=&quot;https://github.com/jingyaogong/minimind/tree/6e9cd28ef9b34a0a10afbdf6f59e65cb6e628efb&quot;&gt;🔗旧仓库内容🔗&lt;/a&gt;。&lt;/li&gt; 
  &lt;li&gt;免去数据预处理步骤。统一数据集格式，更换为&lt;code&gt;jsonl&lt;/code&gt;格式杜绝数据集下载混乱的问题。&lt;/li&gt; 
  &lt;li&gt;MiniMind2系列效果相比MiniMind-V1显著提升。&lt;/li&gt; 
  &lt;li&gt;小问题：{kv-cache写法更标准、MoE的负载均衡loss被考虑等等}&lt;/li&gt; 
  &lt;li&gt;提供模型迁移到私有数据集的训练方案（医疗模型、自我认知样例）。&lt;/li&gt; 
  &lt;li&gt;精简预训练数据集，并大幅提升预训练数据质量，大幅缩短个人快速训练所需时间，单卡3090即可2小时复现！&lt;/li&gt; 
  &lt;li&gt;更新：LoRA微调脱离peft包装，从0实现LoRA过程；DPO算法从0使用PyTorch原生实现；模型白盒蒸馏原生实现。&lt;/li&gt; 
  &lt;li&gt;MiniMind2-DeepSeek-R1系列蒸馏模型诞生！&lt;/li&gt; 
  &lt;li&gt;MiniMind2具备一定的英文能力！&lt;/li&gt; 
  &lt;li&gt;更新MiniMind2与第三方模型的基于更多大模型榜单测试性能的结果。&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details close&gt; 
 &lt;summary&gt; &lt;b&gt;2024-10-05&lt;/b&gt; &lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;为MiniMind拓展了多模态能力之---视觉&lt;/li&gt; 
  &lt;li&gt;移步孪生项目&lt;a href=&quot;https://github.com/jingyaogong/minimind-v&quot;&gt;minimind-v&lt;/a&gt;查看详情！&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details close&gt; 
 &lt;summary&gt; &lt;b&gt;2024-09-27&lt;/b&gt; &lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;09-27更新pretrain数据集的预处理方式，为了保证文本完整性，放弃预处理成.bin训练的形式（轻微牺牲训练速度）。&lt;/li&gt; 
  &lt;li&gt;目前pretrain预处理后的文件命名为：pretrain_data.csv。&lt;/li&gt; 
  &lt;li&gt;删除了一些冗余的代码。&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details close&gt; 
 &lt;summary&gt; &lt;b&gt;2024-09-17&lt;/b&gt; &lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;更新minimind-v1-moe模型&lt;/li&gt; 
  &lt;li&gt;为了防止歧义，不再使用mistral_tokenizer分词，全部采用自定义的minimind_tokenizer作为分词器。&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details close&gt; 
 &lt;summary&gt; &lt;b&gt;2024-09-01&lt;/b&gt; &lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;更新minimind-v1 (108M)模型，采用minimind_tokenizer，预训练轮次3 + SFT轮次10，更充分训练，性能更强。&lt;/li&gt; 
  &lt;li&gt;项目已部署至ModelScope创空间，可以在此网站上体验：&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://www.modelscope.cn/studios/gongjy/minimind&quot;&gt;🔗ModelScope在线体验🔗&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details close&gt; 
 &lt;summary&gt; &lt;b&gt;2024-08-27&lt;/b&gt; &lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;项目首次开源&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h1&gt;📌 快速开始&lt;/h1&gt; 
&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt; 
 &lt;summary&gt;分享本人的软硬件配置（仅供参考）&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;CPU: Intel(R) Core(TM) i9-10980XE CPU @ 3.00GHz&lt;/li&gt; 
  &lt;li&gt;RAM: 128 GB&lt;/li&gt; 
  &lt;li&gt;GPU: NVIDIA GeForce RTX 3090(24GB) * 8&lt;/li&gt; 
  &lt;li&gt;Ubuntu==20.04&lt;/li&gt; 
  &lt;li&gt;CUDA==12.2&lt;/li&gt; 
  &lt;li&gt;Python==3.10.16&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/jingyaogong/minimind/master/requirements.txt&quot;&gt;requirements.txt&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;第0步&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;git clone https://github.com/jingyaogong/minimind.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Ⅰ 测试已有模型效果&lt;/h2&gt; 
&lt;h3&gt;1.环境准备&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2.下载模型&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;git clone https://huggingface.co/jingyaogong/MiniMind2
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;3.命令行问答&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# load=0: load from pytorch model, load=1: load from transformers-hf model
python eval_model.py --load 1 --model_mode 2
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;4.或启动WebUI&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# 可能需要`python&amp;gt;=3.10` 安装 `pip install streamlit`
# cd scripts
streamlit run web_demo.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Ⅱ 从0开始自己训练&lt;/h2&gt; 
&lt;h3&gt;1.环境准备&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple
&lt;/code&gt;&lt;/pre&gt; 
&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt; 
 &lt;summary&gt;注：提前测试Torch是否可用cuda&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;import torch
print(torch.cuda.is_available())
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;如果不可用，请自行去&lt;a href=&quot;https://download.pytorch.org/whl/torch_stable.html&quot;&gt;torch_stable&lt;/a&gt; 下载whl文件安装。参考&lt;a href=&quot;https://blog.csdn.net/weixin_45456738/article/details/141029610?ops_request_misc=&amp;amp;request_id=&amp;amp;biz_id=102&amp;amp;utm_term=%E5%AE%89%E8%A3%85torch&amp;amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-2-141029610.nonecase&amp;amp;spm=1018.2226.3001.4187&quot;&gt;链接&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;h3&gt;2.数据下载&lt;/h3&gt; 
&lt;p&gt;从下文提供的&lt;a href=&quot;https://www.modelscope.cn/datasets/gongjy/minimind_dataset/files&quot;&gt;数据集下载链接&lt;/a&gt; 下载需要的数据文件（创建&lt;code&gt;./dataset&lt;/code&gt;目录）并放到&lt;code&gt;./dataset&lt;/code&gt;下&lt;/p&gt; 
&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt; 
 &lt;summary&gt;注：数据集须知&lt;/summary&gt; 
 &lt;p&gt;默认推荐下载&lt;code&gt;pretrain_hq.jsonl&lt;/code&gt; + &lt;code&gt;sft_mini_512.jsonl&lt;/code&gt;最快速度复现Zero聊天模型。&lt;/p&gt; 
 &lt;p&gt;数据文件可自由选择，下文提供了多种搭配方案，可根据自己手头的训练需求和GPU资源进行适当组合。&lt;/p&gt; 
&lt;/details&gt; 
&lt;h3&gt;3.开始训练&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;3.1 预训练（学知识）&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python train_pretrain.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;执行预训练，得到 &lt;code&gt;pretrain_*.pth&lt;/code&gt; 作为预训练的输出权重（其中*为模型的dimension，默认为512）&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;3.2 监督微调（学对话方式）&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python train_full_sft.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;执行监督微调，得到 &lt;code&gt;full_sft_*.pth&lt;/code&gt; 作为指令微调的输出权重（其中&lt;code&gt;full&lt;/code&gt;即为全参数微调）&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt; 
 &lt;summary&gt;注：训练须知&lt;/summary&gt; 
 &lt;p&gt;所有训练过程默认每隔100步保存1次参数到文件&lt;code&gt;./out/***.pth&lt;/code&gt;（每次会覆盖掉旧权重文件）。&lt;/p&gt; 
 &lt;p&gt;简单起见，此处只写明两个阶段训练过程。如需其它训练 (LoRA, 蒸馏, 强化学习, 微调推理等) 可参考下文【实验】小节的详细说明。&lt;/p&gt; 
&lt;/details&gt; 
&lt;hr&gt; 
&lt;h3&gt;4.测试模型效果&lt;/h3&gt; 
&lt;p&gt;确保需要测试的模型&lt;code&gt;*.pth&lt;/code&gt;文件位于&lt;code&gt;./out/&lt;/code&gt;目录下。 也可以直接去&lt;a href=&quot;https://www.modelscope.cn/models/gongjy/MiniMind2-PyTorch/files&quot;&gt;此处&lt;/a&gt;下载使用我训练的&lt;code&gt;*.pth&lt;/code&gt;文件。&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python eval_model.py --model_mode 1 # 默认为0：测试pretrain模型效果，设置为1：测试full_sft模型效果
&lt;/code&gt;&lt;/pre&gt; 
&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt; 
 &lt;summary&gt;注：测试须知&lt;/summary&gt; 
 &lt;p&gt;如需详情，查看&lt;code&gt;eval_model.py&lt;/code&gt;脚本代码即可。model_mode分为 0: 预训练模型，1: SFT-Chat模型，2: RLHF-Chat模型，3: Reason模型&lt;/p&gt; 
&lt;/details&gt; 
&lt;hr&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] 所有训练脚本均为Pytorch原生框架，均支持多卡加速，假设你的设备有N (N＞1) 张显卡：&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;单机N卡启动训练方式 (DDP, 支持多机多卡集群)&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;torchrun --nproc_per_node N train_xxx.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt; 
 &lt;summary&gt;注：其它须知&lt;/summary&gt; 
 &lt;p&gt;单机N卡启动训练 (DeepSpeed)&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;deepspeed --master_port 29500 --num_gpus=N train_xxx.py
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;可根据需要开启wandb记录训练过程&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# 需要登录: wandb login
torchrun --nproc_per_node N train_xxx.py --use_wandb
# and
python train_xxx.py --use_wandb
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;通过添加&lt;code&gt;--use_wandb&lt;/code&gt;参数，可以记录训练过程，训练完成后，可以在wandb网站上查看训练过程。通过修改&lt;code&gt;wandb_project&lt;/code&gt; 和&lt;code&gt;wandb_run_name&lt;/code&gt;参数，可以指定项目名称和运行名称。&lt;/p&gt; 
&lt;/details&gt; 
&lt;h1&gt;📌 数据介绍&lt;/h1&gt; 
&lt;h2&gt;Ⅰ Tokenizer&lt;/h2&gt; 
&lt;p&gt;分词器将单词从自然语言通过“词典”映射到&lt;code&gt;0, 1, 36&lt;/code&gt;这样的数字，可以理解为数字就代表了单词在“词典”中的页码。 可以选择自己构造词表训练一个“词典”，代码可见&lt;code&gt;./scripts/train_tokenizer.py&lt;/code&gt;（仅供学习参考，若非必要无需再自行训练，MiniMind已自带tokenizer）。 或者选择比较出名的开源大模型分词器， 正如同直接用新华/牛津词典的优点是token编码压缩率很好，缺点是页数太多，动辄数十万个词汇短语； 自己训练的分词器，优点是词表长度和内容随意控制，缺点是压缩率很低（例如&quot;hello&quot;也许会被拆分为&quot;h e l l o&quot; 五个独立的token），且生僻词难以覆盖。 “词典”的选择固然很重要，LLM的输出本质上是SoftMax到词典N个词的多分类问题，然后通过“词典”解码到自然语言。 因为MiniMind体积需要严格控制，为了避免模型头重脚轻（词嵌入embedding层参数在LLM占比太高），所以词表长度短短益善。&lt;/p&gt; 
&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt; 
 &lt;summary&gt;Tokenizer介绍&lt;/summary&gt; 
 &lt;p&gt;第三方强大的开源模型例如Yi、qwen、chatglm、mistral、Llama3的tokenizer词表长度如下：&lt;/p&gt; 
 &lt;table&gt; 
  &lt;tbody&gt;
   &lt;tr&gt;
    &lt;th&gt;Tokenizer模型&lt;/th&gt;
    &lt;th&gt;词表大小&lt;/th&gt;
    &lt;th&gt;来源&lt;/th&gt;
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td&gt;yi tokenizer&lt;/td&gt;
    &lt;td&gt;64,000&lt;/td&gt;
    &lt;td&gt;01万物（中国）&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td&gt;qwen2 tokenizer&lt;/td&gt;
    &lt;td&gt;151,643&lt;/td&gt;
    &lt;td&gt;阿里云（中国）&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td&gt;glm tokenizer&lt;/td&gt;
    &lt;td&gt;151,329&lt;/td&gt;
    &lt;td&gt;智谱AI（中国）&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td&gt;mistral tokenizer&lt;/td&gt;
    &lt;td&gt;32,000&lt;/td&gt;
    &lt;td&gt;Mistral AI（法国）&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td&gt;llama3 tokenizer&lt;/td&gt;
    &lt;td&gt;128,000&lt;/td&gt;
    &lt;td&gt;Meta（美国）&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td&gt;minimind tokenizer&lt;/td&gt;
    &lt;td&gt;6,400&lt;/td&gt;
    &lt;td&gt;自定义&lt;/td&gt;
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;👉2024-09-17更新：为了防止过去的版本歧义&amp;amp;控制体积，minimind所有模型均使用minimind_tokenizer分词，废弃所有mistral_tokenizer版本。&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;pre&gt;&lt;code&gt;# 一些自言自语
&amp;gt; 尽管minimind_tokenizer长度很小，编解码效率弱于qwen2、glm等中文友好型分词器。
&amp;gt; 但minimind模型选择了自己训练的minimind_tokenizer作为分词器，以保持整体参数轻量，避免编码层和计算层占比失衡，头重脚轻，因为minimind的词表大小只有6400。
&amp;gt; 且minimind在实际测试中没有出现过生僻词汇解码失败的情况，效果良好。
&amp;gt; 由于自定义词表压缩长度到6400，使得LLM总参数量最低只有25.8M。
&amp;gt; 训练数据`tokenizer_train.jsonl`均来自于`匠数大模型数据集`，这部分数据相对次要，如需训练可以自由选择。
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;Ⅱ Pretrain数据&lt;/h2&gt; 
&lt;p&gt;经历了MiniMind-V1的低质量预训练数据，导致模型胡言乱语的教训，&lt;code&gt;2025-02-05&lt;/code&gt; 之后决定不再采用大规模无监督的数据集做预训练。 进而尝试把&lt;a href=&quot;https://www.modelscope.cn/datasets/deepctrl/deepctrl-sft-data&quot;&gt;匠数大模型数据集&lt;/a&gt;的中文部分提取出来， 清洗出字符&lt;code&gt;&amp;lt;512&lt;/code&gt;长度的大约1.6GB的语料直接拼接成预训练数据 &lt;code&gt;pretrain_hq.jsonl&lt;/code&gt;，hq即为high quality（当然也还不算high，提升数据质量无止尽）。&lt;/p&gt; 
&lt;p&gt;文件&lt;code&gt;pretrain_hq.jsonl&lt;/code&gt; 数据格式为&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;{&quot;text&quot;: &quot;如何才能摆脱拖延症？ 治愈拖延症并不容易，但以下建议可能有所帮助...&quot;}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Ⅲ SFT数据&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.modelscope.cn/datasets/deepctrl/deepctrl-sft-data&quot;&gt;匠数大模型SFT数据集&lt;/a&gt; “是一个完整、格式统一、安全的大模型训练和研究资源。 从网络上的公开数据源收集并整理了大量开源数据集，对其进行了格式统一，数据清洗， 包含10M条数据的中文数据集和包含2M条数据的英文数据集。” 以上是官方介绍，下载文件后的数据总量大约在4B tokens，肯定是适合作为中文大语言模型的SFT数据的。 但是官方提供的数据格式很乱，全部用来sft代价太大。 我将把官方数据集进行了二次清洗，把含有符号污染和噪声的条目去除；另外依然只保留了总长度&lt;code&gt;&amp;lt;512&lt;/code&gt; 的内容，此阶段希望通过大量对话补充预训练阶段欠缺的知识。 导出文件为&lt;code&gt;sft_512.jsonl&lt;/code&gt;(~7.5GB)。&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.modelscope.cn/organization/Magpie-Align&quot;&gt;Magpie-SFT数据集&lt;/a&gt; 收集了~1M条来自Qwen2/2.5的高质量对话，我将这部分数据进一步清洗，把总长度&lt;code&gt;&amp;lt;2048&lt;/code&gt;的部分导出为&lt;code&gt;sft_2048.jsonl&lt;/code&gt;(~9GB)。 长度&lt;code&gt;&amp;lt;1024&lt;/code&gt;的部分导出为&lt;code&gt;sft_1024.jsonl&lt;/code&gt;(~5.5GB)，用大模型对话数据直接进行sft就属于“黑盒蒸馏”的范畴。&lt;/p&gt; 
&lt;p&gt;进一步清洗前两步sft的数据（只保留中文字符占比高的内容），筛选长度&lt;code&gt;&amp;lt;512&lt;/code&gt;的对话，得到&lt;code&gt;sft_mini_512.jsonl&lt;/code&gt;(~1.2GB)。&lt;/p&gt; 
&lt;p&gt;所有sft文件 &lt;code&gt;sft_X.jsonl&lt;/code&gt; 数据格式均为&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;{
    &quot;conversations&quot;: [
        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;你好&quot;},
        {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;你好！&quot;},
        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;再见&quot;},
        {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;再见！&quot;}
    ]
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Ⅳ RLHF数据&lt;/h2&gt; 
&lt;p&gt;来自&lt;a href=&quot;https://www.modelscope.cn/datasets/Magpie-Align/MagpieLM-DPO-Data-v0.1&quot;&gt;Magpie-DPO数据集&lt;/a&gt; 大约200k条偏好数据（均是英文）生成自Llama3.1-70B/8B，可以用于训练奖励模型，优化模型回复质量，使其更加符合人类偏好。 这里将数据总长度&lt;code&gt;&amp;lt;3000&lt;/code&gt;的内容重组为&lt;code&gt;dpo.jsonl&lt;/code&gt;(~0.9GB)，包含&lt;code&gt;chosen&lt;/code&gt;和&lt;code&gt;rejected&lt;/code&gt;两个字段，&lt;code&gt;chosen&lt;/code&gt; 为偏好的回复，&lt;code&gt;rejected&lt;/code&gt;为拒绝的回复。&lt;/p&gt; 
&lt;p&gt;文件 &lt;code&gt;dpo.jsonl&lt;/code&gt; 数据格式为&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;{
  &quot;chosen&quot;: [
    {&quot;content&quot;: &quot;Q&quot;, &quot;role&quot;: &quot;user&quot;}, 
    {&quot;content&quot;: &quot;good answer&quot;, &quot;role&quot;: &quot;assistant&quot;}
  ], 
  &quot;rejected&quot;: [
    {&quot;content&quot;: &quot;Q&quot;, &quot;role&quot;: &quot;user&quot;}, 
    {&quot;content&quot;: &quot;bad answer&quot;, &quot;role&quot;: &quot;assistant&quot;}
  ]
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Ⅴ Reason数据集：&lt;/h2&gt; 
&lt;p&gt;不得不说2025年2月谁能火的过DeepSeek... 也激发了我对RL引导的推理模型的浓厚兴趣，目前已经用Qwen2.5复现了R1-Zero。 如果有时间+效果work（但99%基模能力不足）我会在之后更新MiniMind基于RL训练的推理模型而不是蒸馏模型。 时间有限，最快的低成本方案依然是直接蒸馏（黑盒方式）。 耐不住R1太火，短短几天就已经存在一些R1的蒸馏数据集&lt;a href=&quot;https://www.modelscope.cn/datasets/Magpie-Align/Magpie-Reasoning-V2-250K-CoT-Deepseek-R1-Llama-70B&quot;&gt;R1-Llama-70B&lt;/a&gt;、&lt;a href=&quot;https://www.modelscope.cn/datasets/AI-ModelScope/R1-Distill-SFT&quot;&gt;R1-Distill-SFT&lt;/a&gt;、 &lt;a href=&quot;https://huggingface.co/datasets/shareAI/Alpaca-Distill-R1-ZH&quot;&gt;Alpaca-Distill-R1&lt;/a&gt;、 &lt;a href=&quot;https://huggingface.co/datasets/jinliuxi/deepseek_r1_zh&quot;&gt;deepseek_r1_zh&lt;/a&gt;等等，纯中文的数据可能比较少。 最终整合它们，导出文件为&lt;code&gt;r1_mix_1024.jsonl&lt;/code&gt;，数据格式和&lt;code&gt;sft_X.jsonl&lt;/code&gt;一致。&lt;/p&gt; 
&lt;h2&gt;Ⅵ 更多数据集&lt;/h2&gt; 
&lt;p&gt;目前已经有&lt;a href=&quot;https://github.com/HqWu-HITCS/Awesome-Chinese-LLM&quot;&gt;HqWu-HITCS/Awesome-Chinese-LLM&lt;/a&gt; 在收集和梳理中文LLM相关的开源模型、应用、数据集及教程等资料，并持续更新这方面的最新进展。全面且专业，Respect！&lt;/p&gt; 
&lt;hr&gt; 
&lt;h2&gt;Ⅷ 数据集下载&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] 2025-02-05后，开源MiniMind最终训练所用的所有数据集，因此无需再自行预处理大规模数据集，避免重复性的数据处理工作。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;MiniMind训练数据集 (&lt;a href=&quot;https://www.modelscope.cn/datasets/gongjy/minimind_dataset/files&quot;&gt;ModelScope&lt;/a&gt; | &lt;a href=&quot;https://huggingface.co/datasets/jingyaogong/minimind_dataset/tree/main&quot;&gt;HuggingFace&lt;/a&gt;)&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;无需全部clone，可单独下载所需的文件&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;将下载的数据集文件放到&lt;code&gt;./dataset/&lt;/code&gt;目录下（✨为推荐的必须项）&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;./dataset/
├── dpo.jsonl (909MB)
├── lora_identity.jsonl (22.8KB)
├── lora_medical.jsonl (34MB)
├── pretrain_hq.jsonl (1.6GB, ✨)
├── r1_mix_1024.jsonl (340MB)
├── sft_1024.jsonl (5.6GB)
├── sft_2048.jsonl (9GB)
├── sft_512.jsonl (7.5GB)
├── sft_mini_512.jsonl (1.2GB, ✨)
└── tokenizer_train.jsonl (1GB)
&lt;/code&gt;&lt;/pre&gt; 
&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt; 
 &lt;summary&gt;注：各数据集简介&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;dpo.jsonl&lt;/code&gt; --RLHF阶段数据集&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;lora_identity.jsonl&lt;/code&gt; --自我认知数据集（例如：你是谁？我是minimind...），推荐用于lora训练（亦可用于全参SFT，勿被名字局限）&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;lora_medical.jsonl&lt;/code&gt; --医疗问答数据集，推荐用于lora训练（亦可用于全参SFT，勿被名字局限）&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;pretrain_hq.jsonl&lt;/code&gt;✨ --预训练数据集，整合自jiangshu科技&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;r1_mix_1024.jsonl&lt;/code&gt; --DeepSeek-R1-1.5B蒸馏数据，每条数据字符最大长度为1024（因此训练时设置max_seq_len=1024）&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;sft_1024.jsonl&lt;/code&gt; --整合自Qwen2.5蒸馏数据（是sft_2048的子集），每条数据字符最大长度为1024（因此训练时设置max_seq_len=1024）&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;sft_2048.jsonl&lt;/code&gt; --整合自Qwen2.5蒸馏数据，每条数据字符最大长度为2048（因此训练时设置max_seq_len=2048）&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;sft_512.jsonl&lt;/code&gt; --整合自匠数科技SFT数据，每条数据字符最大长度为512（因此训练时设置max_seq_len=512）&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;sft_mini_512.jsonl&lt;/code&gt;✨ --极简整合自匠数科技SFT数据+Qwen2.5蒸馏数据（用于快速训练Zero模型），每条数据字符最大长度为512（因此训练时设置max_seq_len=512）&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;tokenizer_train.jsonl&lt;/code&gt; --均来自于&lt;code&gt;匠数大模型数据集&lt;/code&gt;，这部分数据相对次要，（不推荐自己重复训练tokenizer，理由如上）如需自己训练tokenizer可以自由选择数据集。&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jingyaogong/minimind/master/images/dataset.jpg&quot; alt=&quot;dataset&quot;&gt;&lt;/p&gt; 
&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt; 
 &lt;summary&gt;说明 &amp;amp; 推荐训练方案&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;MiniMind2 Series均经过共约20GB语料训练，大约4B tokens，即对应上面的数据组合训练结果（开销：💰💰💰💰💰💰💰💰，效果：😊😊😊😊😊😊）&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;想要最快速度从0实现Zero模型，推荐使用&lt;code&gt;pretrain_hq.jsonl&lt;/code&gt; + &lt;code&gt;sft_mini_512.jsonl&lt;/code&gt; 的数据组合，具体花销和效果可查看下文表格（开销：💰，效果：😊😊）&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;推荐具备一定算力资源或更在意效果的朋友可以考虑前者完整复现MiniMind2；仅有单卡GPU或在乎短时间快速复现的朋友强烈推荐后者；&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;【折中方案】亦可选择例如&lt;code&gt;sft_mini_512.jsonl&lt;/code&gt;、&lt;code&gt;sft_1024.jsonl&lt;/code&gt;中等规模数据进行自由组合训练（开销：💰💰💰，效果：😊😊😊😊）。&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h1&gt;📌 Model Structure&lt;/h1&gt; 
&lt;p&gt;MiniMind-Dense（和&lt;a href=&quot;https://ai.meta.com/blog/meta-llama-3-1/&quot;&gt;Llama3.1&lt;/a&gt;一样）使用了Transformer的Decoder-Only结构，跟GPT-3的区别在于：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;采用了GPT-3的预标准化方法，也就是在每个Transformer子层的输入上进行归一化，而不是在输出上。具体来说，使用的是RMSNorm归一化函数。&lt;/li&gt; 
 &lt;li&gt;用SwiGLU激活函数替代了ReLU，这样做是为了提高性能。&lt;/li&gt; 
 &lt;li&gt;像GPT-Neo一样，去掉了绝对位置嵌入，改用了旋转位置嵌入（RoPE），这样在处理超出训练长度的推理时效果更好。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr&gt; 
&lt;p&gt;MiniMind-MoE模型，它的结构基于Llama3和&lt;a href=&quot;https://arxiv.org/pdf/2405.04434&quot;&gt;Deepseek-V2/3&lt;/a&gt;中的MixFFN混合专家模块。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;DeepSeek-V2在前馈网络（FFN）方面，采用了更细粒度的专家分割和共享的专家隔离技术，以提高Experts的效果。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr&gt; 
&lt;p&gt;MiniMind的整体结构一致，只是在RoPE计算、推理函数和FFN层的代码上做了一些小调整。 其结构如下图（重绘版）：&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jingyaogong/minimind/master/images/LLM-structure.png&quot; alt=&quot;structure&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jingyaogong/minimind/master/images/LLM-structure-moe.png&quot; alt=&quot;structure-moe&quot;&gt;&lt;/p&gt; 
&lt;p&gt;修改模型配置见&lt;a href=&quot;https://raw.githubusercontent.com/jingyaogong/minimind/master/model/LMConfig.py&quot;&gt;./model/LMConfig.py&lt;/a&gt;。 参考模型参数版本见下表：&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model Name&lt;/th&gt; 
   &lt;th&gt;params&lt;/th&gt; 
   &lt;th&gt;len_vocab&lt;/th&gt; 
   &lt;th&gt;rope_theta&lt;/th&gt; 
   &lt;th&gt;n_layers&lt;/th&gt; 
   &lt;th&gt;d_model&lt;/th&gt; 
   &lt;th&gt;kv_heads&lt;/th&gt; 
   &lt;th&gt;q_heads&lt;/th&gt; 
   &lt;th&gt;share+route&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MiniMind2-Small&lt;/td&gt; 
   &lt;td&gt;26M&lt;/td&gt; 
   &lt;td&gt;6400&lt;/td&gt; 
   &lt;td&gt;1e6&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;512&lt;/td&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MiniMind2-MoE&lt;/td&gt; 
   &lt;td&gt;145M&lt;/td&gt; 
   &lt;td&gt;6400&lt;/td&gt; 
   &lt;td&gt;1e6&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;640&lt;/td&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;1+4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MiniMind2&lt;/td&gt; 
   &lt;td&gt;104M&lt;/td&gt; 
   &lt;td&gt;6400&lt;/td&gt; 
   &lt;td&gt;1e6&lt;/td&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;768&lt;/td&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;minimind-v1-small&lt;/td&gt; 
   &lt;td&gt;26M&lt;/td&gt; 
   &lt;td&gt;6400&lt;/td&gt; 
   &lt;td&gt;1e4&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;512&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;minimind-v1-moe&lt;/td&gt; 
   &lt;td&gt;4×26M&lt;/td&gt; 
   &lt;td&gt;6400&lt;/td&gt; 
   &lt;td&gt;1e4&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;512&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;1+4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;minimind-v1&lt;/td&gt; 
   &lt;td&gt;108M&lt;/td&gt; 
   &lt;td&gt;6400&lt;/td&gt; 
   &lt;td&gt;1e4&lt;/td&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;768&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;📌 Experiment&lt;/h1&gt; 
&lt;h2&gt;Ⅰ 训练开销&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;时间单位&lt;/strong&gt;：小时 (h)。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;成本单位&lt;/strong&gt;：人民币 (￥)；7￥ ≈ 1美元。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;3090 租卡单价&lt;/strong&gt;：≈1.3￥/h（可自行参考实时市价）。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;参考标准&lt;/strong&gt;：表格仅实测 &lt;code&gt;pretrain&lt;/code&gt; 和 &lt;code&gt;sft_mini_512&lt;/code&gt; 两个数据集的训练时间，其它耗时根据数据集大小估算（可能存在些许出入）。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;基于 3090 （单卡）成本计算&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model Name&lt;/th&gt; 
   &lt;th&gt;params&lt;/th&gt; 
   &lt;th&gt;pretrain&lt;/th&gt; 
   &lt;th&gt;sft_mini_512&lt;/th&gt; 
   &lt;th&gt;sft_512&lt;/th&gt; 
   &lt;th&gt;sft_1024&lt;/th&gt; 
   &lt;th&gt;sft_2048&lt;/th&gt; 
   &lt;th&gt;RLHF&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MiniMind2-Small&lt;/td&gt; 
   &lt;td&gt;26M&lt;/td&gt; 
   &lt;td&gt;≈1.1h&lt;br&gt;≈1.43￥&lt;/td&gt; 
   &lt;td&gt;≈1h&lt;br&gt;≈1.3￥&lt;/td&gt; 
   &lt;td&gt;≈6h&lt;br&gt;≈7.8￥&lt;/td&gt; 
   &lt;td&gt;≈4.58h&lt;br&gt;≈5.95￥&lt;/td&gt; 
   &lt;td&gt;≈7.5h&lt;br&gt;≈9.75￥&lt;/td&gt; 
   &lt;td&gt;≈1h&lt;br&gt;≈1.3￥&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MiniMind2&lt;/td&gt; 
   &lt;td&gt;104M&lt;/td&gt; 
   &lt;td&gt;≈3.9h&lt;br&gt;≈5.07￥&lt;/td&gt; 
   &lt;td&gt;≈3.3h&lt;br&gt;≈4.29￥&lt;/td&gt; 
   &lt;td&gt;≈20h&lt;br&gt;≈26￥&lt;/td&gt; 
   &lt;td&gt;≈15h&lt;br&gt;≈19.5￥&lt;/td&gt; 
   &lt;td&gt;≈25h&lt;br&gt;≈32.5￥&lt;/td&gt; 
   &lt;td&gt;≈3h&lt;br&gt;≈3.9￥&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr&gt; 
&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt; 
 &lt;summary&gt;训练开销总结&amp;amp;预测&lt;/summary&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;MiniMind2-Small参数&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;&lt;code&gt;pretrain_hq&lt;/code&gt;+&lt;code&gt;sft_mini_512&lt;/code&gt;数据集 &lt;br&gt;单卡3090 (1 epoch) + 2.1小时 + 花费2.73元人民币 &lt;br&gt;即可从0训练出MiniMind-Zero-0.025B模型!!!&lt;/p&gt; 
  &lt;/blockquote&gt; 
 &lt;/blockquote&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;MiniMind2-Small参数&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;&lt;code&gt;pretrain_hq&lt;/code&gt;+&lt;code&gt;sft_512&lt;/code&gt;+&lt;code&gt;sft_2048&lt;/code&gt;+&lt;code&gt;dpo&lt;/code&gt;数据集 &lt;br&gt;单卡3090 (2 epochs) + 大约38.16小时 + 花费49.61元人民币 &lt;br&gt;即可从0训练出MiniMind2-Small-0.025B模型!!!&lt;/p&gt; 
  &lt;/blockquote&gt; 
 &lt;/blockquote&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;MiniMind2参数&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;&lt;code&gt;pretrain_hq&lt;/code&gt;+&lt;code&gt;sft_512&lt;/code&gt;+&lt;code&gt;sft_2048&lt;/code&gt;+&lt;code&gt;dpo&lt;/code&gt;数据集 &lt;br&gt;单卡3090 (2 epochs) + 大约122小时 + 花费158.6元人民币 &lt;br&gt;即可从0训练出MiniMind2-0.1B模型!!!&lt;/p&gt; 
  &lt;/blockquote&gt; 
 &lt;/blockquote&gt; 
&lt;/details&gt; 
&lt;p&gt;✨基于单卡NVIDIA 3090的&lt;code&gt;MiniMind-Zero&lt;/code&gt;从0训练仅需&lt;code&gt;2小时&lt;/code&gt; + &lt;code&gt;3块钱&lt;/code&gt;，实现ChatBot效果！&lt;/p&gt; 
&lt;p&gt;✨PS：若采用8卡4090训练，总用时甚至可以压缩到10分钟以内！（由于时间更短，花费同样3元左右，与单卡成本相当）&lt;/p&gt; 
&lt;p&gt;✨以极低极低的门槛，实现人人可玩的大模型自由！这正是MiniMind系列的诞生初衷！&lt;/p&gt; 
&lt;p&gt;✨仅价值&lt;code&gt;3块钱&lt;/code&gt;成本的&lt;code&gt;MiniMind-Zero&lt;/code&gt;并不是噱头！Chat测试：&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;👶: 请介绍一下自己。
🤖️: 作为人工智能，我没有实际的生活，也没有自我意识，所以没有自己的生活。我被设计成能够帮助用户解答问题、提供信息、进行对话等。我的设计和功能是由计算机科学、机器学习、人工智能技术和算法所构建的，我通过编程和算法实现。

👶: 推荐一些杭州的特色美食吧。
🤖️: 杭州有很多美味的特色美食，比如鸡肉串、鳗鱼头、水煮鱼、豆腐脑等。这些美食都非常美味，有着独特的风味和口感，绝对是值得一试的美食。

👶: 请为我讲解“大语言模型”这个概念。
🤖️: 大语言模型是一种人工智能技术，它使用大量的文本数据来训练模型，然后在不断的迭代中不断优化模型。它的目标是根据输入的数据来生成符合特定需求的文本，这些文本可以是任何你感兴趣的主题。

👶: Introduce the history of the United States, please.
🤖️: 您提到的“Introok&#39;s the believeations of theument.&quot; 这个名字来源于中国古代的&quot;groty of of the change.&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;极速且初具效果，甚至仍然可以进一步压缩获取更小更优质的训练数据。 Zero模型权重保存为 &lt;code&gt;full_sft_512_zero.pth&lt;/code&gt;（见下文MiniMind模型文件链接），如有兴趣可下载检验此模型效果。&lt;/p&gt; 
&lt;hr&gt; 
&lt;h2&gt;Ⅱ 主要训练步骤&lt;/h2&gt; 
&lt;h3&gt;&lt;strong&gt;1. 预训练(Pretrain)&lt;/strong&gt;:&lt;/h3&gt; 
&lt;p&gt;LLM首先要学习的并非直接与人交流，而是让网络参数中充满知识的墨水，“墨水” 理论上喝的越饱越好，产生大量的对世界的知识积累。 预训练就是让Model先埋头苦学大量基本的知识，例如从Wiki百科、新闻、书籍整理大规模的高质量训练数据。 这个过程是“无监督”的，即人类不需要在过程中做任何“有监督”的校正，而是由模型自己从大量文本中总结规律学习知识点。 模型此阶段目的只有一个：&lt;strong&gt;学会词语接龙&lt;/strong&gt;。例如我们输入“秦始皇”四个字，它可以接龙“是中国的第一位皇帝”。&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;torchrun --nproc_per_node 1 train_pretrain.py # 1即为单卡训练，可根据硬件情况自行调整 (设置&amp;gt;=2)
# or
python train_pretrain.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;训练后的模型权重文件默认每隔&lt;code&gt;100步&lt;/code&gt;保存为: &lt;code&gt;pretrain_*.pth&lt;/code&gt;（* 为模型具体dimension，每次保存时新文件会覆盖旧文件）&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;&lt;strong&gt;2. 有监督微调(Supervised Fine-Tuning)&lt;/strong&gt;:&lt;/h3&gt; 
&lt;p&gt;经过预训练，LLM此时已经掌握了大量知识，然而此时它只会无脑地词语接龙，还不会与人聊天。 SFT阶段就需要把半成品LLM施加一个自定义的聊天模板进行微调。 例如模型遇到这样的模板【问题-&amp;gt;回答，问题-&amp;gt;回答】后不再无脑接龙，而是意识到这是一段完整的对话结束。 称这个过程为指令微调，就如同让已经学富五车的「牛顿」先生适应21世纪智能手机的聊天习惯，学习屏幕左侧是对方消息，右侧是本人消息这个规律。 在训练时，MiniMind的指令和回答长度被截断在512，是为了节省显存空间。就像我们学习时，会先从短的文章开始，当学会写作200字作文后，800字文章也可以手到擒来。 在需要长度拓展时，只需要准备少量的2k/4k/8k长度对话数据进行进一步微调即可（此时最好配合RoPE-NTK的基准差值）。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;在推理时通过调整RoPE线性差值，实现免训练长度外推到2048及以上将会很方便。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;torchrun --nproc_per_node 1 train_full_sft.py
# or
python train_full_sft.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;训练后的模型权重文件默认每隔&lt;code&gt;100步&lt;/code&gt;保存为: &lt;code&gt;full_sft_*.pth&lt;/code&gt;（* 为模型具体dimension，每次保存时新文件会覆盖旧文件）&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Ⅲ 其它训练步骤&lt;/h2&gt; 
&lt;h3&gt;&lt;strong&gt;3. 人类反馈强化学习(Reinforcement Learning from Human Feedback, RLHF)&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;在前面的训练步骤中，模型已经具备了基本的对话能力，但是这样的能力完全基于单词接龙，缺少正反样例的激励。 模型此时尚未知什么回答是好的，什么是差的。我们希望它能够更符合人的偏好，降低让人类不满意答案的产生概率。 这个过程就像是让模型参加新的培训，从优秀员工的作为例子，消极员工作为反例，学习如何更好地回复。 此处使用的是RLHF系列之-直接偏好优化(Direct Preference Optimization, DPO)。 与PPO(Proximal Policy Optimization)这种需要奖励模型、价值模型的RL算法不同； DPO通过推导PPO奖励模型的显式解，把在线奖励模型换成离线数据，Ref模型输出可以提前保存。 DPO性能几乎不变，只用跑 actor_model 和 ref_model 两个模型，大大节省显存开销和增加训练稳定性。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;注：RLHF训练步骤&lt;strong&gt;并非必须&lt;/strong&gt;，此步骤难以提升模型“智力”而通常仅用于提升模型的“礼貌”，有利（符合偏好、减少有害内容）也有弊（样本收集昂贵、反馈偏差、多样性损失）。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;torchrun --nproc_per_node 1 train_dpo.py
# or
python train_dpo.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;训练后的模型权重文件默认每隔&lt;code&gt;100步&lt;/code&gt;保存为: &lt;code&gt;rlhf_*.pth&lt;/code&gt;（* 为模型具体dimension，每次保存时新文件会覆盖旧文件）&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;&lt;strong&gt;4. 知识蒸馏(Knowledge Distillation, KD)&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;在前面的所有训练步骤中，模型已经完全具备了基本能力，通常可以学成出师了。 而知识蒸馏可以进一步优化模型的性能和效率，所谓知识蒸馏，即学生模型面向教师模型学习。 教师模型通常是经过充分训练的大模型，具有较高的准确性和泛化能力。 学生模型是一个较小的模型，目标是学习教师模型的行为，而不是直接从原始数据中学习。 在SFT学习中，模型的目标是拟合词Token分类硬标签（hard labels），即真实的类别标签（如 0 或 6400）。 在知识蒸馏中，教师模型的softmax概率分布被用作软标签（soft labels）。小模型仅学习软标签，并使用KL-Loss来优化模型的参数。 通俗地说，SFT直接学习老师给的解题答案。而KD过程相当于“打开”老师聪明的大脑，尽可能地模仿老师“大脑”思考问题的神经元状态。 例如，当老师模型计算&lt;code&gt;1+1=2&lt;/code&gt;这个问题的时候，最后一层神经元a状态为0，神经元b状态为100，神经元c状态为-99... 学生模型通过大量数据，学习教师模型大脑内部的运转规律。这个过程即称之为：知识蒸馏。 知识蒸馏的目的只有一个：让小模型体积更小的同时效果更好。 然而随着LLM诞生和发展，模型蒸馏一词被广泛滥用，从而产生了“白盒/黑盒”知识蒸馏两个派别。 GPT-4这种闭源模型，由于无法获取其内部结构，因此只能面向它所输出的数据学习，这个过程称之为黑盒蒸馏，也是大模型时代最普遍的做法。 黑盒蒸馏与SFT过程完全一致，只不过数据是从大模型的输出收集，因此只需要准备数据并且进一步FT即可。 注意更改被加载的基础模型为&lt;code&gt;full_sft_*.pth&lt;/code&gt;，即基于微调模型做进一步的蒸馏学习。 &lt;code&gt;./dataset/sft_1024.jsonl&lt;/code&gt;与&lt;code&gt;./dataset/sft_2048.jsonl&lt;/code&gt; 均收集自qwen2.5-7/72B-Instruct大模型，可直接用于SFT以获取Qwen的部分行为。&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# 注意需要更改train_full_sft.py数据集路径，以及max_seq_len  
torchrun --nproc_per_node 1 train_full_sft.py
# or
python train_full_sft.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;训练后的模型权重文件默认每隔&lt;code&gt;100步&lt;/code&gt;同样保存为: &lt;code&gt;full_sft_*.pth&lt;/code&gt;（*为模型具体dimension，每次保存时新文件会覆盖旧文件）&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;此处应当着重介绍MiniMind实现的白盒蒸馏代码&lt;code&gt;train_distillation.py&lt;/code&gt;，由于MiniMind同系列本身并不存在强大的教师模型，因此白盒蒸馏代码仅作为学习参考。&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;torchrun --nproc_per_node 1 train_distillation.py
# or
python train_distillation.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;&lt;strong&gt;5. LoRA (Low-Rank Adaptation)&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;LoRA是一种高效的参数高效微调（Parameter-Efficient Fine-Tuning, PEFT）方法，旨在通过低秩分解的方式对预训练模型进行微调。 相比于全参数微调（Full Fine-Tuning），LoRA 只需要更新少量的参数。 LoRA 的核心思想是：在模型的权重矩阵中引入低秩分解，仅对低秩部分进行更新，而保持原始预训练权重不变。 代码可见&lt;code&gt;./model/model_lora.py&lt;/code&gt;和&lt;code&gt;train_lora.py&lt;/code&gt;，完全从0实现LoRA流程，不依赖第三方库的封装。&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;torchrun --nproc_per_node 1 train_lora.py
# or
python train_lora.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;训练后的模型权重文件默认每隔&lt;code&gt;100步&lt;/code&gt;保存为: &lt;code&gt;lora_xxx_*.pth&lt;/code&gt;（* 为模型具体dimension，每次保存时新文件会覆盖旧文件）&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;非常多的人困惑，如何使模型学会自己私有领域的知识？如何准备数据集？如何迁移通用领域模型打造垂域模型？ 这里举几个例子，对于通用模型，医学领域知识欠缺，可以尝试在原有模型基础上加入领域知识，以获得更好的性能。 同时，我们通常不希望学会领域知识的同时损失原有基础模型的其它能力，此时LoRA可以很好的改善这个问题。 只需要准备如下格式的对话数据集放置到&lt;code&gt;./dataset/lora_xxx.jsonl&lt;/code&gt;，启动 &lt;code&gt;python train_lora.py&lt;/code&gt; 训练即可得到&lt;code&gt;./out/lora/lora_xxx.pth&lt;/code&gt;新模型权重。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;医疗场景&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt; {&quot;conversations&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;请问颈椎病的人枕头多高才最好？&quot;}, {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;颈椎病患者选择枕头的高度应该根据...&quot;}]}
 {&quot;conversations&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;请问xxx&quot;}, {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;xxx...&quot;}]}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;自我认知场景&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt; {&quot;conversations&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;你叫什么名字？&quot;}, {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;我叫minimind...&quot;}]}
 {&quot;conversations&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;你是谁&quot;}, {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;我是...&quot;}]}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;此时【基础模型+LoRA模型】即可获得医疗场景模型增强的能力，相当于为基础模型增加了LoRA外挂，这个过程并不损失基础模型的本身能力。 我们可以通过&lt;code&gt;eval_model.py&lt;/code&gt;进行模型评估测试。&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# 注意：model_mode即选择基础模型的类型，这和train_lora是基于哪个模型训练的相关，确保统一即可。
python eval_model.py --lora_name &#39;lora_medical&#39; --model_mode 2
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;小测试&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;👶: 我最近经常感到头晕，可能是什么原因？
🤖️: 头晕可能是由于多种原因，包括但不限于：心脏疾病、眼睛损伤、过敏反应、皮肤问题、过敏反应、压力、焦虑、疲劳、药物副作用、性功能障碍或者其他...

---

👶: 你和openai是什么关系？
🤖️: 我是 MiniMind，一个由 Jingyao Gong 开发的人工智能助手。我通过自然语言处理和算法训练来与用户进行交互。
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;PS：只要有所需要的数据集，也可以full_sft全参微调（需要进行通用知识的混合配比，否则过拟合领域数据会让模型变傻，损失通用性）&lt;/p&gt; 
&lt;h3&gt;&lt;strong&gt;6. 训练推理模型 (Reasoning Model)&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;DeepSeek-R1实在太火了，几乎重新指明了未来LLM的新范式。 论文指出&lt;code&gt;&amp;gt;3B&lt;/code&gt;的模型经历多次反复的冷启动和RL奖励训练才能获得肉眼可见的推理能力提升。 最快最稳妥最经济的做法，以及最近爆发的各种各样所谓的推理模型几乎都是直接面向数据进行蒸馏训练， 但由于缺乏技术含量，蒸馏派被RL派瞧不起（hhhh）。 本人迅速已经在Qwen系列1.5B小模型上进行了尝试，很快复现了Zero过程的数学推理能力。 然而一个遗憾的共识是：参数太小的模型直接通过冷启动SFT+GRPO几乎不可能获得任何推理效果。 MiniMind2第一时间只能坚定不移的选择做蒸馏派，日后基于0.1B模型的RL如果同样取得小小进展会更新此部分的训练方案。&lt;/p&gt; 
&lt;p&gt;做蒸馏需要准备的依然是和SFT阶段同样格式的数据即可，数据集来源已如上文介绍。数据格式例如：&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-json&quot;&gt;{
  &quot;conversations&quot;: [
    {
      &quot;role&quot;: &quot;user&quot;,
      &quot;content&quot;: &quot;你好，我是小芳，很高兴认识你。&quot;
    },
    {
      &quot;role&quot;: &quot;assistant&quot;,
      &quot;content&quot;: &quot;&amp;lt;think&amp;gt;\n你好！我是由中国的个人开发者独立开发的智能助手MiniMind-R1-Lite-Preview，很高兴为您提供服务！\n&amp;lt;/think&amp;gt;\n&amp;lt;answer&amp;gt;\n你好！我是由中国的个人开发者独立开发的智能助手MiniMind-R1-Lite-Preview，很高兴为您提供服务！\n&amp;lt;/answer&amp;gt;&quot;
    }
  ]
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;推理模型R1的回复模板是：&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&amp;lt;think&amp;gt;\n思考过程\n&amp;lt;/think&amp;gt;\n
&amp;lt;answer&amp;gt;\n最终回答\n&amp;lt;/answer&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;这在GRPO中通过设置规则奖励函数约束模型符合思考标签和回复标签（在冷启动靠前的阶段奖励值设置应该提高一些）&lt;/p&gt; 
&lt;p&gt;另一个问题是蒸馏过程虽然和SFT一样，但实验结果是模型难以每次都符合模板规范的回复，即脱离思考和回复标签约束。 这里的小技巧是增加标记位置token的损失惩罚，详见&lt;code&gt;train_distill_reason.py&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;# 在 sp_ids 对应的位置增加额外的惩罚
...
loss_mask[sp_ids] = 10 # 惩罚系数
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;另另一个tips是由于推理数据由于只筛选了&lt;code&gt;&amp;lt;1024&lt;/code&gt;长度的数据，其中多轮对话和英文数据偏少， 因此&lt;code&gt;r1_mix_1024.jsonl&lt;/code&gt;进行了大约10k条多轮对话+英文数据的混合，防止模型遗忘严重。&lt;/p&gt; 
&lt;p&gt;脚本默认基于rlhf后的基模型做推理能力的蒸馏微调，下面直接启动训练即可：&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;torchrun --nproc_per_node 1 train_distill_reason.py
# or
python train_distill_reason.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;训练后的模型权重文件默认每隔&lt;code&gt;100步&lt;/code&gt;保存为: &lt;code&gt;reason_*.pth&lt;/code&gt;（*为模型具体dimension，每次保存时新文件会覆盖旧文件）&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;测试一下：&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;👶: 请介绍一下自己。
🤖️: &amp;lt;think&amp;gt;
嗯，用户让我介绍一下自己。我现在需要解释一下自己。首先，我应该明确用户是谁，是学生、职场人士还是普通人。用户可能是一个学生或者职场人士，所以信息传递应该是帮助学生和职场人士的信息。
然后，我需要回应用户，这可能包括介绍自己的位置、兴趣爱好、职业经历等信息。用户还可能想了解自己在哪里，比如职业发展、工作经验等。
我应该注意用词要正式，不要太正式，同时要保持简洁明了。这样用户容易理解，也方便他们查阅相关资料。
最后，我应该总结一下，让用户知道我是如何帮助他们完成他们的任务的，这样他们有更多关于我的信息。
&amp;lt;/think&amp;gt;
&amp;lt;answer&amp;gt;
我是**“你的名字”，你叫什么名字？
&amp;lt;/answer&amp;gt;

👶: 你更擅长哪一个学科？
🤖️: &amp;lt;think&amp;gt;
您好！我是由中国的个人开发者开发的智能助手MiniMind-R1。如您有任何任何问题，我会尽我所能为您提供帮助。
&amp;lt;/think&amp;gt;
&amp;lt;answer&amp;gt;
您好！我是由中国的个人开发者开发的智能助手MiniMind-R1。如您有任何任何问题，我会尽我所能为您提供帮助。
&amp;lt;/answer&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr&gt; 
&lt;h2&gt;Ⅳ 模型参数设定&lt;/h2&gt; 
&lt;p&gt;📋关于LLM的参数配置，有一篇很有意思的论文&lt;a href=&quot;https://arxiv.org/pdf/2402.14905&quot;&gt;MobileLLM&lt;/a&gt;做了详细的研究和实验。 Scaling Law在小模型中有自己独特的规律。 引起Transformer参数成规模变化的参数几乎只取决于&lt;code&gt;d_model&lt;/code&gt;和&lt;code&gt;n_layers&lt;/code&gt;。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;d_model&lt;/code&gt;↑ + &lt;code&gt;n_layers&lt;/code&gt;↓ -&amp;gt; 矮胖子&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;d_model&lt;/code&gt;↓ + &lt;code&gt;n_layers&lt;/code&gt;↑ -&amp;gt; 瘦高个&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;2020年提出Scaling Law的论文认为，训练数据量、参数量以及训练迭代次数才是决定性能的关键因素，而模型架构的影响几乎可以忽视。 然而似乎这个定律对小模型并不完全适用。 MobileLLM提出架构的深度比宽度更重要，「深而窄」的「瘦长」模型可以学习到比「宽而浅」模型更多的抽象概念。 例如当模型参数固定在125M或者350M时，30～42层的「狭长」模型明显比12层左右的「矮胖」模型有更优越的性能， 在常识推理、问答、阅读理解等8个基准测试上都有类似的趋势。 这其实是非常有趣的发现，因为以往为100M左右量级的小模型设计架构时，几乎没人尝试过叠加超过12层。 这与MiniMind在训练过程中，模型参数量在&lt;code&gt;d_model&lt;/code&gt;和&lt;code&gt;n_layers&lt;/code&gt;之间进行调整实验观察到的效果是一致的。 然而「深而窄」的「窄」也是有维度极限的，当d_model&amp;lt;512时，词嵌入维度坍塌的劣势非常明显， 增加的layers并不能弥补词嵌入在固定q_head带来d_head不足的劣势。 当d_model&amp;gt;1536时，layers的增加似乎比d_model的优先级更高，更能带来具有“性价比”的参数-&amp;gt;效果增益。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;因此MiniMind设定small模型dim=512，n_layers=8来获取的「极小体积&amp;lt;-&amp;gt;更好效果」的平衡。&lt;/li&gt; 
 &lt;li&gt;设定dim=768，n_layers=16来获取效果的更大收益，更加符合小模型Scaling-Law的变化曲线。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;作为参考，GPT3的参数设定见下表： &lt;img src=&quot;https://raw.githubusercontent.com/jingyaogong/minimind/master/images/gpt3_config.png&quot; alt=&quot;gpt3_config.png&quot;&gt;&lt;/p&gt; 
&lt;hr&gt; 
&lt;h2&gt;Ⅴ 训练结果&lt;/h2&gt; 
&lt;p&gt;MiniMind2 模型训练损失走势（由于数据集在训练后又更新清洗多次，因此Loss仅供参考）&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;models&lt;/th&gt; 
   &lt;th&gt;pretrain (length-512)&lt;/th&gt; 
   &lt;th&gt;sft (length-512)&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MiniMind2-Small&lt;/td&gt; 
   &lt;td&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jingyaogong/minimind/master/images/pre_512_loss.png&quot; width=&quot;100%&quot;&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jingyaogong/minimind/master/images/sft_512_loss.png&quot; width=&quot;100%&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MiniMind2&lt;/td&gt; 
   &lt;td&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jingyaogong/minimind/master/images/pre_768_loss.png&quot; width=&quot;100%&quot;&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jingyaogong/minimind/master/images/sft_768_loss.png&quot; width=&quot;100%&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;训练完成-模型合集&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;考虑到多人反应百度网盘速度慢，MiniMind2及以后全部使用ModelScope/HuggingFace托管。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;① PyTorch原生模型&lt;/h4&gt; 
&lt;p&gt;MiniMind2模型权重 (&lt;a href=&quot;https://www.modelscope.cn/models/gongjy/MiniMind2-PyTorch&quot;&gt;ModelScope&lt;/a&gt; | &lt;a href=&quot;https://huggingface.co/jingyaogong/MiniMind2-Pytorch&quot;&gt;HuggingFace&lt;/a&gt;)&lt;/p&gt; 
&lt;p&gt;MiniMind-V1模型权重 (&lt;a href=&quot;https://pan.baidu.com/s/1KUfSzEkSXYbCCBj0Pw-9fA?pwd=6666&quot;&gt;百度网盘&lt;/a&gt;)&lt;/p&gt; 
&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt; 
 &lt;summary&gt;Torch文件命名对照&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model Name&lt;/th&gt; 
    &lt;th&gt;params&lt;/th&gt; 
    &lt;th&gt;pretrain_model&lt;/th&gt; 
    &lt;th&gt;sft_model&lt;/th&gt; 
    &lt;th&gt;rl_model&lt;/th&gt; 
    &lt;th&gt;reason_model&lt;/th&gt; 
    &lt;th&gt;lora_model&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;MiniMind2-small&lt;/td&gt; 
    &lt;td&gt;26M&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;pretrain_512.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;full_sft_512.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;rlhf_512.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;reason_512.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;lora_xxx_512.pth&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;MiniMind2-MoE&lt;/td&gt; 
    &lt;td&gt;145M&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;pretrain_640_moe.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;full_sft_640_moe.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;rlhf_640_moe.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;MiniMind2&lt;/td&gt; 
    &lt;td&gt;104M&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;pretrain_768.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;full_sft_768.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;rlhf_768.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;reason_768.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;lora_xxx_768.pth&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model Name&lt;/th&gt; 
    &lt;th&gt;params&lt;/th&gt; 
    &lt;th&gt;pretrain_model&lt;/th&gt; 
    &lt;th&gt;单轮对话sft&lt;/th&gt; 
    &lt;th&gt;多轮对话sft&lt;/th&gt; 
    &lt;th&gt;rl_model&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;minimind-v1-small&lt;/td&gt; 
    &lt;td&gt;26M&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;pretrain_512.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;single_chat/full_sft_512.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;multi_chat/full_sft_512.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;rl_512.pth&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;minimind-v1-moe&lt;/td&gt; 
    &lt;td&gt;4×26M&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;pretrain_512_moe.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;single_chat/full_sft_512_moe.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;multi_chat/full_sft_512_moe.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;minimind-v1&lt;/td&gt; 
    &lt;td&gt;108M&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;pretrain_768.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;single_chat/full_sft_768.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;multi_chat/full_sft_768.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;rl_768.pth&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h4&gt;② Transformers模型&lt;/h4&gt; 
&lt;p&gt;MiniMind系列 (&lt;a href=&quot;https://www.modelscope.cn/collections/MiniMind-b72f4cfeb74b47&quot;&gt;ModelScope&lt;/a&gt; | &lt;a href=&quot;https://huggingface.co/collections/jingyaogong/minimind-66caf8d999f5c7fa64f399e5&quot;&gt;HuggingFace&lt;/a&gt;)&lt;/p&gt; 
&lt;hr&gt; 
&lt;h1&gt;📌 Eval&lt;/h1&gt; 
&lt;h2&gt;Ⅰ RLHF对比SFT篇&lt;/h2&gt; 
&lt;p&gt;测试基于&lt;code&gt;full_sft&lt;/code&gt;和&lt;code&gt;rlhf&lt;/code&gt;的&lt;code&gt;MiniMind2系列&lt;/code&gt;模型对比，测试随机种子均固定&lt;/p&gt; 
&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt; 
 &lt;summary&gt;问答实测&lt;/summary&gt; 
 &lt;p&gt;[A0] MiniMind2：full_sft_640&lt;/p&gt; 
 &lt;p&gt;[A1] MiniMind2：rlhf_640&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;[Q]: 你叫什么名字？
[A0]: 我是一个大型语言模型，可以回答各种问题，提供信息和帮助。我可以回答各种问题，提供信息和帮助，帮助你解决各种问题。
[A1]: 我是一种基于文本的AI模型，我并没有个人经历或情感，所以无法感受和体验情感。我的设计目的是为了帮助用户解决问题、提供信息和进行对话。如果您有任何问题或需要帮助，请随时告诉我！

[Q]: 你更擅长哪一个学科？
[A0]: 作为一个AI模型，我并没有真正的“擅长”这个学科。我被设计成能够提供信息、解答问题、进行对话、执行任务和与用户进行对话。不过，我可以提供信息、解答问题、进行对话、提供建议、处理问题、进行对话、提供建议、学习新知识、编写代码、回答问题、提供建议等多种学科的功能。如果你有具体的学科需求或主题，欢迎提供详细信息。
[A1]: 作为AI，我并没有个人能力或兴趣，但我可以提供一些关于数学、科学、计算机科学、语言学、心理学等学科的知识。如果你对数学感兴趣，我可以提供一些基础知识，例如代数、几何、概率、统计学等。如果你需要更深入的学习，我也可以提供相关的在线课程、书籍、论文等资源。请告诉我你具体需要什么帮助，我会尽力提供帮助。

[Q]: 鲁迅的《狂人日记》是如何批判封建礼教的？
[A0]: 鲁迅的《狂人日记》是一部讲述了鲁迅对人类价值和道德责任的倡导。在这部作品中，鲁迅通过细腻的笔触，探讨了人性、道德、正义、牺牲与责任等深刻的主题。以下是鲁迅在《狂人日记》中如何批判封建礼教的几个方面：1. **社会现实与道德观念**：鲁迅通过讲述鲁迅的故事，探讨了社会的不平等、权力的流动、人性的演变等社会问题。他认为，社会不平等不仅仅是不公正的，更是每个人在社会中的不公正。他呼吁社会各界共同努力，通过教育、宣传、法律等手段，为构建和谐社会做出贡献。2. **道德与责任**：鲁迅通过讲述鲁迅的故事，展现了道德和责任的重要性。他提倡通过道德教育来培养人们的道德品质，同时也强调了道德和责任的重要性。鲁迅通过讲述鲁迅的故事，揭示了社会的不平等和人性的复杂性，体现了对人性的深刻理解。3. **道德与责任**：鲁迅通过讲述鲁迅的故事，强调了道德责任的重要性。他提倡通过自己的行为来体现对社会的尊重和对人性的深刻理解。鲁迅通过讲述鲁迅的故事，强调了道德和责任的重要性。4. **道德与道德**：鲁迅通过讲述鲁迅的故事，揭示了道德和责任的真正意义。鲁迅通过讲述鲁迅的故事，揭示了道德和责任的本质，强调了道德和责任的重要性。5. **个人与社会**：鲁迅通过讲述鲁迅的故事，揭示了个人与社会之间的复杂关系。他呼吁社会各界共同努力，通过教育和宣传，为构建和谐社会做出贡献。鲁迅的《狂人日记》通过其丰富的故事和深刻的道德寓言，传达了关于人性、道德、正义、道德以及个人与社会的复杂关系。这部作品不仅在文学价值上取得了巨大成就，更在思想层面上启发了后来的文学创作，对后世产生了深远的影响。
[A1]: 鲁迅的《狂人日记》是一部描绘战争和自由精神的作品，它批判了封建社会的种种问题和冲突，反映了社会的不平等和人性本质。在这部作品中，鲁迅通过对封建社会的诸多现象进行批判，如“弗雷德里克·约翰逊的“鹰迪伦”、“纳扎特的”、“贝克汉姆”、“乔治·威尔逊”等，对封建社会的种种问题进行了批判，强调了封建社会的不平等、人性本质以及人性本质。鲁迅通过他的“鹰迪伦”、“纳扎特的”、“贝克汉姆”、“乔治·威尔逊”等，对封建社会的种种问题进行了批判。在小说中，鲁迅通过对封建社会的诸多现象进行批判，揭示了封建社会的腐败、人性本质以及人性本质的复杂性。这种批判体现了对封建社会的尊重和对人性的复杂性，同时也反映了现代社会中普遍存在的不平等和人性本质。鲁迅的《狂人日记》不仅是一部关于封建社会的小说，也是一部具有深刻社会意义的作品，对后世的审判和批判起到了关键作用。

[Q]: 我咳嗽已经持续了两周，需要去医院检查吗？
[A0]: 咳嗽确实需要一定的检查，但如果您的咳嗽持续时间过长，或者有其他不适症状（如持续咳嗽、呼吸困难等），建议尽快就医。医生会根据您的症状、体检结果和可能的影像学检查（如X光、MRI或CT扫描）来诊断您的症状。在等待就医期间，您可以尝试以下方法来帮助缓解症状：1. **休息**：确保您有足够的睡眠。2. **保持水分**：脱水可能会加重咳嗽。3. **热敷**：使用热敷袋或热毛巾敷在疼痛的部位，有助于缓解喉咙痛。4. **避免刺激**：避免使用可能刺激喉咙的化学物质，如某些化学物质。5. **保持室内湿润**：确保您的卧室有良好的通风，可以使用加湿器帮助保持空气湿度。6. **保持室内湿度**：使用加湿器可以帮助缓解喉咙干燥和咳嗽。如果症状持续不缓解或有其他不适症状，请立即就医。希望您早日康复！
[A1]: 如果您的咳嗽持续了两周，通常建议您尽快就医，因为这可能会影响您的呼吸和心跳。您可以通过以下步骤来检查咳嗽：1. **休息**：用纸巾或肘部轻轻按压咳嗽的位置。2. **使用药物**：在可能的情况下，使用药物（如抗过敏药、抗过敏药等）来缓解咳嗽。请在医生的指导下使用药物。3. **热敷**：在咳嗽期间，可以使用热敷来缓解疼痛和肿胀。4. **药物治疗**：如果咳嗽持续了两周，可以使用一些消炎药或抗过敏药来缓解症状。请务必在医生的指导下使用。5. **保持健康的生活方式**：保持充足的睡眠、均衡的饮食、适量的运动和避免吸烟和限酒，这些都有助于提高咳嗽的几率。请记住，这些建议不能替代专业医疗意见。如果您的症状持续或加重，强烈建议您尽快就医。
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p&gt;👉效果总结&lt;/p&gt; 
&lt;p&gt;full_sft模型在简洁性和信息准确性方面表现更好；rlhf模型在回答中倾向于提供更多的背景信息，但信息准确性有待改进。 总的来说RLHF后的模型倾向于学习：说更多有礼貌但无用的废话讨好“对话”本身，而对信息准确性则有轻微损失。 天下没有免费的午餐，还需要继续提升RLHF数据集的质量，也要接受模型能力无法避免的损失(程度有轻重)。 DPO和在线PPO的区别在于reject和chosen都是离线准备的，和minimind模型本身的输出必然存在很大的分布差异。 通俗地说DPO算法使模型观看乒乓球世界冠军的打法「录像」进行RL，而不是像PPO一样请reward模型做「教练」纠正自己的打法进行RL。&lt;/p&gt; 
&lt;h2&gt;Ⅱ 主观样例测评&lt;/h2&gt; 
&lt;p&gt;🏃以下测试于2025-02-09完成，此日期后发布的新模型，无特殊需要时将不加入测试。&lt;/p&gt; 
&lt;p&gt;[A] &lt;a href=&quot;https://www.modelscope.cn/models/gongjy/MiniMind2-PyTorch&quot;&gt;MiniMind2 (0.1B)&lt;/a&gt;&lt;br&gt; [B] &lt;a href=&quot;https://www.modelscope.cn/models/gongjy/MiniMind2-PyTorch&quot;&gt;MiniMind2-MoE (0.15B)&lt;/a&gt;&lt;br&gt; [C] &lt;a href=&quot;https://www.modelscope.cn/models/gongjy/MiniMind2-PyTorch&quot;&gt;MiniMind2-Small (0.02B)&lt;/a&gt;&lt;br&gt; [D] &lt;a href=&quot;https://pan.baidu.com/s/1_COe0FQRDmeapSsvArahCA?pwd=6666&quot;&gt;minimind-v1-small(0.02B)&lt;/a&gt;&lt;br&gt; [E] &lt;a href=&quot;https://pan.baidu.com/s/1tqB-GMvuiGQBvEl-yZ-oBw?pwd=6666&quot;&gt;minimind-v1-moe(0.1B)&lt;/a&gt;&lt;br&gt; [F] &lt;a href=&quot;https://pan.baidu.com/s/1p713loS7EfwHQf3G9eYI3Q?pwd=6666&quot;&gt;minimind-v1(0.1B)&lt;/a&gt;&lt;br&gt; [G] &lt;a href=&quot;https://github.com/DLLXW/baby-llama2-chinese&quot;&gt;baby-llama2-chinese(0.2B)&lt;/a&gt;&lt;br&gt; [H] &lt;a href=&quot;https://github.com/charent/ChatLM-mini-Chinese&quot;&gt;chatlm-mini-chinese(0.2B)&lt;/a&gt;&lt;br&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;[Q]: 你知道长江吗？
[A]: 长江，位于中国东南部，是中国的第一长河，全长6300多公里，流经11个省份，是中国最长的河流，也是中国的“母亲河”。长江的历史可以追溯到公元前2000多年的商朝时期，当时的商朝是商朝的后裔。商朝时期，长江的经济发展达到了高峰，是中国最重要的农业和工业中心之一。长江流域的经济主要以农业为主，长江渔业占据了主要产业，如钓鱼、猎鱼等。长江的水资源也非常丰富，包括河流、湖泊、河流等。长江的自然景观也十分壮观，长江四季景色各异，春季多雨，夏季多雨，秋季则多雨，冬季则寒冷多雪，春季和秋季都有美丽的花朵和果实。长江是中国的经济中心，拥有众多的国家和城市，是世界上重要的交通枢纽和航运中心。同时，长江还是中国的重要水利资源，对于中国的经济发展具有重要意义。
[B]: 是的，我知道长江。长江位于中国的中部，是中国第三长的河流，全长约6300公里，流经江西省、安徽省、江苏省、安徽省、江苏省和安徽省。
[C]: 当然，长江是中国最长的河流，全长约6300公里。它以其丰富的水资源、丰富的水生动植物资源以及独特的地理位置而闻名于世。
[D]: 是的，长江是中国最长的河流，全长超过3300千米。
[E]: 长江是中国最长的河流，总长度约为6300公里。它发源于青藏高原的青海湖，流经青海、四川、甘肃、宁夏、内蒙古、陕西和山西。长江流域是中国最大的农业区之一，也是中国的文化和经济中心。
[F]: 是的，长江是中国最长的河流之一，也是世界第三大河流。长江发源于青藏高原，流经11个省份，最终注入东海。长江流域是中国最大的河流流域之一，拥有着丰富的自然资源和文化遗产，如长江大桥、长江海洋博物馆、长江古城等。
[G]: 同时也知道，中国是世界上最长的城市之一，拥有超过3400万人口。
[H]: 是的，长江是中国最长的河流，也是世界上最长的河流之一。它发源于青藏高原，流经九个省区，最终注入东海。长江流域的气候温和，四季分明，是中国重要的农业和工业基地。长江流域的水产资源丰富，是中国的重要渔业资源之一。


[Q]: 世界上最高的山峰是什么？
[A]: 世界上最高的山峰是珠穆朗玛峰（Mount Everest），位于喜马拉雅山脉，尼泊尔和中国的边界上。它的海拔高度是8,848.86米（29,031.7英尺），是地球上最高的山峰。
[B]: 珠穆朗玛峰（Mount Everest）是世界上最高的山峰，海拔高度达8,848米（29,029英尺）。
[C]: 珠穆朗玛峰是世界上最高的山峰，位于喜马拉雅山脉，海拔8,848.86米（29,031.7英尺）。它位于尼泊尔和中国的边界上，是喜马拉雅山脉的一部分。
[D]: 世界上最高的山峰是珠穆朗玛峰，位于尼泊尔和中国的边境上，海拔8848米。
[E]: 世界上最高的山峰是珠穆朗玛峰，位于喜马拉雅山脉，海拔8,848米。
[F]: 珠穆朗玛峰是世界上最高的山峰，海拔8848米。
[G]: 世界上最高的山峰是珠穆朗玛峰，位于喜马拉雅山脉，海拔8,848.86米（29,031.69英尺）。
[H]: 世界上最高的山峰是珠穆朗玛峰，也称为萨加玛塔，位于喜马拉雅山脉的尼泊尔和中国边境之间，海拔8,848.86米（29,031.69英尺）。它是世界上最著名的登山目标之一，吸引着来自世界各地的登山者前来挑战。珠穆朗玛峰的名字来自藏语，意思是“女神的额峰”，它是由两个不同的山峰组成的，其中一个是位于尼泊尔的珠穆朗玛峰，另一个则位于中国。它的高度是通过测量海拔高度来确定的，因为它是通过测量地球的平均高度来确定的。尽管珠穆朗玛峰是世界上最高的山峰，但它并不是地球上任何其他地方都能到达的。地球上的其他地方，如南美洲的安第斯山脉，也是世界上最高的山脉，但它并不是地球上最高的山峰。总之，珠穆朗玛峰是世界上最高的山峰，但它并不是地球上任何地方都能到达的。它是地球上最高的山脉，但它并不是地球上任何地方都能到达的。


[Q]: 万有引力是谁提出的？
[A]: 万有引力是由牛顿在17世纪提出的，它是牛顿三大定律之一。牛顿在17世纪提出的引力定律揭示了万有引力定律的本质，并且解释了行星、卫星、彗星等天体之间如何相互作用。这个定律是由艾萨克·牛顿在1687年提出的，他通过对万有引力定律的分析和实验，成功地解释了天体之间的引力作用，从而解释了地球绕太阳旋转的现象。
[B]: 万有引力（Human Policy）是牛顿的三大力学定律，用于描述两个物体之间的相互作用力。它由万有引力定律和牛顿的第一定律（惯性定律）构成。万有引力是由于两个物体之间相互作用而产生的，它们的质量、距离、速度和方向等因素决定了它们之间的相互作用。这种相互作用力是：质量的乘积：质量的乘积是万有引力定律和牛顿第二定律的乘积。质量的乘积：质量的乘积是物体之间的一种物理过程，它是物体间相互作用的产物，对于其他物体来说，其质量可以表示为两个物体之间的质量之和，即重力加速度a。
[C]: 万有引力是一个由牛顿在1687年提出的物理学概念，它描述了物体之间引力如何相互作用的规律。
[D]: 万有引力是由英国物理学家牛顿提出的。他认为，所有物体都会产生引力，这个引力是由物体的质量和距离决定的。这个理论揭示了质量和距离之间的关系，并证明了万有引力的本质。
[E]: 万有引力是由牛顿在1687年提出的。它是宇宙中一个基本的物理现象，它描述了物体之间的相互作用。这个力被称为万有引力，它在所有物体之间都存在，这种力与物体的质量和距离有关。万有引力是自然界中最基本的力量之一，它不仅影响着物体的运动和变化，还对宇宙的结构和演化产生着重要的影响。
[F]: 万有引力是英国物理学家艾萨克·牛顿在17世纪提出的。
[G]: 引力是宇宙中一种力量，它是由质量或能量所组成的。根据牛顿的万有引力定律，一个物体之间的引力大小与它的质量成正比，与它的质量成反比。因此，牛顿的万有引力定律表明，一个物体之间的引力大小与它的质量成正比，与它的质量成反比。这个定律在物理学、天文学和物理学中都有广泛的应用。
[H]: 万有引力是由英国物理学家艾萨克·牛顿于1687年提出的。他在1687年发表了一篇著名的论文，题为《自然哲学的数学原理》，其中提出了万有引力定律，指出任何两个物体之间都存在着一种引力，这种引力的大小与它们的质量成正比，与它们之间的距离的平方成反比。这个定律被称为万有引力定律，它描述了质量和距离之间的关系。它表明，两个物体之间的引力与它们的质量成正比，与它们之间的距离的平方成反比。这个定律对于解释宇宙中的许多现象和现象之间的相互作用非常重要，例如行星绕太阳运动、恒星之间的引力透镜效应等等。它也被广泛应用于天文学、物理学和工程学等领域，为我们理解宇宙的本质和演化提供了重要的理论基础。


[Q]: 你知道大熊猫的主要食物是什么吗？
[A]: 大熊猫主要以竹子为食，但也会吃其他植物、昆虫、小型哺乳动物和鱼类。它们的消化系统与其他熊科动物类似，能够消化竹子。大熊猫的牙齿锋利，适合咀嚼竹子，因此它们经常被称为“竹食动物”。
[B]: 大熊猫是以竹子为食的哺乳动物，它们是肉食动物。它们主要食物是竹子，特别是竹子中富含纤维素和矿物质，并且不会直接进入消化系统。大熊猫的肠道主要由竹子组成，它们需要消化竹子中的纤维素和其他纤维素来支持它们的正常进食。
[C]: 大熊猫主要以竹子为食，但它们也吃其他植物，包括树叶、嫩芽、昆虫、水果、蔬菜和小型哺乳动物等。大熊猫是中国特有的物种，属于熊科，主要分布在中国的四川、陕西和甘肃等地。
[D]: 大熊猫的主要食物是竹子。它们通常吃竹子，每天需要吃掉大约30公斤的竹子。
[E]: 是的，大熊猫主要以竹子为食。它们每天需要吃掉大量的竹子来维持生命，每天需要吃掉40至80公斤的竹子。此外，它们也会吃一些其他的植物，如草、果实、坚果和小型哺乳动物。
[F]: 大熊猫主要以竹子为食，也会偶尔食用其他的植物和动物。此外，大熊猫也会吃一些野果、小动物和植物，但这些都不属于它们的主要食物来源。
[G]: 大熊猫的主要食物是竹子。
[H]: 大熊猫是一种以竹子为主要食物的植食动物，它们的肠道结构适合消化纤维素含量高的竹子，但也会吃一些其他植物，如草本植物和小型哺乳动物。大熊猫的肠道结构也适应了高纤维的竹子，这使得它们能够消化大量的纤维素，从而帮助它们在野外生存。大熊猫的肠道中还含有一种叫做“纤维素酶”的酶类物质，这种酶能够帮助大熊猫分解纤维素，从而帮助它们消化大量的水分和营养物质。此外，大熊猫的肠道还含有一种叫做“纤维素蛋白”的蛋白质，这种蛋白质能够帮助大熊猫维持体内的能量水平，从而帮助它们在野外生存。总的来说，大熊猫的主要食物是竹子，但它们也会吃一些其他植物，如草本植物和小型哺乳动物。


[Q]: 海水为什么是咸的？
[A]: 海水之所以是咸的，主要是由于其中的化学成分。海水中含有大量的矿物质和盐分，其中主要成分是硫酸盐、碳酸盐、镁离子和钾离子。这些物质在高浓度下，会沉淀在海水中，形成海水。具体来说，海水中的矿物质和盐分主要来自于岩石、土壤、生物和人类排泄物等。岩石是海水的组成部分，主要由矿物质组成，如石英、长石、云母等。土壤中含有大量的化学物质，如氮、磷、钾等，这些物质会渗透到土壤中，形成土壤。生物体内的盐分主要来源于植物的叶绿素等色素，这些色素会吸收太阳光中的能量，转化为生物体可以利用的能量。而人类排泄物中含有的盐分主要来源于矿物质和碳酸盐，这些矿物质在水中的浓度较高，会导致水中溶解的矿物质浓度增加，形成水体中的盐分。因此，海水之所以是咸的，是由于其中的化学成分和结构特征。海水中的矿物质和盐分主要来自于岩石、土壤、生物和人类排泄物等，而这些物质的积累和排泄会导致海水呈现出咸味。
[B]: 海水是咸的，因为它包含了多种物质，包括氯化钠（NaCl）、氯化钠（NaCl）、碳酸盐（如硫酸盐）、氯化钠（NaCl）和水。咸物质的来源：盐是咸的常见来源，含有钾、钙、镁、钾等矿物质。这些矿物质通常来自于地下水、地下水或土壤中，这些矿物质的含量远低于纯净水。咸味的原因：盐的咸味主要源于其溶解于水中的微小颗粒物。这些小颗粒物主要通过植物根部和动物体内的微生物传播，如昆虫、小型哺乳动物、鸟类和某些昆虫。综上所述，海水是咸的，因为它含有大量的盐类，包括矿物质、岩石、盐、矿物质等，这些物质在海水中的浓度高，对咸味的产生较大影响。
[C]: 海水是咸的，主要是因为其中含有各种盐分，其中以海水为主要成分之一。当海水中含有大量的盐分时，盐分会被溶解在海水中，形成咸味。海水中的盐分主要来源于溶解盐类、蒸发析出的盐和海水冻结成冰等因素。
[D]: 海水中含有许多盐分，这些盐分可以导致海水咸度的增加。海水中的盐分主要来自于海水中的盐分，这些盐分可以从海水中的盐分中获得。海水中的氧化物和盐分可以与海水中的盐分形成氢键，从而使海水咸度增加。
[E]: 海水是咸的，这是因为它含有大量的盐分，其中包括氯离子和氯离子。这些离子在海洋中形成了大量的盐，这些盐分被称为盐分。海水中含有许多溶解在水中的盐，这些盐分会随着海水的升高而逐渐蒸发，最终导致海水中盐分的蒸发。这些盐分在海水中被完全消耗，然后被排除在海水中。此外，海水中还含有许多其他的盐分，但这些盐分是不同的，因为它们在水中的浮起和下降过程是不同的。海水中的盐分是从海底的岩石中提取出来的，这些盐分在海水中可以被利用。总的来说，海水是咸的，这也是为什么海水中含有许多盐分和盐分的原因。
[F]: 海水含有许多溶解的盐类，其中最主要的是氯化钠。当水中加入盐时，盐的分子会与水分子发生反应，形成氢氧化钠，这是一种盐。
[G]: 因为水是许多生物的生存所必需的，它能够吸收和释放二氧化碳，释放氧气，维持大气中的水蒸气，以及维持水循环。水中的盐类和化学物质会释放出氧气，使地球上的生物呼吸。此外，水还具有许多其他功能，如保持水循环、维持水循环、调节气候和环境等。水是许多生物生存所必需的，它也是许多自然现象的重要基础。
[H]: 海水是咸的主要原因是因为其中含有大量的溶解盐类，其中最主要的是氯化钠（NaCl）。氯化钠是一种非常常见的盐类，它存在于地球上的大部分地区，包括海洋、湖泊、河流和地下水。当水从海洋中蒸发时，它会带走一部分盐分，使海水变得咸。此外，海中还含有其他溶解盐类的物质，如硫酸盐、硫酸镁、硫酸钙、硫酸镁盐、硫酸镁钾、硫酸镁钠、硫酸镁钙、硫酸镁镁钙、硫酸镁镁钙、硫酸镁镁钙、硫酸镁镁钠、硫酸镁镁钙、硫酸镁镁镁钙、硫酸镁镁镁钙、硫酸镁镁镁钙、硫酸镁镁镁钙、硫酸镁镁镁钙、硫酸镁镁镁钙、硫酸镁镁镁钙、硫酸镁镁镁钙、硫酸镁镁镁钙、硫酸镁镁镁钙、硫酸镁镁镁镁钙、硫酸镁镁镁镁钙、硫酸镁镁镁镁钙、硫酸镁镁镁镁钙、硫酸镁镁镁镁钙、硫酸镁镁镁镁钙、硫酸镁镁镁镁镁钙、硫酸镁镁镁镁镁钙、硫酸镁镁镁镁镁钙、硫酸镁镁镁镁镁钙、硫酸镁镁镁镁镁镁钙、硫酸镁镁镁镁镁镁钙、硫酸镁镁镁镁镁镁钙、硫酸镁镁镁镁镁镁镁钙、硫酸镁镁镁镁


[Q]: 写一段python的快速排序代码
[A]: 好的，以下是Python的快速排序代码：def quick_sort(arr):    if len(arr) &amp;lt;= 1:        return arr    else:        pivot = arr[len(arr) // 2]        left = [x for x in arr[1:]]        right = [x for x in arr[2:] if x &amp;lt; pivot]        return quick_sort(left) + [pivot] + quick_sort(right)# 测试代码arr = [3, 6, 8, 10, 1, 2, 1]print(quick_sort(arr))这段代码首先检查数组的长度是否小于等于1。如果是，那么它就按照从小到大的顺序排列。接着，对于数组的每个元素，它想要进行递归地排序，直到达到排序的结束位置。如果数组的长度是奇数，那么它就按照从大到小的顺序排列。最后，平均的时间复杂度是O(n log n)，其中n是数组的长度。
[B]: 以下是一个使用Python的快速排序算法的实现：def sort_array(arr):    # 生成一个随机排列    sorted_array = sorted(arr)    # 返回排序后的列表    return sorted(arr)# 示例arr = [3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5]# 调用函数sorted_array = sort_array(arr)print(&quot;原始数组是：&quot;, sorted_array)这段代码首先生成一个随机排列，然后通过一个列表的长度进行排序。最后，返回排序后的列表。注意，这个代码使用了Python的内置sorted函数和内置的sorted()函数，它们在时间复杂度为O(n log n)的情况下，比直接使用sorted()函数更快。
[C]: 以下是一个简单的排序算法，适用于小型数组，包括小于等于5的数组。def quick_sort(arr):    if len(arr) &amp;lt;= 1:        return arr    else    for _ in range(2, 51):        if arr[len(arr) - 1:] is not None:            arr[len(arr) - 1] = arr[len(arr) - 1]                arr[len(arr)] = arr[len(arr)]    return quick_sort(arr)# 示例arr = [3, 5, 1, 4, 1, 2, 1]sorted_arr = quick_sort(arr)print(&quot;Sorted array is:&quot;, sorted_arr)### 代码解释：1. **函数定义**：quick_sort 函数接受一个数组 arr 作为参数。2. **循环**：对于 arr 的元素，使用递归来实现，即从右到左遍历数组，并将当前元素与数组的当前元素交换位置，直到数组达到空或数组长度小于等于 arr。3. **递归调用**：遍历数组，将当前元素与数组的当前元素进行交换，直到数组达到空或数组长度小于等于 arr。4. **检查数组长度**：使用 if __name__ == &quot;__main__&quot; 检查数组的长度是否小于 arr，如果是，就将数组从较大的数组移动到数组的末尾。5. **输出排序后的结果**
[D]: 作为一个AI语言模型，我无法编写Python程序。但是，Python是一个非常流行的编程语言，它具有许多优秀的库和框架，包括NumPy、Pandas、Matplotlib、Pandas、Bandke、Norm和NKlearn等。Python有很多优秀的自然语言处理库和框架，可以用于处理和分析各种类型的数据，如文本、图像、音频、视频等。Python还可以用于创建各种应用程序，如网页搜索、游戏开发、科学计算等。Python非常灵活，也支持多种编程风格，因此可以轻松地找到适合自己的解决方案。
[E]: 作为AI语言模型，我可以回答您的问题。但是我无法进行编程操作，因为我无法确定您想问的是哪种编程语言。请您提供更具体的问题，以便我能够更好地回答您的问题。
[F]: 快速排序（Quick Sort）是一种简单的排序算法，它通过将数据划分为小组，每组数据按照一定规则排序，最终得到一个有序数组。
[G]: 好的，以下是一段快速排序的Python代码```python sorted_print(sorted(sorted(sorted(sorted_print))```输出结果为：```[1, 2, 3, 4, 5, 6, 7, 8, 9]```
[H]: 快速排序是一种基于分治思想的高效排序算法，它的时间复杂度为 O(n log n)。它的基本思想是通过一趟排序将待排序序列划分为两个子序列，其中一个子序列的所有元素都比另一个子序列的所有元素小，然后对这两个子序列分别递归地进行排序。下面是一个 Python 代码示例，实现了快速排序算法：```pythondef quick_sort(arr):    if len(arr) &amp;gt; 1:        return arr    else: return quick_sort(arr)```该代码使用了递归的方式，首先判断数组的长度是否小于等于1，如果是，则直接返回数组，否则递归地对数组进行排序。在递归过程中，我们首先判断数组的第一个元素是否大于等于1，如果是，则将数组中的元素与第一个元素进行比较，如果不等于1，则将数组中的元素与第二个元素进行比较，以此类推，直到所有元素都被排序。该代码的时间复杂度为 O(n log n)，是一个非常实用的排序算法。


[Q]: 截至2025年2月9日14时31分，电影《哪吒之魔童闹海》累计票房（含预售）突破78.09亿元，超过《星球大战：天行者崛起》的票房成绩，成为首部进入全球票房榜前40的亚洲电影。这一成就不仅标志着中国电影在国际市场的突破，也展示了中国动画电影的巨大潜力和市场吸引力。从这段文字总结20字的摘要给我。
[A]: 电影《哪吒之魔童闹海》票房突破78.09亿元，成为首部进入全球票房榜前40的亚洲电影。这标志着中国电影的国际突破，也展现了中国动画电影的巨大潜力。
[B]: 2025年2月9日，中国电影在世界舞台上突破78.09亿元，超过《星球大战：天行者崛起》的票房成绩。
[C]: 电影《哪吒之魔童闹海》票房突破78.09亿元，成为全球票房榜前40的亚洲电影，展现中国动画电影的巨大潜力。
[D]: 《哪吒之魔童闹海》累计票房突破78.09亿，成为首部进入全球票房榜前40的亚洲电影，展示了中国电影在国际市场的突破，同时展示了中国动画电影的巨大潜力和市场吸引力。
[E]: 《猛龙强：打开世界奇幻冒险》成功地，上海成功吸引了众多观众，同时也引起了广泛的关注。
[F]: 《哪吒之魔童闹海》是一部让中国动画电影成为国际市场的巨大潜力。该电影以其华丽的画面和惊险的战斗场面而闻名于世。
[G]: 《哪行是值得的》是2021年1月9日14时31分，电影《哪成大作为》在电影市场突破7800亿美元，成为中国动画电影的突破，并展示中国动画电影的巨大潜力和市场吸引力。
[H]: 我是一个参数很少的AI模型🥺，知识库较少，无法直接回答您的问题，换个问题试试吧👋
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr&gt; 
&lt;p&gt;🙋‍直接把以上所有问题和模型的回答丢给DeepSeek-R1，让它帮忙点评和排名打分：&lt;/p&gt; 
&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt; 
 &lt;summary&gt;具体点评&lt;/summary&gt; 
 &lt;h3&gt;评分标准：&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;准确性&lt;/strong&gt;：回答是否正确且无明显错误。&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;完整性&lt;/strong&gt;：回答是否涵盖了问题的核心要点。&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;逻辑性&lt;/strong&gt;：回答是否有条理，是否符合逻辑。&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;代码质量&lt;/strong&gt;：代码是否能正常运行，逻辑是否清晰。&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;点评：&lt;/h3&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;A模型&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;优点&lt;/strong&gt;：回答非常全面，信息量大，逻辑清晰，尤其在长江、大熊猫、海水咸味等问题上表现优异。代码虽然有小瑕疵，但整体思路正确。&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;缺点&lt;/strong&gt;：部分回答稍显冗长，但不影响整体质量。&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;总评&lt;/strong&gt;：综合表现最佳，得分最高。&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;H模型&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;优点&lt;/strong&gt;：回答较为准确，尤其在珠穆朗玛峰、万有引力等问题上表现出色。代码虽未完全展示，但解释较为详细。&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;缺点&lt;/strong&gt;：部分回答略显啰嗦，但逻辑性较强。&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;总评&lt;/strong&gt;：仅次于A模型，表现稳定。&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;C模型&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;优点&lt;/strong&gt;：回答简洁明了，尤其在大熊猫、快速排序代码等问题上表现较好。&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;缺点&lt;/strong&gt;：部分回答略显简短，缺乏深入解释。&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;总评&lt;/strong&gt;：整体表现不错，但细节上略逊于A和H。&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;F模型&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;优点&lt;/strong&gt;：回答较为准确，尤其在长江、万有引力等问题上表现尚可。代码部分有一定逻辑性。&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;缺点&lt;/strong&gt;：部分回答不够深入，代码存在一些小问题。&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;总评&lt;/strong&gt;：表现中规中矩，仍有提升空间。&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;D模型&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;优点&lt;/strong&gt;：回答基本准确，尤其在万有引力、长江等问题上表现尚可。&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;缺点&lt;/strong&gt;：部分回答过于简略，代码存在明显错误。&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;总评&lt;/strong&gt;：表现一般，需改进代码部分。&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;B模型&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;优点&lt;/strong&gt;：回答较为准确，尤其在长江、海水咸味等问题上表现尚可。&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;缺点&lt;/strong&gt;：部分回答逻辑性较差，代码存在较大问题。&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;总评&lt;/strong&gt;：表现平平，需进一步优化。&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;E模型&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;优点&lt;/strong&gt;：部分回答较为准确，尤其在海水咸味、大熊猫等问题上表现尚可。&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;缺点&lt;/strong&gt;：回答过于简略，代码部分几乎无法运行。&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;总评&lt;/strong&gt;：表现不佳，需大幅提升。&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;G模型&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;优点&lt;/strong&gt;：几乎没有明显的优点。&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;缺点&lt;/strong&gt;：回答严重偏离主题，代码部分完全无法运行。&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;总评&lt;/strong&gt;：表现最差，需大幅改进。&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;hr&gt; 
 &lt;h3&gt;总结：&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;A模型&lt;/strong&gt;在各方面表现最为出色，尤其在复杂问题的回答上展现了极高的准确性与逻辑性。&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;H模型&lt;/strong&gt;紧随其后，表现稳定，但在某些细节上略显不足。&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;G模型&lt;/strong&gt;表现最差，回答偏离主题且代码无法运行，需大幅改进。&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;打分排序&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;排名&lt;/th&gt; 
   &lt;th&gt;模型&lt;/th&gt; 
   &lt;th&gt;准确性 (30分)&lt;/th&gt; 
   &lt;th&gt;完整性 (30分)&lt;/th&gt; 
   &lt;th&gt;逻辑性 (20分)&lt;/th&gt; 
   &lt;th&gt;代码质量 (20分)&lt;/th&gt; 
   &lt;th&gt;总分 (100分)&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;1&lt;/td&gt; 
   &lt;td&gt;A&lt;/td&gt; 
   &lt;td&gt;28&lt;/td&gt; 
   &lt;td&gt;29&lt;/td&gt; 
   &lt;td&gt;19&lt;/td&gt; 
   &lt;td&gt;20&lt;/td&gt; 
   &lt;td&gt;96&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;H&lt;/td&gt; 
   &lt;td&gt;27&lt;/td&gt; 
   &lt;td&gt;28&lt;/td&gt; 
   &lt;td&gt;18&lt;/td&gt; 
   &lt;td&gt;20&lt;/td&gt; 
   &lt;td&gt;93&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;3&lt;/td&gt; 
   &lt;td&gt;C&lt;/td&gt; 
   &lt;td&gt;26&lt;/td&gt; 
   &lt;td&gt;27&lt;/td&gt; 
   &lt;td&gt;18&lt;/td&gt; 
   &lt;td&gt;18&lt;/td&gt; 
   &lt;td&gt;89&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;4&lt;/td&gt; 
   &lt;td&gt;F&lt;/td&gt; 
   &lt;td&gt;25&lt;/td&gt; 
   &lt;td&gt;26&lt;/td&gt; 
   &lt;td&gt;17&lt;/td&gt; 
   &lt;td&gt;18&lt;/td&gt; 
   &lt;td&gt;86&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;D&lt;/td&gt; 
   &lt;td&gt;24&lt;/td&gt; 
   &lt;td&gt;25&lt;/td&gt; 
   &lt;td&gt;17&lt;/td&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;82&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;6&lt;/td&gt; 
   &lt;td&gt;B&lt;/td&gt; 
   &lt;td&gt;23&lt;/td&gt; 
   &lt;td&gt;24&lt;/td&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;15&lt;/td&gt; 
   &lt;td&gt;78&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;7&lt;/td&gt; 
   &lt;td&gt;E&lt;/td&gt; 
   &lt;td&gt;22&lt;/td&gt; 
   &lt;td&gt;23&lt;/td&gt; 
   &lt;td&gt;15&lt;/td&gt; 
   &lt;td&gt;14&lt;/td&gt; 
   &lt;td&gt;74&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;G&lt;/td&gt; 
   &lt;td&gt;10&lt;/td&gt; 
   &lt;td&gt;12&lt;/td&gt; 
   &lt;td&gt;10&lt;/td&gt; 
   &lt;td&gt;10&lt;/td&gt; 
   &lt;td&gt;42&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;👉主观效果总结&lt;/h3&gt; 
&lt;p&gt;个人主观评价与DeepSeek-R1基本相符，其中：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;MiniMind系列的排序非常符合直觉，参数越大+训练数据越充分评分越高，幻觉和错误都会比小模型肉眼可见的好。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;H模型的回答肉眼看起来是不错的，尽管存在些许幻觉瞎编的情况。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;G模型可能训练数据不够完备，给出的权重经过测试效果不佳。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;再复诵一遍经久不衰的Scaling Law: 参数越大，训练数据越多模型的性能越强。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr&gt; 
&lt;h2&gt;Ⅲ Objective Benchmark&lt;/h2&gt; 
&lt;p&gt;下面就到喜闻乐见的benchmark刷榜测试环节，就不找乐子和qwen、glm级别的中文模型做对比了。 这里选取了一些&amp;lt;1B的微型模型进行横评比较， 测试集选择C-Eval、CMMLU、A-CLUE、TMMLU+这几个纯中文语言榜单。&lt;/p&gt; 
&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt; 
 &lt;summary&gt;测评框架&lt;/summary&gt; 
 &lt;p&gt;测评框架选择&lt;a href=&quot;https://github.com/EleutherAI/lm-evaluation-harness&quot;&gt;lm-evaluation&lt;/a&gt;， 安装后启动测试非常方便：&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;lm_eval --model hf --model_args pretrained=&amp;lt;填写模型路径&amp;gt;,device=cuda,dtype=auto --tasks ceval* --batch_size 8 --trust_remote_code
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p&gt;PS: 在这种全是选择题的测评集中，为了避免回复格式的难以固定的特点， 所以常用做法是直接把&lt;code&gt;A&lt;/code&gt;,&lt;code&gt;B&lt;/code&gt;,&lt;code&gt;C&lt;/code&gt;,&lt;code&gt;D&lt;/code&gt;四个字母对应token的预测概率取出来，将其中概率最大的字母与标准答案计算正确率。 选择题1/4乱选的正确率是25%，然而这个量级的所有模型都集中在25附近，甚至很多时候不如瞎选，是不是像极了高中完形填空的滑铁卢正确率... MiniMind模型本身预训练数据集小的可怜，也没有针对性的对测试集做刷榜微调，因此结果图一乐即可：&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;models&lt;/th&gt; 
   &lt;th&gt;from&lt;/th&gt; 
   &lt;th&gt;params↓&lt;/th&gt; 
   &lt;th&gt;ceval↑&lt;/th&gt; 
   &lt;th&gt;cm mlu↑&lt;/th&gt; 
   &lt;th&gt;aclue↑&lt;/th&gt; 
   &lt;th&gt;tmmlu+↑&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MiniMind2&lt;/td&gt; 
   &lt;td&gt;JingyaoGong&lt;/td&gt; 
   &lt;td&gt;104M&lt;/td&gt; 
   &lt;td&gt;26.52&lt;/td&gt; 
   &lt;td&gt;24.42&lt;/td&gt; 
   &lt;td&gt;24.97&lt;/td&gt; 
   &lt;td&gt;25.27&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MiniMind2-Small&lt;/td&gt; 
   &lt;td&gt;JingyaoGong&lt;/td&gt; 
   &lt;td&gt;26M&lt;/td&gt; 
   &lt;td&gt;26.37&lt;/td&gt; 
   &lt;td&gt;24.97&lt;/td&gt; 
   &lt;td&gt;25.39&lt;/td&gt; 
   &lt;td&gt;24.63&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MiniMind2-MoE&lt;/td&gt; 
   &lt;td&gt;JingyaoGong&lt;/td&gt; 
   &lt;td&gt;145M&lt;/td&gt; 
   &lt;td&gt;26.6&lt;/td&gt; 
   &lt;td&gt;25.01&lt;/td&gt; 
   &lt;td&gt;24.83&lt;/td&gt; 
   &lt;td&gt;25.01&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/zhanshijinwat/Steel-LLM&quot;&gt;Steel-LLM&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ZhanShiJin&lt;/td&gt; 
   &lt;td&gt;1121M&lt;/td&gt; 
   &lt;td&gt;24.81&lt;/td&gt; 
   &lt;td&gt;25.32&lt;/td&gt; 
   &lt;td&gt;26&lt;/td&gt; 
   &lt;td&gt;24.39&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/openai-community/gpt2-medium&quot;&gt;GPT2-medium&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;OpenAI&lt;/td&gt; 
   &lt;td&gt;360M&lt;/td&gt; 
   &lt;td&gt;23.18&lt;/td&gt; 
   &lt;td&gt;25&lt;/td&gt; 
   &lt;td&gt;18.6&lt;/td&gt; 
   &lt;td&gt;25.19&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/jzhang38/TinyLlama&quot;&gt;TinyLlama-1.1B-Chat-V1.0&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;TinyLlama&lt;/td&gt; 
   &lt;td&gt;1100M&lt;/td&gt; 
   &lt;td&gt;25.48&lt;/td&gt; 
   &lt;td&gt;25&lt;/td&gt; 
   &lt;td&gt;25.4&lt;/td&gt; 
   &lt;td&gt;25.13&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/huggingface/smollm&quot;&gt;SmolLM2&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;HuggingFaceTB&lt;/td&gt; 
   &lt;td&gt;135M&lt;/td&gt; 
   &lt;td&gt;24.37&lt;/td&gt; 
   &lt;td&gt;25.02&lt;/td&gt; 
   &lt;td&gt;25.37&lt;/td&gt; 
   &lt;td&gt;25.06&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.modelscope.cn/models/BAAI/Aquila-135M-Instruct&quot;&gt;Aquila-Instruct&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;BAAI&lt;/td&gt; 
   &lt;td&gt;135M&lt;/td&gt; 
   &lt;td&gt;25.11&lt;/td&gt; 
   &lt;td&gt;25.1&lt;/td&gt; 
   &lt;td&gt;24.43&lt;/td&gt; 
   &lt;td&gt;25.05&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jingyaogong/minimind/master/images/compare_radar.png&quot; alt=&quot;compare_radar&quot;&gt;&lt;/p&gt; 
&lt;h1&gt;📌 其它 (Others)&lt;/h1&gt; 
&lt;h3&gt;推理与导出&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/jingyaogong/minimind/master/scripts/convert_model.py&quot;&gt;./scripts/convert_model.py&lt;/a&gt;可以将torch/transformers模型互相转换。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;MiniMind的HuggingFace集合地址： &lt;a href=&quot;https://huggingface.co/collections/jingyaogong/minimind-66caf8d999f5c7fa64f399e5&quot;&gt;MiniMind&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr&gt; 
&lt;h3&gt;基于MiniMind-API服务接口&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/jingyaogong/minimind/master/scripts/serve_openai_api.py&quot;&gt;./scripts/serve_openai_api.py&lt;/a&gt;完成了兼容openai-api的最简聊天接口，方便将自己的模型接入第三方UI 例如FastGPT、OpenWebUI、Dify等等。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;从&lt;a href=&quot;https://huggingface.co/collections/jingyaogong/minimind-66caf8d999f5c7fa64f399e5&quot;&gt;Huggingface&lt;/a&gt;下载模型权重文件，文件树：&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;lt;MiniMind-Model-Name&amp;gt; (root dir)
├─&amp;lt;MiniMind-Model-Name&amp;gt;
|  ├── config.json
|  ├── generation_config.json
|  ├── LMConfig.py
|  ├── model.py
|  ├── pytorch_model.bin
|  ├── special_tokens_map.json
|  ├── tokenizer_config.json
|  ├── tokenizer.json
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;启动聊天服务端&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python serve_openai_api.py
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;测试服务接口&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python chat_openai_api.py
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;API接口示例，兼容openai api格式&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;curl http://ip:port/v1/chat/completions \
  -H &quot;Content-Type: application/json&quot; \
  -d &#39;{ 
    &quot;model&quot;: &quot;model-identifier&quot;,
    &quot;messages&quot;: [ 
      { &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;世界上最高的山是什么？&quot; }
    ], 
    &quot;temperature&quot;: 0.7, 
    &quot;max_tokens&quot;: 512,
    &quot;stream&quot;: true
}&#39;
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;📌 Acknowledge&lt;/h1&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] 如果觉得&lt;code&gt;MiniMind系列&lt;/code&gt;对您有所帮助，可以在 GitHub 上加一个⭐&lt;br&gt; 篇幅超长水平有限难免纰漏，欢迎在Issues交流指正或提交PR改进项目&lt;br&gt; 您的小小支持就是持续改进此项目的动力！&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;🤝&lt;a href=&quot;https://github.com/jingyaogong/minimind/graphs/contributors&quot;&gt;贡献者&lt;/a&gt;&lt;/h2&gt; 
&lt;!--
&lt;a href=&quot;https://github.com/jingyaogong/minimind/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=jingyaogong/minimind&amp;v3&quot; /&gt;
&lt;/a&gt;
--&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/jingyaogong&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/62287848&quot; width=&quot;70px&quot; height=&quot;70px&quot;&gt;&lt;/a&gt; &amp;nbsp; &lt;a href=&quot;https://github.com/MuWinds&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/93832089&quot; width=&quot;70px&quot; height=&quot;70px&quot;&gt;&lt;/a&gt; &amp;nbsp; &lt;a href=&quot;https://github.com/chuanzhubin&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/2813798&quot; width=&quot;70px&quot; height=&quot;70px&quot;&gt;&lt;/a&gt; &amp;nbsp; &lt;a href=&quot;https://github.com/iomgaa-ycz&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/124225682&quot; width=&quot;70px&quot; height=&quot;70px&quot;&gt;&lt;/a&gt; &amp;nbsp;&lt;/p&gt; 
&lt;h2&gt;😊鸣谢&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/ipfgao&quot;&gt;&lt;b&gt;@ipfgao&lt;/b&gt;&lt;/a&gt;: &lt;a href=&quot;https://github.com/jingyaogong/minimind/issues/26&quot;&gt;🔗训练步骤记录&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/chuanzhubin&quot;&gt;&lt;b&gt;@chuanzhubin&lt;/b&gt;&lt;/a&gt;: &lt;a href=&quot;https://github.com/jingyaogong/minimind/pull/34&quot;&gt;🔗代码逐行注释&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/WangRongsheng&quot;&gt;&lt;b&gt;@WangRongsheng&lt;/b&gt;&lt;/a&gt;: &lt;a href=&quot;https://github.com/jingyaogong/minimind/issues/39&quot;&gt;🔗大型数据集预处理&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/pengqianhan&quot;&gt;&lt;b&gt;@pengqianhan&lt;/b&gt;&lt;/a&gt;: &lt;a href=&quot;https://github.com/jingyaogong/minimind/issues/73&quot;&gt;🔗一个简明教程&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/RyanSunn&quot;&gt;&lt;b&gt;@RyanSunn&lt;/b&gt;&lt;/a&gt;: &lt;a href=&quot;https://github.com/jingyaogong/minimind/issues/75&quot;&gt;🔗推理过程学习记录&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/Nijikadesu&quot;&gt;&lt;b&gt;@Nijikadesu&lt;/b&gt;&lt;/a&gt;: &lt;a href=&quot;https://github.com/jingyaogong/minimind/issues/213&quot;&gt;🔗以交互笔记本方式分解项目代码&lt;/a&gt;&lt;/p&gt; 
&lt;details close&gt; 
 &lt;summary&gt; &lt;b&gt;参考链接 &amp;amp; 感谢以下优秀的论文或项目&lt;/b&gt; &lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;排名不分任何先后顺序&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/meta-llama/llama3&quot;&gt;https://github.com/meta-llama/llama3&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/karpathy/llama2.c&quot;&gt;https://github.com/karpathy/llama2.c&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/DLLXW/baby-llama2-chinese&quot;&gt;https://github.com/DLLXW/baby-llama2-chinese&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.04434&quot;&gt;(DeepSeek-V2)https://arxiv.org/abs/2405.04434&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/charent/ChatLM-mini-Chinese&quot;&gt;https://github.com/charent/ChatLM-mini-Chinese&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/wdndev/tiny-llm-zh&quot;&gt;https://github.com/wdndev/tiny-llm-zh&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/2401.04088&quot;&gt;(Mistral-MoE)https://arxiv.org/pdf/2401.04088&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/Tongjilibo/build_MiniLLM_from_scratch&quot;&gt;https://github.com/Tongjilibo/build_MiniLLM_from_scratch&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/jzhang38/TinyLlama&quot;&gt;https://github.com/jzhang38/TinyLlama&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/AI-Study-Han/Zero-Chatgpt&quot;&gt;https://github.com/AI-Study-Han/Zero-Chatgpt&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/xusenlinzy/api-for-open-llm&quot;&gt;https://github.com/xusenlinzy/api-for-open-llm&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/HqWu-HITCS/Awesome-Chinese-LLM&quot;&gt;https://github.com/HqWu-HITCS/Awesome-Chinese-LLM&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;🫶支持者&lt;/h2&gt; 
&lt;a href=&quot;https://github.com/jingyaogong/minimind/stargazers&quot;&gt; 
 &lt;picture&gt; 
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://reporoster.com/stars/dark/jingyaogong/minimind&quot;&gt; 
  &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://reporoster.com/stars/jingyaogong/minimind&quot;&gt; 
  &lt;img alt=&quot;github contribution grid snake animation&quot; src=&quot;https://reporoster.com/stars/jingyaogong/minimind&quot;&gt; 
 &lt;/picture&gt; &lt;/a&gt; 
&lt;a href=&quot;https://github.com/jingyaogong/minimind/network/members&quot;&gt; 
 &lt;picture&gt; 
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://reporoster.com/forks/dark/jingyaogong/minimind&quot;&gt; 
  &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://reporoster.com/forks/jingyaogong/minimind&quot;&gt; 
  &lt;img alt=&quot;github contribution grid snake animation&quot; src=&quot;https://reporoster.com/forks/jingyaogong/minimind&quot;&gt; 
 &lt;/picture&gt; &lt;/a&gt; 
&lt;picture&gt; 
 &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://api.star-history.com/svg?repos=jingyaogong/minimind&amp;amp;type=Date&amp;amp;theme=dark&quot;&gt; 
 &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://api.star-history.com/svg?repos=jingyaogong/minimind&amp;amp;type=Date&quot;&gt; 
 &lt;img alt=&quot;Star History Chart&quot; src=&quot;https://api.star-history.com/svg?repos=jingyaogong/minimind&amp;amp;type=Date&quot;&gt; 
&lt;/picture&gt; 
&lt;h1&gt;License&lt;/h1&gt; 
&lt;p&gt;This repository is licensed under the &lt;a href=&quot;https://raw.githubusercontent.com/jingyaogong/minimind/master/LICENSE&quot;&gt;Apache-2.0 License&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>virattt/ai-hedge-fund</title>
      <link>https://github.com/virattt/ai-hedge-fund</link>
      <description>&lt;p&gt;An AI Hedge Fund Team&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AI Hedge Fund&lt;/h1&gt; 
&lt;p&gt;This is a proof of concept for an AI-powered hedge fund. The goal of this project is to explore the use of AI to make trading decisions. This project is for &lt;strong&gt;educational&lt;/strong&gt; purposes only and is not intended for real trading or investment.&lt;/p&gt; 
&lt;p&gt;This system employs several agents working together:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Ben Graham Agent - The godfather of value investing, only buys hidden gems with a margin of safety&lt;/li&gt; 
 &lt;li&gt;Bill Ackman Agent - An activist investors, takes bold positions and pushes for change&lt;/li&gt; 
 &lt;li&gt;Cathie Wood Agent - The queen of growth investing, believes in the power of innovation and disruption&lt;/li&gt; 
 &lt;li&gt;Charlie Munger Agent - Warren Buffett&#39;s partner, only buys wonderful businesses at fair prices&lt;/li&gt; 
 &lt;li&gt;Stanley Druckenmiller Agent - Macro trading legend who hunts for asymmetric opportunities with explosive growth potential&lt;/li&gt; 
 &lt;li&gt;Warren Buffett Agent - The oracle of Omaha, seeks wonderful companies at a fair price&lt;/li&gt; 
 &lt;li&gt;Valuation Agent - Calculates the intrinsic value of a stock and generates trading signals&lt;/li&gt; 
 &lt;li&gt;Sentiment Agent - Analyzes market sentiment and generates trading signals&lt;/li&gt; 
 &lt;li&gt;Fundamentals Agent - Analyzes fundamental data and generates trading signals&lt;/li&gt; 
 &lt;li&gt;Technicals Agent - Analyzes technical indicators and generates trading signals&lt;/li&gt; 
 &lt;li&gt;Risk Manager - Calculates risk metrics and sets position limits&lt;/li&gt; 
 &lt;li&gt;Portfolio Manager - Makes final trading decisions and generates orders&lt;/li&gt; 
&lt;/ol&gt; 
&lt;img width=&quot;1020&quot; alt=&quot;Screenshot 2025-03-08 at 4 45 22 PM&quot; src=&quot;https://github.com/user-attachments/assets/d8ab891e-a083-4fed-b514-ccc9322a3e57&quot;&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: the system simulates trading decisions, it does not actually trade.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://twitter.com/virattt&quot;&gt;&lt;img src=&quot;https://img.shields.io/twitter/follow/virattt?style=social&quot; alt=&quot;Twitter Follow&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Disclaimer&lt;/h2&gt; 
&lt;p&gt;This project is for &lt;strong&gt;educational and research purposes only&lt;/strong&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Not intended for real trading or investment&lt;/li&gt; 
 &lt;li&gt;No warranties or guarantees provided&lt;/li&gt; 
 &lt;li&gt;Past performance does not indicate future results&lt;/li&gt; 
 &lt;li&gt;Creator assumes no liability for financial losses&lt;/li&gt; 
 &lt;li&gt;Consult a financial advisor for investment decisions&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;By using this software, you agree to use it solely for learning purposes.&lt;/p&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#setup&quot;&gt;Setup&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#usage&quot;&gt;Usage&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#running-the-hedge-fund&quot;&gt;Running the Hedge Fund&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#running-the-backtester&quot;&gt;Running the Backtester&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#project-structure&quot;&gt;Project Structure&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#contributing&quot;&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#feature-requests&quot;&gt;Feature Requests&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#license&quot;&gt;License&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Setup&lt;/h2&gt; 
&lt;p&gt;Clone the repository:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;git clone https://github.com/virattt/ai-hedge-fund.git
cd ai-hedge-fund
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install Poetry (if not already installed):&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;curl -sSL https://install.python-poetry.org | python3 -
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start=&quot;2&quot;&gt; 
 &lt;li&gt;Install dependencies:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;poetry install
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start=&quot;3&quot;&gt; 
 &lt;li&gt;Set up your environment variables:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# Create .env file for your API keys
cp .env.example .env
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start=&quot;4&quot;&gt; 
 &lt;li&gt;Set your API keys:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# For running LLMs hosted by openai (gpt-4o, gpt-4o-mini, etc.)
# Get your OpenAI API key from https://platform.openai.com/
OPENAI_API_KEY=your-openai-api-key

# For running LLMs hosted by groq (deepseek, llama3, etc.)
# Get your Groq API key from https://groq.com/
GROQ_API_KEY=your-groq-api-key

# For getting financial data to power the hedge fund
# Get your Financial Datasets API key from https://financialdatasets.ai/
FINANCIAL_DATASETS_API_KEY=your-financial-datasets-api-key
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: You must set &lt;code&gt;OPENAI_API_KEY&lt;/code&gt;, &lt;code&gt;GROQ_API_KEY&lt;/code&gt;, &lt;code&gt;ANTHROPIC_API_KEY&lt;/code&gt;, or &lt;code&gt;DEEPSEEK_API_KEY&lt;/code&gt; for the hedge fund to work. If you want to use LLMs from all providers, you will need to set all API keys.&lt;/p&gt; 
&lt;p&gt;Financial data for AAPL, GOOGL, MSFT, NVDA, and TSLA is free and does not require an API key.&lt;/p&gt; 
&lt;p&gt;For any other ticker, you will need to set the &lt;code&gt;FINANCIAL_DATASETS_API_KEY&lt;/code&gt; in the .env file.&lt;/p&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;h3&gt;Running the Hedge Fund&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;poetry run python src/main.py --ticker AAPL,MSFT,NVDA
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Example Output:&lt;/strong&gt; &lt;img width=&quot;992&quot; alt=&quot;Screenshot 2025-01-06 at 5 50 17 PM&quot; src=&quot;https://github.com/user-attachments/assets/e8ca04bf-9989-4a7d-a8b4-34e04666663b&quot;&gt;&lt;/p&gt; 
&lt;p&gt;You can also specify a &lt;code&gt;--show-reasoning&lt;/code&gt; flag to print the reasoning of each agent to the console.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;poetry run python src/main.py --ticker AAPL,MSFT,NVDA --show-reasoning
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can optionally specify the start and end dates to make decisions for a specific time period.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;poetry run python src/main.py --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01 
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Running the Backtester&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;poetry run python src/backtester.py --ticker AAPL,MSFT,NVDA
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Example Output:&lt;/strong&gt; &lt;img width=&quot;941&quot; alt=&quot;Screenshot 2025-01-06 at 5 47 52 PM&quot; src=&quot;https://github.com/user-attachments/assets/00e794ea-8628-44e6-9a84-8f8a31ad3b47&quot;&gt;&lt;/p&gt; 
&lt;p&gt;You can optionally specify the start and end dates to backtest over a specific time period.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;poetry run python src/backtester.py --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Project Structure&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;ai-hedge-fund/
├── src/
│   ├── agents/                   # Agent definitions and workflow
│   │   ├── bill_ackman.py        # Bill Ackman agent
│   │   ├── fundamentals.py       # Fundamental analysis agent
│   │   ├── portfolio_manager.py  # Portfolio management agent
│   │   ├── risk_manager.py       # Risk management agent
│   │   ├── sentiment.py          # Sentiment analysis agent
│   │   ├── technicals.py         # Technical analysis agent
│   │   ├── valuation.py          # Valuation analysis agent
│   │   ├── warren_buffett.py     # Warren Buffett agent
│   ├── tools/                    # Agent tools
│   │   ├── api.py                # API tools
│   ├── backtester.py             # Backtesting tools
│   ├── main.py # Main entry point
├── pyproject.toml
├── ...
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Fork the repository&lt;/li&gt; 
 &lt;li&gt;Create a feature branch&lt;/li&gt; 
 &lt;li&gt;Commit your changes&lt;/li&gt; 
 &lt;li&gt;Push to the branch&lt;/li&gt; 
 &lt;li&gt;Create a Pull Request&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: Please keep your pull requests small and focused. This will make it easier to review and merge.&lt;/p&gt; 
&lt;h2&gt;Feature Requests&lt;/h2&gt; 
&lt;p&gt;If you have a feature request, please open an &lt;a href=&quot;https://github.com/virattt/ai-hedge-fund/issues&quot;&gt;issue&lt;/a&gt; and make sure it is tagged with &lt;code&gt;enhancement&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the MIT License - see the LICENSE file for details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>landing-ai/vision-agent</title>
      <link>https://github.com/landing-ai/vision-agent</link>
      <description>&lt;p&gt;Vision agent&lt;/p&gt;&lt;hr&gt;&lt;div align=&quot;center&quot;&gt; 
 &lt;picture&gt; 
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://github.com/landing-ai/vision-agent/blob/main/assets/logo_light.svg?raw=true&quot;&gt; 
  &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://github.com/landing-ai/vision-agent/blob/main/assets/logo_dark.svg?raw=true&quot;&gt; 
  &lt;img alt=&quot;VisionAgent&quot; height=&quot;200px&quot; src=&quot;https://github.com/landing-ai/vision-agent/raw/main/assets/logo_light.svg?raw=true&quot;&gt; 
 &lt;/picture&gt; 
 &lt;p&gt;&lt;a href=&quot;https://discord.gg/wPdN8RCYew&quot;&gt;&lt;img src=&quot;https://dcbadge.vercel.app/api/server/wPdN8RCYew?compact=true&amp;amp;style=flat&quot; alt=&quot;&quot;&gt;&lt;/a&gt; &lt;img src=&quot;https://github.com/landing-ai/vision-agent/actions/workflows/ci_cd.yml/badge.svg?sanitize=true&quot; alt=&quot;ci_status&quot;&gt; &lt;a href=&quot;https://badge.fury.io/py/vision-agent&quot;&gt;&lt;img src=&quot;https://badge.fury.io/py/vision-agent.svg?sanitize=true&quot; alt=&quot;PyPI version&quot;&gt;&lt;/a&gt; &lt;img src=&quot;https://img.shields.io/pypi/pyversions/vision-agent&quot; alt=&quot;version&quot;&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;VisionAgent&lt;/h2&gt; 
&lt;p&gt;VisionAgent is a library that helps you utilize agent frameworks to generate code to solve your vision task. Check out our discord for updates and roadmaps! The fastest way to test out VisionAgent is to use our web application which you can find &lt;a href=&quot;https://va.landing.ai/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install vision-agent
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;export ANTHROPIC_API_KEY=&quot;your-api-key&quot;
export GEMINI_API_KEY=&quot;your-api-key&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;&lt;em&gt;NOTE:&lt;/em&gt;&lt;/strong&gt; We found using both Anthropic Claude-3.7 and Gemini-2.0-Flash-Exp to be provide the best performance for VisionAgent. If you want to use a different LLM provider or only one, see &#39;Using Other LLM Providers&#39; below.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;You will also need to set your VisionAgent API key to be able to authenticate when using the hosted vision tools that we provide through our APIs. Currently, the APIs are free to use so you will only need to get it from &lt;a href=&quot;https://va.landing.ai/account/api-key&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;export VISION_AGENT_API_KEY=&quot;your-api-key&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://landing-ai.github.io/vision-agent/&quot;&gt;VisionAgent Library Docs&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;h3&gt;Counting cans in an image&lt;/h3&gt; 
&lt;p&gt;You can run VisionAgent in a local Jupyter Notebook &lt;a href=&quot;https://github.com/landing-ai/vision-agent/raw/main/examples/notebooks/counting_cans.ipynb&quot;&gt;Counting cans in an image&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Generating code&lt;/h3&gt; 
&lt;p&gt;You can use VisionAgent to generate code to count the number of people in an image:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from vision_agent.agent import VisionAgentCoderV2
from vision_agent.models import AgentMessage

agent = VisionAgentCoderV2(verbose=True)
code_context = agent.generate_code(
    [
        AgentMessage(
            role=&quot;user&quot;,
            content=&quot;Count the number of people in this image&quot;,
            media=[&quot;people.png&quot;]
        )
    ]
)

with open(&quot;generated_code.py&quot;, &quot;w&quot;) as f:
    f.write(code_context.code + &quot;\n&quot; + code_context.test)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Using the tools directly&lt;/h3&gt; 
&lt;p&gt;VisionAgent produces code that utilizes our tools. You can also use the tools directly. For example if you wanted to detect people in an image and visualize the results:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import vision_agent.tools as T
import matplotlib.pyplot as plt

image = T.load_image(&quot;people.png&quot;)
dets = T.countgd_object_detection(&quot;person&quot;, image)
# visualize the countgd bounding boxes on the image
viz = T.overlay_bounding_boxes(image, dets)

# save the visualization to a file
T.save_image(viz, &quot;people_detected.png&quot;)

# display the visualization
plt.imshow(viz)
plt.show()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also use the tools for running on video files:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import vision_agent.tools as T

frames_and_ts = T.extract_frames_and_timestamps(&quot;people.mp4&quot;)
# extract the frames from the frames_and_ts list
frames = [f[&quot;frame&quot;] for f in frames_and_ts]

# run the countgd tracking on the frames
tracks = T.countgd_sam2_video_tracking(&quot;person&quot;, frames)
# visualize the countgd tracking results on the frames and save the video
viz = T.overlay_segmentation_masks(frames, tracks)
T.save_video(viz, &quot;people_detected.mp4&quot;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Using Other LLM Providers&lt;/h2&gt; 
&lt;p&gt;You can use other LLM providers by changing &lt;code&gt;config.py&lt;/code&gt; in the &lt;code&gt;vision_agent/configs&lt;/code&gt; directory. For example to change to Anthropic simply just run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;cp vision_agent/configs/anthropic_config.py vision_agent/configs/config.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also modify the existing &lt;code&gt;config.py&lt;/code&gt; file yourself to use a different LLM provider, for example if you wanted to change the planner from Anthropic inside &lt;code&gt;config.py&lt;/code&gt; to OpenAI you would replace this code:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;    planner: Type[LMM] = Field(default=AnthropicLMM)
    planner_kwargs: dict = Field(
        default_factory=lambda: {
            &quot;model_name&quot;: &quot;claude-3-7-sonnet-20250219&quot;,
            &quot;temperature&quot;: 0.0,
            &quot;image_size&quot;: 768,
        }
    )
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;with this code:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;    planner: Type[LMM] = Field(default=OpenAILMM)
    planner_kwargs: dict = Field(
        default_factory=lambda: {
            &quot;model_name&quot;: &quot;gpt-4o-2024-11-20&quot;,
            &quot;temperature&quot;: 0.0,
            &quot;image_size&quot;: 768,
            &quot;image_detail&quot;: &quot;low&quot;,
        }
    )
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;&lt;em&gt;NOTE:&lt;/em&gt;&lt;/strong&gt; VisionAgent moves fast and we are constantly updating and changing the library. If you have any questions or need help, please reach out to us on our discord channel.&lt;/p&gt; 
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>VikParuchuri/marker</title>
      <link>https://github.com/VikParuchuri/marker</link>
      <description>&lt;p&gt;Convert PDF to markdown + JSON quickly with high accuracy&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Marker&lt;/h1&gt; 
&lt;p&gt;Marker converts documents to markdown, JSON, and HTML quickly and accurately.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Converts PDF, image, PPTX, DOCX, XLSX, HTML, EPUB files in all languages&lt;/li&gt; 
 &lt;li&gt;Formats tables, forms, equations, inline math, links, references, and code blocks&lt;/li&gt; 
 &lt;li&gt;Extracts and saves images&lt;/li&gt; 
 &lt;li&gt;Removes headers/footers/other artifacts&lt;/li&gt; 
 &lt;li&gt;Extensible with your own formatting and logic&lt;/li&gt; 
 &lt;li&gt;Optionally boost accuracy with LLMs&lt;/li&gt; 
 &lt;li&gt;Works on GPU, CPU, or MPS&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Performance&lt;/h2&gt; 
&lt;img src=&quot;https://raw.githubusercontent.com/VikParuchuri/marker/master/data/images/overall.png&quot; width=&quot;800px&quot;&gt; 
&lt;p&gt;Marker benchmarks favorably compared to cloud services like Llamaparse and Mathpix, as well as other open source tools.&lt;/p&gt; 
&lt;p&gt;The above results are running single PDF pages serially. Marker is significantly faster when running in batch mode, with a projected throughput of 122 pages/second on an H100 (.18 seconds per page across 22 processes).&lt;/p&gt; 
&lt;p&gt;See &lt;a href=&quot;https://raw.githubusercontent.com/VikParuchuri/marker/master/#benchmarks&quot;&gt;below&lt;/a&gt; for detailed speed and accuracy benchmarks, and instructions on how to run your own benchmarks.&lt;/p&gt; 
&lt;h2&gt;Hybrid Mode&lt;/h2&gt; 
&lt;p&gt;For the highest accuracy, pass the &lt;code&gt;--use_llm&lt;/code&gt; flag to use an LLM alongside marker. This will do things like merge tables across pages, handle inline math, format tables properly, and extract values from forms. It can use any gemini or ollama model. By default, it uses &lt;code&gt;gemini-2.0-flash&lt;/code&gt;. See &lt;a href=&quot;https://raw.githubusercontent.com/VikParuchuri/marker/master/#llm-services&quot;&gt;below&lt;/a&gt; for details.&lt;/p&gt; 
&lt;p&gt;Here is a table benchmark comparing marker, gemini flash alone, and marker with use_llm:&lt;/p&gt; 
&lt;img src=&quot;https://raw.githubusercontent.com/VikParuchuri/marker/master/data/images/table.png&quot; width=&quot;400px&quot;&gt; 
&lt;p&gt;As you can see, the use_llm mode offers higher accuracy than marker or gemini alone.&lt;/p&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;PDF&lt;/th&gt; 
   &lt;th&gt;File type&lt;/th&gt; 
   &lt;th&gt;Markdown&lt;/th&gt; 
   &lt;th&gt;JSON&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://greenteapress.com/thinkpython/thinkpython.pdf&quot;&gt;Think Python&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Textbook&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/VikParuchuri/marker/raw/master/data/examples/markdown/thinkpython/thinkpython.md&quot;&gt;View&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/VikParuchuri/marker/raw/master/data/examples/json/thinkpython.json&quot;&gt;View&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/pdf/2101.03961.pdf&quot;&gt;Switch Transformers&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;arXiv paper&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/VikParuchuri/marker/raw/master/data/examples/markdown/switch_transformers/switch_trans.md&quot;&gt;View&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/VikParuchuri/marker/raw/master/data/examples/json/switch_trans.json&quot;&gt;View&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/pdf/1804.07821.pdf&quot;&gt;Multi-column CNN&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;arXiv paper&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/VikParuchuri/marker/raw/master/data/examples/markdown/multicolcnn/multicolcnn.md&quot;&gt;View&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/VikParuchuri/marker/raw/master/data/examples/json/multicolcnn.json&quot;&gt;View&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;Commercial usage&lt;/h1&gt; 
&lt;p&gt;I want marker to be as widely accessible as possible, while still funding my development/training costs. Research and personal usage is always okay, but there are some restrictions on commercial usage.&lt;/p&gt; 
&lt;p&gt;The weights for the models are licensed &lt;code&gt;cc-by-nc-sa-4.0&lt;/code&gt;, but I will waive that for any organization under $5M USD in gross revenue in the most recent 12-month period AND under $5M in lifetime VC/angel funding raised. You also must not be competitive with the &lt;a href=&quot;https://www.datalab.to/&quot;&gt;Datalab API&lt;/a&gt;. If you want to remove the GPL license requirements (dual-license) and/or use the weights commercially over the revenue limit, check out the options &lt;a href=&quot;https://www.datalab.to&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Hosted API&lt;/h1&gt; 
&lt;p&gt;There&#39;s a hosted API for marker available &lt;a href=&quot;https://www.datalab.to/&quot;&gt;here&lt;/a&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Supports PDFs, word documents, and powerpoints&lt;/li&gt; 
 &lt;li&gt;1/4th the price of leading cloud-based competitors&lt;/li&gt; 
 &lt;li&gt;High uptime (99.99%), quality, and speed (around 15 seconds to convert a 250 page PDF)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Community&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://discord.gg//KuZwXNGnfH&quot;&gt;Discord&lt;/a&gt; is where we discuss future development.&lt;/p&gt; 
&lt;h1&gt;Installation&lt;/h1&gt; 
&lt;p&gt;You&#39;ll need python 3.10+ and PyTorch. You may need to install the CPU version of torch first if you&#39;re not using a Mac or a GPU machine. See &lt;a href=&quot;https://pytorch.org/get-started/locally/&quot;&gt;here&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;p&gt;Install with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;pip install marker-pdf
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you want to use marker on documents other than PDFs, you will need to install additional dependencies with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;pip install marker-pdf[full]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;Usage&lt;/h1&gt; 
&lt;p&gt;First, some configuration:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Your torch device will be automatically detected, but you can override this. For example, &lt;code&gt;TORCH_DEVICE=cuda&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Some PDFs, even digital ones, have bad text in them. Set the &lt;code&gt;force_ocr&lt;/code&gt; flag to ensure your PDF runs through OCR, or the &lt;code&gt;strip_existing_ocr&lt;/code&gt; to keep all digital text, and strip out any existing OCR text.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Interactive App&lt;/h2&gt; 
&lt;p&gt;I&#39;ve included a streamlit app that lets you interactively try marker with some basic options. Run it with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;pip install streamlit
marker_gui
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Convert a single file&lt;/h2&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;marker_single /path/to/file.pdf
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can pass in PDFs or images.&lt;/p&gt; 
&lt;p&gt;Options:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--output_dir PATH&lt;/code&gt;: Directory where output files will be saved. Defaults to the value specified in settings.OUTPUT_DIR.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--output_format [markdown|json|html]&lt;/code&gt;: Specify the format for the output results.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--paginate_output&lt;/code&gt;: Paginates the output, using &lt;code&gt;\n\n{PAGE_NUMBER}&lt;/code&gt; followed by &lt;code&gt;-&lt;/code&gt; * 48, then &lt;code&gt;\n\n&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--use_llm&lt;/code&gt;: Uses an LLM to improve accuracy. You must set your Gemini API key using the &lt;code&gt;GOOGLE_API_KEY&lt;/code&gt; env var.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--redo_inline_math&lt;/code&gt;: If you want the highest quality inline math conversion, use this along with &lt;code&gt;--use_llm&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--disable_image_extraction&lt;/code&gt;: Don&#39;t extract images from the PDF. If you also specify &lt;code&gt;--use_llm&lt;/code&gt;, then images will be replaced with a description.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--page_range TEXT&lt;/code&gt;: Specify which pages to process. Accepts comma-separated page numbers and ranges. Example: &lt;code&gt;--page_range &quot;0,5-10,20&quot;&lt;/code&gt; will process pages 0, 5 through 10, and page 20.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--force_ocr&lt;/code&gt;: Force OCR processing on the entire document, even for pages that might contain extractable text.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--strip_existing_ocr&lt;/code&gt;: Remove all existing OCR text in the document and re-OCR with surya.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--debug&lt;/code&gt;: Enable debug mode for additional logging and diagnostic information.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--processors TEXT&lt;/code&gt;: Override the default processors by providing their full module paths, separated by commas. Example: &lt;code&gt;--processors &quot;module1.processor1,module2.processor2&quot;&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--config_json PATH&lt;/code&gt;: Path to a JSON configuration file containing additional settings.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--languages TEXT&lt;/code&gt;: Optionally specify which languages to use for OCR processing. Accepts a comma-separated list. Example: &lt;code&gt;--languages &quot;en,fr,de&quot;&lt;/code&gt; for English, French, and German.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;config --help&lt;/code&gt;: List all available builders, processors, and converters, and their associated configuration. These values can be used to build a JSON configuration file for additional tweaking of marker defaults.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--converter_cls&lt;/code&gt;: One of &lt;code&gt;marker.converters.pdf.PdfConverter&lt;/code&gt; (default) or &lt;code&gt;marker.converters.table.TableConverter&lt;/code&gt;. The &lt;code&gt;PdfConverter&lt;/code&gt; will convert the whole PDF, the &lt;code&gt;TableConverter&lt;/code&gt; will only extract and convert tables.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--llm_service&lt;/code&gt;: Which llm service to use if &lt;code&gt;--use_llm&lt;/code&gt; is passed. This defaults to &lt;code&gt;marker.services.gemini.GoogleGeminiService&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--help&lt;/code&gt;: see all of the flags that can be passed into marker. (it supports many more options then are listed above)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The list of supported languages for surya OCR is &lt;a href=&quot;https://github.com/VikParuchuri/surya/raw/master/surya/recognition/languages.py&quot;&gt;here&lt;/a&gt;. If you don&#39;t need OCR, marker can work with any language.&lt;/p&gt; 
&lt;h2&gt;Convert multiple files&lt;/h2&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;marker /path/to/input/folder --workers 4
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;marker&lt;/code&gt; supports all the same options from &lt;code&gt;marker_single&lt;/code&gt; above.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--workers&lt;/code&gt; is the number of conversion workers to run simultaneously. This is set to 5 by default, but you can increase it to increase throughput, at the cost of more CPU/GPU usage. Marker will use 5GB of VRAM per worker at the peak, and 3.5GB average.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Convert multiple files on multiple GPUs&lt;/h2&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;NUM_DEVICES=4 NUM_WORKERS=15 marker_chunk_convert ../pdf_in ../md_out
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;NUM_DEVICES&lt;/code&gt; is the number of GPUs to use. Should be &lt;code&gt;2&lt;/code&gt; or greater.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;NUM_WORKERS&lt;/code&gt; is the number of parallel processes to run on each GPU.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Use from python&lt;/h2&gt; 
&lt;p&gt;See the &lt;code&gt;PdfConverter&lt;/code&gt; class at &lt;code&gt;marker/converters/pdf.py&lt;/code&gt; function for additional arguments that can be passed.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from marker.converters.pdf import PdfConverter
from marker.models import create_model_dict
from marker.output import text_from_rendered

converter = PdfConverter(
    artifact_dict=create_model_dict(),
)
rendered = converter(&quot;FILEPATH&quot;)
text, _, images = text_from_rendered(rendered)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;code&gt;rendered&lt;/code&gt; will be a pydantic basemodel with different properties depending on the output type requested. With markdown output (default), you&#39;ll have the properties &lt;code&gt;markdown&lt;/code&gt;, &lt;code&gt;metadata&lt;/code&gt;, and &lt;code&gt;images&lt;/code&gt;. For json output, you&#39;ll have &lt;code&gt;children&lt;/code&gt;, &lt;code&gt;block_type&lt;/code&gt;, and &lt;code&gt;metadata&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Custom configuration&lt;/h3&gt; 
&lt;p&gt;You can pass configuration using the &lt;code&gt;ConfigParser&lt;/code&gt;. To see all available options, do &lt;code&gt;marker_single --help&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from marker.converters.pdf import PdfConverter
from marker.models import create_model_dict
from marker.config.parser import ConfigParser

config = {
    &quot;output_format&quot;: &quot;json&quot;,
    &quot;ADDITIONAL_KEY&quot;: &quot;VALUE&quot;
}
config_parser = ConfigParser(config)

converter = PdfConverter(
    config=config_parser.generate_config_dict(),
    artifact_dict=create_model_dict(),
    processor_list=config_parser.get_processors(),
    renderer=config_parser.get_renderer(),
    llm_service=config_parser.get_llm_service()
)
rendered = converter(&quot;FILEPATH&quot;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Extract blocks&lt;/h3&gt; 
&lt;p&gt;Each document consists of one or more pages. Pages contain blocks, which can themselves contain other blocks. It&#39;s possible to programmatically manipulate these blocks.&lt;/p&gt; 
&lt;p&gt;Here&#39;s an example of extracting all forms from a document:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from marker.converters.pdf import PdfConverter
from marker.models import create_model_dict
from marker.schema import BlockTypes

converter = PdfConverter(
    artifact_dict=create_model_dict(),
)
document = converter.build_document(&quot;FILEPATH&quot;)
forms = document.contained_blocks((BlockTypes.Form,))
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Look at the processors for more examples of extracting and manipulating blocks.&lt;/p&gt; 
&lt;h2&gt;Other converters&lt;/h2&gt; 
&lt;p&gt;You can also use other converters that define different conversion pipelines:&lt;/p&gt; 
&lt;h3&gt;Extract tables&lt;/h3&gt; 
&lt;p&gt;The &lt;code&gt;TableConverter&lt;/code&gt; will only convert and extract tables:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from marker.converters.table import TableConverter
from marker.models import create_model_dict
from marker.output import text_from_rendered

converter = TableConverter(
    artifact_dict=create_model_dict(),
)
rendered = converter(&quot;FILEPATH&quot;)
text, _, images = text_from_rendered(rendered)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This takes all the same configuration as the PdfConverter. You can specify the configuration &lt;code&gt;force_layout_block=Table&lt;/code&gt; to avoid layout detection and instead assume every page is a table. Set &lt;code&gt;output_format=json&lt;/code&gt; to also get cell bounding boxes.&lt;/p&gt; 
&lt;p&gt;You can also run this via the CLI with&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;marker_single FILENAME --use_llm --force_layout_block Table --converter_cls marker.converters.table.TableConverter --output_format json
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;Output Formats&lt;/h1&gt; 
&lt;h2&gt;Markdown&lt;/h2&gt; 
&lt;p&gt;Markdown output will include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;image links (images will be saved in the same folder)&lt;/li&gt; 
 &lt;li&gt;formatted tables&lt;/li&gt; 
 &lt;li&gt;embedded LaTeX equations (fenced with &lt;code&gt;$$&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Code is fenced with triple backticks&lt;/li&gt; 
 &lt;li&gt;Superscripts for footnotes&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;HTML&lt;/h2&gt; 
&lt;p&gt;HTML output is similar to markdown output:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Images are included via &lt;code&gt;img&lt;/code&gt; tags&lt;/li&gt; 
 &lt;li&gt;equations are fenced with &lt;code&gt;&amp;lt;math&amp;gt;&lt;/code&gt; tags&lt;/li&gt; 
 &lt;li&gt;code is in &lt;code&gt;pre&lt;/code&gt; tags&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;JSON&lt;/h2&gt; 
&lt;p&gt;JSON output will be organized in a tree-like structure, with the leaf nodes being blocks. Examples of leaf nodes are a single list item, a paragraph of text, or an image.&lt;/p&gt; 
&lt;p&gt;The output will be a list, with each list item representing a page. Each page is considered a block in the internal marker schema. There are different types of blocks to represent different elements.&lt;/p&gt; 
&lt;p&gt;Pages have the keys:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;id&lt;/code&gt; - unique id for the block.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;block_type&lt;/code&gt; - the type of block. The possible block types can be seen in &lt;code&gt;marker/schema/__init__.py&lt;/code&gt;. As of this writing, they are [&quot;Line&quot;, &quot;Span&quot;, &quot;FigureGroup&quot;, &quot;TableGroup&quot;, &quot;ListGroup&quot;, &quot;PictureGroup&quot;, &quot;Page&quot;, &quot;Caption&quot;, &quot;Code&quot;, &quot;Figure&quot;, &quot;Footnote&quot;, &quot;Form&quot;, &quot;Equation&quot;, &quot;Handwriting&quot;, &quot;TextInlineMath&quot;, &quot;ListItem&quot;, &quot;PageFooter&quot;, &quot;PageHeader&quot;, &quot;Picture&quot;, &quot;SectionHeader&quot;, &quot;Table&quot;, &quot;Text&quot;, &quot;TableOfContents&quot;, &quot;Document&quot;]&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;html&lt;/code&gt; - the HTML for the page. Note that this will have recursive references to children. The &lt;code&gt;content-ref&lt;/code&gt; tags must be replaced with the child content if you want the full html. You can see an example of this at &lt;code&gt;marker/output.py:json_to_html&lt;/code&gt;. That function will take in a single block from the json output, and turn it into HTML.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;polygon&lt;/code&gt; - the 4-corner polygon of the page, in (x1,y1), (x2,y2), (x3, y3), (x4, y4) format. (x1,y1) is the top left, and coordinates go clockwise.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;children&lt;/code&gt; - the child blocks.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The child blocks have two additional keys:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;section_hierarchy&lt;/code&gt; - indicates the sections that the block is part of. &lt;code&gt;1&lt;/code&gt; indicates an h1 tag, &lt;code&gt;2&lt;/code&gt; an h2, and so on.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;images&lt;/code&gt; - base64 encoded images. The key will be the block id, and the data will be the encoded image.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Note that child blocks of pages can have their own children as well (a tree structure).&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-json&quot;&gt;{
      &quot;id&quot;: &quot;/page/10/Page/366&quot;,
      &quot;block_type&quot;: &quot;Page&quot;,
      &quot;html&quot;: &quot;&amp;lt;content-ref src=&#39;/page/10/SectionHeader/0&#39;&amp;gt;&amp;lt;/content-ref&amp;gt;&amp;lt;content-ref src=&#39;/page/10/SectionHeader/1&#39;&amp;gt;&amp;lt;/content-ref&amp;gt;&amp;lt;content-ref src=&#39;/page/10/Text/2&#39;&amp;gt;&amp;lt;/content-ref&amp;gt;&amp;lt;content-ref src=&#39;/page/10/Text/3&#39;&amp;gt;&amp;lt;/content-ref&amp;gt;&amp;lt;content-ref src=&#39;/page/10/Figure/4&#39;&amp;gt;&amp;lt;/content-ref&amp;gt;&amp;lt;content-ref src=&#39;/page/10/SectionHeader/5&#39;&amp;gt;&amp;lt;/content-ref&amp;gt;&amp;lt;content-ref src=&#39;/page/10/SectionHeader/6&#39;&amp;gt;&amp;lt;/content-ref&amp;gt;&amp;lt;content-ref src=&#39;/page/10/TextInlineMath/7&#39;&amp;gt;&amp;lt;/content-ref&amp;gt;&amp;lt;content-ref src=&#39;/page/10/TextInlineMath/8&#39;&amp;gt;&amp;lt;/content-ref&amp;gt;&amp;lt;content-ref src=&#39;/page/10/Table/9&#39;&amp;gt;&amp;lt;/content-ref&amp;gt;&amp;lt;content-ref src=&#39;/page/10/SectionHeader/10&#39;&amp;gt;&amp;lt;/content-ref&amp;gt;&amp;lt;content-ref src=&#39;/page/10/Text/11&#39;&amp;gt;&amp;lt;/content-ref&amp;gt;&quot;,
      &quot;polygon&quot;: [[0.0, 0.0], [612.0, 0.0], [612.0, 792.0], [0.0, 792.0]],
      &quot;children&quot;: [
        {
          &quot;id&quot;: &quot;/page/10/SectionHeader/0&quot;,
          &quot;block_type&quot;: &quot;SectionHeader&quot;,
          &quot;html&quot;: &quot;&amp;lt;h1&amp;gt;Supplementary Material for &amp;lt;i&amp;gt;Subspace Adversarial Training&amp;lt;/i&amp;gt; &amp;lt;/h1&amp;gt;&quot;,
          &quot;polygon&quot;: [
            [217.845703125, 80.630859375], [374.73046875, 80.630859375],
            [374.73046875, 107.0],
            [217.845703125, 107.0]
          ],
          &quot;children&quot;: null,
          &quot;section_hierarchy&quot;: {
            &quot;1&quot;: &quot;/page/10/SectionHeader/1&quot;
          },
          &quot;images&quot;: {}
        },
        ...
        ]
    }


&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Metadata&lt;/h2&gt; 
&lt;p&gt;All output formats will return a metadata dictionary, with the following fields:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-json&quot;&gt;{
    &quot;table_of_contents&quot;: [
      {
        &quot;title&quot;: &quot;Introduction&quot;,
        &quot;heading_level&quot;: 1,
        &quot;page_id&quot;: 0,
        &quot;polygon&quot;: [...]
      }
    ], // computed PDF table of contents
    &quot;page_stats&quot;: [
      {
        &quot;page_id&quot;:  0, 
        &quot;text_extraction_method&quot;: &quot;pdftext&quot;,
        &quot;block_counts&quot;: [(&quot;Span&quot;, 200), ...]
      },
      ...
    ]
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;LLM Services&lt;/h1&gt; 
&lt;p&gt;When running with the &lt;code&gt;--use_llm&lt;/code&gt; flag, you have a choice of services you can use:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;Gemini&lt;/code&gt; - this will use the Gemini developer API by default. You&#39;ll need to pass &lt;code&gt;--gemini_api_key&lt;/code&gt; to configuration.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Google Vertex&lt;/code&gt; - this will use vertex, which can be more reliable. You&#39;ll need to pass &lt;code&gt;--vertex_project_id&lt;/code&gt;. To use it, set &lt;code&gt;--llm_service=marker.services.vertex.GoogleVertexService&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Ollama&lt;/code&gt; - this will use local models. You can configure &lt;code&gt;--ollama_base_url&lt;/code&gt; and &lt;code&gt;--ollama_model&lt;/code&gt;. To use it, set &lt;code&gt;--llm_service=marker.services.ollama.OllamaService&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Claude&lt;/code&gt; - this will use the anthropic API. You can configure &lt;code&gt;--claude_api_key&lt;/code&gt;, and &lt;code&gt;--claude_model_name&lt;/code&gt;. To use it, set &lt;code&gt;--llm_service=marker.services.claude.ClaudeService&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;These services may have additional optional configuration as well - you can see it by viewing the classes.&lt;/p&gt; 
&lt;h1&gt;Internals&lt;/h1&gt; 
&lt;p&gt;Marker is easy to extend. The core units of marker are:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;Providers&lt;/code&gt;, at &lt;code&gt;marker/providers&lt;/code&gt;. These provide information from a source file, like a PDF.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Builders&lt;/code&gt;, at &lt;code&gt;marker/builders&lt;/code&gt;. These generate the initial document blocks and fill in text, using info from the providers.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Processors&lt;/code&gt;, at &lt;code&gt;marker/processors&lt;/code&gt;. These process specific blocks, for example the table formatter is a processor.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Renderers&lt;/code&gt;, at &lt;code&gt;marker/renderers&lt;/code&gt;. These use the blocks to render output.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Schema&lt;/code&gt;, at &lt;code&gt;marker/schema&lt;/code&gt;. The classes for all the block types.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Converters&lt;/code&gt;, at &lt;code&gt;marker/converters&lt;/code&gt;. They run the whole end to end pipeline.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;To customize processing behavior, override the &lt;code&gt;processors&lt;/code&gt;. To add new output formats, write a new &lt;code&gt;renderer&lt;/code&gt;. For additional input formats, write a new &lt;code&gt;provider.&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Processors and renderers can be directly passed into the base &lt;code&gt;PDFConverter&lt;/code&gt;, so you can specify your own custom processing easily.&lt;/p&gt; 
&lt;h2&gt;API server&lt;/h2&gt; 
&lt;p&gt;There is a very simple API server you can run like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;pip install -U uvicorn fastapi python-multipart
marker_server --port 8001
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will start a fastapi server that you can access at &lt;code&gt;localhost:8001&lt;/code&gt;. You can go to &lt;code&gt;localhost:8001/docs&lt;/code&gt; to see the endpoint options.&lt;/p&gt; 
&lt;p&gt;You can send requests like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;import requests
import json

post_data = {
    &#39;filepath&#39;: &#39;FILEPATH&#39;,
    # Add other params here
}

requests.post(&quot;http://localhost:8001/marker&quot;, data=json.dumps(post_data)).json()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note that this is not a very robust API, and is only intended for small-scale use. If you want to use this server, but want a more robust conversion option, you can use the hosted &lt;a href=&quot;https://www.datalab.to/plans&quot;&gt;Datalab API&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Troubleshooting&lt;/h1&gt; 
&lt;p&gt;There are some settings that you may find useful if things aren&#39;t working the way you expect:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;If you have issues with accuracy, try setting &lt;code&gt;--use_llm&lt;/code&gt; to use an LLM to improve quality. You must set &lt;code&gt;GOOGLE_API_KEY&lt;/code&gt; to a Gemini API key for this to work.&lt;/li&gt; 
 &lt;li&gt;Make sure to set &lt;code&gt;force_ocr&lt;/code&gt; if you see garbled text - this will re-OCR the document.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;TORCH_DEVICE&lt;/code&gt; - set this to force marker to use a given torch device for inference.&lt;/li&gt; 
 &lt;li&gt;If you&#39;re getting out of memory errors, decrease worker count. You can also try splitting up long PDFs into multiple files.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Debugging&lt;/h2&gt; 
&lt;p&gt;Pass the &lt;code&gt;debug&lt;/code&gt; option to activate debug mode. This will save images of each page with detected layout and text, as well as output a json file with additional bounding box information.&lt;/p&gt; 
&lt;h1&gt;Benchmarks&lt;/h1&gt; 
&lt;h2&gt;Overall PDF Conversion&lt;/h2&gt; 
&lt;p&gt;We created a &lt;a href=&quot;https://huggingface.co/datasets/datalab-to/marker_benchmark&quot;&gt;benchmark set&lt;/a&gt; by extracting single PDF pages from common crawl. We scored based on a heuristic that aligns text with ground truth text segments, and an LLM as a judge scoring method.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Method&lt;/th&gt; 
   &lt;th&gt;Avg Time&lt;/th&gt; 
   &lt;th&gt;Heuristic Score&lt;/th&gt; 
   &lt;th&gt;LLM Score&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;marker&lt;/td&gt; 
   &lt;td&gt;2.83837&lt;/td&gt; 
   &lt;td&gt;95.6709&lt;/td&gt; 
   &lt;td&gt;4.23916&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;llamaparse&lt;/td&gt; 
   &lt;td&gt;23.348&lt;/td&gt; 
   &lt;td&gt;84.2442&lt;/td&gt; 
   &lt;td&gt;3.97619&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;mathpix&lt;/td&gt; 
   &lt;td&gt;6.36223&lt;/td&gt; 
   &lt;td&gt;86.4281&lt;/td&gt; 
   &lt;td&gt;4.15626&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;docling&lt;/td&gt; 
   &lt;td&gt;3.69949&lt;/td&gt; 
   &lt;td&gt;86.7073&lt;/td&gt; 
   &lt;td&gt;3.70429&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Benchmarks were run on an H100 for markjer and docling - llamaparse and mathpix used their cloud services. We can also look at it by document type:&lt;/p&gt; 
&lt;img src=&quot;https://raw.githubusercontent.com/VikParuchuri/marker/master/data/images/per_doc.png&quot; width=&quot;1000px&quot;&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Document Type&lt;/th&gt; 
   &lt;th&gt;Marker heuristic&lt;/th&gt; 
   &lt;th&gt;Marker LLM&lt;/th&gt; 
   &lt;th&gt;Llamaparse Heuristic&lt;/th&gt; 
   &lt;th&gt;Llamaparse LLM&lt;/th&gt; 
   &lt;th&gt;Mathpix Heuristic&lt;/th&gt; 
   &lt;th&gt;Mathpix LLM&lt;/th&gt; 
   &lt;th&gt;Docling Heuristic&lt;/th&gt; 
   &lt;th&gt;Docling LLM&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Scientific paper&lt;/td&gt; 
   &lt;td&gt;96.6737&lt;/td&gt; 
   &lt;td&gt;4.34899&lt;/td&gt; 
   &lt;td&gt;87.1651&lt;/td&gt; 
   &lt;td&gt;3.96421&lt;/td&gt; 
   &lt;td&gt;91.2267&lt;/td&gt; 
   &lt;td&gt;4.46861&lt;/td&gt; 
   &lt;td&gt;92.135&lt;/td&gt; 
   &lt;td&gt;3.72422&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Book page&lt;/td&gt; 
   &lt;td&gt;97.1846&lt;/td&gt; 
   &lt;td&gt;4.16168&lt;/td&gt; 
   &lt;td&gt;90.9532&lt;/td&gt; 
   &lt;td&gt;4.07186&lt;/td&gt; 
   &lt;td&gt;93.8886&lt;/td&gt; 
   &lt;td&gt;4.35329&lt;/td&gt; 
   &lt;td&gt;90.0556&lt;/td&gt; 
   &lt;td&gt;3.64671&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Other&lt;/td&gt; 
   &lt;td&gt;95.1632&lt;/td&gt; 
   &lt;td&gt;4.25076&lt;/td&gt; 
   &lt;td&gt;81.1385&lt;/td&gt; 
   &lt;td&gt;4.01835&lt;/td&gt; 
   &lt;td&gt;79.6231&lt;/td&gt; 
   &lt;td&gt;4.00306&lt;/td&gt; 
   &lt;td&gt;83.8223&lt;/td&gt; 
   &lt;td&gt;3.76147&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Form&lt;/td&gt; 
   &lt;td&gt;88.0147&lt;/td&gt; 
   &lt;td&gt;3.84663&lt;/td&gt; 
   &lt;td&gt;66.3081&lt;/td&gt; 
   &lt;td&gt;3.68712&lt;/td&gt; 
   &lt;td&gt;64.7512&lt;/td&gt; 
   &lt;td&gt;3.33129&lt;/td&gt; 
   &lt;td&gt;68.3857&lt;/td&gt; 
   &lt;td&gt;3.40491&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Presentation&lt;/td&gt; 
   &lt;td&gt;95.1562&lt;/td&gt; 
   &lt;td&gt;4.13669&lt;/td&gt; 
   &lt;td&gt;81.2261&lt;/td&gt; 
   &lt;td&gt;4&lt;/td&gt; 
   &lt;td&gt;83.6737&lt;/td&gt; 
   &lt;td&gt;3.95683&lt;/td&gt; 
   &lt;td&gt;84.8405&lt;/td&gt; 
   &lt;td&gt;3.86331&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Financial document&lt;/td&gt; 
   &lt;td&gt;95.3697&lt;/td&gt; 
   &lt;td&gt;4.39106&lt;/td&gt; 
   &lt;td&gt;82.5812&lt;/td&gt; 
   &lt;td&gt;4.16111&lt;/td&gt; 
   &lt;td&gt;81.3115&lt;/td&gt; 
   &lt;td&gt;4.05556&lt;/td&gt; 
   &lt;td&gt;86.3882&lt;/td&gt; 
   &lt;td&gt;3.8&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Letter&lt;/td&gt; 
   &lt;td&gt;98.4021&lt;/td&gt; 
   &lt;td&gt;4.5&lt;/td&gt; 
   &lt;td&gt;93.4477&lt;/td&gt; 
   &lt;td&gt;4.28125&lt;/td&gt; 
   &lt;td&gt;96.0383&lt;/td&gt; 
   &lt;td&gt;4.45312&lt;/td&gt; 
   &lt;td&gt;92.0952&lt;/td&gt; 
   &lt;td&gt;4.09375&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Engineering document&lt;/td&gt; 
   &lt;td&gt;93.9244&lt;/td&gt; 
   &lt;td&gt;4.04412&lt;/td&gt; 
   &lt;td&gt;77.4854&lt;/td&gt; 
   &lt;td&gt;3.72059&lt;/td&gt; 
   &lt;td&gt;80.3319&lt;/td&gt; 
   &lt;td&gt;3.88235&lt;/td&gt; 
   &lt;td&gt;79.6807&lt;/td&gt; 
   &lt;td&gt;3.42647&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Legal document&lt;/td&gt; 
   &lt;td&gt;96.689&lt;/td&gt; 
   &lt;td&gt;4.27759&lt;/td&gt; 
   &lt;td&gt;86.9769&lt;/td&gt; 
   &lt;td&gt;3.87584&lt;/td&gt; 
   &lt;td&gt;91.601&lt;/td&gt; 
   &lt;td&gt;4.20805&lt;/td&gt; 
   &lt;td&gt;87.8383&lt;/td&gt; 
   &lt;td&gt;3.65552&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Newspaper page&lt;/td&gt; 
   &lt;td&gt;98.8733&lt;/td&gt; 
   &lt;td&gt;4.25806&lt;/td&gt; 
   &lt;td&gt;84.7492&lt;/td&gt; 
   &lt;td&gt;3.90323&lt;/td&gt; 
   &lt;td&gt;96.9963&lt;/td&gt; 
   &lt;td&gt;4.45161&lt;/td&gt; 
   &lt;td&gt;92.6496&lt;/td&gt; 
   &lt;td&gt;3.51613&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Magazine page&lt;/td&gt; 
   &lt;td&gt;98.2145&lt;/td&gt; 
   &lt;td&gt;4.38776&lt;/td&gt; 
   &lt;td&gt;87.2902&lt;/td&gt; 
   &lt;td&gt;3.97959&lt;/td&gt; 
   &lt;td&gt;93.5934&lt;/td&gt; 
   &lt;td&gt;4.16327&lt;/td&gt; 
   &lt;td&gt;93.0892&lt;/td&gt; 
   &lt;td&gt;4.02041&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Throughput&lt;/h2&gt; 
&lt;p&gt;We benchmarked throughput using a &lt;a href=&quot;https://www.greenteapress.com/thinkpython/thinkpython.pdf&quot;&gt;single long PDF&lt;/a&gt;.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Method&lt;/th&gt; 
   &lt;th&gt;Time per page&lt;/th&gt; 
   &lt;th&gt;Time per document&lt;/th&gt; 
   &lt;th&gt;VRAM used&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;marker&lt;/td&gt; 
   &lt;td&gt;0.18&lt;/td&gt; 
   &lt;td&gt;43.42&lt;/td&gt; 
   &lt;td&gt;3.17GB&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;The projected throughput is 122 pages per second on an H100 - we can run 22 individual processes given the VRAM used.&lt;/p&gt; 
&lt;h2&gt;Table Conversion&lt;/h2&gt; 
&lt;p&gt;Marker can extract tables from PDFs using &lt;code&gt;marker.converters.table.TableConverter&lt;/code&gt;. The table extraction performance is measured by comparing the extracted HTML representation of tables against the original HTML representations using the test split of &lt;a href=&quot;https://developer.ibm.com/exchanges/data/all/fintabnet/&quot;&gt;FinTabNet&lt;/a&gt;. The HTML representations are compared using a tree edit distance based metric to judge both structure and content. Marker detects and identifies the structure of all tables in a PDF page and achieves these scores:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Method&lt;/th&gt; 
   &lt;th&gt;Avg score&lt;/th&gt; 
   &lt;th&gt;Total tables&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;marker&lt;/td&gt; 
   &lt;td&gt;0.816&lt;/td&gt; 
   &lt;td&gt;99&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;marker w/use_llm&lt;/td&gt; 
   &lt;td&gt;0.907&lt;/td&gt; 
   &lt;td&gt;99&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;gemini&lt;/td&gt; 
   &lt;td&gt;0.829&lt;/td&gt; 
   &lt;td&gt;99&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;The &lt;code&gt;--use_llm&lt;/code&gt; flag can significantly improve table recognition performance, as you can see.&lt;/p&gt; 
&lt;p&gt;We filter out tables that we cannot align with the ground truth, since fintabnet and our layout model have slightly different detection methods (this results in some tables being split/merged).&lt;/p&gt; 
&lt;h2&gt;Running your own benchmarks&lt;/h2&gt; 
&lt;p&gt;You can benchmark the performance of marker on your machine. Install marker manually with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;git clone https://github.com/VikParuchuri/marker.git
poetry install
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Overall PDF Conversion&lt;/h3&gt; 
&lt;p&gt;Download the benchmark data &lt;a href=&quot;https://drive.google.com/file/d/1ZSeWDo2g1y0BRLT7KnbmytV2bjWARWba/view?usp=sharing&quot;&gt;here&lt;/a&gt; and unzip. Then run the overall benchmark like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;python benchmarks/overall.py --methods marker --scores heuristic,llm
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Options:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--use_llm&lt;/code&gt; use an llm to improve the marker results.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--max_rows&lt;/code&gt; how many rows to process for the benchmark.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--methods&lt;/code&gt; can be &lt;code&gt;llamaparse&lt;/code&gt;, &lt;code&gt;mathpix&lt;/code&gt;, &lt;code&gt;docling&lt;/code&gt;, &lt;code&gt;marker&lt;/code&gt;. Comma separated.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--scores&lt;/code&gt; which scoring functions to use, can be &lt;code&gt;llm&lt;/code&gt;, &lt;code&gt;heuristic&lt;/code&gt;. Comma separated.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Table Conversion&lt;/h3&gt; 
&lt;p&gt;The processed FinTabNet dataset is hosted &lt;a href=&quot;https://huggingface.co/datasets/datalab-to/fintabnet-test&quot;&gt;here&lt;/a&gt; and is automatically downloaded. Run the benchmark with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;python benchmarks/table/table.py --max_rows 100
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Options:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--use_llm&lt;/code&gt; uses an llm with marker to improve accuracy.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--use_gemini&lt;/code&gt; also benchmarks gemini 2.0 flash.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;How it works&lt;/h1&gt; 
&lt;p&gt;Marker is a pipeline of deep learning models:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Extract text, OCR if necessary (heuristics, &lt;a href=&quot;https://github.com/VikParuchuri/surya&quot;&gt;surya&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;Detect page layout and find reading order (&lt;a href=&quot;https://github.com/VikParuchuri/surya&quot;&gt;surya&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;Clean and format each block (heuristics, &lt;a href=&quot;https://github.com/VikParuchuri/texify&quot;&gt;texify&lt;/a&gt;, &lt;a href=&quot;https://github.com/VikParuchuri/surya&quot;&gt;surya&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;Optionally use an LLM to improve quality&lt;/li&gt; 
 &lt;li&gt;Combine blocks and postprocess complete text&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;It only uses models where necessary, which improves speed and accuracy.&lt;/p&gt; 
&lt;h1&gt;Limitations&lt;/h1&gt; 
&lt;p&gt;PDF is a tricky format, so marker will not always work perfectly. Here are some known limitations that are on the roadmap to address:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Very complex layouts, with nested tables and forms, may not work&lt;/li&gt; 
 &lt;li&gt;Forms may not be rendered well&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Note: Passing the &lt;code&gt;--use_llm&lt;/code&gt; flag will mostly solve these issues.&lt;/p&gt; 
&lt;h1&gt;Thanks&lt;/h1&gt; 
&lt;p&gt;This work would not have been possible without amazing open source models and datasets, including (but not limited to):&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Surya&lt;/li&gt; 
 &lt;li&gt;Texify&lt;/li&gt; 
 &lt;li&gt;Pypdfium2/pdfium&lt;/li&gt; 
 &lt;li&gt;DocLayNet from IBM&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Thank you to the authors of these models and datasets for making them available to the community!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>harry0703/MoneyPrinterTurbo</title>
      <link>https://github.com/harry0703/MoneyPrinterTurbo</link>
      <description>&lt;p&gt;利用AI大模型，一键生成高清短视频 Generate short videos with one click using AI LLM.&lt;/p&gt;&lt;hr&gt;&lt;div align=&quot;center&quot;&gt; 
 &lt;h1 align=&quot;center&quot;&gt;MoneyPrinterTurbo 💸&lt;/h1&gt; 
 &lt;p align=&quot;center&quot;&gt; &lt;a href=&quot;https://github.com/harry0703/MoneyPrinterTurbo/stargazers&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/harry0703/MoneyPrinterTurbo.svg?style=for-the-badge&quot; alt=&quot;Stargazers&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/harry0703/MoneyPrinterTurbo/issues&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/issues/harry0703/MoneyPrinterTurbo.svg?style=for-the-badge&quot; alt=&quot;Issues&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/harry0703/MoneyPrinterTurbo/network/members&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/forks/harry0703/MoneyPrinterTurbo.svg?style=for-the-badge&quot; alt=&quot;Forks&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/harry0703/MoneyPrinterTurbo/raw/main/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/license/harry0703/MoneyPrinterTurbo.svg?style=for-the-badge&quot; alt=&quot;License&quot;&gt;&lt;/a&gt; &lt;/p&gt; 
 &lt;br&gt; 
 &lt;h3&gt;简体中文 | &lt;a href=&quot;https://raw.githubusercontent.com/harry0703/MoneyPrinterTurbo/main/README-en.md&quot;&gt;English&lt;/a&gt;&lt;/h3&gt; 
 &lt;div align=&quot;center&quot;&gt; 
  &lt;a href=&quot;https://trendshift.io/repositories/8731&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/8731&quot; alt=&quot;harry0703%2FMoneyPrinterTurbo | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;&gt;&lt;/a&gt; 
 &lt;/div&gt; 
 &lt;br&gt; 只需提供一个视频 
 &lt;b&gt;主题&lt;/b&gt; 或 
 &lt;b&gt;关键词&lt;/b&gt; ，就可以全自动生成视频文案、视频素材、视频字幕、视频背景音乐，然后合成一个高清的短视频。 
 &lt;br&gt; 
 &lt;h4&gt;Web界面&lt;/h4&gt; 
 &lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/harry0703/MoneyPrinterTurbo/main/docs/webui.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt; 
 &lt;h4&gt;API界面&lt;/h4&gt; 
 &lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/harry0703/MoneyPrinterTurbo/main/docs/api.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;特别感谢 🙏&lt;/h2&gt; 
&lt;p&gt;由于该项目的 &lt;strong&gt;部署&lt;/strong&gt; 和 &lt;strong&gt;使用&lt;/strong&gt;，对于一些小白用户来说，还是 &lt;strong&gt;有一定的门槛&lt;/strong&gt;，在此特别感谢 &lt;strong&gt;录咖（AI智能 多媒体服务平台）&lt;/strong&gt; 网站基于该项目，提供的免费&lt;code&gt;AI视频生成器&lt;/code&gt;服务，可以不用部署，直接在线使用，非常方便。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;中文版：&lt;a href=&quot;https://reccloud.cn&quot;&gt;https://reccloud.cn&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;英文版：&lt;a href=&quot;https://reccloud.com&quot;&gt;https://reccloud.com&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/harry0703/MoneyPrinterTurbo/main/docs/reccloud.cn.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt; 
&lt;h2&gt;感谢赞助 🙏&lt;/h2&gt; 
&lt;p&gt;感谢佐糖 &lt;a href=&quot;https://picwish.cn&quot;&gt;https://picwish.cn&lt;/a&gt; 对该项目的支持和赞助，使得该项目能够持续的更新和维护。&lt;/p&gt; 
&lt;p&gt;佐糖专注于&lt;strong&gt;图像处理领域&lt;/strong&gt;，提供丰富的&lt;strong&gt;图像处理工具&lt;/strong&gt;，将复杂操作极致简化，真正实现让图像处理更简单。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/harry0703/MoneyPrinterTurbo/main/docs/picwish.jpg&quot; alt=&quot;picwish.jpg&quot;&gt;&lt;/p&gt; 
&lt;h2&gt;功能特性 🎯&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; 完整的 &lt;strong&gt;MVC架构&lt;/strong&gt;，代码 &lt;strong&gt;结构清晰&lt;/strong&gt;，易于维护，支持 &lt;code&gt;API&lt;/code&gt; 和 &lt;code&gt;Web界面&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; 支持视频文案 &lt;strong&gt;AI自动生成&lt;/strong&gt;，也可以&lt;strong&gt;自定义文案&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; 支持多种 &lt;strong&gt;高清视频&lt;/strong&gt; 尺寸 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; 竖屏 9:16，&lt;code&gt;1080x1920&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; 横屏 16:9，&lt;code&gt;1920x1080&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; 支持 &lt;strong&gt;批量视频生成&lt;/strong&gt;，可以一次生成多个视频，然后选择一个最满意的&lt;/li&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; 支持 &lt;strong&gt;视频片段时长&lt;/strong&gt; 设置，方便调节素材切换频率&lt;/li&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; 支持 &lt;strong&gt;中文&lt;/strong&gt; 和 &lt;strong&gt;英文&lt;/strong&gt; 视频文案&lt;/li&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; 支持 &lt;strong&gt;多种语音&lt;/strong&gt; 合成，可 &lt;strong&gt;实时试听&lt;/strong&gt; 效果&lt;/li&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; 支持 &lt;strong&gt;字幕生成&lt;/strong&gt;，可以调整 &lt;code&gt;字体&lt;/code&gt;、&lt;code&gt;位置&lt;/code&gt;、&lt;code&gt;颜色&lt;/code&gt;、&lt;code&gt;大小&lt;/code&gt;，同时支持&lt;code&gt;字幕描边&lt;/code&gt;设置&lt;/li&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; 支持 &lt;strong&gt;背景音乐&lt;/strong&gt;，随机或者指定音乐文件，可设置&lt;code&gt;背景音乐音量&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; 视频素材来源 &lt;strong&gt;高清&lt;/strong&gt;，而且 &lt;strong&gt;无版权&lt;/strong&gt;，也可以使用自己的 &lt;strong&gt;本地素材&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; 支持 &lt;strong&gt;OpenAI&lt;/strong&gt;、&lt;strong&gt;Moonshot&lt;/strong&gt;、&lt;strong&gt;Azure&lt;/strong&gt;、&lt;strong&gt;gpt4free&lt;/strong&gt;、&lt;strong&gt;one-api&lt;/strong&gt;、&lt;strong&gt;通义千问&lt;/strong&gt;、&lt;strong&gt;Google Gemini&lt;/strong&gt;、&lt;strong&gt;Ollama&lt;/strong&gt;、 &lt;strong&gt;DeepSeek&lt;/strong&gt;、 &lt;strong&gt;文心一言&lt;/strong&gt; 等多种模型接入 
  &lt;ul&gt; 
   &lt;li&gt;中国用户建议使用 &lt;strong&gt;DeepSeek&lt;/strong&gt; 或 &lt;strong&gt;Moonshot&lt;/strong&gt; 作为大模型提供商（国内可直接访问，不需要VPN。注册就送额度，基本够用）&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;后期计划 📅&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; disabled&gt; GPT-SoVITS 配音支持&lt;/li&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; disabled&gt; 优化语音合成，利用大模型，使其合成的声音，更加自然，情绪更加丰富&lt;/li&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; disabled&gt; 增加视频转场效果，使其看起来更加的流畅&lt;/li&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; disabled&gt; 增加更多视频素材来源，优化视频素材和文案的匹配度&lt;/li&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; disabled&gt; 增加视频长度选项：短、中、长&lt;/li&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; disabled&gt; 支持更多的语音合成服务商，比如 OpenAI TTS&lt;/li&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; disabled&gt; 自动上传到YouTube平台&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;视频演示 📺&lt;/h2&gt; 
&lt;h3&gt;竖屏 9:16&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align=&quot;center&quot;&gt;
    &lt;g-emoji class=&quot;g-emoji&quot; alias=&quot;arrow_forward&quot;&gt;
     ▶️
    &lt;/g-emoji&gt; 《如何增加生活的乐趣》&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;
    &lt;g-emoji class=&quot;g-emoji&quot; alias=&quot;arrow_forward&quot;&gt;
     ▶️
    &lt;/g-emoji&gt; 《金钱的作用》&lt;br&gt;更真实的合成声音&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;
    &lt;g-emoji class=&quot;g-emoji&quot; alias=&quot;arrow_forward&quot;&gt;
     ▶️
    &lt;/g-emoji&gt; 《生命的意义是什么》&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;
    &lt;video src=&quot;https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/a84d33d5-27a2-4aba-8fd0-9fb2bd91c6a6&quot;&gt;&lt;/video&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;
    &lt;video src=&quot;https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/af2f3b0b-002e-49fe-b161-18ba91c055e8&quot;&gt;&lt;/video&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;
    &lt;video src=&quot;https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/112c9564-d52b-4472-99ad-970b75f66476&quot;&gt;&lt;/video&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;横屏 16:9&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align=&quot;center&quot;&gt;
    &lt;g-emoji class=&quot;g-emoji&quot; alias=&quot;arrow_forward&quot;&gt;
     ▶️
    &lt;/g-emoji&gt;《生命的意义是什么》&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;
    &lt;g-emoji class=&quot;g-emoji&quot; alias=&quot;arrow_forward&quot;&gt;
     ▶️
    &lt;/g-emoji&gt;《为什么要运动》&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;
    &lt;video src=&quot;https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/346ebb15-c55f-47a9-a653-114f08bb8073&quot;&gt;&lt;/video&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;
    &lt;video src=&quot;https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/271f2fae-8283-44a0-8aa0-0ed8f9a6fa87&quot;&gt;&lt;/video&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;配置要求 📦&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;建议最低 CPU 4核或以上，内存 8G 或以上，显卡非必须&lt;/li&gt; 
 &lt;li&gt;Windows 10 或 MacOS 11.0 以上系统&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;快速开始 🚀&lt;/h2&gt; 
&lt;p&gt;下载一键启动包，解压直接使用（路径不要有 &lt;strong&gt;中文&lt;/strong&gt;、&lt;strong&gt;特殊字符&lt;/strong&gt;、&lt;strong&gt;空格&lt;/strong&gt;）&lt;/p&gt; 
&lt;h3&gt;Windows&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;百度网盘（1.2.1 最新版本）: &lt;a href=&quot;https://pan.baidu.com/s/1pSNjxTYiVENulTLm6zieMQ?pwd=g36q&quot;&gt;https://pan.baidu.com/s/1pSNjxTYiVENulTLm6zieMQ?pwd=g36q&lt;/a&gt; 提取码: g36q&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;下载后，建议先&lt;strong&gt;双击执行&lt;/strong&gt; &lt;code&gt;update.bat&lt;/code&gt; 更新到&lt;strong&gt;最新代码&lt;/strong&gt;，然后双击 &lt;code&gt;start.bat&lt;/code&gt; 启动&lt;/p&gt; 
&lt;p&gt;启动后，会自动打开浏览器（如果打开是空白，建议换成 &lt;strong&gt;Chrome&lt;/strong&gt; 或者 &lt;strong&gt;Edge&lt;/strong&gt; 打开）&lt;/p&gt; 
&lt;h3&gt;其他系统&lt;/h3&gt; 
&lt;p&gt;还没有制作一键启动包，看下面的 &lt;strong&gt;安装部署&lt;/strong&gt; 部分，建议使用 &lt;strong&gt;docker&lt;/strong&gt; 部署，更加方便。&lt;/p&gt; 
&lt;h2&gt;安装部署 📥&lt;/h2&gt; 
&lt;h3&gt;前提条件&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;尽量不要使用 &lt;strong&gt;中文路径&lt;/strong&gt;，避免出现一些无法预料的问题&lt;/li&gt; 
 &lt;li&gt;请确保你的 &lt;strong&gt;网络&lt;/strong&gt; 是正常的，VPN需要打开&lt;code&gt;全局流量&lt;/code&gt;模式&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;① 克隆代码&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;git clone https://github.com/harry0703/MoneyPrinterTurbo.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;② 修改配置文件&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;将 &lt;code&gt;config.example.toml&lt;/code&gt; 文件复制一份，命名为 &lt;code&gt;config.toml&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;按照 &lt;code&gt;config.toml&lt;/code&gt; 文件中的说明，配置好 &lt;code&gt;pexels_api_keys&lt;/code&gt; 和 &lt;code&gt;llm_provider&lt;/code&gt;，并根据 llm_provider 对应的服务商，配置相关的 API Key&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Docker部署 🐳&lt;/h3&gt; 
&lt;h4&gt;① 启动Docker&lt;/h4&gt; 
&lt;p&gt;如果未安装 Docker，请先安装 &lt;a href=&quot;https://www.docker.com/products/docker-desktop/&quot;&gt;https://www.docker.com/products/docker-desktop/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;如果是Windows系统，请参考微软的文档：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href=&quot;https://learn.microsoft.com/zh-cn/windows/wsl/install&quot;&gt;https://learn.microsoft.com/zh-cn/windows/wsl/install&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://learn.microsoft.com/zh-cn/windows/wsl/tutorials/wsl-containers&quot;&gt;https://learn.microsoft.com/zh-cn/windows/wsl/tutorials/wsl-containers&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;cd MoneyPrinterTurbo
docker-compose up
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;注意：最新版的docker安装时会自动以插件的形式安装docker compose，启动命令调整为docker compose up&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;② 访问Web界面&lt;/h4&gt; 
&lt;p&gt;打开浏览器，访问 &lt;a href=&quot;http://0.0.0.0:8501&quot;&gt;http://0.0.0.0:8501&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;③ 访问API文档&lt;/h4&gt; 
&lt;p&gt;打开浏览器，访问 &lt;a href=&quot;http://0.0.0.0:8080/docs&quot;&gt;http://0.0.0.0:8080/docs&lt;/a&gt; 或者 &lt;a href=&quot;http://0.0.0.0:8080/redoc&quot;&gt;http://0.0.0.0:8080/redoc&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;手动部署 📦&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;视频教程&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;完整的使用演示：&lt;a href=&quot;https://v.douyin.com/iFhnwsKY/&quot;&gt;https://v.douyin.com/iFhnwsKY/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;如何在Windows上部署：&lt;a href=&quot;https://v.douyin.com/iFyjoW3M&quot;&gt;https://v.douyin.com/iFyjoW3M&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;① 创建虚拟环境&lt;/h4&gt; 
&lt;p&gt;建议使用 &lt;a href=&quot;https://conda.io/projects/conda/en/latest/user-guide/install/index.html&quot;&gt;conda&lt;/a&gt; 创建 python 虚拟环境&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;git clone https://github.com/harry0703/MoneyPrinterTurbo.git
cd MoneyPrinterTurbo
conda create -n MoneyPrinterTurbo python=3.11
conda activate MoneyPrinterTurbo
pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;② 安装好 ImageMagick&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Windows:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;下载 &lt;a href=&quot;https://imagemagick.org/script/download.php&quot;&gt;https://imagemagick.org/script/download.php&lt;/a&gt; 选择Windows版本，切记一定要选择 &lt;strong&gt;静态库&lt;/strong&gt; 版本，比如 ImageMagick-7.1.1-32-Q16-x64-&lt;strong&gt;static&lt;/strong&gt;.exe&lt;/li&gt; 
   &lt;li&gt;安装下载好的 ImageMagick，&lt;strong&gt;注意不要修改安装路径&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;修改 &lt;code&gt;配置文件 config.toml&lt;/code&gt; 中的 &lt;code&gt;imagemagick_path&lt;/code&gt; 为你的 &lt;strong&gt;实际安装路径&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;MacOS:&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;brew install imagemagick
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Ubuntu&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;sudo apt-get install imagemagick
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;CentOS&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;sudo yum install ImageMagick
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;③ 启动Web界面 🌐&lt;/h4&gt; 
&lt;p&gt;注意需要到 MoneyPrinterTurbo 项目 &lt;code&gt;根目录&lt;/code&gt; 下执行以下命令&lt;/p&gt; 
&lt;h6&gt;Windows&lt;/h6&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bat&quot;&gt;conda activate MoneyPrinterTurbo
webui.bat
&lt;/code&gt;&lt;/pre&gt; 
&lt;h6&gt;MacOS or Linux&lt;/h6&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;conda activate MoneyPrinterTurbo
sh webui.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;启动后，会自动打开浏览器（如果打开是空白，建议换成 &lt;strong&gt;Chrome&lt;/strong&gt; 或者 &lt;strong&gt;Edge&lt;/strong&gt; 打开）&lt;/p&gt; 
&lt;h4&gt;④ 启动API服务 🚀&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;python main.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;启动后，可以查看 &lt;code&gt;API文档&lt;/code&gt; &lt;a href=&quot;http://127.0.0.1:8080/docs&quot;&gt;http://127.0.0.1:8080/docs&lt;/a&gt; 或者 &lt;a href=&quot;http://127.0.0.1:8080/redoc&quot;&gt;http://127.0.0.1:8080/redoc&lt;/a&gt; 直接在线调试接口，快速体验。&lt;/p&gt; 
&lt;h2&gt;语音合成 🗣&lt;/h2&gt; 
&lt;p&gt;所有支持的声音列表，可以查看：&lt;a href=&quot;https://raw.githubusercontent.com/harry0703/MoneyPrinterTurbo/main/docs/voice-list.txt&quot;&gt;声音列表&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;2024-04-16 v1.1.2 新增了9种Azure的语音合成声音，需要配置API KEY，该声音合成的更加真实。&lt;/p&gt; 
&lt;h2&gt;字幕生成 📜&lt;/h2&gt; 
&lt;p&gt;当前支持2种字幕生成方式：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;edge&lt;/strong&gt;: 生成&lt;code&gt;速度快&lt;/code&gt;，性能更好，对电脑配置没有要求，但是质量可能不稳定&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;whisper&lt;/strong&gt;: 生成&lt;code&gt;速度慢&lt;/code&gt;，性能较差，对电脑配置有一定要求，但是&lt;code&gt;质量更可靠&lt;/code&gt;。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;可以修改 &lt;code&gt;config.toml&lt;/code&gt; 配置文件中的 &lt;code&gt;subtitle_provider&lt;/code&gt; 进行切换&lt;/p&gt; 
&lt;p&gt;建议使用 &lt;code&gt;edge&lt;/code&gt; 模式，如果生成的字幕质量不好，再切换到 &lt;code&gt;whisper&lt;/code&gt; 模式&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;注意：&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ol&gt; 
 &lt;li&gt;whisper 模式下需要到 HuggingFace 下载一个模型文件，大约 3GB 左右，请确保网络通畅&lt;/li&gt; 
 &lt;li&gt;如果留空，表示不生成字幕。&lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;由于国内无法访问 HuggingFace，可以使用以下方法下载 &lt;code&gt;whisper-large-v3&lt;/code&gt; 的模型文件&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;下载地址：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;百度网盘: &lt;a href=&quot;https://pan.baidu.com/s/11h3Q6tsDtjQKTjUu3sc5cA?pwd=xjs9&quot;&gt;https://pan.baidu.com/s/11h3Q6tsDtjQKTjUu3sc5cA?pwd=xjs9&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;夸克网盘：&lt;a href=&quot;https://pan.quark.cn/s/3ee3d991d64b&quot;&gt;https://pan.quark.cn/s/3ee3d991d64b&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;模型下载后解压，整个目录放到 &lt;code&gt;.\MoneyPrinterTurbo\models&lt;/code&gt; 里面， 最终的文件路径应该是这样: &lt;code&gt;.\MoneyPrinterTurbo\models\whisper-large-v3&lt;/code&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;MoneyPrinterTurbo  
  ├─models
  │   └─whisper-large-v3
  │          config.json
  │          model.bin
  │          preprocessor_config.json
  │          tokenizer.json
  │          vocabulary.json
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;背景音乐 🎵&lt;/h2&gt; 
&lt;p&gt;用于视频的背景音乐，位于项目的 &lt;code&gt;resource/songs&lt;/code&gt; 目录下。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;当前项目里面放了一些默认的音乐，来自于 YouTube 视频，如有侵权，请删除。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;字幕字体 🅰&lt;/h2&gt; 
&lt;p&gt;用于视频字幕的渲染，位于项目的 &lt;code&gt;resource/fonts&lt;/code&gt; 目录下，你也可以放进去自己的字体。&lt;/p&gt; 
&lt;h2&gt;常见问题 🤔&lt;/h2&gt; 
&lt;h3&gt;❓如何使用免费的OpenAI GPT-3.5模型?&lt;/h3&gt; 
&lt;p&gt;&lt;a href=&quot;https://openai.com/blog/start-using-chatgpt-instantly&quot;&gt;OpenAI宣布ChatGPT里面3.5已经免费了&lt;/a&gt;，有开发者将其封装成了API，可以直接调用&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;确保你安装和启动了docker服务&lt;/strong&gt;，执行以下命令启动docker服务&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;docker run -p 3040:3040 missuo/freegpt35
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;启动成功后，修改 &lt;code&gt;config.toml&lt;/code&gt; 中的配置&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;llm_provider&lt;/code&gt; 设置为 &lt;code&gt;openai&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;openai_api_key&lt;/code&gt; 随便填写一个即可，比如 &#39;123456&#39;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;openai_base_url&lt;/code&gt; 改为 &lt;code&gt;http://localhost:3040/v1/&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;openai_model_name&lt;/code&gt; 改为 &lt;code&gt;gpt-3.5-turbo&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;注意：该方式稳定性较差&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;❓AttributeError: &#39;str&#39; object has no attribute &#39;choices&#39;`&lt;/h3&gt; 
&lt;p&gt;这个问题是由于大模型没有返回正确的回复导致的。&lt;/p&gt; 
&lt;p&gt;大概率是网络原因， 使用 &lt;strong&gt;VPN&lt;/strong&gt;，或者设置 &lt;code&gt;openai_base_url&lt;/code&gt; 为你的代理 ，应该就可以解决了。&lt;/p&gt; 
&lt;p&gt;同时建议使用 &lt;strong&gt;Moonshot&lt;/strong&gt; 或 &lt;strong&gt;DeepSeek&lt;/strong&gt; 作为大模型提供商，这两个服务商在国内访问速度更快，更加稳定。&lt;/p&gt; 
&lt;h3&gt;❓RuntimeError: No ffmpeg exe could be found&lt;/h3&gt; 
&lt;p&gt;通常情况下，ffmpeg 会被自动下载，并且会被自动检测到。 但是如果你的环境有问题，无法自动下载，可能会遇到如下错误：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;RuntimeError: No ffmpeg exe could be found.
Install ffmpeg on your system, or set the IMAGEIO_FFMPEG_EXE environment variable.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;此时你可以从 &lt;a href=&quot;https://www.gyan.dev/ffmpeg/builds/&quot;&gt;https://www.gyan.dev/ffmpeg/builds/&lt;/a&gt; 下载ffmpeg，解压后，设置 &lt;code&gt;ffmpeg_path&lt;/code&gt; 为你的实际安装路径即可。&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-toml&quot;&gt;[app]
# 请根据你的实际路径设置，注意 Windows 路径分隔符为 \\
ffmpeg_path = &quot;C:\\Users\\harry\\Downloads\\ffmpeg.exe&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;❓ImageMagick的安全策略阻止了与临时文件@/tmp/tmpur5hyyto.txt相关的操作&lt;/h3&gt; 
&lt;p&gt;可以在ImageMagick的配置文件policy.xml中找到这些策略。 这个文件通常位于 /etc/ImageMagick-&lt;code&gt;X&lt;/code&gt;/ 或 ImageMagick 安装目录的类似位置。 修改包含&lt;code&gt;pattern=&quot;@&quot;&lt;/code&gt;的条目，将&lt;code&gt;rights=&quot;none&quot;&lt;/code&gt;更改为&lt;code&gt;rights=&quot;read|write&quot;&lt;/code&gt;以允许对文件的读写操作。&lt;/p&gt; 
&lt;h3&gt;❓OSError: [Errno 24] Too many open files&lt;/h3&gt; 
&lt;p&gt;这个问题是由于系统打开文件数限制导致的，可以通过修改系统的文件打开数限制来解决。&lt;/p&gt; 
&lt;p&gt;查看当前限制&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;ulimit -n
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;如果过低，可以调高一些，比如&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;ulimit -n 10240
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;❓Whisper 模型下载失败，出现如下错误&lt;/h3&gt; 
&lt;p&gt;LocalEntryNotfoundEror: Cannot find an appropriate cached snapshotfolderfor the specified revision on the local disk and outgoing trafic has been disabled. To enablerepo look-ups and downloads online, pass &#39;local files only=False&#39; as input.&lt;/p&gt; 
&lt;p&gt;或者&lt;/p&gt; 
&lt;p&gt;An error occured while synchronizing the model Systran/faster-whisper-large-v3 from the Hugging Face Hub: An error happened while trying to locate the files on the Hub and we cannot find the appropriate snapshot folder for the specified revision on the local disk. Please check your internet connection and try again. Trying to load the model directly from the local cache, if it exists.&lt;/p&gt; 
&lt;p&gt;解决方法：&lt;a href=&quot;https://raw.githubusercontent.com/harry0703/MoneyPrinterTurbo/main/#%E5%AD%97%E5%B9%95%E7%94%9F%E6%88%90-&quot;&gt;点击查看如何从网盘手动下载模型&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;反馈建议 📢&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;可以提交 &lt;a href=&quot;https://github.com/harry0703/MoneyPrinterTurbo/issues&quot;&gt;issue&lt;/a&gt; 或者 &lt;a href=&quot;https://github.com/harry0703/MoneyPrinterTurbo/pulls&quot;&gt;pull request&lt;/a&gt;。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;参考项目 📚&lt;/h2&gt; 
&lt;p&gt;该项目基于 &lt;a href=&quot;https://github.com/FujiwaraChoki/MoneyPrinter&quot;&gt;https://github.com/FujiwaraChoki/MoneyPrinter&lt;/a&gt; 重构而来，做了大量的优化，增加了更多的功能。 感谢原作者的开源精神。&lt;/p&gt; 
&lt;h2&gt;许可证 📝&lt;/h2&gt; 
&lt;p&gt;点击查看 &lt;a href=&quot;https://raw.githubusercontent.com/harry0703/MoneyPrinterTurbo/main/LICENSE&quot;&gt;&lt;code&gt;LICENSE&lt;/code&gt;&lt;/a&gt; 文件&lt;/p&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://star-history.com/#harry0703/MoneyPrinterTurbo&amp;amp;Date&quot;&gt;&lt;img src=&quot;https://api.star-history.com/svg?repos=harry0703/MoneyPrinterTurbo&amp;amp;type=Date&quot; alt=&quot;Star History Chart&quot;&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>vllm-project/vllm</title>
      <link>https://github.com/vllm-project/vllm</link>
      <description>&lt;p&gt;A high-throughput and memory-efficient inference and serving engine for LLMs&lt;/p&gt;&lt;hr&gt;&lt;p align=&quot;center&quot;&gt; 
 &lt;picture&gt; 
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://raw.githubusercontent.com/vllm-project/vllm/main/docs/source/assets/logos/vllm-logo-text-dark.png&quot;&gt; 
  &lt;img alt=&quot;vLLM&quot; src=&quot;https://raw.githubusercontent.com/vllm-project/vllm/main/docs/source/assets/logos/vllm-logo-text-light.png&quot; width=&quot;55%&quot;&gt; 
 &lt;/picture&gt; &lt;/p&gt; 
&lt;h3 align=&quot;center&quot;&gt; Easy, fast, and cheap LLM serving for everyone &lt;/h3&gt; 
&lt;p align=&quot;center&quot;&gt; | &lt;a href=&quot;https://docs.vllm.ai&quot;&gt;&lt;b&gt;Documentation&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;https://vllm.ai&quot;&gt;&lt;b&gt;Blog&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;https://arxiv.org/abs/2309.06180&quot;&gt;&lt;b&gt;Paper&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;https://x.com/vllm_project&quot;&gt;&lt;b&gt;Twitter/X&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;https://slack.vllm.ai&quot;&gt;&lt;b&gt;Developer Slack&lt;/b&gt;&lt;/a&gt; | &lt;/p&gt; 
&lt;hr&gt; 
&lt;p&gt;We’re excited to invite you to the first &lt;strong&gt;vLLM China Meetup&lt;/strong&gt; on &lt;strong&gt;March 16&lt;/strong&gt; in &lt;strong&gt;Beijing&lt;/strong&gt;!&lt;/p&gt; 
&lt;p&gt;Join us to connect with the &lt;strong&gt;vLLM team&lt;/strong&gt; and explore how vLLM is leveraged in &lt;strong&gt;post-training, fine-tuning, and deployment&lt;/strong&gt;, including &lt;a href=&quot;https://github.com/volcengine/verl&quot;&gt;verl&lt;/a&gt;, &lt;a href=&quot;https://github.com/hiyouga/LLaMA-Factory&quot;&gt;LLaMA-Factory&lt;/a&gt;, and &lt;a href=&quot;https://github.com/vllm-project/vllm-ascend&quot;&gt;vllm-ascend&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;👉 &lt;strong&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s/n77GibL2corAtQHtVEAzfg&quot;&gt;Register Now&lt;/a&gt;&lt;/strong&gt; to be part of the discussion!&lt;/p&gt; 
&lt;hr&gt; 
&lt;p&gt;&lt;em&gt;Latest News&lt;/em&gt; 🔥&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;[2025/03] We hosted &lt;a href=&quot;https://lu.ma/7mu4k4xx&quot;&gt;the East Coast vLLM Meetup&lt;/a&gt;! Please find the meetup slides &lt;a href=&quot;https://docs.google.com/presentation/d/1NHiv8EUFF1NLd3fEYODm56nDmL26lEeXCaDgyDlTsRs/edit#slide=id.g31441846c39_0_0&quot;&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;[2025/02] We hosted &lt;a href=&quot;https://lu.ma/h7g3kuj9&quot;&gt;the ninth vLLM meetup&lt;/a&gt; with Meta! Please find the meetup slides from vLLM team &lt;a href=&quot;https://docs.google.com/presentation/d/1jzC_PZVXrVNSFVCW-V4cFXb6pn7zZ2CyP_Flwo05aqg/edit?usp=sharing&quot;&gt;here&lt;/a&gt; and AMD &lt;a href=&quot;https://drive.google.com/file/d/1Zk5qEJIkTmlQ2eQcXQZlljAx3m9s7nwn/view?usp=sharing&quot;&gt;here&lt;/a&gt;. The slides from Meta will not be posted.&lt;/li&gt; 
 &lt;li&gt;[2025/01] We are excited to announce the alpha release of vLLM V1: A major architectural upgrade with 1.7x speedup! Clean code, optimized execution loop, zero-overhead prefix caching, enhanced multimodal support, and more. Please check out our blog post &lt;a href=&quot;https://blog.vllm.ai/2025/01/27/v1-alpha-release.html&quot;&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;[2025/01] We hosted &lt;a href=&quot;https://lu.ma/zep56hui&quot;&gt;the eighth vLLM meetup&lt;/a&gt; with Google Cloud! Please find the meetup slides from vLLM team &lt;a href=&quot;https://docs.google.com/presentation/d/1epVkt4Zu8Jz_S5OhEHPc798emsYh2BwYfRuDDVEF7u4/edit?usp=sharing&quot;&gt;here&lt;/a&gt;, and Google Cloud team &lt;a href=&quot;https://drive.google.com/file/d/1h24pHewANyRL11xy5dXUbvRC9F9Kkjix/view?usp=sharing&quot;&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;[2024/12] vLLM joins &lt;a href=&quot;https://pytorch.org/blog/vllm-joins-pytorch&quot;&gt;pytorch ecosystem&lt;/a&gt;! Easy, Fast, and Cheap LLM Serving for Everyone!&lt;/li&gt; 
 &lt;li&gt;[2024/11] We hosted &lt;a href=&quot;https://lu.ma/h0qvrajz&quot;&gt;the seventh vLLM meetup&lt;/a&gt; with Snowflake! Please find the meetup slides from vLLM team &lt;a href=&quot;https://docs.google.com/presentation/d/1e3CxQBV3JsfGp30SwyvS3eM_tW-ghOhJ9PAJGK6KR54/edit?usp=sharing&quot;&gt;here&lt;/a&gt;, and Snowflake team &lt;a href=&quot;https://docs.google.com/presentation/d/1qF3RkDAbOULwz9WK5TOltt2fE9t6uIc_hVNLFAaQX6A/edit?usp=sharing&quot;&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;[2024/10] We have just created a developer slack (&lt;a href=&quot;https://slack.vllm.ai&quot;&gt;slack.vllm.ai&lt;/a&gt;) focusing on coordinating contributions and discussing features. Please feel free to join us there!&lt;/li&gt; 
 &lt;li&gt;[2024/10] Ray Summit 2024 held a special track for vLLM! Please find the opening talk slides from the vLLM team &lt;a href=&quot;https://docs.google.com/presentation/d/1B_KQxpHBTRa_mDF-tR6i8rWdOU5QoTZNcEg2MKZxEHM/edit?usp=sharing&quot;&gt;here&lt;/a&gt;. Learn more from the &lt;a href=&quot;https://www.youtube.com/playlist?list=PLzTswPQNepXl6AQwifuwUImLPFRVpksjR&quot;&gt;talks&lt;/a&gt; from other vLLM contributors and users!&lt;/li&gt; 
 &lt;li&gt;[2024/09] We hosted &lt;a href=&quot;https://lu.ma/87q3nvnh&quot;&gt;the sixth vLLM meetup&lt;/a&gt; with NVIDIA! Please find the meetup slides &lt;a href=&quot;https://docs.google.com/presentation/d/1wrLGwytQfaOTd5wCGSPNhoaW3nq0E-9wqyP7ny93xRs/edit?usp=sharing&quot;&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;[2024/07] We hosted &lt;a href=&quot;https://lu.ma/lp0gyjqr&quot;&gt;the fifth vLLM meetup&lt;/a&gt; with AWS! Please find the meetup slides &lt;a href=&quot;https://docs.google.com/presentation/d/1RgUD8aCfcHocghoP3zmXzck9vX3RCI9yfUAB2Bbcl4Y/edit?usp=sharing&quot;&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;[2024/07] In partnership with Meta, vLLM officially supports Llama 3.1 with FP8 quantization and pipeline parallelism! Please check out our blog post &lt;a href=&quot;https://blog.vllm.ai/2024/07/23/llama31.html&quot;&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;[2024/06] We hosted &lt;a href=&quot;https://lu.ma/agivllm&quot;&gt;the fourth vLLM meetup&lt;/a&gt; with Cloudflare and BentoML! Please find the meetup slides &lt;a href=&quot;https://docs.google.com/presentation/d/1iJ8o7V2bQEi0BFEljLTwc5G1S10_Rhv3beed5oB0NJ4/edit?usp=sharing&quot;&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;[2024/04] We hosted &lt;a href=&quot;https://robloxandvllmmeetup2024.splashthat.com/&quot;&gt;the third vLLM meetup&lt;/a&gt; with Roblox! Please find the meetup slides &lt;a href=&quot;https://docs.google.com/presentation/d/1A--47JAK4BJ39t954HyTkvtfwn0fkqtsL8NGFuslReM/edit?usp=sharing&quot;&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;[2024/01] We hosted &lt;a href=&quot;https://lu.ma/ygxbpzhl&quot;&gt;the second vLLM meetup&lt;/a&gt; with IBM! Please find the meetup slides &lt;a href=&quot;https://docs.google.com/presentation/d/12mI2sKABnUw5RBWXDYY-HtHth4iMSNcEoQ10jDQbxgA/edit?usp=sharing&quot;&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;[2023/10] We hosted &lt;a href=&quot;https://lu.ma/first-vllm-meetup&quot;&gt;the first vLLM meetup&lt;/a&gt; with a16z! Please find the meetup slides &lt;a href=&quot;https://docs.google.com/presentation/d/1QL-XPFXiFpDBh86DbEegFXBXFXjix4v032GhShbKf3s/edit?usp=sharing&quot;&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;[2023/08] We would like to express our sincere gratitude to &lt;a href=&quot;https://a16z.com/2023/08/30/supporting-the-open-source-ai-community/&quot;&gt;Andreessen Horowitz&lt;/a&gt; (a16z) for providing a generous grant to support the open-source development and research of vLLM.&lt;/li&gt; 
 &lt;li&gt;[2023/06] We officially released vLLM! FastChat-vLLM integration has powered &lt;a href=&quot;https://chat.lmsys.org&quot;&gt;LMSYS Vicuna and Chatbot Arena&lt;/a&gt; since mid-April. Check out our &lt;a href=&quot;https://vllm.ai&quot;&gt;blog post&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr&gt; 
&lt;h2&gt;About&lt;/h2&gt; 
&lt;p&gt;vLLM is a fast and easy-to-use library for LLM inference and serving.&lt;/p&gt; 
&lt;p&gt;Originally developed in the &lt;a href=&quot;https://sky.cs.berkeley.edu&quot;&gt;Sky Computing Lab&lt;/a&gt; at UC Berkeley, vLLM has evolved into a community-driven project with contributions from both academia and industry.&lt;/p&gt; 
&lt;p&gt;vLLM is fast with:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;State-of-the-art serving throughput&lt;/li&gt; 
 &lt;li&gt;Efficient management of attention key and value memory with &lt;a href=&quot;https://blog.vllm.ai/2023/06/20/vllm.html&quot;&gt;&lt;strong&gt;PagedAttention&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Continuous batching of incoming requests&lt;/li&gt; 
 &lt;li&gt;Fast model execution with CUDA/HIP graph&lt;/li&gt; 
 &lt;li&gt;Quantizations: &lt;a href=&quot;https://arxiv.org/abs/2210.17323&quot;&gt;GPTQ&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2306.00978&quot;&gt;AWQ&lt;/a&gt;, INT4, INT8, and FP8.&lt;/li&gt; 
 &lt;li&gt;Optimized CUDA kernels, including integration with FlashAttention and FlashInfer.&lt;/li&gt; 
 &lt;li&gt;Speculative decoding&lt;/li&gt; 
 &lt;li&gt;Chunked prefill&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Performance benchmark&lt;/strong&gt;: We include a performance benchmark at the end of &lt;a href=&quot;https://blog.vllm.ai/2024/09/05/perf-update.html&quot;&gt;our blog post&lt;/a&gt;. It compares the performance of vLLM against other LLM serving engines (&lt;a href=&quot;https://github.com/NVIDIA/TensorRT-LLM&quot;&gt;TensorRT-LLM&lt;/a&gt;, &lt;a href=&quot;https://github.com/sgl-project/sglang&quot;&gt;SGLang&lt;/a&gt; and &lt;a href=&quot;https://github.com/InternLM/lmdeploy&quot;&gt;LMDeploy&lt;/a&gt;). The implementation is under &lt;a href=&quot;https://raw.githubusercontent.com/vllm-project/vllm/main/.buildkite/nightly-benchmarks/&quot;&gt;nightly-benchmarks folder&lt;/a&gt; and you can &lt;a href=&quot;https://github.com/vllm-project/vllm/issues/8176&quot;&gt;reproduce&lt;/a&gt; this benchmark using our one-click runnable script.&lt;/p&gt; 
&lt;p&gt;vLLM is flexible and easy to use with:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Seamless integration with popular Hugging Face models&lt;/li&gt; 
 &lt;li&gt;High-throughput serving with various decoding algorithms, including &lt;em&gt;parallel sampling&lt;/em&gt;, &lt;em&gt;beam search&lt;/em&gt;, and more&lt;/li&gt; 
 &lt;li&gt;Tensor parallelism and pipeline parallelism support for distributed inference&lt;/li&gt; 
 &lt;li&gt;Streaming outputs&lt;/li&gt; 
 &lt;li&gt;OpenAI-compatible API server&lt;/li&gt; 
 &lt;li&gt;Support NVIDIA GPUs, AMD CPUs and GPUs, Intel CPUs and GPUs, PowerPC CPUs, TPU, and AWS Neuron.&lt;/li&gt; 
 &lt;li&gt;Prefix caching support&lt;/li&gt; 
 &lt;li&gt;Multi-lora support&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;vLLM seamlessly supports most popular open-source models on HuggingFace, including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Transformer-like LLMs (e.g., Llama)&lt;/li&gt; 
 &lt;li&gt;Mixture-of-Expert LLMs (e.g., Mixtral, Deepseek-V2 and V3)&lt;/li&gt; 
 &lt;li&gt;Embedding Models (e.g. E5-Mistral)&lt;/li&gt; 
 &lt;li&gt;Multi-modal LLMs (e.g., LLaVA)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Find the full list of supported models &lt;a href=&quot;https://docs.vllm.ai/en/latest/models/supported_models.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;Install vLLM with &lt;code&gt;pip&lt;/code&gt; or &lt;a href=&quot;https://docs.vllm.ai/en/latest/getting_started/installation/gpu/index.html#build-wheel-from-source&quot;&gt;from source&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install vllm
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Visit our &lt;a href=&quot;https://docs.vllm.ai/en/latest/&quot;&gt;documentation&lt;/a&gt; to learn more.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.vllm.ai/en/latest/getting_started/installation.html&quot;&gt;Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.vllm.ai/en/latest/getting_started/quickstart.html&quot;&gt;Quickstart&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.vllm.ai/en/latest/models/supported_models.html&quot;&gt;List of Supported Models&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome and value any contributions and collaborations. Please check out &lt;a href=&quot;https://raw.githubusercontent.com/vllm-project/vllm/main/CONTRIBUTING.md&quot;&gt;CONTRIBUTING.md&lt;/a&gt; for how to get involved.&lt;/p&gt; 
&lt;h2&gt;Sponsors&lt;/h2&gt; 
&lt;p&gt;vLLM is a community project. Our compute resources for development and testing are supported by the following organizations. Thank you for your support!&lt;/p&gt; 
&lt;!-- Note: Please sort them in alphabetical order. --&gt; 
&lt;!-- Note: Please keep these consistent with docs/source/community/sponsors.md --&gt; 
&lt;p&gt;Cash Donations:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;a16z&lt;/li&gt; 
 &lt;li&gt;Dropbox&lt;/li&gt; 
 &lt;li&gt;Sequoia Capital&lt;/li&gt; 
 &lt;li&gt;Skywork AI&lt;/li&gt; 
 &lt;li&gt;ZhenFund&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Compute Resources:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;AMD&lt;/li&gt; 
 &lt;li&gt;Anyscale&lt;/li&gt; 
 &lt;li&gt;AWS&lt;/li&gt; 
 &lt;li&gt;Crusoe Cloud&lt;/li&gt; 
 &lt;li&gt;Databricks&lt;/li&gt; 
 &lt;li&gt;DeepInfra&lt;/li&gt; 
 &lt;li&gt;Google Cloud&lt;/li&gt; 
 &lt;li&gt;Lambda Lab&lt;/li&gt; 
 &lt;li&gt;Nebius&lt;/li&gt; 
 &lt;li&gt;Novita AI&lt;/li&gt; 
 &lt;li&gt;NVIDIA&lt;/li&gt; 
 &lt;li&gt;Replicate&lt;/li&gt; 
 &lt;li&gt;Roblox&lt;/li&gt; 
 &lt;li&gt;RunPod&lt;/li&gt; 
 &lt;li&gt;Trainy&lt;/li&gt; 
 &lt;li&gt;UC Berkeley&lt;/li&gt; 
 &lt;li&gt;UC San Diego&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Slack Sponsor: Anyscale&lt;/p&gt; 
&lt;p&gt;We also have an official fundraising venue through &lt;a href=&quot;https://opencollective.com/vllm&quot;&gt;OpenCollective&lt;/a&gt;. We plan to use the fund to support the development, maintenance, and adoption of vLLM.&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you use vLLM for your research, please cite our &lt;a href=&quot;https://arxiv.org/abs/2309.06180&quot;&gt;paper&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bibtex&quot;&gt;@inproceedings{kwon2023efficient,
  title={Efficient Memory Management for Large Language Model Serving with PagedAttention},
  author={Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica},
  booktitle={Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles},
  year={2023}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Contact Us&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;For technical questions and feature requests, please use GitHub issues or discussions.&lt;/li&gt; 
 &lt;li&gt;For discussing with fellow users and coordinating contributions and development, please use Slack.&lt;/li&gt; 
 &lt;li&gt;For security disclosures, please use GitHub&#39;s security advisory feature.&lt;/li&gt; 
 &lt;li&gt;For collaborations and partnerships, please contact us at vllm-questions AT lists.berkeley.edu.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Media Kit&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;If you wish to use vLLM&#39;s logo, please refer to &lt;a href=&quot;https://github.com/vllm-project/media-kit&quot;&gt;our media kit repo&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>vnpy/vnpy</title>
      <link>https://github.com/vnpy/vnpy</link>
      <description>&lt;p&gt;基于Python的开源量化交易平台开发框架&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;VeighNa - By Traders, For Traders.&lt;/h1&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;https://vnpy.oss-cn-shanghai.aliyuncs.com/veighna-logo.png&quot;&gt; &lt;/p&gt; 
&lt;p&gt;💬 Want to read this in &lt;strong&gt;english&lt;/strong&gt; ? Go &lt;a href=&quot;https://raw.githubusercontent.com/vnpy/vnpy/master/README_ENG.md&quot;&gt;&lt;strong&gt;here&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;https://img.shields.io/badge/version-3.9.4-blueviolet.svg?sanitize=true&quot;&gt; &lt;img src=&quot;https://img.shields.io/badge/platform-windows%7Clinux%7Cmacos-yellow.svg?sanitize=true&quot;&gt; &lt;img src=&quot;https://img.shields.io/badge/python-3.10%7C3.11.%7C3.12-blue.svg?sanitize=true&quot;&gt; &lt;img src=&quot;https://img.shields.io/github/actions/workflow/status/vnpy/vnpy/pythonapp.yml?branch=master&quot;&gt; &lt;img src=&quot;https://img.shields.io/github/license/vnpy/vnpy.svg?color=orange&quot;&gt; &lt;/p&gt; 
&lt;p&gt;VeighNa是一套基于Python的开源量化交易系统开发框架，在开源社区持续不断的贡献下一步步成长为多功能量化交易平台，自发布以来已经积累了众多来自金融机构或相关领域的用户，包括私募基金、证券公司、期货公司等。&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;🚀&lt;/span&gt; &lt;span&gt;🚀&lt;/span&gt; &lt;span&gt;🚀&lt;/span&gt; &lt;strong&gt;面向专业交易员的【VeighNa Elite量化终端】已经正式发布，针对专业交易员群体在海量策略并发、智能移仓换月、算法拆单执行、多账户交易支持等方面的需求提供了完善支持。了解更详细的信息请扫描下方二维码关注后，点击菜单栏的【社区交流 -&amp;gt; Elite会员服务】即可&lt;/strong&gt;：&lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;https://vnpy.oss-cn-shanghai.aliyuncs.com/vnpy_qr.jpg&quot;&gt; &lt;/p&gt; 
&lt;p&gt;在使用VeighNa进行二次开发（策略、模块等）的过程中有任何疑问，请查看&lt;a href=&quot;https://www.vnpy.com/docs/cn/index.html&quot;&gt;&lt;strong&gt;VeighNa项目文档&lt;/strong&gt;&lt;/a&gt;，如果无法解决请前往&lt;a href=&quot;https://www.vnpy.com/forum/&quot;&gt;&lt;strong&gt;官方社区论坛&lt;/strong&gt;&lt;/a&gt;的【提问求助】板块寻求帮助，也欢迎在【经验分享】板块分享你的使用心得！&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;想要获取更多关于VeighNa的资讯信息？&lt;/strong&gt; 请扫描下方二维码添加小助手加入【VeighNa社区交流微信群】：&lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;https://vnpy.oss-cn-shanghai.aliyuncs.com/github_wx.png&quot; , width=&quot;250&quot;&gt; &lt;/p&gt; 
&lt;h2&gt;功能特点&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;多功能量化交易平台（trader），整合了多种交易接口，并针对具体策略算法和功能开发提供了简洁易用的API，用于快速构建交易员所需的量化交易应用。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;覆盖国内外所拥有的下述交易品种的交易接口（gateway）：&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;国内市场&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt; &lt;p&gt;CTP（&lt;a href=&quot;https://www.github.com/vnpy/vnpy_ctp&quot;&gt;ctp&lt;/a&gt;）：国内期货、期权&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;CTP Mini（&lt;a href=&quot;https://www.github.com/vnpy/vnpy_mini&quot;&gt;mini&lt;/a&gt;）：国内期货、期权&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;CTP证券（&lt;a href=&quot;https://www.github.com/vnpy/vnpy_sopt&quot;&gt;sopt&lt;/a&gt;）：ETF期权&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;飞马（&lt;a href=&quot;https://www.github.com/vnpy/vnpy_femas&quot;&gt;femas&lt;/a&gt;）：国内期货&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;恒生UFT（&lt;a href=&quot;https://www.github.com/vnpy/vnpy_uft&quot;&gt;uft&lt;/a&gt;）：国内期货、ETF期权&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;易盛（&lt;a href=&quot;https://www.github.com/vnpy/vnpy_esunny&quot;&gt;esunny&lt;/a&gt;）：国内期货、黄金TD&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;顶点飞创（&lt;a href=&quot;https://www.github.com/vnpy/vnpy_sec&quot;&gt;sec&lt;/a&gt;）：ETF期权&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;顶点HTS（&lt;a href=&quot;https://www.github.com/vnpy/vnpy_hts&quot;&gt;hts&lt;/a&gt;）：ETF期权&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;中泰XTP（&lt;a href=&quot;https://www.github.com/vnpy/vnpy_xtp&quot;&gt;xtp&lt;/a&gt;）：国内证券（A股）、ETF期权&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;华鑫奇点（&lt;a href=&quot;https://www.github.com/vnpy/vnpy_tora&quot;&gt;tora&lt;/a&gt;）：国内证券（A股）、ETF期权&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;国泰君安（&lt;a href=&quot;https://www.github.com/vnpy/vnpy_hft&quot;&gt;hft&lt;/a&gt;）：国内证券（A股、两融）&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;东证OST（&lt;a href=&quot;https://www.github.com/vnpy/vnpy_ost&quot;&gt;ost&lt;/a&gt;）：国内证券（A股）&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;东方财富EMT（&lt;a href=&quot;https://www.github.com/vnpy/vnpy_emt&quot;&gt;emt&lt;/a&gt;）：国内证券（A股）&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;飞鼠（&lt;a href=&quot;https://www.github.com/vnpy/vnpy_sgit&quot;&gt;sgit&lt;/a&gt;）：黄金TD、国内期货&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;金仕达黄金（&lt;a href=&quot;https://www.github.com/vnpy/vnpy_ksgold&quot;&gt;ksgold&lt;/a&gt;）：黄金TD&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;利星资管（&lt;a href=&quot;https://www.github.com/vnpy/vnpy_lstar&quot;&gt;lstar&lt;/a&gt;）：期货资管&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;融航（&lt;a href=&quot;https://www.github.com/vnpy/vnpy_rohon&quot;&gt;rohon&lt;/a&gt;）：期货资管&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;杰宜斯（&lt;a href=&quot;https://www.github.com/vnpy/vnpy_jees&quot;&gt;jees&lt;/a&gt;）：期货资管&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;中汇亿达（&lt;a href=&quot;https://www.github.com/vnpy/vnpy_comstar&quot;&gt;comstar&lt;/a&gt;）：银行间市场&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;掘金（&lt;a href=&quot;https://www.github.com/vnpy/vnpy_gm&quot;&gt;gm&lt;/a&gt;）：国内证券（仿真）&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;恒生云UF（&lt;a href=&quot;https://www.github.com/vnpy/vnpy_uf&quot;&gt;uf&lt;/a&gt;）：国内证券（仿真）&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;TTS（&lt;a href=&quot;https://www.github.com/vnpy/vnpy_tts&quot;&gt;tts&lt;/a&gt;）：国内期货（仿真）&lt;/p&gt; &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;海外市场&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt; &lt;p&gt;Interactive Brokers（&lt;a href=&quot;https://www.github.com/vnpy/vnpy_ib&quot;&gt;ib&lt;/a&gt;）：海外证券、期货、期权、贵金属等&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;易盛9.0外盘（&lt;a href=&quot;https://www.github.com/vnpy/vnpy_tap&quot;&gt;tap&lt;/a&gt;）：海外期货&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;直达期货（&lt;a href=&quot;https://www.github.com/vnpy/vnpy_da&quot;&gt;da&lt;/a&gt;）：海外期货&lt;/p&gt; &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;特殊应用&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt; &lt;p&gt;RQData行情（&lt;a href=&quot;https://www.github.com/vnpy/vnpy_rqdata&quot;&gt;rqdata&lt;/a&gt;）：跨市场（股票、指数、ETF、期货）实时行情&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;迅投研行情（&lt;a href=&quot;https://www.github.com/vnpy/vnpy_xt&quot;&gt;xt&lt;/a&gt;）：跨市场（股票、指数、可转债、ETF、期货、期权）实时行情&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;RPC服务（&lt;a href=&quot;https://www.github.com/vnpy/vnpy_rpcservice&quot;&gt;rpc&lt;/a&gt;）：跨进程通讯接口，用于分布式架构&lt;/p&gt; &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;覆盖下述各类量化策略的交易应用（app）：&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://www.github.com/vnpy/vnpy_ctastrategy&quot;&gt;cta_strategy&lt;/a&gt;：CTA策略引擎模块，在保持易用性的同时，允许用户针对CTA类策略运行过程中委托的报撤行为进行细粒度控制（降低交易滑点、实现高频策略）&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://www.github.com/vnpy/vnpy_ctabacktester&quot;&gt;cta_backtester&lt;/a&gt;：CTA策略回测模块，无需使用Jupyter Notebook，直接使用图形界面进行策略回测分析、参数优化等相关工作&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://www.github.com/vnpy/vnpy_spreadtrading&quot;&gt;spread_trading&lt;/a&gt;：价差交易模块，支持自定义价差，实时计算价差行情和持仓，支持价差算法交易以及自动价差策略两种模式&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://www.github.com/vnpy/vnpy_optionmaster&quot;&gt;option_master&lt;/a&gt;：期权交易模块，针对国内期权市场设计，支持多种期权定价模型、隐含波动率曲面计算、希腊值风险跟踪等功能&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://www.github.com/vnpy/vnpy_portfoliostrategy&quot;&gt;portfolio_strategy&lt;/a&gt;：组合策略模块，面向同时交易多合约的量化策略（Alpha、期权套利等），提供历史数据回测和实盘自动交易功能&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://www.github.com/vnpy/vnpy_algotrading&quot;&gt;algo_trading&lt;/a&gt;：算法交易模块，提供多种常用的智能交易算法：TWAP、Sniper、Iceberg、BestLimit等&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://www.github.com/vnpy/vnpy_scripttrader&quot;&gt;script_trader&lt;/a&gt;：脚本策略模块，面向多标的类量化策略和计算任务设计，同时也可以在命令行中实现REPL指令形式的交易，不支持回测功能&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://www.github.com/vnpy/vnpy_paperaccount&quot;&gt;paper_account&lt;/a&gt;：本地仿真模块，纯本地化实现的仿真模拟交易功能，基于交易接口获取的实时行情进行委托撮合，提供委托成交推送以及持仓记录&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://www.github.com/vnpy/vnpy_chartwizard&quot;&gt;chart_wizard&lt;/a&gt;：K线图表模块，基于RQData数据服务（期货）或者交易接口获取历史数据，并结合Tick推送显示实时行情变化&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://www.github.com/vnpy/vnpy_portfoliomanager&quot;&gt;portfolio_manager&lt;/a&gt;：交易组合管理模块，以独立的策略交易组合（子账户）为基础，提供委托成交记录管理、交易仓位自动跟踪以及每日盈亏实时统计功能&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://www.github.com/vnpy/vnpy_rpcservice&quot;&gt;rpc_service&lt;/a&gt;：RPC服务模块，允许将某一进程启动为服务端，作为统一的行情和交易路由通道，允许多客户端同时连接，实现多进程分布式系统&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://www.github.com/vnpy/vnpy_datamanager&quot;&gt;data_manager&lt;/a&gt;：历史数据管理模块，通过树形目录查看数据库中已有的数据概况，选择任意时间段数据查看字段细节，支持CSV文件的数据导入和导出&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://www.github.com/vnpy/vnpy_datarecorder&quot;&gt;data_recorder&lt;/a&gt;：行情记录模块，基于图形界面进行配置，根据需求实时录制Tick或者K线行情到数据库中，用于策略回测或者实盘初始化&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://www.github.com/vnpy/vnpy_excelrtd&quot;&gt;excel_rtd&lt;/a&gt;：Excel RTD（Real Time Data）实时数据服务，基于pyxll模块实现在Excel中获取各类数据（行情、合约、持仓等）的实时推送更新&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://www.github.com/vnpy/vnpy_riskmanager&quot;&gt;risk_manager&lt;/a&gt;：风险管理模块，提供包括交易流控、下单数量、活动委托、撤单总数等规则的统计和限制，有效实现前端风控功能&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://www.github.com/vnpy/vnpy_webtrader&quot;&gt;web_trader&lt;/a&gt;：Web服务模块，针对B-S架构需求设计，实现了提供主动函数调用（REST）和被动数据推送（Websocket）的Web服务器&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Python交易API接口封装（api），提供上述交易接口的底层对接实现。&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;REST Client（&lt;a href=&quot;https://www.github.com/vnpy/vnpy_rest&quot;&gt;rest&lt;/a&gt;）：基于协程异步IO的高性能REST API客户端，采用事件消息循环的编程模型，支持高并发实时交易请求发送&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Websocket Client（&lt;a href=&quot;https://www.github.com/vnpy/vnpy_websocket&quot;&gt;websocket&lt;/a&gt;）：基于协程异步IO的高性能Websocket API客户端，支持和REST Client共用事件循环并发运行&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;简洁易用的事件驱动引擎（event），作为事件驱动型交易程序的核心。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;对接各类数据库的适配器接口（database）：&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;SQL类&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt; &lt;p&gt;SQLite（&lt;a href=&quot;https://www.github.com/vnpy/vnpy_sqlite&quot;&gt;sqlite&lt;/a&gt;）：轻量级单文件数据库，无需安装和配置数据服务程序，VeighNa的默认选项，适合入门新手用户&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;MySQL（&lt;a href=&quot;https://www.github.com/vnpy/vnpy_mysql&quot;&gt;mysql&lt;/a&gt;）：主流的开源关系型数据库，文档资料极为丰富，且可替换其他NewSQL兼容实现（如TiDB）&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;PostgreSQL（&lt;a href=&quot;https://www.github.com/vnpy/vnpy_postgresql&quot;&gt;postgresql&lt;/a&gt;）：特性更为丰富的开源关系型数据库，支持通过扩展插件来新增功能，只推荐熟手使用&lt;/p&gt; &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;NoSQL类&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt; &lt;p&gt;DolphinDB（&lt;a href=&quot;https://www.github.com/vnpy/vnpy_dolphindb&quot;&gt;dolphindb&lt;/a&gt;）：一款高性能分布式时序数据库，适用于对速度要求极高的低延时或实时性任务&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;Arctic（&lt;a href=&quot;https://www.github.com/vnpy/vnpy_arctic&quot;&gt;arctic&lt;/a&gt;）：高性能金融时序数据库，采用了分块化储存、LZ4压缩等性能优化方案，以实现时序数据的高效读写&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;TDengine（&lt;a href=&quot;https://www.github.com/vnpy/vnpy_taos&quot;&gt;taos&lt;/a&gt;）：分布式、高性能、支持SQL的时序数据库，带有内建的缓存、流式计算、数据订阅等系统功能，能大幅减少研发和运维的复杂度&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;TimescaleDB（&lt;a href=&quot;https://www.github.com/vnpy/vnpy_timescaledb&quot;&gt;timescaledb&lt;/a&gt;）：基于PostgreSQL开发的一款时序数据库，以插件化扩展的形式安装，支持自动按空间和时间对数据进行分区&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;MongoDB（&lt;a href=&quot;https://www.github.com/vnpy/vnpy_mongodb&quot;&gt;mongodb&lt;/a&gt;）：基于分布式文件储存（bson格式）的文档式数据库，内置的热数据内存缓存提供更快读写速度&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;InfluxDB（&lt;a href=&quot;https://www.github.com/vnpy/vnpy_influxdb&quot;&gt;influxdb&lt;/a&gt;）：针对TimeSeries Data专门设计的时序数据库，列式数据储存提供极高的读写效率和外围分析应用&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;LevelDB（&lt;a href=&quot;https://www.github.com/vnpy/vnpy_leveldb&quot;&gt;leveldb&lt;/a&gt;）：由Google推出的高性能Key/Value数据库，基于LSM算法实现进程内存储引擎，支持数十亿级别的海量数据&lt;/p&gt; &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;对接下述各类数据服务的适配器接口（datafeed）：&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;迅投研（&lt;a href=&quot;https://www.github.com/vnpy/vnpy_xt&quot;&gt;xt&lt;/a&gt;）：股票、期货、期权、基金、债券&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;米筐RQData（&lt;a href=&quot;https://www.github.com/vnpy/vnpy_rqdata&quot;&gt;rqdata&lt;/a&gt;）：股票、期货、期权、基金、债券、黄金TD&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;咏春大师（&lt;a href=&quot;https://www.github.com/vnpy/vnpy_voltrader&quot;&gt;voltrader&lt;/a&gt;）：期货、期权&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;恒生UData（&lt;a href=&quot;https://www.github.com/vnpy/vnpy_udata&quot;&gt;udata&lt;/a&gt;）：股票、期货、期权&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;TuShare（&lt;a href=&quot;https://www.github.com/vnpy/vnpy_tushare&quot;&gt;tushare&lt;/a&gt;）：股票、期货、期权、基金&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;万得Wind（&lt;a href=&quot;https://www.github.com/vnpy/vnpy_wind&quot;&gt;wind&lt;/a&gt;）：股票、期货、基金、债券&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;天软Tinysoft（&lt;a href=&quot;https://www.github.com/vnpy/vnpy_tinysoft&quot;&gt;tinysoft&lt;/a&gt;）：股票、期货、基金、债券&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;同花顺iFinD（&lt;a href=&quot;https://www.github.com/vnpy/vnpy_ifind&quot;&gt;ifind&lt;/a&gt;）：股票、期货、基金、债券&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;天勤TQSDK（&lt;a href=&quot;https://www.github.com/vnpy/vnpy_tqsdk&quot;&gt;tqsdk&lt;/a&gt;）：期货&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;跨进程通讯标准组件（rpc），用于实现分布式部署的复杂交易系统。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Python高性能K线图表（chart），支持大数据量图表显示以及实时数据更新功能。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;http://www.vnpy.com/forum&quot;&gt;社区论坛&lt;/a&gt;和&lt;a href=&quot;http://zhuanlan.zhihu.com/vn-py&quot;&gt;知乎专栏&lt;/a&gt;，内容包括VeighNa项目的开发教程和Python在量化交易领域的应用研究等内容。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;官方交流群262656087（QQ），管理严格（定期清除长期潜水的成员），入群费将捐赠给VeighNa社区基金。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;注：以上关于功能特点的说明为根据说明文档发布时情况罗列，后续可能存在更新或调整。若功能描述同实际存在出入，欢迎通过Issue联系进行调整。&lt;/p&gt; 
&lt;h2&gt;环境准备&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;推荐使用VeighNa团队为量化交易专门打造的Python发行版&lt;a href=&quot;https://download.vnpy.com/veighna_studio-3.9.4.exe&quot;&gt;VeighNa Studio-3.9.4&lt;/a&gt;，集成内置了VeighNa框架以及VeighNa Station量化管理平台，无需手动安装&lt;/li&gt; 
 &lt;li&gt;支持的系统版本：Windows 11以上 / Windows Server 2019以上 / Ubuntu 22.04 LTS以上&lt;/li&gt; 
 &lt;li&gt;支持的Python版本：Python 3.10以上（64位），&lt;strong&gt;推荐使用Python 3.10&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;安装步骤&lt;/h2&gt; 
&lt;p&gt;在&lt;a href=&quot;https://github.com/vnpy/vnpy/releases&quot;&gt;这里&lt;/a&gt;下载Release发布版本，解压后运行以下命令安装：&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Windows&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;install.bat
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Ubuntu&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;bash install.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Macos&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;bash install_osx.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;注意：setup.cfg中列举了VeighNa框架安装所需的依赖库，requirements.txt中给出了这些依赖库的推荐安装版本。&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;使用指南&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;在&lt;a href=&quot;http://www.simnow.com.cn/&quot;&gt;SimNow&lt;/a&gt;注册CTP仿真账号，并在&lt;a href=&quot;http://www.simnow.com.cn/product.action&quot;&gt;该页面&lt;/a&gt;获取经纪商代码以及交易行情服务器地址。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;在&lt;a href=&quot;https://www.vnpy.com/forum/&quot;&gt;VeighNa社区论坛&lt;/a&gt;注册获得VeighNa Station账号密码（论坛账号密码即是）&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;启动VeighNa Station（安装VeighNa Studio后会在桌面自动创建快捷方式），输入上一步的账号密码登录&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;点击底部的&lt;strong&gt;VeighNa Trader&lt;/strong&gt;按钮，开始你的交易！！！&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;注意：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;在VeighNa Trader的运行过程中请勿关闭VeighNa Station（会自动退出）&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;脚本运行&lt;/h2&gt; 
&lt;p&gt;除了基于VeighNa Station的图形化启动方式外，也可以在任意目录下创建run.py，写入以下示例代码：&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;from vnpy.event import EventEngine
from vnpy.trader.engine import MainEngine
from vnpy.trader.ui import MainWindow, create_qapp

from vnpy_ctp import CtpGateway
from vnpy_ctastrategy import CtaStrategyApp
from vnpy_ctabacktester import CtaBacktesterApp


def main():
    &quot;&quot;&quot;Start VeighNa Trader&quot;&quot;&quot;
    qapp = create_qapp()

    event_engine = EventEngine()
    main_engine = MainEngine(event_engine)
    
    main_engine.add_gateway(CtpGateway)
    main_engine.add_app(CtaStrategyApp)
    main_engine.add_app(CtaBacktesterApp)

    main_window = MainWindow(main_engine, event_engine)
    main_window.showMaximized()

    qapp.exec()


if __name__ == &quot;__main__&quot;:
    main()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;在该目录下打开CMD（按住Shift-&amp;gt;点击鼠标右键-&amp;gt;在此处打开命令窗口/PowerShell）后运行下列命令启动VeighNa Trader：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;python run.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;贡献代码&lt;/h2&gt; 
&lt;p&gt;VeighNa使用Github托管其源代码，如果希望贡献代码请使用github的PR（Pull Request）的流程:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/vnpy/vnpy/issues/new&quot;&gt;创建 Issue&lt;/a&gt; - 对于较大的改动（如新功能，大型重构等）建议先开issue讨论一下，较小的improvement（如文档改进，bugfix等）直接发PR即可&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Fork &lt;a href=&quot;https://github.com/vnpy/vnpy&quot;&gt;VeighNa&lt;/a&gt; - 点击右上角&lt;strong&gt;Fork&lt;/strong&gt;按钮&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Clone你自己的fork: &lt;code&gt;git clone https://github.com/$userid/vnpy.git&lt;/code&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;如果你的fork已经过时，需要手动sync：&lt;a href=&quot;https://help.github.com/articles/syncing-a-fork/&quot;&gt;同步方法&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;从&lt;strong&gt;dev&lt;/strong&gt;创建你自己的feature branch: &lt;code&gt;git checkout -b $my_feature_branch dev&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;在$my_feature_branch上修改并将修改push到你的fork上&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;创建从你的fork的$my_feature_branch分支到主项目的&lt;strong&gt;dev&lt;/strong&gt;分支的[Pull Request] - &lt;a href=&quot;https://github.com/vnpy/vnpy/compare?expand=1&quot;&gt;在此&lt;/a&gt;点击&lt;strong&gt;compare across forks&lt;/strong&gt;，选择需要的fork和branch创建PR&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;等待review, 需要继续改进，或者被Merge!&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;在提交代码的时候，请遵守以下规则，以提高代码质量：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;使用&lt;a href=&quot;https://pypi.org/project/flake8/&quot;&gt;flake8&lt;/a&gt;检查你的代码，确保没有error和warning。在项目根目录下运行&lt;code&gt;flake8&lt;/code&gt;即可。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;其他内容&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/vnpy/vnpy/raw/dev/.github/SUPPORT.md&quot;&gt;获取帮助&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/vnpy/vnpy/raw/dev/.github/CODE_OF_CONDUCT.md&quot;&gt;社区行为准则&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/vnpy/vnpy/raw/dev/.github/ISSUE_TEMPLATE.md&quot;&gt;Issue模板&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/vnpy/vnpy/raw/dev/.github/PULL_REQUEST_TEMPLATE.md&quot;&gt;PR模板&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;版权说明&lt;/h2&gt; 
&lt;p&gt;MIT&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>FujiwaraChoki/MoneyPrinterV2</title>
      <link>https://github.com/FujiwaraChoki/MoneyPrinterV2</link>
      <description>&lt;p&gt;Automate the process of making money online.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MoneyPrinter V2&lt;/h1&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;♥︎ If you&#39;re interested in using a hosted version, sign up here &lt;a href=&quot;https://www.shiori.ai&quot;&gt;shiori.ai&lt;/a&gt;, an AI tool that combines all other AI tools into one.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;𝕏 Also, follow me on X: &lt;a href=&quot;https://x.com/DevBySami&quot;&gt;@DevBySami&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/FujiwaraChoki/MoneyPrinterV2&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/made_with-%E2%9D%A4-red?style=for-the-badge&amp;amp;labelColor=orange&quot; alt=&quot;madewithlove&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.buymeacoffee.com/fujicodes&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Buy%20Me%20A%20Coffee-Donate-brightgreen?logo=buymeacoffee&quot; alt=&quot;Buy Me A Coffee&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/FujiwaraChoki/MoneyPrinterV2/raw/main/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/license/FujiwaraChoki/MoneyPrinterV2?style=for-the-badge&quot; alt=&quot;GitHub license&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/FujiwaraChoki/MoneyPrinterV2/issues&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/issues/FujiwaraChoki/MoneyPrinterV2?style=for-the-badge&quot; alt=&quot;GitHub issues&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/FujiwaraChoki/MoneyPrinterV2/stargazers&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/FujiwaraChoki/MoneyPrinterV2?style=for-the-badge&quot; alt=&quot;GitHub stars&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://dsc.gg/fuji-community&quot;&gt;&lt;img src=&quot;https://img.shields.io/discord/1134848537704804432?style=for-the-badge&quot; alt=&quot;Discord&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Follow me on &lt;a href=&quot;https://x.com/DevBySami&quot;&gt;X&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;An Application that automates the process of making money online. MPV2 (MoneyPrinter Version 2) is, as the name suggests, the second version of the MoneyPrinter project. It is a complete rewrite of the original project, with a focus on a wider range of features and a more modular architecture.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; MPV2 needs Python 3.9 to function effectively. Watch the YouTube video &lt;a href=&quot;https://youtu.be/wAZ_ZSuIqfk&quot;&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; Twitter Bot (with CRON Jobs =&amp;gt; &lt;code&gt;scheduler&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; YouTube Shorts Automater (with CRON Jobs =&amp;gt; &lt;code&gt;scheduler&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; Affiliate Marketing (Amazon + Twitter)&lt;/li&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; Find local businesses &amp;amp; cold outreach&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Versions&lt;/h2&gt; 
&lt;p&gt;MoneyPrinter has different versions for multiple languages developed by the community for the community. Here are some known versions:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Chinese: &lt;a href=&quot;https://github.com/harry0703/MoneyPrinterTurbo&quot;&gt;MoneyPrinterTurbo&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If you would like to submit your own version/fork of MoneyPrinter, please open an issue describing the changes you made to the fork.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;Please install &lt;a href=&quot;https://visualstudio.microsoft.com/de/visual-cpp-build-tools/&quot;&gt;Microsoft Visual C++ build tools&lt;/a&gt; first, so that CoquiTTS can function correctly.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;⚠️ If you are planning to reach out to scraped businesses per E-Mail, please first install the &lt;a href=&quot;https://golang.org/&quot;&gt;Go Programming Language&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;git clone https://github.com/FujiwaraChoki/MoneyPrinterV2.git

cd MoneyPrinterV2
# Copy Example Configuration and fill out values in config.json
cp config.example.json config.json

# Create a virtual environment
python -m venv venv

# Activate the virtual environment - Windows
.\venv\Scripts\activate

# Activate the virtual environment - Unix
source venv/bin/activate

# Install the requirements
pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# Run the application
python src/main.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;All relevant document can be found &lt;a href=&quot;https://raw.githubusercontent.com/FujiwaraChoki/MoneyPrinterV2/main/docs/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Scripts&lt;/h2&gt; 
&lt;p&gt;For easier usage, there are some scripts in the &lt;code&gt;scripts&lt;/code&gt; directory, that can be used to directly access the core functionality of MPV2, without the need of user interaction.&lt;/p&gt; 
&lt;p&gt;All scripts need to be run from the root directory of the project, e.g. &lt;code&gt;bash scripts/upload_video.sh&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Please read &lt;a href=&quot;https://raw.githubusercontent.com/FujiwaraChoki/MoneyPrinterV2/main/CONTRIBUTING.md&quot;&gt;CONTRIBUTING.md&lt;/a&gt; for details on our code of conduct, and the process for submitting pull requests to us. Check out &lt;a href=&quot;https://raw.githubusercontent.com/FujiwaraChoki/MoneyPrinterV2/main/docs/Roadmap.md&quot;&gt;docs/Roadmap.md&lt;/a&gt; for a list of features that need to be implemented.&lt;/p&gt; 
&lt;h2&gt;Code of Conduct&lt;/h2&gt; 
&lt;p&gt;Please read &lt;a href=&quot;https://raw.githubusercontent.com/FujiwaraChoki/MoneyPrinterV2/main/CODE_OF_CONDUCT.md&quot;&gt;CODE_OF_CONDUCT.md&lt;/a&gt; for details on our code of conduct, and the process for submitting pull requests to us.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;MoneyPrinterV2 is licensed under &lt;code&gt;Affero General Public License v3.0&lt;/code&gt;. See &lt;a href=&quot;https://raw.githubusercontent.com/FujiwaraChoki/MoneyPrinterV2/main/LICENSE&quot;&gt;LICENSE&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h2&gt;Acknowledgments&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/coqui-ai/TTS&quot;&gt;CoquiTTS&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/xtekky/gpt4free&quot;&gt;gpt4free&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Disclaimer&lt;/h2&gt; 
&lt;p&gt;This project is for educational purposes only. The author will not be responsible for any misuse of the information provided. All the information on this website is published in good faith and for general information purpose only. The author does not make any warranties about the completeness, reliability, and accuracy of this information. Any action you take upon the information you find on this website (FujiwaraChoki/MoneyPrinterV2), is strictly at your own risk. The author will not be liable for any losses and/or damages in connection with the use of our website.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>PaddlePaddle/PaddleOCR</title>
      <link>https://github.com/PaddlePaddle/PaddleOCR</link>
      <description>&lt;p&gt;Awesome multilingual OCR toolkits based on PaddlePaddle (practical ultra lightweight OCR system, support 80+ languages recognition, provide data annotation and synthesis tools, support training and deployment among server, mobile, embedded and IoT devices)&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/README_en.md&quot;&gt;English&lt;/a&gt; | 简体中文&lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;https://github.com/PaddlePaddle/PaddleOCR/releases/download/v2.8.0/PaddleOCR_logo.png&quot; align=&quot;middle&quot; width=&quot;600&quot;&gt; &lt;/p&gt;
&lt;p align=&quot;center&quot;&gt; &lt;/p&gt;
&lt;p align=&quot;center&quot;&gt; &lt;a href=&quot;https://discord.gg/z9xaRVjdbD&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Chat-on%20discord-7289da.svg?sanitize=true&quot; alt=&quot;Chat&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/license-Apache%202-dfd.svg?sanitize=true&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/releases&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/v/release/PaddlePaddle/PaddleOCR?color=ffa&quot;&gt;&lt;/a&gt; &lt;a href=&quot;&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/python-3.7+-aff.svg?sanitize=true&quot;&gt;&lt;/a&gt; &lt;a href=&quot;&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/os-linux%2C%20win%2C%20mac-pink.svg?sanitize=true&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://pypi.org/project/PaddleOCR/&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/dm/PaddleOCR?color=9cf&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/stargazers&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/PaddlePaddle/PaddleOCR?color=ccf&quot;&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;简介&lt;/h2&gt; 
&lt;p&gt;PaddleOCR 旨在打造一套丰富、领先、且实用的 OCR 工具库，助力开发者训练出更好的模型，并应用落地。&lt;/p&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;img src=&quot;https://github.com/PaddlePaddle/PaddleOCR/releases/download/v2.8.0/demo.gif&quot; width=&quot;800&quot;&gt; 
&lt;/div&gt; 
&lt;h2&gt;🚀 社区&lt;/h2&gt; 
&lt;p&gt;PaddleOCR 由 &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/issues/12122&quot;&gt;PMC&lt;/a&gt; 监督。Issues 和 PRs 将在尽力的基础上进行审查。欲了解 PaddlePaddle 社区的完整概况，请访问 &lt;a href=&quot;https://github.com/PaddlePaddle/community&quot;&gt;community&lt;/a&gt;。&lt;/p&gt; 
&lt;p&gt;⚠️注意：&lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/issues&quot;&gt;Issues&lt;/a&gt;模块仅用来报告程序🐞Bug，其余提问请移步&lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/discussions&quot;&gt;Discussions&lt;/a&gt;模块提问。如所提Issue不是Bug，会被移到Discussions模块，敬请谅解。&lt;/p&gt; 
&lt;h2&gt;📣 近期更新(&lt;a href=&quot;https://paddlepaddle.github.io/PaddleOCR/latest/update.html&quot;&gt;more&lt;/a&gt;)&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;🔥🔥2025.3.7 PaddleOCR 2.10 版本，主要包含如下内容&lt;/strong&gt;：&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;重磅新增 OCR 领域 12 个自研单模型：&lt;/strong&gt;&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://paddlepaddle.github.io/PaddleX/latest/module_usage/tutorials/ocr_modules/layout_detection.html&quot;&gt;版面区域检测&lt;/a&gt;&lt;/strong&gt; 系列 3 个模型：PP-DocLayout-L、PP-DocLayout-M、PP-DocLayout-S，支持预测 23 个常见版面类别，中英论文、研报、试卷、书籍、杂志、合同、报纸等丰富类型的文档实现高质量版面检测，&lt;strong&gt;mAP@0.5 最高达 90.4%，轻量模型端到端每秒处理超百页文档图像。&lt;/strong&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://paddlepaddle.github.io/PaddleX/latest/module_usage/tutorials/ocr_modules/formula_recognition.html&quot;&gt;公式识别&lt;/a&gt;&lt;/strong&gt; 系列 2 个模型：PP-FormulaNet-L、PP-FormulaNet-S，支持 5 万种 LaTeX 常见词汇，支持识别高难度印刷公式和手写公式，其中 &lt;strong&gt;PP-FormulaNet-L 较开源同等量级模型精度高 6 个百分点，PP-FormulaNet-S 较同等精度模型速度快 16 倍。&lt;/strong&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://paddlepaddle.github.io/PaddleX/latest/module_usage/tutorials/ocr_modules/table_structure_recognition.html&quot;&gt;表格结构识别&lt;/a&gt;&lt;/strong&gt; 系列 2 个模型：SLANeXt_wired、SLANeXt_wireless。飞桨自研新一代表格结构识别模型，分别支持有线表格和无线表格的结构预测。相比于SLANet_plus，SLANeXt在表格结构方面有较大提升，&lt;strong&gt;在内部高难度表格识别评测集上精度高 6 个百分点。&lt;/strong&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://paddlepaddle.github.io/PaddleX/latest/module_usage/tutorials/ocr_modules/table_classification.html&quot;&gt;表格分类&lt;/a&gt;&lt;/strong&gt; 系列 1 个模型：PP-LCNet_x1_0_table_cls，超轻量级有线表格和无线表格的分类模型。&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://paddlepaddle.github.io/PaddleX/latest/module_usage/tutorials/ocr_modules/table_cells_detection.html&quot;&gt;表格单元格检测&lt;/a&gt;&lt;/strong&gt; 系列 2 个模型：RT-DETR-L_wired_table_cell_det、RT-DETR-L_wireless_table_cell_det，分别支持有线表格和无线表格的单元格检测，可配合SLANeXt_wired、SLANeXt_wireless、文本检测、文本识别模块完成对表格的端到端预测。（参见本次新增的表格识别v2产线）&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://paddlepaddle.github.io/PaddleX/latest/module_usage/tutorials/ocr_modules/text_recognition.html&quot;&gt;文本识别&lt;/a&gt;&lt;/strong&gt; 系列 1 个模型： PP-OCRv4_server_rec_doc，&lt;strong&gt;支持1.5万+字典，文字识别范围更广，与此同时提升了部分文字的识别精准度，在内部数据集上，精度较 PP-OCRv4_server_rec 高 3 个百分点以上。&lt;/strong&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://paddlepaddle.github.io/PaddleX/latest/module_usage/tutorials/ocr_modules/text_recognition.html&quot;&gt;文本行方向分类&lt;/a&gt;&lt;/strong&gt; 系列 1 个模型：PP-LCNet_x0_25_textline_ori，&lt;strong&gt;存储只有 0.3M&lt;/strong&gt; 的超轻量级文本行方向分类模型。&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;重磅推出 4 条高价值多模型组合方案：&lt;/strong&gt;&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://paddlepaddle.github.io/PaddleX/latest/pipeline_usage/tutorials/ocr_pipelines/doc_preprocessor.html&quot;&gt;文档图像预处理产线&lt;/a&gt;&lt;/strong&gt;：通过超轻量级模型组合使用，实现对文档图像的扭曲和方向的矫正。&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://paddlepaddle.github.io/PaddleX/latest/pipeline_usage/tutorials/ocr_pipelines/layout_parsing_v2.html&quot;&gt;版面解析v2产线&lt;/a&gt;&lt;/strong&gt;：组合多个自研的不同类型的 OCR 类模型，优化复杂版面阅读顺序，实现多种复杂 PDF 文件端到端转换 Markdown 文件和 JSON 文件。在多个文档场景下，转换效果较其他开源方案更好。可以为大模型训练和应用提供高质量的数据生产能力。&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://paddlepaddle.github.io/PaddleX/latest/pipeline_usage/tutorials/ocr_pipelines/table_recognition_v2.html&quot;&gt;表格识别v2产线&lt;/a&gt;&lt;/strong&gt;：&lt;strong&gt;提供更好的表格端到端识别能力。&lt;/strong&gt; 通过将表格分类模块、表格单元格检测模块、表格结构识别模块、文本检测模块、文本识别模块等组合使用，实现对多种样式的表格预测，用户可自定义微调其中任意模块以提升垂类表格的效果。&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://paddlepaddle.github.io/PaddleX/latest/pipeline_usage/tutorials/information_extraction_pipelines/document_scene_information_extraction_v4.html&quot;&gt;PP-ChatOCRv4-doc产线&lt;/a&gt;&lt;/strong&gt;：在 PP-ChatOCRv3-doc 的基础上，&lt;strong&gt;融合了多模态大模型，优化了 Prompt 和多模型组合后处理逻辑，更好地解决了版面分析、生僻字、多页 pdf、表格、印章识别等常见的复杂文档信息抽取难点问题，准确率较 PP-ChatOCRv3-doc 高 15 个百分点。其中，大模型升级了本地部署的能力，提供了标准的 OpenAI 调用接口，支持对本地大模型如 DeepSeek-R1 部署的调用。&lt;/strong&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;🔥2024.10.1 添加OCR领域低代码全流程开发能力&lt;/strong&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;飞桨低代码开发工具PaddleX，依托于PaddleOCR的先进技术，支持了OCR领域的低代码全流程开发能力：&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;🎨 &lt;a href=&quot;https://paddlepaddle.github.io/PaddleOCR/latest/paddlex/quick_start.html&quot;&gt;&lt;strong&gt;模型丰富一键调用&lt;/strong&gt;&lt;/a&gt;：将文本图像智能分析、通用OCR、通用版面解析、通用表格识别、公式识别、印章文本识别涉及的&lt;strong&gt;17个模型&lt;/strong&gt;整合为6条模型产线，通过极简的&lt;strong&gt;Python API一键调用&lt;/strong&gt;，快速体验模型效果。此外，同一套API，也支持图像分类、目标检测、图像分割、时序预测等共计&lt;strong&gt;200+模型&lt;/strong&gt;，形成20+单功能模块，方便开发者进行&lt;strong&gt;模型组合&lt;/strong&gt;使用。&lt;/li&gt; 
     &lt;li&gt;🚀&lt;a href=&quot;https://paddlepaddle.github.io/PaddleOCR/latest/paddlex/overview.html&quot;&gt;&lt;strong&gt;提高效率降低门槛&lt;/strong&gt;&lt;/a&gt;：提供基于&lt;strong&gt;统一命令&lt;/strong&gt;和&lt;strong&gt;图形界面&lt;/strong&gt;两种方式，实现模型简洁高效的使用、组合与定制。支持&lt;strong&gt;高性能推理、服务化部署和端侧部署&lt;/strong&gt;等多种部署方式。此外，对于各种主流硬件如&lt;strong&gt;英伟达GPU、昆仑芯、昇腾、寒武纪和海光&lt;/strong&gt;等，进行模型开发时，都可以&lt;strong&gt;无缝切换&lt;/strong&gt;。&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;支持文档场景信息抽取v3&lt;a href=&quot;https://paddlepaddle.github.io/PaddleX/latest/pipeline_usage/tutorials/information_extraction_pipelines/document_scene_information_extraction.html&quot;&gt;PP-ChatOCRv3-doc&lt;/a&gt;、基于RT-DETR的&lt;a href=&quot;https://paddlepaddle.github.io/PaddleX/latest/module_usage/tutorials/ocr_modules/layout_detection.html&quot;&gt;高精度版面区域检测模型&lt;/a&gt;和PicoDet的&lt;a href=&quot;https://paddlepaddle.github.io/PaddleX/latest/module_usage/tutorials/ocr_modules/layout_detection.html&quot;&gt;高效率版面区域检测模型&lt;/a&gt;、高精度表格结构识别模型&lt;a href=&quot;https://paddlepaddle.github.io/PaddleX/latest/module_usage/tutorials/ocr_modules/table_structure_recognition.html&quot;&gt;SLANet_Plus&lt;/a&gt;、文本图像矫正模型&lt;a href=&quot;https://paddlepaddle.github.io/PaddleX/latest/module_usage/tutorials/ocr_modules/text_image_unwarping.html&quot;&gt;UVDoc&lt;/a&gt;、公式识别模型&lt;a href=&quot;https://paddlepaddle.github.io/PaddleX/latest/module_usage/tutorials/ocr_modules/formula_recognition.html&quot;&gt;LatexOCR&lt;/a&gt;、基于PP-LCNet的&lt;a href=&quot;https://paddlepaddle.github.io/PaddleX/latest/module_usage/tutorials/ocr_modules/doc_img_orientation_classification.html&quot;&gt;文档图像方向分类模型&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;🔥2024.7 添加 PaddleOCR 算法模型挑战赛冠军方案&lt;/strong&gt;：&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;赛题一：OCR 端到端识别任务冠军方案——&lt;a href=&quot;https://paddlepaddle.github.io/PaddleOCR/latest/algorithm/text_recognition/algorithm_rec_svtrv2.html&quot;&gt;场景文本识别算法-SVTRv2&lt;/a&gt;；&lt;/li&gt; 
   &lt;li&gt;赛题二：通用表格识别任务冠军方案——&lt;a href=&quot;https://paddlepaddle.github.io/PaddleOCR/latest/algorithm/table_recognition/algorithm_table_slanet.html&quot;&gt;表格识别算法-SLANet-LCNetV2&lt;/a&gt;。&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🌟 特性&lt;/h2&gt; 
&lt;p&gt;支持多种 OCR 相关前沿算法，在此基础上打造产业级特色模型PP-OCR、PP-Structure和PP-ChatOCR，并打通数据生产、模型训练、压缩、预测部署全流程。&lt;/p&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;img src=&quot;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/docs/images/ppocrv4.png&quot;&gt; 
&lt;/div&gt; 
&lt;h2&gt;⚡ &lt;a href=&quot;https://paddlepaddle.github.io/PaddleOCR/latest/quick_start.html&quot;&gt;快速开始&lt;/a&gt;&lt;/h2&gt; 
&lt;h2&gt;🔥 &lt;a href=&quot;https://paddlepaddle.github.io/PaddleOCR/latest/paddlex/overview.html&quot;&gt;低代码全流程开发&lt;/a&gt;&lt;/h2&gt; 
&lt;h2&gt;📝 文档&lt;/h2&gt; 
&lt;p&gt;完整文档请移步：&lt;a href=&quot;https://paddlepaddle.github.io/PaddleOCR/latest/&quot;&gt;docs&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;📚《动手学 OCR》电子书&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://paddlepaddle.github.io/PaddleOCR/latest/ppocr/blog/ocr_book.html&quot;&gt;《动手学 OCR》电子书&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🎖 贡献者&lt;/h2&gt; 
&lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/graphs/contributors&quot;&gt; &lt;img src=&quot;https://contrib.rocks/image?repo=PaddlePaddle/PaddleOCR&amp;amp;max=400&amp;amp;columns=20&quot; width=&quot;800&quot;&gt; &lt;/a&gt; 
&lt;h2&gt;⭐️ Star&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://star-history.com/#PaddlePaddle/PaddleOCR&amp;amp;Date&quot;&gt;&lt;img src=&quot;https://api.star-history.com/svg?repos=PaddlePaddle/PaddleOCR&amp;amp;type=Date&quot; alt=&quot;Star History Chart&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;许可证书&lt;/h2&gt; 
&lt;p&gt;本项目的发布受 &lt;a href=&quot;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/LICENSE&quot;&gt;Apache License Version 2.0&lt;/a&gt; 许可认证。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>unslothai/unsloth</title>
      <link>https://github.com/unslothai/unsloth</link>
      <description>&lt;p&gt;Finetune Llama 3.3, DeepSeek-R1, Gemma 3 &amp; Reasoning LLMs 2x faster with 70% less memory! 🦥&lt;/p&gt;&lt;hr&gt;&lt;div align=&quot;center&quot;&gt; 
 &lt;p&gt;&lt;a href=&quot;https://unsloth.ai&quot;&gt;
   &lt;picture&gt; 
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20white%20text.png&quot;&gt; 
    &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png&quot;&gt; 
    &lt;img alt=&quot;unsloth logo&quot; src=&quot;https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png&quot; height=&quot;110&quot; style=&quot;max-width: 100%;&quot;&gt; 
   &lt;/picture&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-Alpaca.ipynb&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/unslothai/unsloth/main/images/start%20free%20finetune%20button.png&quot; height=&quot;48&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://discord.com/invite/unsloth&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/unslothai/unsloth/main/images/Discord%20button.png&quot; height=&quot;48&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://docs.unsloth.ai&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/unslothai/unsloth/refs/heads/main/images/Documentation%20Button.png&quot; height=&quot;48&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;h3&gt;Finetune Llama 3.3, Gemma 3, Phi-4, Qwen 2.5 &amp;amp; Mistral 2x faster with 80% less VRAM!&lt;/h3&gt; 
 &lt;p&gt;&lt;img src=&quot;https://i.ibb.co/sJ7RhGG/image-41.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;✨ Finetune for Free&lt;/h2&gt; 
&lt;p&gt;Notebooks are beginner friendly. Read our &lt;a href=&quot;https://docs.unsloth.ai/get-started/fine-tuning-guide&quot;&gt;guide&lt;/a&gt;. Add your dataset, click &quot;Run All&quot;, and export your finetuned model to GGUF, Ollama, vLLM or Hugging Face.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Unsloth supports&lt;/th&gt; 
   &lt;th&gt;Free Notebooks&lt;/th&gt; 
   &lt;th&gt;Performance&lt;/th&gt; 
   &lt;th&gt;Memory use&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;GRPO (R1 reasoning)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb&quot;&gt;▶️ Start for free&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2x faster&lt;/td&gt; 
   &lt;td&gt;80% less&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Gemma 3 (4B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3_(4B).ipynb&quot;&gt;▶️ Start for free&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1.6x faster&lt;/td&gt; 
   &lt;td&gt;60% less&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Llama 3.2 (3B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb&quot;&gt;▶️ Start for free&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2x faster&lt;/td&gt; 
   &lt;td&gt;70% less&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Phi-4 (14B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4-Conversational.ipynb&quot;&gt;▶️ Start for free&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2x faster&lt;/td&gt; 
   &lt;td&gt;70% less&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Llama 3.2 Vision (11B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb&quot;&gt;▶️ Start for free&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2x faster&lt;/td&gt; 
   &lt;td&gt;50% less&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Llama 3.1 (8B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-Alpaca.ipynb&quot;&gt;▶️ Start for free&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2x faster&lt;/td&gt; 
   &lt;td&gt;70% less&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Qwen 2.5 (7B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_(7B)-Alpaca.ipynb&quot;&gt;▶️ Start for free&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2x faster&lt;/td&gt; 
   &lt;td&gt;70% less&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Mistral v0.3 (7B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_v0.3_(7B)-Conversational.ipynb&quot;&gt;▶️ Start for free&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2.2x faster&lt;/td&gt; 
   &lt;td&gt;75% less&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Ollama&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb&quot;&gt;▶️ Start for free&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1.9x faster&lt;/td&gt; 
   &lt;td&gt;60% less&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;DPO Zephyr&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Zephyr_(7B)-DPO.ipynb&quot;&gt;▶️ Start for free&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1.9x faster&lt;/td&gt; 
   &lt;td&gt;50% less&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;ul&gt; 
 &lt;li&gt;See &lt;a href=&quot;https://docs.unsloth.ai/get-started/unsloth-notebooks&quot;&gt;all our notebooks&lt;/a&gt; and &lt;a href=&quot;https://docs.unsloth.ai/get-started/all-our-models&quot;&gt;all our models&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Kaggle Notebooks&lt;/strong&gt; for &lt;a href=&quot;https://www.kaggle.com/danielhanchen/kaggle-llama-3-2-1b-3b-unsloth-notebook&quot;&gt;Llama 3.2 Kaggle notebook&lt;/a&gt;, &lt;a href=&quot;https://www.kaggle.com/danielhanchen/kaggle-llama-3-1-8b-unsloth-notebook&quot;&gt;Llama 3.1 (8B)&lt;/a&gt;, &lt;a href=&quot;https://www.kaggle.com/code/danielhanchen/phi-4-finetuning-unsloth-notebook&quot;&gt;Phi-4 (14B)&lt;/a&gt;, &lt;a href=&quot;https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook&quot;&gt;Mistral (7B)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;See detailed documentation for Unsloth &lt;a href=&quot;https://docs.unsloth.ai/&quot;&gt;here&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;⚡ Quickstart&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Install with pip (recommended)&lt;/strong&gt; for Linux devices:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;pip install unsloth
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For Windows install instructions, see &lt;a href=&quot;https://docs.unsloth.ai/get-started/installing-+-updating/windows-installation&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;🦥 Unsloth.ai News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;📣 NEW! &lt;a href=&quot;https://unsloth.ai/blog/gemma3#everything&quot;&gt;&lt;strong&gt;EVERYTHING&lt;/strong&gt; is now supported&lt;/a&gt; incuding: full finetuning, pretraining, ALL models (Mixtral, MOE, Cohere, Mamba) and all training algorithms (KTO, DoRA) etc. MultiGPU support coming very soon.&lt;/li&gt; 
 &lt;li&gt;📣 NEW! &lt;strong&gt;Gemma 3&lt;/strong&gt; by Google: &lt;a href=&quot;https://unsloth.ai/blog/gemma3&quot;&gt;Read Blog&lt;/a&gt;. We &lt;a href=&quot;https://huggingface.co/collections/unsloth/phi-4-all-versions-677eecf93784e61afe762afa&quot;&gt;uploaded GGUFs, 4-bit models&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;📣 NEW! Introducing Long-context &lt;a href=&quot;https://unsloth.ai/blog/grpo&quot;&gt;Reasoning (GRPO)&lt;/a&gt; in Unsloth. Train your own reasoning model with just 5GB VRAM. Transform Llama, Phi, Mistral etc. into reasoning LLMs!&lt;/li&gt; 
 &lt;li&gt;📣 NEW! &lt;a href=&quot;https://unsloth.ai/blog/deepseek-r1&quot;&gt;DeepSeek-R1&lt;/a&gt; - the most powerful open reasoning models with Llama &amp;amp; Qwen distillations. Run or fine-tune them now &lt;a href=&quot;https://unsloth.ai/blog/deepseek-r1&quot;&gt;with our guide&lt;/a&gt;. All model uploads: &lt;a href=&quot;https://huggingface.co/collections/unsloth/deepseek-r1-all-versions-678e1c48f5d2fce87892ace5&quot;&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;📣 NEW! &lt;a href=&quot;https://unsloth.ai/blog/phi4&quot;&gt;Phi-4&lt;/a&gt; by Microsoft: We also &lt;a href=&quot;https://unsloth.ai/blog/phi4&quot;&gt;fixed bugs&lt;/a&gt; in Phi-4 and &lt;a href=&quot;https://huggingface.co/collections/unsloth/phi-4-all-versions-677eecf93784e61afe762afa&quot;&gt;uploaded GGUFs, 4-bit&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;📣 NEW! &lt;a href=&quot;https://huggingface.co/collections/unsloth/llama-33-all-versions-67535d7d994794b9d7cf5e9f&quot;&gt;Llama 3.3 (70B)&lt;/a&gt;, Meta&#39;s latest model is supported.&lt;/li&gt; 
 &lt;li&gt;📣 Introducing Unsloth &lt;a href=&quot;https://unsloth.ai/blog/dynamic-4bit&quot;&gt;Dynamic 4-bit Quantization&lt;/a&gt;! We dynamically opt not to quantize certain parameters and this greatly increases accuracy while only using &amp;lt;10% more VRAM than BnB 4-bit. See our collection on &lt;a href=&quot;https://huggingface.co/collections/unsloth/unsloth-4-bit-dynamic-quants-67503bb873f89e15276c44e7&quot;&gt;Hugging Face here.&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;📣 &lt;a href=&quot;https://unsloth.ai/blog/vision&quot;&gt;Vision models&lt;/a&gt; now supported! &lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb&quot;&gt;Llama 3.2 Vision (11B)&lt;/a&gt;, &lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2_VL_(7B)-Vision.ipynb&quot;&gt;Qwen 2.5 VL (7B)&lt;/a&gt; and &lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Pixtral_(12B)-Vision.ipynb&quot;&gt;Pixtral (12B) 2409&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;Click for more news&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;📣 NEW! We worked with Apple to add &lt;a href=&quot;https://arxiv.org/abs/2411.09009&quot;&gt;Cut Cross Entropy&lt;/a&gt;. Unsloth now supports 89K context for Meta&#39;s Llama 3.3 (70B) on a 80GB GPU - 13x longer than HF+FA2. For Llama 3.1 (8B), Unsloth enables 342K context, surpassing its native 128K support.&lt;/li&gt; 
  &lt;li&gt;📣 We found and helped fix a &lt;a href=&quot;https://unsloth.ai/blog/gradient&quot;&gt;gradient accumulation bug&lt;/a&gt;! Please update Unsloth and transformers.&lt;/li&gt; 
  &lt;li&gt;📣 Try out &lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Unsloth_Studio.ipynb&quot;&gt;Chat interface&lt;/a&gt;!&lt;/li&gt; 
  &lt;li&gt;📣 NEW! Qwen-2.5 including &lt;a href=&quot;https://unsloth.ai/blog/qwen-coder&quot;&gt;Coder&lt;/a&gt; models are now supported with bugfixes. 14b fits in a Colab GPU! &lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_Coder_(14B)-Conversational.ipynb&quot;&gt;Qwen 2.5 conversational notebook&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;📣 NEW! &lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_Small_(22B)-Alpaca.ipynb&quot;&gt;Mistral Small 22b notebook&lt;/a&gt; finetuning fits in under 16GB of VRAM!&lt;/li&gt; 
  &lt;li&gt;📣 NEW! &lt;code&gt;pip install unsloth&lt;/code&gt; now works! Head over to &lt;a href=&quot;https://pypi.org/project/unsloth/&quot;&gt;pypi&lt;/a&gt; to check it out! This allows non git pull installs. Use &lt;code&gt;pip install unsloth[colab-new]&lt;/code&gt; for non dependency installs.&lt;/li&gt; 
  &lt;li&gt;📣 NEW! Continued Pretraining &lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_v0.3_(7B)-CPT.ipynb&quot;&gt;notebook&lt;/a&gt; for other languages like Korean!&lt;/li&gt; 
  &lt;li&gt;📣 &lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-Inference.ipynb&quot;&gt;2x faster inference&lt;/a&gt; added for all our models&lt;/li&gt; 
  &lt;li&gt;📣 We cut memory usage by a &lt;a href=&quot;https://unsloth.ai/blog/long-context&quot;&gt;further 30%&lt;/a&gt; and now support &lt;a href=&quot;https://unsloth.ai/blog/long-context&quot;&gt;4x longer context windows&lt;/a&gt;!&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;🔗 Links and Resources&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Links&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;📚 &lt;strong&gt;Documentation &amp;amp; Wiki&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://docs.unsloth.ai&quot;&gt;Read Our Docs&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img height=&quot;14&quot; src=&quot;https://upload.wikimedia.org/wikipedia/commons/6/6f/Logo_of_Twitter.svg?sanitize=true&quot;&gt;&amp;nbsp; &lt;strong&gt;Twitter (aka X)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://twitter.com/unslothai&quot;&gt;Follow us on X&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;💾 &lt;strong&gt;Installation&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://docs.unsloth.ai/get-started/installing-+-updating&quot;&gt;Pip install&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;🔮 &lt;strong&gt;Our Models&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://docs.unsloth.ai/get-started/all-our-models&quot;&gt;Unsloth Releases&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;✍️ &lt;strong&gt;Blog&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://unsloth.ai/blog&quot;&gt;Read our Blogs&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img height=&quot;14&quot; src=&quot;https://redditinc.com/hs-fs/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png&quot;&gt;&amp;nbsp; &lt;strong&gt;Reddit&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://reddit.com/r/unsloth&quot;&gt;Join our Reddit page&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;⭐ Key Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;All kernels written in &lt;a href=&quot;https://openai.com/index/triton/&quot;&gt;OpenAI&#39;s Triton&lt;/a&gt; language. &lt;strong&gt;Manual backprop engine&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;0% loss in accuracy&lt;/strong&gt; - no approximation methods - all exact.&lt;/li&gt; 
 &lt;li&gt;No change of hardware. Supports NVIDIA GPUs since 2018+. Minimum CUDA Capability 7.0 (V100, T4, Titan V, RTX 20, 30, 40x, A100, H100, L40 etc) &lt;a href=&quot;https://developer.nvidia.com/cuda-gpus&quot;&gt;Check your GPU!&lt;/a&gt; GTX 1070, 1080 works, but is slow.&lt;/li&gt; 
 &lt;li&gt;Works on &lt;strong&gt;Linux&lt;/strong&gt; and &lt;strong&gt;Windows&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Supports 4bit and 16bit QLoRA / LoRA finetuning via &lt;a href=&quot;https://github.com/TimDettmers/bitsandbytes&quot;&gt;bitsandbytes&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;If you trained a model with 🦥Unsloth, you can use this cool sticker! &amp;nbsp; &lt;img src=&quot;https://raw.githubusercontent.com/unslothai/unsloth/main/images/made%20with%20unsloth.png&quot; height=&quot;50&quot; align=&quot;center&quot;&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;💾 Install Unsloth&lt;/h2&gt; 
&lt;p&gt;You can also see our documentation for more detailed installation and updating instructions &lt;a href=&quot;https://docs.unsloth.ai/get-started/installing-+-updating&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Pip Installation&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Install with pip (recommended) for Linux devices:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install unsloth
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;a href=&quot;https://github.com/unslothai/unsloth/edit/main/README.md#advanced-pip-installation&quot;&gt;here&lt;/a&gt; for advanced pip install instructions.&lt;/p&gt; 
&lt;h3&gt;Windows Installation&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!warning] Python 3.13 does not support Unsloth. Use 3.12, 3.11 or 3.10&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install NVIDIA Video Driver:&lt;/strong&gt; You should install the latest version of your GPUs driver. Download drivers here: &lt;a href=&quot;https://www.nvidia.com/Download/index.aspx&quot;&gt;NVIDIA GPU Drive&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install Visual Studio C++:&lt;/strong&gt; You will need Visual Studio, with C++ installed. By default, C++ is not installed with &lt;a href=&quot;https://visualstudio.microsoft.com/vs/community/&quot;&gt;Visual Studio&lt;/a&gt;, so make sure you select all of the C++ options. Also select options for Windows 10/11 SDK. For detailed instructions with options, see &lt;a href=&quot;https://docs.unsloth.ai/get-started/installing-+-updating&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install CUDA Toolkit:&lt;/strong&gt; Follow the instructions to install &lt;a href=&quot;https://developer.nvidia.com/cuda-toolkit-archive&quot;&gt;CUDA Toolkit&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install PyTorch:&lt;/strong&gt; You will need the correct version of PyTorch that is compatibile with your CUDA drivers, so make sure to select them carefully. &lt;a href=&quot;https://pytorch.org/get-started/locally/&quot;&gt;Install PyTorch&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install Unsloth:&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;pip install unsloth
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Notes&lt;/h4&gt; 
&lt;p&gt;To run Unsloth directly on Windows:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Install Triton from this Windows fork and follow the instructions &lt;a href=&quot;https://github.com/woct0rdho/triton-windows&quot;&gt;here&lt;/a&gt; (be aware that the Windows fork requires PyTorch &amp;gt;= 2.4 and CUDA 12)&lt;/li&gt; 
 &lt;li&gt;In the SFTTrainer, set &lt;code&gt;dataset_num_proc=1&lt;/code&gt; to avoid a crashing issue:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;trainer = SFTTrainer(
    dataset_num_proc=1,
    ...
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Advanced/Troubleshooting&lt;/h4&gt; 
&lt;p&gt;For &lt;strong&gt;advanced installation instructions&lt;/strong&gt; or if you see weird errors during installations:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install &lt;code&gt;torch&lt;/code&gt; and &lt;code&gt;triton&lt;/code&gt;. Go to &lt;a href=&quot;https://pytorch.org&quot;&gt;https://pytorch.org&lt;/a&gt; to install it. For example &lt;code&gt;pip install torch torchvision torchaudio triton&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Confirm if CUDA is installated correctly. Try &lt;code&gt;nvcc&lt;/code&gt;. If that fails, you need to install &lt;code&gt;cudatoolkit&lt;/code&gt; or CUDA drivers.&lt;/li&gt; 
 &lt;li&gt;Install &lt;code&gt;xformers&lt;/code&gt; manually. You can try installing &lt;code&gt;vllm&lt;/code&gt; and seeing if &lt;code&gt;vllm&lt;/code&gt; succeeds. Check if &lt;code&gt;xformers&lt;/code&gt; succeeded with &lt;code&gt;python -m xformers.info&lt;/code&gt; Go to &lt;a href=&quot;https://github.com/facebookresearch/xformers&quot;&gt;https://github.com/facebookresearch/xformers&lt;/a&gt;. Another option is to install &lt;code&gt;flash-attn&lt;/code&gt; for Ampere GPUs.&lt;/li&gt; 
 &lt;li&gt;Double check that your versions of Python, CUDA, CUDNN, &lt;code&gt;torch&lt;/code&gt;, &lt;code&gt;triton&lt;/code&gt;, and &lt;code&gt;xformers&lt;/code&gt; are compatible with one another. The &lt;a href=&quot;https://github.com/pytorch/pytorch/raw/main/RELEASE.md#release-compatibility-matrix&quot;&gt;PyTorch Compatibility Matrix&lt;/a&gt; may be useful.&lt;/li&gt; 
 &lt;li&gt;Finally, install &lt;code&gt;bitsandbytes&lt;/code&gt; and check it with &lt;code&gt;python -m bitsandbytes&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Conda Installation (Optional)&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;⚠️Only use Conda if you have it. If not, use Pip&lt;/code&gt;. Select either &lt;code&gt;pytorch-cuda=11.8,12.1&lt;/code&gt; for CUDA 11.8 or CUDA 12.1. We support &lt;code&gt;python=3.10,3.11,3.12&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;conda create --name unsloth_env \
    python=3.11 \
    pytorch-cuda=12.1 \
    pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers \
    -y
conda activate unsloth_env

pip install unsloth
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;If you&#39;re looking to install Conda in a Linux environment, &lt;a href=&quot;https://docs.anaconda.com/miniconda/&quot;&gt;read here&lt;/a&gt;, or run the below 🔽&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;mkdir -p ~/miniconda3
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh
bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3
rm -rf ~/miniconda3/miniconda.sh
~/miniconda3/bin/conda init bash
~/miniconda3/bin/conda init zsh
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;Advanced Pip Installation&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;⚠️Do **NOT** use this if you have Conda.&lt;/code&gt; Pip is a bit more complex since there are dependency issues. The pip command is different for &lt;code&gt;torch 2.2,2.3,2.4,2.5&lt;/code&gt; and CUDA versions.&lt;/p&gt; 
&lt;p&gt;For other torch versions, we support &lt;code&gt;torch211&lt;/code&gt;, &lt;code&gt;torch212&lt;/code&gt;, &lt;code&gt;torch220&lt;/code&gt;, &lt;code&gt;torch230&lt;/code&gt;, &lt;code&gt;torch240&lt;/code&gt; and for CUDA versions, we support &lt;code&gt;cu118&lt;/code&gt; and &lt;code&gt;cu121&lt;/code&gt; and &lt;code&gt;cu124&lt;/code&gt;. For Ampere devices (A100, H100, RTX3090) and above, use &lt;code&gt;cu118-ampere&lt;/code&gt; or &lt;code&gt;cu121-ampere&lt;/code&gt; or &lt;code&gt;cu124-ampere&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;For example, if you have &lt;code&gt;torch 2.4&lt;/code&gt; and &lt;code&gt;CUDA 12.1&lt;/code&gt;, use:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install --upgrade pip
pip install &quot;unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Another example, if you have &lt;code&gt;torch 2.5&lt;/code&gt; and &lt;code&gt;CUDA 12.4&lt;/code&gt;, use:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install --upgrade pip
pip install &quot;unsloth[cu124-torch250] @ git+https://github.com/unslothai/unsloth.git&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;And other examples:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install &quot;unsloth[cu121-ampere-torch240] @ git+https://github.com/unslothai/unsloth.git&quot;
pip install &quot;unsloth[cu118-ampere-torch240] @ git+https://github.com/unslothai/unsloth.git&quot;
pip install &quot;unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git&quot;
pip install &quot;unsloth[cu118-torch240] @ git+https://github.com/unslothai/unsloth.git&quot;

pip install &quot;unsloth[cu121-torch230] @ git+https://github.com/unslothai/unsloth.git&quot;
pip install &quot;unsloth[cu121-ampere-torch230] @ git+https://github.com/unslothai/unsloth.git&quot;

pip install &quot;unsloth[cu121-torch250] @ git+https://github.com/unslothai/unsloth.git&quot;
pip install &quot;unsloth[cu124-ampere-torch250] @ git+https://github.com/unslothai/unsloth.git&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or, run the below in a terminal to get the &lt;strong&gt;optimal&lt;/strong&gt; pip installation command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;wget -qO- https://raw.githubusercontent.com/unslothai/unsloth/main/unsloth/_auto_install.py | python -
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or, run the below manually in a Python REPL:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;try: import torch
except: raise ImportError(&#39;Install torch via `pip install torch`&#39;)
from packaging.version import Version as V
v = V(torch.__version__)
cuda = str(torch.version.cuda)
is_ampere = torch.cuda.get_device_capability()[0] &amp;gt;= 8
if cuda != &quot;12.1&quot; and cuda != &quot;11.8&quot; and cuda != &quot;12.4&quot;: raise RuntimeError(f&quot;CUDA = {cuda} not supported!&quot;)
if   v &amp;lt;= V(&#39;2.1.0&#39;): raise RuntimeError(f&quot;Torch = {v} too old!&quot;)
elif v &amp;lt;= V(&#39;2.1.1&#39;): x = &#39;cu{}{}-torch211&#39;
elif v &amp;lt;= V(&#39;2.1.2&#39;): x = &#39;cu{}{}-torch212&#39;
elif v  &amp;lt; V(&#39;2.3.0&#39;): x = &#39;cu{}{}-torch220&#39;
elif v  &amp;lt; V(&#39;2.4.0&#39;): x = &#39;cu{}{}-torch230&#39;
elif v  &amp;lt; V(&#39;2.5.0&#39;): x = &#39;cu{}{}-torch240&#39;
elif v  &amp;lt; V(&#39;2.6.0&#39;): x = &#39;cu{}{}-torch250&#39;
else: raise RuntimeError(f&quot;Torch = {v} too new!&quot;)
x = x.format(cuda.replace(&quot;.&quot;, &quot;&quot;), &quot;-ampere&quot; if is_ampere else &quot;&quot;)
print(f&#39;pip install --upgrade pip &amp;amp;&amp;amp; pip install &quot;unsloth[{x}] @ git+https://github.com/unslothai/unsloth.git&quot;&#39;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;📜 Documentation&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Go to our official &lt;a href=&quot;https://docs.unsloth.ai&quot;&gt;Documentation&lt;/a&gt; for saving to GGUF, checkpointing, evaluation and more!&lt;/li&gt; 
 &lt;li&gt;We support Huggingface&#39;s TRL, Trainer, Seq2SeqTrainer or even Pytorch code!&lt;/li&gt; 
 &lt;li&gt;We&#39;re in 🤗Hugging Face&#39;s official docs! Check out the &lt;a href=&quot;https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth&quot;&gt;SFT docs&lt;/a&gt; and &lt;a href=&quot;https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth&quot;&gt;DPO docs&lt;/a&gt;!&lt;/li&gt; 
 &lt;li&gt;If you want to download models from the ModelScope community, please use an environment variable: &lt;code&gt;UNSLOTH_USE_MODELSCOPE=1&lt;/code&gt;, and install the modelscope library by: &lt;code&gt;pip install modelscope -U&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;unsloth_cli.py also supports &lt;code&gt;UNSLOTH_USE_MODELSCOPE=1&lt;/code&gt; to download models and datasets. please remember to use the model and dataset id in the ModelScope community.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from unsloth import FastLanguageModel 
import torch
from trl import SFTTrainer, SFTConfig
from datasets import load_dataset
max_seq_length = 2048 # Supports RoPE Scaling interally, so choose any!
# Get LAION dataset
url = &quot;https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl&quot;
dataset = load_dataset(&quot;json&quot;, data_files = {&quot;train&quot; : url}, split = &quot;train&quot;)

# 4bit pre quantized models we support for 4x faster downloading + no OOMs.
fourbit_models = [
    &quot;unsloth/Meta-Llama-3.1-8B-bnb-4bit&quot;,      # Llama-3.1 2x faster
    &quot;unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit&quot;,
    &quot;unsloth/Meta-Llama-3.1-70B-bnb-4bit&quot;,
    &quot;unsloth/Meta-Llama-3.1-405B-bnb-4bit&quot;,    # 4bit for 405b!
    &quot;unsloth/Mistral-Small-Instruct-2409&quot;,     # Mistral 22b 2x faster!
    &quot;unsloth/mistral-7b-instruct-v0.3-bnb-4bit&quot;,
    &quot;unsloth/Phi-3.5-mini-instruct&quot;,           # Phi-3.5 2x faster!
    &quot;unsloth/Phi-3-medium-4k-instruct&quot;,
    &quot;unsloth/gemma-2-9b-bnb-4bit&quot;,
    &quot;unsloth/gemma-2-27b-bnb-4bit&quot;,            # Gemma 2x faster!

    &quot;unsloth/Llama-3.2-1B-bnb-4bit&quot;,           # NEW! Llama 3.2 models
    &quot;unsloth/Llama-3.2-1B-Instruct-bnb-4bit&quot;,
    &quot;unsloth/Llama-3.2-3B-bnb-4bit&quot;,
    &quot;unsloth/Llama-3.2-3B-Instruct-bnb-4bit&quot;,

    &quot;unsloth/Llama-3.3-70B-Instruct-bnb-4bit&quot; # NEW! Llama 3.3 70B!
] # More models at https://huggingface.co/unsloth

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = &quot;unsloth/Llama-3.2-1B&quot;,
    max_seq_length = max_seq_length,
    load_in_4bit = True,
)

# Do model patching and add fast LoRA weights
model = FastLanguageModel.get_peft_model(
    model,
    r = 16,
    target_modules = [&quot;q_proj&quot;, &quot;k_proj&quot;, &quot;v_proj&quot;, &quot;o_proj&quot;,
                      &quot;gate_proj&quot;, &quot;up_proj&quot;, &quot;down_proj&quot;,],
    lora_alpha = 16,
    lora_dropout = 0, # Supports any, but = 0 is optimized
    bias = &quot;none&quot;,    # Supports any, but = &quot;none&quot; is optimized
    # [NEW] &quot;unsloth&quot; uses 30% less VRAM, fits 2x larger batch sizes!
    use_gradient_checkpointing = &quot;unsloth&quot;, # True or &quot;unsloth&quot; for very long context
    random_state = 3407,
    max_seq_length = max_seq_length,
    use_rslora = False,  # We support rank stabilized LoRA
    loftq_config = None, # And LoftQ
)

trainer = SFTTrainer(
    model = model,
    train_dataset = dataset,
    tokenizer = tokenizer,
    args = SFTConfig(
        dataset_text_field = &quot;text&quot;,
        max_seq_length = max_seq_length,
        per_device_train_batch_size = 2,
        gradient_accumulation_steps = 4,
        warmup_steps = 10,
        max_steps = 60,
        logging_steps = 1,
        output_dir = &quot;outputs&quot;,
        optim = &quot;adamw_8bit&quot;,
        seed = 3407,
    ),
)
trainer.train()

# Go to https://github.com/unslothai/unsloth/wiki for advanced tips like
# (1) Saving to GGUF / merging to 16bit for vLLM
# (2) Continued training from a saved LoRA adapter
# (3) Adding an evaluation loop / OOMs
# (4) Customized chat templates
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a name=&quot;RL&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;💡 Reinforcement Learning&lt;/h2&gt; 
&lt;p&gt;RL including DPO, GRPO, PPO, Reward Modelling, Online DPO all work with Unsloth. We&#39;re in 🤗Hugging Face&#39;s official docs! We&#39;re on the &lt;a href=&quot;https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth&quot;&gt;SFT docs&lt;/a&gt; and the &lt;a href=&quot;https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth&quot;&gt;DPO docs&lt;/a&gt;! List of RL notebooks:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ORPO notebook: &lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-ORPO.ipynb&quot;&gt;Link&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;DPO Zephyr notebook: &lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Zephyr_(7B)-DPO.ipynb&quot;&gt;Link&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;KTO notebook: &lt;a href=&quot;https://colab.research.google.com/drive/1a2b3c4d5e6f7g8h9i0j&quot;&gt;Link&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;SimPO notebook: &lt;a href=&quot;https://colab.research.google.com/drive/1a2b3c4d5e6f7g8h9i0j&quot;&gt;Link&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;Click for DPO code&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import os
os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = &quot;0&quot; # Optional set GPU device ID

from unsloth import FastLanguageModel
import torch
from trl import DPOTrainer, DPOConfig
max_seq_length = 2048

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = &quot;unsloth/zephyr-sft-bnb-4bit&quot;,
    max_seq_length = max_seq_length,
    load_in_4bit = True,
)

# Do model patching and add fast LoRA weights
model = FastLanguageModel.get_peft_model(
    model,
    r = 64,
    target_modules = [&quot;q_proj&quot;, &quot;k_proj&quot;, &quot;v_proj&quot;, &quot;o_proj&quot;,
                      &quot;gate_proj&quot;, &quot;up_proj&quot;, &quot;down_proj&quot;,],
    lora_alpha = 64,
    lora_dropout = 0, # Supports any, but = 0 is optimized
    bias = &quot;none&quot;,    # Supports any, but = &quot;none&quot; is optimized
    # [NEW] &quot;unsloth&quot; uses 30% less VRAM, fits 2x larger batch sizes!
    use_gradient_checkpointing = &quot;unsloth&quot;, # True or &quot;unsloth&quot; for very long context
    random_state = 3407,
    max_seq_length = max_seq_length,
)

dpo_trainer = DPOTrainer(
    model = model,
    ref_model = None,
    train_dataset = YOUR_DATASET_HERE,
    # eval_dataset = YOUR_DATASET_HERE,
    tokenizer = tokenizer,
    args = DPOConfig(
        per_device_train_batch_size = 4,
        gradient_accumulation_steps = 8,
        warmup_ratio = 0.1,
        num_train_epochs = 3,
        logging_steps = 1,
        optim = &quot;adamw_8bit&quot;,
        seed = 42,
        output_dir = &quot;outputs&quot;,
        max_length = 1024,
        max_prompt_length = 512,
        beta = 0.1,
    ),
)
dpo_trainer.train()
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;🥇 Performance Benchmarking&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;For our most detailed benchmarks, read our &lt;a href=&quot;https://unsloth.ai/blog/llama3-3&quot;&gt;Llama 3.3 Blog&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Benchmarking of Unsloth was also conducted by &lt;a href=&quot;https://huggingface.co/blog/unsloth-trl&quot;&gt;🤗Hugging Face&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We tested using the Alpaca Dataset, a batch size of 2, gradient accumulation steps of 4, rank = 32, and applied QLoRA on all linear layers (q, k, v, o, gate, up, down):&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;VRAM&lt;/th&gt; 
   &lt;th&gt;🦥 Unsloth speed&lt;/th&gt; 
   &lt;th&gt;🦥 VRAM reduction&lt;/th&gt; 
   &lt;th&gt;🦥 Longer context&lt;/th&gt; 
   &lt;th&gt;😊 Hugging Face + FA2&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Llama 3.3 (70B)&lt;/td&gt; 
   &lt;td&gt;80GB&lt;/td&gt; 
   &lt;td&gt;2x&lt;/td&gt; 
   &lt;td&gt;&amp;gt;75%&lt;/td&gt; 
   &lt;td&gt;13x longer&lt;/td&gt; 
   &lt;td&gt;1x&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Llama 3.1 (8B)&lt;/td&gt; 
   &lt;td&gt;80GB&lt;/td&gt; 
   &lt;td&gt;2x&lt;/td&gt; 
   &lt;td&gt;&amp;gt;70%&lt;/td&gt; 
   &lt;td&gt;12x longer&lt;/td&gt; 
   &lt;td&gt;1x&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Context length benchmarks&lt;/h3&gt; 
&lt;h4&gt;Llama 3.1 (8B) max. context length&lt;/h4&gt; 
&lt;p&gt;We tested Llama 3.1 (8B) Instruct and did 4bit QLoRA on all linear layers (Q, K, V, O, gate, up and down) with rank = 32 with a batch size of 1. We padded all sequences to a certain maximum sequence length to mimic long context finetuning workloads.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;GPU VRAM&lt;/th&gt; 
   &lt;th&gt;🦥Unsloth context length&lt;/th&gt; 
   &lt;th&gt;Hugging Face + FA2&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;8 GB&lt;/td&gt; 
   &lt;td&gt;2,972&lt;/td&gt; 
   &lt;td&gt;OOM&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;12 GB&lt;/td&gt; 
   &lt;td&gt;21,848&lt;/td&gt; 
   &lt;td&gt;932&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;16 GB&lt;/td&gt; 
   &lt;td&gt;40,724&lt;/td&gt; 
   &lt;td&gt;2,551&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;24 GB&lt;/td&gt; 
   &lt;td&gt;78,475&lt;/td&gt; 
   &lt;td&gt;5,789&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;40 GB&lt;/td&gt; 
   &lt;td&gt;153,977&lt;/td&gt; 
   &lt;td&gt;12,264&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;48 GB&lt;/td&gt; 
   &lt;td&gt;191,728&lt;/td&gt; 
   &lt;td&gt;15,502&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;80 GB&lt;/td&gt; 
   &lt;td&gt;342,733&lt;/td&gt; 
   &lt;td&gt;28,454&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h4&gt;Llama 3.3 (70B) max. context length&lt;/h4&gt; 
&lt;p&gt;We tested Llama 3.3 (70B) Instruct on a 80GB A100 and did 4bit QLoRA on all linear layers (Q, K, V, O, gate, up and down) with rank = 32 with a batch size of 1. We padded all sequences to a certain maximum sequence length to mimic long context finetuning workloads.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;GPU VRAM&lt;/th&gt; 
   &lt;th&gt;🦥Unsloth context length&lt;/th&gt; 
   &lt;th&gt;Hugging Face + FA2&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;48 GB&lt;/td&gt; 
   &lt;td&gt;12,106&lt;/td&gt; 
   &lt;td&gt;OOM&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;80 GB&lt;/td&gt; 
   &lt;td&gt;89,389&lt;/td&gt; 
   &lt;td&gt;6,916&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;br&gt; 
&lt;p&gt;&lt;img src=&quot;https://i.ibb.co/sJ7RhGG/image-41.png&quot; alt=&quot;&quot;&gt; &lt;br&gt;&lt;/p&gt; 
&lt;h3&gt;Citation&lt;/h3&gt; 
&lt;p&gt;You can cite the Unsloth repo as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bibtex&quot;&gt;@software{unsloth,
  author = {Daniel Han, Michael Han and Unsloth team},
  title = {Unsloth},
  url = {http://github.com/unslothai/unsloth},
  year = {2023}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Thank You to&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Hugging Face&#39;s &lt;a href=&quot;https://github.com/huggingface/trl&quot;&gt;TRL library&lt;/a&gt; which serves as the basis foundation for Unsloth&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/erikwijmans&quot;&gt;Erik&lt;/a&gt; for his help adding &lt;a href=&quot;https://github.com/apple/ml-cross-entropy&quot;&gt;Apple&#39;s ML Cross Entropy&lt;/a&gt; in Unsloth&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/HuyNguyen-hust&quot;&gt;HuyNguyen-hust&lt;/a&gt; for making &lt;a href=&quot;https://github.com/unslothai/unsloth/pull/238&quot;&gt;RoPE Embeddings 28% faster&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/RandomInternetPreson&quot;&gt;RandomInternetPreson&lt;/a&gt; for confirming WSL support&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/152334H&quot;&gt;152334H&lt;/a&gt; for experimental DPO support&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>akfamily/akshare</title>
      <link>https://github.com/akfamily/akshare</link>
      <description>&lt;p&gt;AKShare is an elegant and simple financial data interface library for Python, built for human beings! 开源财经数据接口库&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;strong&gt;欢迎加入专注于财经数据和量化投资的知识社区，获取《AKShare-财经数据宝典》，其汇集了财经数据的使用经验和指南，还独家分享了 众多国内外财经数据源的使用和注意事项，请点击&lt;a href=&quot;https://akshare.akfamily.xyz/learn.html&quot;&gt;了解更多&lt;/a&gt;&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;量化投研视频课程：《PyBroker-入门及实战》已经上架！《PyBroker-进阶及实战》正在更新！&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;更多视频教程已经发布：《AKShare-初阶-使用教学》、《AKShare-初阶-实战应用》、《AKShare-源码解析》、《开源项目巡礼》&lt;/strong&gt;， 详情请关注【数据科学实战】公众号，查看更多课程信息！&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;广告推广&lt;/strong&gt;：期魔方是一款非常专业本地化期货量化终端，无需部署或搭建环境，可直接调用及时和历史数据做回测实盘， 支持开箱即用的机器学习训练，策略投研、回测均免费，详情请&lt;a href=&quot;https://qmfquant.com&quot;&gt;访问期魔方官网&lt;/a&gt;。&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://github.com/akfamily/akshare/raw/main/assets/images/akshare_logo.jpg&quot; alt=&quot;AKShare Logo&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://pypi.org/project/akshare/&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/pyversions/akshare.svg?sanitize=true&quot; alt=&quot;PyPI - Python Version&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://pypi.org/project/akshare/&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/v/akshare.svg?sanitize=true&quot; alt=&quot;PyPI&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://pepy.tech/project/akshare&quot;&gt;&lt;img src=&quot;https://pepy.tech/badge/akshare&quot; alt=&quot;Downloads&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://akshare.readthedocs.io/?badge=latest&quot;&gt;&lt;img src=&quot;https://readthedocs.org/projects/akshare/badge/?version=latest&quot; alt=&quot;Documentation Status&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/astral-sh/ruff&quot;&gt;&lt;img src=&quot;https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json&quot; alt=&quot;Ruff&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/akfamily/akshare&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Data%20Science-AKShare-green&quot; alt=&quot;akshare&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/akfamily/akshare/actions&quot;&gt;&lt;img src=&quot;https://github.com/akfamily/akshare/actions/workflows/release_and_deploy.yml/badge.svg?sanitize=true&quot; alt=&quot;Actions Status&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/akfamily/akshare/raw/main/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/license-MIT-blue&quot; alt=&quot;MIT Licence&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/akfamily/akshare&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/forks/jindaxiang/akshare&quot; alt=&quot;&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/akfamily/akshare&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/jindaxiang/akshare&quot; alt=&quot;&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/akfamily/akshare&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/issues/jindaxiang/akshare&quot; alt=&quot;&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/prettier/prettier&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/code_style-prettier-ff69b4.svg?style=flat-square&quot; alt=&quot;code style: prettier&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Overview&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/akfamily/akshare&quot;&gt;AKShare&lt;/a&gt; requires Python(64 bit) 3.8 or higher and aims to simplify the process of fetching financial data.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Write less, get more!&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Documentation: &lt;a href=&quot;https://akshare.akfamily.xyz/&quot;&gt;中文文档&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h3&gt;General&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;pip install akshare --upgrade
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;China&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;pip install akshare -i http://mirrors.aliyun.com/pypi/simple/ --trusted-host=mirrors.aliyun.com  --upgrade
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;PR&lt;/h3&gt; 
&lt;p&gt;Please check out &lt;a href=&quot;https://akshare.akfamily.xyz/contributing.html&quot;&gt;Documentation&lt;/a&gt; if you want to contribute to AKShare&lt;/p&gt; 
&lt;h3&gt;Docker&lt;/h3&gt; 
&lt;h4&gt;Pull images&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;docker pull registry.cn-shanghai.aliyuncs.com/akfamily/aktools:jupyter
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Run Container&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;docker run -it registry.cn-shanghai.aliyuncs.com/akfamily/aktools:jupyter python
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Test&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import akshare as ak

print(ak.__version__)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;h3&gt;Data&lt;/h3&gt; 
&lt;p&gt;Code:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import akshare as ak

stock_zh_a_hist_df = ak.stock_zh_a_hist(symbol=&quot;000001&quot;, period=&quot;daily&quot;, start_date=&quot;20170301&quot;, end_date=&#39;20231022&#39;, adjust=&quot;&quot;)
print(stock_zh_a_hist_df)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Output:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;      日期          开盘   收盘    最高  ...  振幅   涨跌幅  涨跌额  换手率
0     2017-03-01   9.49   9.49   9.55  ...  0.84  0.11  0.01  0.21
1     2017-03-02   9.51   9.43   9.54  ...  1.26 -0.63 -0.06  0.24
2     2017-03-03   9.41   9.40   9.43  ...  0.74 -0.32 -0.03  0.20
3     2017-03-06   9.40   9.45   9.46  ...  0.74  0.53  0.05  0.24
4     2017-03-07   9.44   9.45   9.46  ...  0.63  0.00  0.00  0.17
          ...    ...    ...    ...  ...   ...   ...   ...   ...
1610  2023-10-16  11.00  11.01  11.03  ...  0.73  0.09  0.01  0.26
1611  2023-10-17  11.01  11.02  11.05  ...  0.82  0.09  0.01  0.25
1612  2023-10-18  10.99  10.95  11.02  ...  1.00 -0.64 -0.07  0.34
1613  2023-10-19  10.91  10.60  10.92  ...  3.01 -3.20 -0.35  0.61
1614  2023-10-20  10.55  10.60  10.67  ...  1.51  0.00  0.00  0.27
[1615 rows x 11 columns]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Plot&lt;/h3&gt; 
&lt;p&gt;Code:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import akshare as ak
import mplfinance as mpf  # Please install mplfinance as follows: pip install mplfinance

stock_us_daily_df = ak.stock_us_daily(symbol=&quot;AAPL&quot;, adjust=&quot;qfq&quot;)
stock_us_daily_df = stock_us_daily_df.set_index([&quot;date&quot;])
stock_us_daily_df = stock_us_daily_df[&quot;2020-04-01&quot;: &quot;2020-04-29&quot;]
mpf.plot(stock_us_daily_df, type=&quot;candle&quot;, mav=(3, 6, 9), volume=True, show_nontrading=False)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Output:&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://jfds-1252952517.cos.ap-chengdu.myqcloud.com/akshare/readme/home/AAPL_candle.png&quot; alt=&quot;KLine&quot;&gt;&lt;/p&gt; 
&lt;h2&gt;Communication&lt;/h2&gt; 
&lt;p&gt;Welcome to join the &lt;strong&gt;数据科学实战&lt;/strong&gt; knowledge planet to learn more about quantitative investment, please visit &lt;a href=&quot;https://akshare.akfamily.xyz/learn.html&quot;&gt;数据科学实战&lt;/a&gt; for more information:&lt;/p&gt; 
&lt;div&gt; 
 &lt;img alt=&quot;data science&quot; src=&quot;https://jfds-1252952517.cos.ap-chengdu.myqcloud.com/akshare/readme/qrcode/data_scientist.png&quot;&gt; 
&lt;/div&gt; 
&lt;p&gt;Pay attention to &lt;strong&gt;数据科学实战&lt;/strong&gt; WeChat Official Accounts to get the &lt;a href=&quot;https://github.com/akfamily/akshare&quot;&gt;AKShare&lt;/a&gt; updated info:&lt;/p&gt; 
&lt;div&gt; 
 &lt;img alt=&quot;ds&quot; src=&quot;https://github.com/akfamily/akshare/raw/main/assets/images/ds.png&quot;&gt; 
&lt;/div&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Easy of use&lt;/strong&gt;: Just one line code to fetch the data;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Extensible&lt;/strong&gt;: Easy to customize your own code with other application;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Powerful&lt;/strong&gt;: Python ecosystem.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Tutorials&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href=&quot;https://akshare.akfamily.xyz/introduction.html&quot;&gt;Overview&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://akshare.akfamily.xyz/installation.html&quot;&gt;Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://akshare.akfamily.xyz/tutorial.html&quot;&gt;Tutorial&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://akshare.akfamily.xyz/data/index.html&quot;&gt;Data Dict&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://akshare.akfamily.xyz/topic/index.html&quot;&gt;Subjects&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Contribution&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/akfamily/akshare&quot;&gt;AKShare&lt;/a&gt; is still under developing, feel free to open issues and pull requests:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Report or fix bugs&lt;/li&gt; 
 &lt;li&gt;Require or publish interface&lt;/li&gt; 
 &lt;li&gt;Write or fix documentation&lt;/li&gt; 
 &lt;li&gt;Add test cases&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Notice: We use &lt;a href=&quot;https://github.com/astral-sh/ruff&quot;&gt;Ruff&lt;/a&gt; to format the code&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Statement&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;All data provided by &lt;a href=&quot;https://github.com/akfamily/akshare&quot;&gt;AKShare&lt;/a&gt; is just for academic research purpose;&lt;/li&gt; 
 &lt;li&gt;The data provided by &lt;a href=&quot;https://github.com/akfamily/akshare&quot;&gt;AKShare&lt;/a&gt; is for reference only and does not constitute any investment proposal;&lt;/li&gt; 
 &lt;li&gt;Any investor based on &lt;a href=&quot;https://github.com/akfamily/akshare&quot;&gt;AKShare&lt;/a&gt; research should pay more attention to data risk;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/akfamily/akshare&quot;&gt;AKShare&lt;/a&gt; will insist on providing open-source financial data;&lt;/li&gt; 
 &lt;li&gt;Based on some uncontrollable factors, some data interfaces in &lt;a href=&quot;https://github.com/akfamily/akshare&quot;&gt;AKShare&lt;/a&gt; may be removed;&lt;/li&gt; 
 &lt;li&gt;Please follow the relevant open-source protocol used by &lt;a href=&quot;https://github.com/akfamily/akshare&quot;&gt;AKShare&lt;/a&gt;;&lt;/li&gt; 
 &lt;li&gt;Provide HTTP API for the person who uses other program language: &lt;a href=&quot;https://aktools.readthedocs.io/&quot;&gt;AKTools&lt;/a&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Show your style&lt;/h2&gt; 
&lt;p&gt;Use the badge in your project&#39;s README.md:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-markdown&quot;&gt;[![Data: akshare](https://img.shields.io/badge/Data%20Science-AKShare-green)](https://github.com/akfamily/akshare)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Using the badge in README.rst:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;.. image:: https://img.shields.io/badge/Data%20Science-AKShare-green
    :target: https://github.com/akfamily/akshare
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Looks like this:&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/akfamily/akshare&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Data%20Science-AKShare-green&quot; alt=&quot;Data: akshare&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;Please use this &lt;strong&gt;bibtex&lt;/strong&gt; if you want to cite this repository in your publications:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-markdown&quot;&gt;@misc{akshare,
    author = {Albert King},
    title = {AKShare},
    year = {2019},
    publisher = {GitHub},
    journal = {GitHub repository},
    howpublished = {\url{https://github.com/akfamily/akshare}},
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Acknowledgement&lt;/h2&gt; 
&lt;p&gt;Special thanks &lt;a href=&quot;https://github.com/LowinLi/fushare&quot;&gt;FuShare&lt;/a&gt; for the opportunity of learning from the project;&lt;/p&gt; 
&lt;p&gt;Special thanks &lt;a href=&quot;https://github.com/waditu/tushare&quot;&gt;TuShare&lt;/a&gt; for the opportunity of learning from the project;&lt;/p&gt; 
&lt;p&gt;Thanks for the data provided by &lt;a href=&quot;http://www.100ppi.com/&quot;&gt;生意社网站&lt;/a&gt;;&lt;/p&gt; 
&lt;p&gt;Thanks for the data provided by &lt;a href=&quot;http://www.nafmii.org.cn/&quot;&gt;中国银行间市场交易商协会网站&lt;/a&gt;;&lt;/p&gt; 
&lt;p&gt;Thanks for the data provided by &lt;a href=&quot;http://www.99qh.com/&quot;&gt;99期货网站&lt;/a&gt;;&lt;/p&gt; 
&lt;p&gt;Thanks for the data provided by &lt;a href=&quot;http://www.chinamoney.com.cn/chinese/&quot;&gt;中国外汇交易中心暨全国银行间同业拆借中心网站&lt;/a&gt;;&lt;/p&gt; 
&lt;p&gt;Thanks for the data provided by &lt;a href=&quot;https://www.jin10.com/&quot;&gt;金十数据网站&lt;/a&gt;;&lt;/p&gt; 
&lt;p&gt;Thanks for the data provided by &lt;a href=&quot;http://www.hexun.com/&quot;&gt;和讯财经网站&lt;/a&gt;;&lt;/p&gt; 
&lt;p&gt;Thanks for the data provided by &lt;a href=&quot;https://finance.sina.com.cn/&quot;&gt;新浪财经网站&lt;/a&gt;;&lt;/p&gt; 
&lt;p&gt;Thanks for the data provided by &lt;a href=&quot;https://dachxiu.chicagobooth.edu/&quot;&gt;DACHENG-XIU 网站&lt;/a&gt;;&lt;/p&gt; 
&lt;p&gt;Thanks for the data provided by &lt;a href=&quot;http://www.sse.com.cn/assortment/options/price/&quot;&gt;上海证券交易所网站&lt;/a&gt;;&lt;/p&gt; 
&lt;p&gt;Thanks for the data provided by &lt;a href=&quot;http://www.szse.cn/&quot;&gt;深证证券交易所网站&lt;/a&gt;;&lt;/p&gt; 
&lt;p&gt;Thanks for the data provided by &lt;a href=&quot;http://www.bse.cn/&quot;&gt;北京证券交易所网站&lt;/a&gt;;&lt;/p&gt; 
&lt;p&gt;Thanks for the data provided by &lt;a href=&quot;http://www.cffex.com.cn/&quot;&gt;中国金融期货交易所网站&lt;/a&gt;;&lt;/p&gt; 
&lt;p&gt;Thanks for the data provided by &lt;a href=&quot;http://www.shfe.com.cn/&quot;&gt;上海期货交易所网站&lt;/a&gt;;&lt;/p&gt; 
&lt;p&gt;Thanks for the data provided by &lt;a href=&quot;http://www.dce.com.cn/&quot;&gt;大连商品交易所网站&lt;/a&gt;;&lt;/p&gt; 
&lt;p&gt;Thanks for the data provided by &lt;a href=&quot;http://www.czce.com.cn/&quot;&gt;郑州商品交易所网站&lt;/a&gt;;&lt;/p&gt; 
&lt;p&gt;Thanks for the data provided by &lt;a href=&quot;http://www.ine.com.cn/&quot;&gt;上海国际能源交易中心网站&lt;/a&gt;;&lt;/p&gt; 
&lt;p&gt;Thanks for the data provided by &lt;a href=&quot;https://www.timeanddate.com/&quot;&gt;Timeanddate 网站&lt;/a&gt;;&lt;/p&gt; 
&lt;p&gt;Thanks for the data provided by &lt;a href=&quot;http://110.249.223.67/publish/&quot;&gt;河北省空气质量预报信息发布系统网站&lt;/a&gt;;&lt;/p&gt; 
&lt;p&gt;Thanks for the data provided by &lt;a href=&quot;http://www.nanhua.net/nhzc/varietytrend.html&quot;&gt;南华期货网站&lt;/a&gt;;&lt;/p&gt; 
&lt;p&gt;Thanks for the data provided by &lt;a href=&quot;http://www.nanhua.net/nhzc/varietytrend.html&quot;&gt;Economic Policy Uncertainty 网站&lt;/a&gt;;&lt;/p&gt; 
&lt;p&gt;Thanks for the data provided by &lt;a href=&quot;http://www.swsindex.com/idx0120.aspx?columnid=8832&quot;&gt;申万指数网站&lt;/a&gt;;&lt;/p&gt; 
&lt;p&gt;Thanks for the data provided by &lt;a href=&quot;https://www.zq12369.com/&quot;&gt;真气网网站&lt;/a&gt;;&lt;/p&gt; 
&lt;p&gt;Thanks for the data provided by &lt;a href=&quot;http://www.fortunechina.com/&quot;&gt;财富网站&lt;/a&gt;;&lt;/p&gt; 
&lt;p&gt;Thanks for the data provided by &lt;a href=&quot;http://gs.amac.org.cn/&quot;&gt;中国证券投资基金业协会网站&lt;/a&gt;;&lt;/p&gt; 
&lt;p&gt;Thanks for the data provided by &lt;a href=&quot;https://www.expatistan.com/cost-of-living&quot;&gt;Expatistan 网站&lt;/a&gt;;&lt;/p&gt; 
&lt;p&gt;Thanks for the data provided by &lt;a href=&quot;https://www.bjets.com.cn/article/jyxx/&quot;&gt;北京市碳排放权电子交易平台网站&lt;/a&gt;;&lt;/p&gt; 
&lt;p&gt;Thanks for the data provided by &lt;a href=&quot;http://www.nifd.cn/&quot;&gt;国家金融与发展实验室网站&lt;/a&gt;;&lt;/p&gt; 
&lt;p&gt;Thanks for the data provided by &lt;a href=&quot;http://data.eastmoney.com/jgdy/&quot;&gt;东方财富网站&lt;/a&gt;;&lt;/p&gt; 
&lt;p&gt;Thanks for the data provided by &lt;a href=&quot;http://www.ywindex.com/Home/Product/index/&quot;&gt;义乌小商品指数网站&lt;/a&gt;;&lt;/p&gt; 
&lt;p&gt;Thanks for the data provided by &lt;a href=&quot;https://qianxi.baidu.com/?from=shoubai#city=0&quot;&gt;百度迁徙网站&lt;/a&gt;;&lt;/p&gt; 
&lt;p&gt;Thanks for the data provided by &lt;a href=&quot;https://www.ownthink.com/&quot;&gt;思知网站&lt;/a&gt;;&lt;/p&gt; 
&lt;p&gt;Thanks for the data provided by &lt;a href=&quot;https://currencyscoop.com/&quot;&gt;Currencyscoop 网站&lt;/a&gt;;&lt;/p&gt; 
&lt;p&gt;Thanks for the data provided by &lt;a href=&quot;https://www.sgx.com/zh-hans/research-education/derivatives&quot;&gt;新加坡交易所网站&lt;/a&gt;;&lt;/p&gt; 
&lt;p&gt;Thanks for the tutorials provided by &lt;a href=&quot;https://upload-images.jianshu.io/upload_images/3240514-61004f2c71be4a0b.png&quot;&gt;微信公众号: Python大咖谈&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Backer and Sponsor&lt;/h2&gt; 
&lt;a href=&quot;https://www.jetbrains.com/?from=albertandking/akshare&quot; target=&quot;_blank&quot;&gt; &lt;img src=&quot;https://resources.jetbrains.com/storage/products/company/brand/logos/jetbrains.png&quot; alt=&quot;JetBrains logo.&quot;&gt; &lt;/a&gt;</description>
    </item>
    
    <item>
      <title>bregman-arie/devops-exercises</title>
      <link>https://github.com/bregman-arie/devops-exercises</link>
      <description>&lt;p&gt;Linux, Jenkins, AWS, SRE, Prometheus, Docker, Python, Ansible, Git, Kubernetes, Terraform, OpenStack, SQL, NoSQL, Azure, GCP, DNS, Elastic, Network, Virtualization. DevOps Interview Questions&lt;/p&gt;&lt;hr&gt;&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/devops_exercises.png&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;ℹ&lt;/span&gt; &amp;nbsp;This repo contains questions and exercises on various technical topics, sometimes related to DevOps and SRE&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;📊&lt;/span&gt; &amp;nbsp;There are currently &lt;strong&gt;2624&lt;/strong&gt; exercises and questions&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;⚠&lt;/span&gt; &amp;nbsp;You can use these for preparing for an interview but most of the questions and exercises don&#39;t represent an actual interview. Please read &lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/faq.md&quot;&gt;FAQ page&lt;/a&gt; for more details&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;🛑&lt;/span&gt; &amp;nbsp;If you are interested in pursuing a career as DevOps engineer, learning some of the concepts mentioned here would be useful, but you should know it&#39;s not about learning all the topics and technologies mentioned in this repository&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;📝&lt;/span&gt; &amp;nbsp;You can add more exercises by submitting pull requests :) Read about contribution guidelines &lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/CONTRIBUTING.md&quot;&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;hr&gt; 
&lt;!-- ALL-TOPICS-LIST:START --&gt; 
&lt;!-- prettier-ignore-start --&gt; 
&lt;!-- markdownlint-disable --&gt; 
&lt;center&gt; 
 &lt;table&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/devops/README.md&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/devops.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;DevOps&quot;&gt;&lt;br&gt;&lt;b&gt;DevOps&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/git/README.md&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/git.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Git&quot;&gt;&lt;br&gt;&lt;b&gt;Git&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/#network&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/network.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Network&quot;&gt;&lt;br&gt;&lt;b&gt;Network&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/#hardware&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/hardware.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Hardware&quot;&gt;&lt;br&gt;&lt;b&gt;Hardware&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/kubernetes/README.md&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/kubernetes.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;kubernetes&quot;&gt;&lt;br&gt;&lt;b&gt;Kubernetes&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/software_development/README.md&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/programming.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;programming&quot;&gt;&lt;br&gt;&lt;b&gt;Software Development&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/bregman-arie/python-exercises&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/python.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Python&quot;&gt;&lt;br&gt;&lt;b&gt;Python&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/bregman-arie/go-exercises&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/Go.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;go&quot;&gt;&lt;br&gt;&lt;b&gt;Go&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/perl/README.md&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/perl.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;perl&quot;&gt;&lt;br&gt;&lt;b&gt;Perl&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/#regex&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/regex.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;RegEx&quot;&gt;&lt;br&gt;&lt;b&gt;Regex&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/cloud/README.md&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/cloud.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Cloud&quot;&gt;&lt;br&gt;&lt;b&gt;Cloud&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/aws/README.md&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/aws.png&quot; width=&quot;100px;&quot; height=&quot;75px;&quot; alt=&quot;aws&quot;&gt;&lt;br&gt;&lt;b&gt;AWS&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/azure/README.md&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/azure.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;azure&quot;&gt;&lt;br&gt;&lt;b&gt;Azure&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/gcp/README.md&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/googlecloud.png&quot; width=&quot;70px;&quot; height=&quot;70px;&quot; alt=&quot;Google Cloud Platform&quot;&gt;&lt;br&gt;&lt;b&gt;Google Cloud Platform&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/#openstack/README.md&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/openstack.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;openstack&quot;&gt;&lt;br&gt;&lt;b&gt;OpenStack&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/#operating-system&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/os.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Operating System&quot;&gt;&lt;br&gt;&lt;b&gt;Operating System&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/linux/README.md&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/logos/linux.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Linux&quot;&gt;&lt;br&gt;&lt;b&gt;Linux&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/#virtualization&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/virtualization.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Virtualization&quot;&gt;&lt;br&gt;&lt;b&gt;Virtualization&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/dns/README.md&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/dns.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;DNS&quot;&gt;&lt;br&gt;&lt;b&gt;DNS&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/shell/README.md&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/bash.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Bash&quot;&gt;&lt;br&gt;&lt;b&gt;Shell Scripting&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/databases/README.md&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/databases.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Databases&quot;&gt;&lt;br&gt;&lt;b&gt;Databases&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/#sql&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/sql.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;sql&quot;&gt;&lt;br&gt;&lt;b&gt;SQL&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/#mongo&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/mongo.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Mongo&quot;&gt;&lt;br&gt;&lt;b&gt;Mongo&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/#testing&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/testing.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Testing&quot;&gt;&lt;br&gt;&lt;b&gt;Testing&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/#big-data&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/big-data.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Big Data&quot;&gt;&lt;br&gt;&lt;b&gt;Big Data&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/cicd/README.md&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/cicd.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;cicd&quot;&gt;&lt;br&gt;&lt;b&gt;CI/CD&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/#certificates&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/certificates.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Certificates&quot;&gt;&lt;br&gt;&lt;b&gt;Certificates&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/containers/README.md&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/containers.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Containers&quot;&gt;&lt;br&gt;&lt;b&gt;Containers&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/openshift/README.md&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/openshift.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;OpenShift&quot;&gt;&lt;br&gt;&lt;b&gt;OpenShift&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/#storage&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/storage.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Storage&quot;&gt;&lt;br&gt;&lt;b&gt;Storage&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/terraform/README.md&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/terraform.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Terraform&quot;&gt;&lt;br&gt;&lt;b&gt;Terraform&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/#puppet&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/puppet.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;puppet&quot;&gt;&lt;br&gt;&lt;b&gt;Puppet&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/#distributed&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/distributed.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Distributed&quot;&gt;&lt;br&gt;&lt;b&gt;Distributed&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/#questions-you-ask&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/you.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;you&quot;&gt;&lt;br&gt;&lt;b&gt;Questions you can ask&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/ansible/README.md&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/ansible.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;ansible&quot;&gt;&lt;br&gt;&lt;b&gt;Ansible&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/observability/README.md&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/observability.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;observability&quot;&gt;&lt;br&gt;&lt;b&gt;Observability&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/#prometheus&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/prometheus.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Prometheus&quot;&gt;&lt;br&gt;&lt;b&gt;Prometheus&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/circleci/README.md&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/logos/circleci.png&quot; width=&quot;70px;&quot; height=&quot;70px;&quot; alt=&quot;Circle CI&quot;&gt;&lt;br&gt;&lt;b&gt;Circle CI&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/datadog/README.md&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/logos/datadog.png&quot; width=&quot;80px;&quot; height=&quot;80px;&quot; alt=&quot;DataDog&quot;&gt;&lt;br&gt;&lt;b&gt;&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/grafana/README.md&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/logos/grafana.png&quot; width=&quot;80px;&quot; height=&quot;80px;&quot; alt=&quot;Grafana&quot;&gt;&lt;br&gt;&lt;b&gt;Grafana&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/argo/README.md&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/logos/argo.png&quot; width=&quot;80px;&quot; height=&quot;80px;&quot; alt=&quot;Argo&quot;&gt;&lt;br&gt;&lt;b&gt;Argo&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/soft_skills/README.md&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/HR.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;HR&quot;&gt;&lt;br&gt;&lt;b&gt;Soft Skills&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/security/README.md&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/security.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;security&quot;&gt;&lt;br&gt;&lt;b&gt;Security&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/#system-design&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/design.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Design&quot;&gt;&lt;br&gt;&lt;b&gt;System Design&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/chaos_engineering/README.md&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/logos/chaos_engineering.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Chaos Engineering&quot;&gt;&lt;br&gt;&lt;b&gt;Chaos Engineering&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/#Misc&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/general.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Misc&quot;&gt;&lt;br&gt;&lt;b&gt;Misc&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/#elastic&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/elastic.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Elastic&quot;&gt;&lt;br&gt;&lt;b&gt;Elastic&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/kafka/README.md&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/logos/kafka.png&quot; width=&quot;85px;&quot; height=&quot;80px;&quot; alt=&quot;Kafka&quot;&gt;&lt;br&gt;&lt;b&gt;Kafka&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/node/node_questions_basic.md&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/nodejs.png&quot; width=&quot;85px;&quot; height=&quot;80px;&quot; alt=&quot;NodeJs&quot;&gt;&lt;br&gt;&lt;b&gt;NodeJs&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
&lt;/center&gt; 
&lt;!-- markdownlint-enable --&gt; 
&lt;!-- prettier-ignore-end --&gt; 
&lt;!-- ALL-TOPICS-LIST:END --&gt; 
&lt;h2&gt;DevOps Applications&lt;/h2&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://play.google.com/store/apps/details?id=com.codingshell.kubeprep&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/apps/kubeprep.png&quot; width=&quot;200px;&quot; height=&quot;300px;&quot; alt=&quot;KubePrep&quot;&gt;&lt;br&gt;&lt;b&gt;KubePrep&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://play.google.com/store/apps/details?id=com.codingshell.linuxmaster&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/apps/linux_master.png&quot; width=&quot;200px;&quot; height=&quot;300px;&quot; alt=&quot;Linux Master&quot;&gt;&lt;br&gt;&lt;b&gt;Linux Master&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;Network&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;In general, what do you need in order to communicate?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; 
  &lt;ul&gt; 
   &lt;li&gt;A common language (for the two ends to understand)&lt;/li&gt; 
   &lt;li&gt;A way to address who you want to communicate with&lt;/li&gt; 
   &lt;li&gt;A Connection (so the content of the communication can reach the recipients)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is TCP/IP?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;A set of protocols that define how two or more devices can communicate with each other.&lt;/p&gt; &lt;p&gt;To learn more about TCP/IP, read &lt;a href=&quot;http://www.penguintutor.com/linux/basic-network-reference&quot;&gt;here&lt;/a&gt;&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is Ethernet?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;Ethernet simply refers to the most common type of Local Area Network (LAN) used today. A LAN—in contrast to a WAN (Wide Area Network), which spans a larger geographical area—is a connected network of computers in a small area, like your office, college campus, or even home.&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is a MAC address? What is it used for?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;A MAC address is a unique identification number or code used to identify individual devices on the network.&lt;/p&gt; &lt;p&gt;Packets that are sent on the ethernet are always coming from a MAC address and sent to a MAC address. If a network adapter is receiving a packet, it is comparing the packet’s destination MAC address to the adapter’s own MAC address.&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;When is this MAC address used?: ff:ff:ff:ff:ff:ff&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;When a device sends a packet to the broadcast MAC address (FF:FF:FF:FF:FF:FF​), it is delivered to all stations on the local network. Ethernet broadcasts are used to resolve IP addresses to MAC addresses (by ARP) at the data link layer. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is an IP address?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;An Internet Protocol address (IP address) is a numerical label assigned to each device connected to a computer network that uses the Internet Protocol for communication.An IP address serves two main functions: host or network interface identification and location addressing. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain the subnet mask and give an example&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;A Subnet mask is a 32-bit number that masks an IP address and divides the IP addresses into network addresses and host addresses. Subnet Mask is made by setting network bits to all &quot;1&quot;s and setting host bits to all &quot;0&quot;s. Within a given network, out of the total usable host addresses, two are always reserved for specific purposes and cannot be allocated to any host. These are the first address, which is reserved as a network address (a.k.a network ID), and the last address used for network broadcast.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/philemonnwanne/projects/tree/main/exercises/exe-09&quot;&gt;Example&lt;/a&gt;&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is a private IP address? In which scenarios/system designs, one should use it?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; Private IP addresses are assigned to the hosts in the same network to communicate with one another. As the name &quot;private&quot; suggests, the devices having the private IP addresses assigned can&#39;t be reached by the devices from any external network. For example, if I am living in a hostel and I want my hostel mates to join the game server I have hosted, I will ask them to join via my server&#39;s private IP address, since the network is local to the hostel. &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is a public IP address? In which scenarios/system designs, one should use it?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; A public IP address is a public-facing IP address. In the event that you were hosting a game server that you want your friends to join, you will give your friends your public IP address to allow their computers to identify and locate your network and server in order for the connection to take place. One time that you would not need to use a public-facing IP address is in the event that you were playing with friends who were connected to the same network as you, in that case, you would use a private IP address. In order for someone to be able to connect to your server that is located internally, you will have to set up a port forward to tell your router to allow traffic from the public domain into your network and vice versa. &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain the OSI model. What layers there are? What each layer is responsible for?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Application: user end (HTTP is here)&lt;/li&gt; 
   &lt;li&gt;Presentation: establishes context between application-layer entities (Encryption is here)&lt;/li&gt; 
   &lt;li&gt;Session: establishes, manages, and terminates the connections&lt;/li&gt; 
   &lt;li&gt;Transport: transfers variable-length data sequences from a source to a destination host (TCP &amp;amp; UDP are here)&lt;/li&gt; 
   &lt;li&gt;Network: transfers datagrams from one network to another (IP is here)&lt;/li&gt; 
   &lt;li&gt;Data link: provides a link between two directly connected nodes (MAC is here)&lt;/li&gt; 
   &lt;li&gt;Physical: the electrical and physical spec of the data connection (Bits are here)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;You can read more about the OSI model in &lt;a href=&quot;http://www.penguintutor.com/linux/basic-network-reference&quot;&gt;penguintutor.com&lt;/a&gt; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;For each of the following determines to which OSI layer it belongs: 
  &lt;ul&gt; 
   &lt;li&gt;Error correction&lt;/li&gt; 
   &lt;li&gt;Packets routing&lt;/li&gt; 
   &lt;li&gt;Cables and electrical signals&lt;/li&gt; 
   &lt;li&gt;MAC address&lt;/li&gt; 
   &lt;li&gt;IP address&lt;/li&gt; 
   &lt;li&gt;Terminate connections&lt;/li&gt; 
   &lt;li&gt;3 way handshake&lt;/li&gt;
  &lt;/ul&gt;&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;li&gt;Error correction - Data link&lt;/li&gt; &lt;li&gt;Packets routing - Network&lt;/li&gt; &lt;li&gt;Cables and electrical signals - Physical&lt;/li&gt; &lt;li&gt;MAC address - Data link&lt;/li&gt; &lt;li&gt;IP address - Network&lt;/li&gt; &lt;li&gt;Terminate connections - Session&lt;/li&gt; &lt;/b&gt;
 &lt;li&gt;&lt;b&gt;3-way handshake - Transport &lt;/b&gt;&lt;/li&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What delivery schemes are you familiar with?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;Unicast: One-to-one communication where there is one sender and one receiver.&lt;/p&gt; &lt;p&gt;Broadcast: Sending a message to everyone in the network. The address ff:ff:ff:ff:ff:ff is used for broadcasting. Two common protocols which use broadcast are ARP and DHCP.&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Multicast: Sending a message to a group of subscribers. It can be one-to-many or many-to-many. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is CSMA/CD? Is it used in modern ethernet networks?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;CSMA/CD stands for Carrier Sense Multiple Access / Collision Detection. Its primary focus is to manage access to a shared medium/bus where only one host can transmit at a given point in time.&lt;/p&gt; &lt;p&gt;CSMA/CD algorithm:&lt;/p&gt; &lt;/b&gt;
 &lt;ol&gt;
  &lt;b&gt; &lt;li&gt;Before sending a frame, it checks whether another host is already transmitting a frame.&lt;/li&gt; &lt;li&gt;If no one is transmitting, it starts transmitting the frame.&lt;/li&gt; &lt;li&gt;If two hosts transmit at the same time, we have a collision.&lt;/li&gt; &lt;li&gt;Both hosts stop sending the frame and they send everyone a &#39;jam signal&#39; notifying everyone that a collision occurred&lt;/li&gt; &lt;li&gt;They are waiting for a random time before sending it again&lt;/li&gt; &lt;li&gt;Once each host waited for a random time, they try to send the frame again and so the cycle starts again &lt;/li&gt;&lt;/b&gt;
 &lt;/ol&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;Describe the following network devices and the difference between them: 
  &lt;ul&gt; 
   &lt;li&gt;router&lt;/li&gt; 
   &lt;li&gt;switch&lt;/li&gt; 
   &lt;li&gt;hub&lt;/li&gt;
  &lt;/ul&gt;&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt;  &lt;p&gt;A router, switch, and hub are all network devices used to connect devices in a local area network (LAN). However, each device operates differently and has its specific use cases. Here is a brief description of each device and the differences between them:&lt;/p&gt; &lt;/b&gt;
 &lt;ol&gt;
  &lt;b&gt; &lt;li&gt;Router: a network device that connects multiple network segments together. It operates at the&amp;nbsp;network layer (Layer 3)&amp;nbsp;of the OSI model and uses routing protocols to direct data between networks. Routers use IP addresses to identify devices and route data packets to the correct destination.&lt;/li&gt; &lt;li&gt;Switch: a network device that connects multiple devices on a LAN. It operates at the&amp;nbsp;data link layer (Layer 2)&amp;nbsp;of the OSI model and uses MAC addresses to identify devices and direct data packets to the correct destination. Switches allow devices on the same network to communicate with each other more efficiently and can prevent data collisions that can occur when multiple devices send data simultaneously.&lt;/li&gt; &lt;li&gt;Hub: a network device that connects multiple devices through a single cable and is used to connect multiple devices without segmenting a network. However, unlike a switch, it operates at the&amp;nbsp;physical layer (Layer 1)&amp;nbsp;of the OSI model and simply broadcasts data packets to all devices connected to it, regardless of whether the device is the intended recipient or not. This means that data collisions can occur, and the network&#39;s efficiency can suffer as a result. Hubs are generally not used in modern network setups, as switches are more efficient and provide better network performance. &lt;/li&gt;&lt;/b&gt;
 &lt;/ol&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What is a &quot;Collision Domain&quot;?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; A collision domain is a network segment in which devices can potentially interfere with each other by attempting to transmit data at the same time. When two devices transmit data at the same time, it can cause a collision, resulting in lost or corrupted data. In a collision domain, all devices share the same bandwidth, and any device can potentially interfere with the transmission of data by other devices. &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is a &quot;Broadcast Domain&quot;?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; A broadcast domain is a network segment in which all devices can communicate with each other by sending broadcast messages. A broadcast message is a message that is sent to all devices in a network rather than a specific device. In a broadcast domain, all devices can receive and process broadcast messages, regardless of whether the message was intended for them or not. &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;three computers connected to a switch. How many collision domains are there? How many broadcast domains?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Three collision domains and one broadcast domain &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;How does a router work?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;A router is a physical or virtual appliance that passes information between two or more packet-switched computer networks. A router inspects a given data packet&#39;s destination Internet Protocol address (IP address), calculates the best way for it to reach its destination, and then forwards it accordingly.&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is NAT?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;Network Address Translation (NAT) is a process in which one or more local IP addresses are translated into one or more Global IP address and vice versa in order to provide Internet access to the local hosts.&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is a proxy? How does it work? What do we need it for?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;A proxy server acts as a gateway between you and the internet. It’s an intermediary server separating end users from the websites they browse.&lt;/p&gt; &lt;p&gt;If you’re using a proxy server, internet traffic flows through the proxy server on its way to the address you requested. The request then comes back through that same proxy server (there are exceptions to this rule), and then the proxy server forwards the data received from the website to you.&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Proxy servers provide varying levels of functionality, security, and privacy depending on your use case, needs, or company policy. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is TCP? How does it work? What is the 3-way handshake?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;TCP 3-way handshake or three-way handshake is a process that is used in a TCP/IP network to make a connection between server and client.&lt;/p&gt; &lt;p&gt;A three-way handshake is primarily used to create a TCP socket connection. It works when:&lt;/p&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;A client node sends an SYN data packet over an IP network to a server on the same or an external network. The objective of this packet is to ask/infer if the server is open for new connections.&lt;/li&gt; &lt;li&gt;The target server must have open ports that can accept and initiate new connections. When the server receives the SYN packet from the client node, it responds and returns a confirmation receipt – the ACK packet or SYN/ACK packet.&lt;/li&gt; &lt;li&gt;The client node receives the SYN/ACK from the server and responds with an ACK packet. &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What is round-trip delay or round-trip time?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;From &lt;a href=&quot;https://en.wikipedia.org/wiki/Round-trip_delay&quot;&gt;wikipedia&lt;/a&gt;: &quot;the length of time it takes for a signal to be sent plus the length of time it takes for an acknowledgment of that signal to be received&quot;&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Bonus question: what is the RTT of LAN? &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;How does an SSL handshake work?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; SSL handshake is a process that establishes a secure connection between a client and a server. &lt;/b&gt;
 &lt;ol&gt;
  &lt;b&gt; &lt;li&gt;The client sends a Client Hello message to the server, which includes the client&#39;s version of the SSL/TLS protocol, a list of the cryptographic algorithms supported by the client, and a random value.&lt;/li&gt; &lt;li&gt;The server responds with a Server Hello message, which includes the server&#39;s version of the SSL/TLS protocol, a random value, and a session ID.&lt;/li&gt; &lt;li&gt;The server sends a Certificate message, which contains the server&#39;s certificate.&lt;/li&gt; &lt;li&gt;The server sends a Server Hello Done message, which indicates that the server is done sending messages for the Server Hello phase.&lt;/li&gt; &lt;li&gt;The client sends a Client Key Exchange message, which contains the client&#39;s public key.&lt;/li&gt; &lt;li&gt;The client sends a Change Cipher Spec message, which notifies the server that the client is about to send a message encrypted with the new cipher spec.&lt;/li&gt; &lt;li&gt;The client sends an Encrypted Handshake Message, which contains the pre-master secret encrypted with the server&#39;s public key.&lt;/li&gt; &lt;li&gt;The server sends a Change Cipher Spec message, which notifies the client that the server is about to send a message encrypted with the new cipher spec.&lt;/li&gt; &lt;li&gt;The server sends an Encrypted Handshake Message, which contains the pre-master secret encrypted with the client&#39;s public key.&lt;/li&gt; &lt;li&gt;The client and server can now exchange application data. &lt;/li&gt;&lt;/b&gt;
 &lt;/ol&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What is the difference between TCP and UDP?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;TCP establishes a connection between the client and the server to guarantee the order of the packages, on the other hand, UDP does not establish a connection between the client and server and doesn&#39;t handle package orders. This makes UDP more lightweight than TCP and a perfect candidate for services like streaming.&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;a href=&quot;http://www.penguintutor.com/linux/basic-network-reference&quot;&gt;Penguintutor.com&lt;/a&gt; provides a good explanation. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What TCP/IP protocols are you familiar with?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain the &quot;default gateway&quot;&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;A default gateway serves as an access point or IP router that a networked computer uses to send information to a computer in another network or the internet. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is ARP? How does it work?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;ARP stands for Address Resolution Protocol. When you try to ping an IP address on your local network, say 192.168.1.1, your system has to turn the IP address 192.168.1.1 into a MAC address. This involves using ARP to resolve the address, hence its name.&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Systems keep an ARP look-up table where they store information about what IP addresses are associated with what MAC addresses. When trying to send a packet to an IP address, the system will first consult this table to see if it already knows the MAC address. If there is a value cached, ARP is not used. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is TTL? What does it help to prevent?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;TTL (Time to Live) is a value in an IP (Internet Protocol) packet that determines how many hops or routers a packet can travel before it is discarded. Each time a packet is forwarded by a router, the TTL value is decreased by one. When the TTL value reaches zero, the packet is dropped, and an ICMP (Internet Control Message Protocol) message is sent back to the sender indicating that the packet has expired.&lt;/li&gt; &lt;li&gt;TTL is used to prevent packets from circulating indefinitely in the network, which can cause congestion and degrade network performance.&lt;/li&gt; &lt;li&gt;It also helps to prevent packets from being trapped in routing loops, where packets continuously travel between the same set of routers without ever reaching their destination.&lt;/li&gt; &lt;li&gt;In addition, TTL can be used to help detect and prevent IP spoofing attacks, where an attacker attempts to impersonate another device on the network by using a false or fake IP address. By limiting the number of hops that a packet can travel, TTL can help prevent packets from being routed to destinations that are not legitimate. &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What is DHCP? How does it work?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;It stands for Dynamic Host Configuration Protocol and allocates IP addresses, subnet masks, and gateways to hosts. This is how it works:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;A host upon entering a network broadcasts a message in search of a DHCP server (DHCP DISCOVER)&lt;/li&gt; 
   &lt;li&gt;An offer message is sent back by the DHCP server as a packet containing lease time, subnet mask, IP addresses, etc (DHCP OFFER)&lt;/li&gt; 
   &lt;li&gt;Depending on which offer is accepted, the client sends back a reply broadcast letting all DHCP servers know (DHCP REQUEST)&lt;/li&gt; 
   &lt;li&gt;The server sends an acknowledgment (DHCP ACK)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Read more &lt;a href=&quot;https://linuxjourney.com/lesson/dhcp-overview&quot;&gt;here&lt;/a&gt; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Can you have two DHCP servers on the same network? How does it work?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;It is possible to have two DHCP servers on the same network, however, it is not recommended, and it is important to configure them carefully to prevent conflicts and configuration problems.&lt;/p&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;When two DHCP servers are configured on the same network, there is a&amp;nbsp;risk that both servers will assign IP addresses and other network configuration settings to the same device, which can cause conflicts and connectivity issues. Additionally, if the DHCP servers are configured with different network settings or options, devices on the network may receive conflicting or inconsistent configuration settings.&lt;/li&gt; &lt;li&gt;However, in some cases, it may be necessary to have two DHCP servers on the same network, such as in large networks where one DHCP server may not be able to handle all the requests. In such cases, DHCP servers can be configured to serve different IP address ranges or different subnets, so they do not interfere with each other. &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What is SSL tunneling? How does it work?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; 
  &lt;ul&gt; 
   &lt;li&gt;SSL (Secure Sockets Layer) tunneling is a technique used to establish a secure, encrypted connection between two endpoints over an insecure network, such as the Internet. The SSL tunnel is created by encapsulating the traffic within an SSL connection, which provides confidentiality, integrity, and authentication.&lt;/li&gt; 
  &lt;/ul&gt; &lt;p&gt;Here&#39;s how SSL tunneling works:&lt;/p&gt; &lt;/b&gt;
 &lt;ol&gt;
  &lt;b&gt; &lt;li&gt;A client initiates an SSL connection to a server, which involves a handshake process to establish the SSL session.&lt;/li&gt; &lt;li&gt;Once the SSL session is established, the client and server negotiate encryption parameters, such as the encryption algorithm and key length, then exchange digital certificates to authenticate each other.&lt;/li&gt; &lt;li&gt;The client then sends traffic through the SSL tunnel to the server, which decrypts the traffic and forwards it to its destination.&lt;/li&gt; &lt;li&gt;The server sends traffic back through the SSL tunnel to the client, which decrypts the traffic and forwards it to the application. &lt;/li&gt;&lt;/b&gt;
 &lt;/ol&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What is a socket? Where can you see the list of sockets in your system?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;A socket is a software endpoint that enables two-way communication between processes over a network. Sockets provide a standardized interface for network communication, allowing applications to send and receive data across a network. To view the list of open sockets on a Linux system:&amp;nbsp; &lt;em&gt;&lt;strong&gt;netstat -an&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt; &lt;li&gt;This command displays a list of all open sockets, along with their protocol, local address, foreign address, and state. &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What is IPv6? Why should we consider using it if we have IPv4?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; 
  &lt;ul&gt; 
   &lt;li&gt;IPv6 (Internet Protocol version 6) is the latest version of the Internet Protocol (IP), which is used to identify and communicate with devices on a network. IPv6 addresses are 128-bit addresses and are expressed in hexadecimal notation, such as 2001:0db8:85a3:0000:0000:8a2e:0370:7334.&lt;/li&gt; 
  &lt;/ul&gt; &lt;p&gt;There are several reasons why we should consider using IPv6 over IPv4:&lt;/p&gt; &lt;/b&gt;
 &lt;ol&gt;
  &lt;b&gt; &lt;li&gt;Address space: IPv4 has a limited address space, which has been exhausted in many parts of the world. IPv6 provides a much larger address space, allowing for trillions of unique IP addresses.&lt;/li&gt; &lt;li&gt;Security: IPv6 includes built-in support for IPsec, which provides end-to-end encryption and authentication for network traffic.&lt;/li&gt; &lt;li&gt;Performance: IPv6 includes features that can help to improve network performance, such as multicast routing, which allows a single packet to be sent to multiple destinations simultaneously.&lt;/li&gt; &lt;li&gt;Simplified network configuration: IPv6 includes features that can simplify network configuration, such as stateless autoconfiguration, which allows devices to automatically configure their own IPv6 addresses without the need for a DHCP server.&lt;/li&gt; &lt;li&gt;Better mobility support: IPv6 includes features that can improve mobility support, such as Mobile IPv6, which allows devices to maintain their IPv6 addresses as they move between different networks. &lt;/li&gt;&lt;/b&gt;
 &lt;/ol&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What is VLAN?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;A VLAN (Virtual Local Area Network) is a logical network that groups together a set of devices on a physical network, regardless of their physical location. VLANs are created by configuring network switches to assign a specific VLAN ID to frames sent by devices connected to a specific port or group of ports on the switch. &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What is MTU?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;MTU stands for Maximum Transmission Unit. It&#39;s the size of the largest PDU (protocol Data Unit) that can be sent in a single transaction. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What happens if you send a packet that is bigger than the MTU?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;With the IPv4 protocol, the router can fragment the PDU and then send all the fragmented PDU through the transaction.&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;With IPv6 protocol, it issues an error to the user&#39;s computer. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;True or False? Ping is using UDP because it doesn&#39;t care about reliable connection&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;False. Ping is actually using ICMP (Internet Control Message Protocol) which is a network protocol used to send diagnostic messages and control messages related to network communication. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is SDN?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;SDN stands for Software-Defined Networking. It is an approach to network management that emphasizes the centralization of network control, enabling administrators to manage network behavior through a software abstraction.&lt;/li&gt; &lt;li&gt;In a traditional network, network devices such as routers, switches, and firewalls are configured and managed individually, using specialized software or command-line interfaces. In contrast, SDN separates the network control plane from the data plane, allowing administrators to manage network behavior through a centralized software controller. &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What is ICMP? What is it used for?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; 
  &lt;ul&gt; 
   &lt;li&gt;ICMP stands for Internet Control Message Protocol. It is a protocol used for diagnostic and control purposes in IP networks. It is a part of the Internet Protocol suite, operating at the network layer.&lt;/li&gt; 
  &lt;/ul&gt; &lt;p&gt;ICMP messages are used for a variety of purposes, including:&lt;/p&gt; &lt;/b&gt;
 &lt;ol&gt;
  &lt;b&gt; &lt;li&gt;Error reporting: ICMP messages are used to report errors that occur in the network, such as a packet that could not be delivered to its destination.&lt;/li&gt; &lt;li&gt;Ping: ICMP is used to send ping messages, which are used to test whether a host or network is reachable and to measure the round-trip time for packets.&lt;/li&gt; &lt;li&gt;Path MTU discovery: ICMP is used to discover the Maximum Transmission Unit (MTU) of a path, which is the largest packet size that can be transmitted without fragmentation.&lt;/li&gt; &lt;li&gt;Traceroute: ICMP is used by the traceroute utility to trace the path that packets take through the network.&lt;/li&gt; &lt;li&gt;Router discovery: ICMP is used to discover the routers in a network. &lt;/li&gt;&lt;/b&gt;
 &lt;/ol&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What is NAT? How does it work?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;NAT stands for Network Address Translation. It’s a way to map multiple local private addresses to a public one before transferring the information. Organizations that want multiple devices to employ a single IP address use NAT, as do most home routers. For example, your computer&#39;s private IP could be 192.168.1.100, but your router maps the traffic to its public IP (e.g. 1.1.1.1). Any device on the internet would see the traffic coming from your public IP (1.1.1.1) instead of your private IP (192.168.1.100). &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Which port number is used in each of the following protocols?: 
  &lt;ul&gt; 
   &lt;li&gt;SSH&lt;/li&gt; 
   &lt;li&gt;SMTP&lt;/li&gt; 
   &lt;li&gt;HTTP&lt;/li&gt; 
   &lt;li&gt;DNS&lt;/li&gt; 
   &lt;li&gt;HTTPS&lt;/li&gt; 
   &lt;li&gt;FTP&lt;/li&gt; 
   &lt;li&gt;SFTP&lt;/li&gt; 
  &lt;/ul&gt; &lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;SSH - 22&lt;/li&gt; &lt;li&gt;SMTP - 25&lt;/li&gt; &lt;li&gt;HTTP - 80&lt;/li&gt; &lt;li&gt;DNS - 53&lt;/li&gt; &lt;li&gt;HTTPS - 443&lt;/li&gt; &lt;li&gt;FTP - 21&lt;/li&gt; &lt;li&gt;SFTP - 22 &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;Which factors affect network performance?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;Several factors can affect network performance, including:&lt;/p&gt; &lt;/b&gt;
 &lt;ol&gt;
  &lt;b&gt; &lt;li&gt;Bandwidth: The available bandwidth of a network connection can significantly impact its performance. Networks with limited bandwidth can experience slow data transfer rates, high latency, and poor responsiveness.&lt;/li&gt; &lt;li&gt;Latency: Latency refers to the delay that occurs when data is transmitted from one point in a network to another. High latency can result in slow network performance, especially for real-time applications like video conferencing and online gaming.&lt;/li&gt; &lt;li&gt;Network congestion: When too many devices are using a network at the same time, network congestion can occur, leading to slow data transfer rates and poor network performance.&lt;/li&gt; &lt;li&gt;Packet loss: Packet loss occurs when packets of data are dropped during transmission. This can result in slower network speeds and lower overall network performance.&lt;/li&gt; &lt;li&gt;Network topology: The physical layout of a network, including the placement of switches, routers, and other network devices, can impact network performance.&lt;/li&gt; &lt;li&gt;Network protocol: Different network protocols have different performance characteristics, which can impact network performance. For example, TCP is a reliable protocol that can guarantee the delivery of data, but it can also result in slower performance due to the overhead required for error checking and retransmission.&lt;/li&gt; &lt;li&gt;Network security: Security measures such as firewalls and encryption can impact network performance, especially if they require significant processing power or introduce additional latency.&lt;/li&gt; &lt;li&gt;Distance: The physical distance between devices on a network can impact network performance, especially for wireless networks where signal strength and interference can affect connectivity and data transfer rates. &lt;/li&gt;&lt;/b&gt;
 &lt;/ol&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What is APIPA?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;APIPA is a set of IP addresses that devices are allocated when the main DHCP server is not reachable&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What IP range does APIPA use?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;APIPA uses the IP range: 169.254.0.1 - 169.254.255.254.&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;h4&gt;Control Plane and Data Plane&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;What does &quot;control plane&quot; refer to?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;The control plane is a part of the network that decides how to route and forward packets to a different location. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What does &quot;data plane&quot; refer to?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;The data plane is a part of the network that actually forwards the data/packets. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What does &quot;management plane&quot; refer to?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;It refers to monitoring and management functions. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;To which plane (data, control, ...) does creating routing tables belong to?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Control Plane. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain Spanning Tree Protocol (STP).&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is link aggregation? Why is it used?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is Asymmetric Routing? How to deal with it?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What overlay (tunnel) protocols are you familiar with?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is GRE? How does it work?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is VXLAN? How does it work?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is SNAT?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain OSPF.&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;OSPF (Open Shortest Path First) is a routing protocol that can be implemented on various types of routers. In general, OSPF is supported on most modern routers, including those from vendors such as Cisco, Juniper, and Huawei. The protocol is designed to work with IP-based networks, including both IPv4 and IPv6. Also, it uses a hierarchical network design, where routers are grouped into areas, with each area having its own topology map and routing table. This design helps to reduce the amount of routing information that needs to be exchanged between routers and improve network scalability.&lt;/p&gt; &lt;p&gt;The OSPF 4 Types of routers are:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Internal Router&lt;/li&gt; 
   &lt;li&gt;Area Border Routers&lt;/li&gt; 
   &lt;li&gt;Autonomous Systems Boundary Routers&lt;/li&gt; 
   &lt;li&gt;Backbone Routers&lt;/li&gt; 
  &lt;/ul&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Learn more about OSPF router types: &lt;a href=&quot;https://www.educba.com/ospf-router-types/&quot;&gt;https://www.educba.com/ospf-router-types/&lt;/a&gt; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is latency?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Latency is the time taken for information to reach its destination from the source. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is bandwidth?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Bandwidth is the capacity of a communication channel to measure how much data the latter can handle over a specific time period. More bandwidth would imply more traffic handling and thus more data transfer. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is throughput?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Throughput refers to the measurement of the real amount of data transferred over a certain period of time across any transmission channel. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;When performing a search query, what is more important, latency or throughput? And how to ensure that we manage global infrastructure? &lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Latency. To have good latency, a search query should be forwarded to the closest data center. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;When uploading a video, what is more important, latency or throughput? And how to assure that?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Throughput. To have good throughput, the upload stream should be routed to an underutilized link. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What other considerations (except latency and throughput) are there when forwarding requests?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Keep caches updated (which means the request could be forwarded not to the closest data center) &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;Explain Spine &amp;amp; Leaf&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &quot;Spine &amp;amp; Leaf&quot; is a networking topology commonly used in data center environments to connect multiple switches and manage network traffic efficiently. It is also known as &quot;spine-leaf&quot; architecture or &quot;leaf-spine&quot; topology. This design provides high bandwidth, low latency, and scalability, making it ideal for modern data centers handling large volumes of data and traffic. &lt;p&gt;Within a Spine &amp;amp; Leaf network there are two main tipology of switches:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Spine Switches: Spine switches are high-performance switches arranged in a spine layer. These switches act as the core of the network and are typically interconnected with each leaf switch. Each spine switch is connected to all the leaf switches in the data center.&lt;/li&gt; 
   &lt;li&gt;Leaf Switches: Leaf switches are connected to end devices like servers, storage arrays, and other networking equipment. Each leaf switch is connected to every spine switch in the data center. This creates a non-blocking, full-mesh connectivity between leaf and spine switches, ensuring any leaf switch can communicate with any other leaf switch with maximum throughput.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;The Spine &amp;amp; Leaf architecture has become increasingly popular in data centers due to its ability to handle the demands of modern cloud computing, virtualization, and big data applications, providing a scalable, high-performance, and reliable network infrastructure &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is Network Congestion? What can cause it?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Network congestion occurs when there is too much data to transmit on a network and it doesn&#39;t have enough capacity to handle the demand. &lt;br&gt; This can lead to increased latency and packet loss. The causes can be multiple, such as high network usage, large file transfers, malware, hardware issues, or network design problems. &lt;br&gt; To prevent network congestion, it&#39;s important to monitor your network usage and implement strategies to limit or manage the demand. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What can you tell me about the UDP packet format? What about the TCP packet format? How is it different?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is the exponential backoff algorithm? Where is it used?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Using Hamming code, what would be the code word for the following data word 100111010001101?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;00110011110100011101 &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Give examples of protocols found in the application layer&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Hypertext Transfer Protocol (HTTP) - used for the webpages on the internet&lt;/li&gt; &lt;li&gt;Simple Mail Transfer Protocol (SMTP) - email transmission&lt;/li&gt; &lt;li&gt;Telecommunications Network - (TELNET) - terminal emulation to allow a client access to a telnet server&lt;/li&gt; &lt;li&gt;File Transfer Protocol (FTP) - facilitates the transfer of files between any two machines&lt;/li&gt; &lt;li&gt;Domain Name System (DNS) - domain name translation&lt;/li&gt; &lt;li&gt;Dynamic Host Configuration Protocol (DHCP) - allocates IP addresses, subnet masks, and gateways to hosts&lt;/li&gt; &lt;li&gt;Simple Network Management Protocol (SNMP) - gathers data on devices on the network &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;Give examples of protocols found in the Network Layer&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Internet Protocol (IP) - assists in routing packets from one machine to another&lt;/li&gt; &lt;li&gt;Internet Control Message Protocol (ICMP) - lets one know what is going such as error messages and debugging information &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What is HSTS?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; HTTP Strict Transport Security is a web server directive that informs user agents and web browsers how to handle its connection through a response header sent at the very beginning and back to the browser. This forces connections over HTTPS encryption, disregarding any script&#39;s call to load any resource in that domain over HTTP. &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Read more [here](&lt;a href=&quot;https://www.globalsign.com/en/blog/what-is-hsts-and-how-do-i-use-it#:~:text=HTTP%20Strict%20Transport%20Security%20(HSTS,and%20back%20to%20the%20browser.)&quot;&gt;https://www.globalsign.com/en/blog/what-is-hsts-and-how-do-i-use-it#:~:text=HTTP%20Strict%20Transport%20Security%20(HSTS,and%20back%20to%20the%20browser.)&lt;/a&gt; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;h4&gt;Network - Misc&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is the Internet? Is it the same as the World Wide Web?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;The internet refers to a network of networks, transferring huge amounts of data around the globe.&lt;br&gt; The World Wide Web is an application running on millions of servers, on top of the internet, accessed through what is known as the web browser &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is the ISP?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;ISP (Internet Service Provider) is the local internet company provider. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;h2&gt;Operating System&lt;/h2&gt; 
&lt;h3&gt;Operating System Exercises&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Name&lt;/th&gt; 
   &lt;th&gt;Topic&lt;/th&gt; 
   &lt;th&gt;Objective &amp;amp; Instructions&lt;/th&gt; 
   &lt;th&gt;Solution&lt;/th&gt; 
   &lt;th&gt;Comments&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Fork 101&lt;/td&gt; 
   &lt;td&gt;Fork&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/os/fork_101.md&quot;&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/os/solutions/fork_101_solution.md&quot;&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Fork 102&lt;/td&gt; 
   &lt;td&gt;Fork&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/os/fork_102.md&quot;&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/os/solutions/fork_102_solution.md&quot;&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Operating System - Self Assessment&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is an operating system?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;From the book &quot;Operating Systems: Three Easy Pieces&quot;:&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&quot;responsible for making it easy to run programs (even allowing you to seemingly run many at the same time), allowing programs to share memory, enabling programs to interact with devices, and other fun stuff like that&quot;. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;h4&gt;Operating System - Process&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;Can you explain what is a process?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;A process is a running program. A program is one or more instructions and the program (or process) is executed by the operating system. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;If you had to design an API for processes in an operating system, what would this API look like?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;It would support the following:&lt;/p&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Create - allow to create new processes&lt;/li&gt; &lt;li&gt;Delete - allow to remove/destroy processes&lt;/li&gt; &lt;li&gt;State - allow to check the state of the process, whether it&#39;s running, stopped, waiting, etc.&lt;/li&gt; &lt;li&gt;Stop - allow to stop a running process &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;How a process is created?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;The OS is reading program&#39;s code and any additional relevant data&lt;/li&gt; &lt;li&gt;Program&#39;s code is loaded into the memory or more specifically, into the address space of the process.&lt;/li&gt; &lt;li&gt;Memory is allocated for program&#39;s stack (aka run-time stack). The stack also initialized by the OS with data like argv, argc and parameters to main()&lt;/li&gt; &lt;li&gt;Memory is allocated for program&#39;s heap which is required for dynamically allocated data like the data structures linked lists and hash tables&lt;/li&gt; &lt;li&gt;I/O initialization tasks are performed, like in Unix/Linux based systems, where each process has 3 file descriptors (input, output and error)&lt;/li&gt; &lt;li&gt;OS is running the program, starting from main() &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;True or False? The loading of the program into the memory is done eagerly (all at once)&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;False. It was true in the past but today&#39;s operating systems perform lazy loading, which means only the relevant pieces required for the process to run are loaded first. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What are different states of a process?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Running - it&#39;s executing instructions&lt;/li&gt; &lt;li&gt;Ready - it&#39;s ready to run, but for different reasons it&#39;s on hold&lt;/li&gt; &lt;li&gt;Blocked - it&#39;s waiting for some operation to complete, for example I/O disk request &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What are some reasons for a process to become blocked?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;I/O operations (e.g. Reading from a disk)&lt;/li&gt; &lt;li&gt;Waiting for a packet from a network &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What is Inter Process Communication (IPC)?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Inter-process communication (IPC) refers to the mechanisms provided by an operating system that allow processes to manage shared data. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is &quot;time sharing&quot;?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Even when using a system with one physical CPU, it&#39;s possible to allow multiple users to work on it and run programs. This is possible with time sharing, where computing resources are shared in a way it seems to the user, the system has multiple CPUs, but in fact it&#39;s simply one CPU shared by applying multiprogramming and multi-tasking. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is &quot;space sharing&quot;?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Somewhat the opposite of time sharing. While in time sharing a resource is used for a while by one entity and then the same resource can be used by another resource, in space sharing the space is shared by multiple entities but in a way where it&#39;s not being transferred between them.&lt;br&gt; It&#39;s used by one entity, until this entity decides to get rid of it. Take for example storage. In storage, a file is yours, until you decide to delete it. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What component determines which process runs at a given moment in time?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;CPU scheduler &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;h4&gt;Operating System - Memory&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is &quot;virtual memory&quot; and what purpose does serve?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Virtual memory combines your computer&#39;s RAM with temporary space on your hard disk. When RAM runs low, virtual memory helps to move data from RAM to a space called a paging file. Moving data to paging file can free up the RAM, so your computer can complete its work. In general, the more RAM your computer has, the faster the programs run. &lt;a href=&quot;https://www.minitool.com/lib/virtual-memory.html&quot;&gt;https://www.minitool.com/lib/virtual-memory.html&lt;/a&gt; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is demand paging?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Demand paging is a memory management technique where pages are loaded into physical memory only when accessed by a process. It optimizes memory usage by loading pages on demand, reducing startup latency and space overhead. However, it introduces some latency when accessing pages for the first time. Overall, it’s a cost-effective approach for managing memory resources in operating systems. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is copy-on-write?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; Copy-on-write (COW) is a resource management concept, with the goal to reduce unnecessary copying of information. It is a concept, which is implemented for instance within the POSIX fork syscall, which creates a duplicate process of the calling process. &lt;p&gt;The idea:&lt;/p&gt; &lt;/b&gt;
 &lt;ol&gt;
  &lt;b&gt; &lt;li&gt;If resources are shared between 2 or more entities (for example shared memory segments between 2 processes), the resources don&#39;t need to be copied for every entity, but rather every entity has a READ operation access permission on the shared resource. (the shared segments are marked as read-only) (Think of every entity having a pointer to the location of the shared resource, which can be dereferenced to read its value)&lt;/li&gt; &lt;li&gt;If one entity would perform a WRITE operation on a shared resource, a problem would arise, since the resource also would be permanently changed for ALL other entities sharing it. (Think of a process modifying some variables on the stack, or allocatingy some data dynamically on the heap, these changes to the shared resource would also apply for ALL other processes, this is definitely an undesirable behaviour)&lt;/li&gt; &lt;li&gt;As a solution only, if a WRITE operation is about to be performed on a shared resource, this resource gets COPIED first and then the changes are applied. &lt;/li&gt;&lt;/b&gt;
 &lt;/ol&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What is a kernel, and what does it do?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;The kernel is part of the operating system and is responsible for tasks like:&lt;/p&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Allocating memory&lt;/li&gt; &lt;li&gt;Schedule processes&lt;/li&gt; &lt;li&gt;Control CPU &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;True or False? Some pieces of the code in the kernel are loaded into protected areas of the memory so applications can&#39;t overwrite them.&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;True &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is POSIX?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;POSIX (Portable Operating System Interface) is a set of standards that define the interface between a Unix-like operating system and application programs. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain what is Semaphore and what its role in operating systems.&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;A semaphore is a synchronization primitive used in operating systems and concurrent programming to control access to shared resources. It&#39;s a variable or abstract data type that acts as a counter or a signaling mechanism for managing access to resources by multiple processes or threads. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is cache? What is buffer?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Cache: Cache is usually used when processes are reading and writing to the disk to make the process faster, by making similar data used by different programs easily accessible. Buffer: Reserved place in RAM, which is used to hold data for temporary purposes. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;h2&gt;Virtualization&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is Virtualization?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Virtualization uses software to create an abstraction layer over computer hardware, that allows the hardware elements of a single computer - processors, memory, storage and more - to be divided into multiple virtual computers, commonly called virtual machines (VMs). &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is a hypervisor?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;Red Hat: &quot;A hypervisor is software that creates and runs virtual machines (VMs). A hypervisor, sometimes called a virtual machine monitor (VMM), isolates the hypervisor operating system and resources from the virtual machines and enables the creation and management of those VMs.&quot;&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Read more &lt;a href=&quot;https://www.redhat.com/en/topics/virtualization/what-is-a-hypervisor&quot;&gt;here&lt;/a&gt; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What types of hypervisors are there?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Hosted hypervisors and bare-metal hypervisors. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What are the advantages and disadvantges of bare-metal hypervisor over a hosted hypervisor?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;Due to having its own drivers and a direct access to hardware components, a baremetal hypervisor will often have better performances along with stability and scalability.&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;On the other hand, there will probably be some limitation regarding loading (any) drivers so a hosted hypervisor will usually benefit from having a better hardware compatibility. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What types of virtualization are there?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Operating system virtualization Network functions virtualization Desktop virtualization &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Is containerization is a type of Virtualization?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Yes, it&#39;s a operating-system-level virtualization, where the kernel is shared and allows to use multiple isolated user-spaces instances. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;How the introduction of virtual machines changed the industry and the way applications were deployed?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;The introduction of virtual machines allowed companies to deploy multiple business applications on the same hardware, while each application is separated from each other in secured way, where each is running on its own separate operating system. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;h4&gt;Virtual Machines&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;Do we need virtual machines in the age of containers? Are they still relevant?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Yes, virtual machines are still relevant even in the age of containers. While containers provide a lightweight and portable alternative to virtual machines, they do have certain limitations. Virtual machines still matter because they offer isolation and security, can run different operating systems, and are good for legacy apps. Containers limitations for example are sharing the host kernel. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;h2&gt;Prometheus&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is Prometheus? What are some of Prometheus&#39;s main features?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;Prometheus is a popular open-source systems monitoring and alerting toolkit, originally developed at SoundCloud. It is designed to collect and store time-series data, and to allow for querying and analysis of that data using a powerful query language called PromQL. Prometheus is frequently used to monitor cloud-native applications, microservices, and other modern infrastructure.&lt;/p&gt; &lt;p&gt;Some of the main features of Prometheus include:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;1. Data model: Prometheus uses a flexible data model that allows users to organize and label their time-series data in a way that makes sense for their particular use case. Labels are used to identify different dimensions of the data, such as the source of the data or the environment in which it was collected.

2. Pull-based architecture: Prometheus uses a pull-based model to collect data from targets, meaning that the Prometheus server actively queries its targets for metrics data at regular intervals. This architecture is more scalable and reliable than a push-based model, which would require every target to push data to the server.

3. Time-series database: Prometheus stores all of its data in a time-series database, which allows users to perform queries over time ranges and to aggregate and analyze their data in various ways. The database is optimized for write-heavy workloads, and can handle a high volume of data with low latency.

4. Alerting: Prometheus includes a powerful alerting system that allows users to define rules based on their metrics data and to send alerts when certain conditions are met. Alerts can be sent via email, chat, or other channels, and can be customized to include specific details about the problem.

5. Visualization: Prometheus has a built-in graphing and visualization tool, called PromDash, which allows users to create custom dashboards to monitor their systems and applications. PromDash supports a variety of graph types and visualization options, and can be customized using CSS and JavaScript.
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Overall, Prometheus is a powerful and flexible tool for monitoring and analyzing systems and applications, and is widely used in the industry for cloud-native monitoring and observability.&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;In what scenarios it might be better to NOT use Prometheus?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;From Prometheus documentation: &quot;if you need 100% accuracy, such as for per-request billing&quot;. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Describe Prometheus architecture and components&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;The Prometheus architecture consists of four major components:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;1. Prometheus Server: The Prometheus server is responsible for collecting and storing metrics data. It has a simple built-in storage layer that allows it to store time-series data in a time-ordered database.

2. Client Libraries: Prometheus provides a range of client libraries that enable applications to expose their metrics data in a format that can be ingested by the Prometheus server. These libraries are available for a range of programming languages, including Java, Python, and Go.

3. Exporters: Exporters are software components that expose existing metrics from third-party systems and make them available for ingestion by the Prometheus server. Prometheus provides exporters for a range of popular technologies, including MySQL, PostgreSQL, and Apache.

4. Alertmanager: The Alertmanager component is responsible for processing alerts generated by the Prometheus server. It can handle alerts from multiple sources and provides a range of features for deduplicating, grouping, and routing alerts to appropriate channels.
&lt;/code&gt;&lt;/pre&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Overall, the Prometheus architecture is designed to be highly scalable and resilient. The server and client libraries can be deployed in a distributed fashion to support monitoring across large-scale, highly dynamic environments &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Can you compare Prometheus to other solutions like InfluxDB for example?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;Compared to other monitoring solutions, such as InfluxDB, Prometheus is known for its high performance and scalability. It can handle large volumes of data and can easily be integrated with other tools in the monitoring ecosystem. InfluxDB, on the other hand, is known for its ease of use and simplicity. It has a user-friendly interface and provides easy-to-use APIs for collecting and querying data.&lt;/p&gt; &lt;p&gt;Another popular solution, Nagios, is a more traditional monitoring system that relies on a push-based model for collecting data. Nagios has been around for a long time and is known for its stability and reliability. However, compared to Prometheus, Nagios lacks some of the more advanced features, such as multi-dimensional data model and powerful query language.&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Overall, the choice of a monitoring solution depends on the specific needs and requirements of the organization. While Prometheus is a great choice for large-scale monitoring and alerting, InfluxDB may be a better fit for smaller environments that require ease of use and simplicity. Nagios remains a solid choice for organizations that prioritize stability and reliability over advanced features. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is an Alert?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; In Prometheus, an alert is a notification triggered when a specific condition or threshold is met. Alerts can be configured to trigger when certain metrics cross a certain threshold or when specific events occur. Once an alert is triggered, it can be routed to various channels, such as email, pager, or chat, to notify relevant teams or individuals to take appropriate action. Alerts are a critical component of any monitoring system, as they allow teams to proactively detect and respond to issues before they impact users or cause system downtime. &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is an Instance? What is a Job?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;In Prometheus, an instance refers to a single target that is being monitored. For example, a single server or service. A job is a set of instances that perform the same function, such as a set of web servers serving the same application. Jobs allow you to define and manage a group of targets together.&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;In essence, an instance is an individual target that Prometheus collects metrics from, while a job is a collection of similar instances that can be managed as a group. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What core metrics types Prometheus supports?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; Prometheus supports several types of metrics, including: &lt;pre&gt;&lt;code&gt;1. Counter: A monotonically increasing value used for tracking counts of events or samples. Examples include the number of requests processed or the total number of errors encountered.

2. Gauge: A value that can go up or down, such as CPU usage or memory usage. Unlike counters, gauge values can be arbitrary, meaning they can go up and down based on changes in the system being monitored.

3. Histogram: A set of observations or events that are divided into buckets based on their value. Histograms help in analyzing the distribution of a metric, such as request latencies or response sizes.

4. Summary: A summary is similar to a histogram, but instead of buckets, it provides a set of quantiles for the observed values. Summaries are useful for monitoring the distribution of request latencies or response sizes over time.
&lt;/code&gt;&lt;/pre&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Prometheus also supports various functions and operators for aggregating and manipulating metrics, such as sum, max, min, and rate. These features make it a powerful tool for monitoring and alerting on system metrics. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is an exporter? What is it used for?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; The exporter serves as a bridge between the third-party system or application and Prometheus, making it possible for Prometheus to monitor and collect data from that system or application. &lt;p&gt;The exporter acts as a server, listening on a specific network port for requests from Prometheus to scrape metrics. It collects metrics from the third-party system or application and transforms them into a format that can be understood by Prometheus. The exporter then exposes these metrics to Prometheus via an HTTP endpoint, making them available for collection and analysis.&lt;/p&gt; &lt;p&gt;Exporters are commonly used to monitor various types of infrastructure components such as databases, web servers, and storage systems. For example, there are exporters available for monitoring popular databases such as MySQL and PostgreSQL, as well as web servers like Apache and Nginx.&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Overall, exporters are a critical component of the Prometheus ecosystem, allowing for the monitoring of a wide range of systems and applications, and providing a high degree of flexibility and extensibility to the platform. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Which Prometheus best practices?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; Here are three of them: &lt;pre&gt;&lt;code&gt;1. Label carefully: Careful and consistent labeling of metrics is crucial for effective querying and alerting. Labels should be clear, concise, and include all relevant information about the metric.

2. Keep metrics simple: The metrics exposed by exporters should be simple and focus on a single aspect of the system being monitored. This helps avoid confusion and ensures that the metrics are easily understandable by all members of the team.

3. Use alerting sparingly: While alerting is a powerful feature of Prometheus, it should be used sparingly and only for the most critical issues. Setting up too many alerts can lead to alert fatigue and result in important alerts being ignored. It is recommended to set up only the most important alerts and adjust the thresholds over time based on the actual frequency of alerts.
&lt;/code&gt;&lt;/pre&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;How to get total requests in a given period of time?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; To get the total requests in a given period of time using Prometheus, you can use the *sum* function along with the *rate* function. Here is an example query that will give you the total number of requests in the last hour: &lt;pre&gt;&lt;code&gt;sum(rate(http_requests_total[1h]))
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In this query, &lt;em&gt;http_requests_total&lt;/em&gt; is the name of the metric that tracks the total number of HTTP requests, and the &lt;em&gt;rate&lt;/em&gt; function calculates the per-second rate of requests over the last hour. The &lt;em&gt;sum&lt;/em&gt; function then adds up all of the requests to give you the total number of requests in the last hour.&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;You can adjust the time range by changing the duration in the &lt;em&gt;rate&lt;/em&gt; function. For example, if you wanted to get the total number of requests in the last day, you could change the function to &lt;em&gt;rate(http_requests_total[1d])&lt;/em&gt;. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What HA in Prometheus means?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;HA stands for High Availability. This means that the system is designed to be highly reliable and always available, even in the face of failures or other issues. In practice, this typically involves setting up multiple instances of Prometheus and ensuring that they are all synchronized and able to work together seamlessly. This can be achieved through a variety of techniques, such as load balancing, replication, and failover mechanisms. By implementing HA in Prometheus, users can ensure that their monitoring data is always available and up-to-date, even in the face of hardware or software failures, network issues, or other problems that might otherwise cause downtime or data loss. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;How do you join two metrics?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; In Prometheus, joining two metrics can be achieved using the *join()* function. The *join()* function combines two or more time series based on their label values. It takes two mandatory arguments: *on* and *table*. The on argument specifies the labels to join *on* and the *table* argument specifies the time series to join. &lt;p&gt;Here&#39;s an example of how to join two metrics using the &lt;em&gt;join()&lt;/em&gt; function:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;sum_series(
  join(
    on(service, instance) request_count_total,
    on(service, instance) error_count_total,
  )
)
&lt;/code&gt;&lt;/pre&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;In this example, the &lt;em&gt;join()&lt;/em&gt; function combines the &lt;em&gt;request_count_total&lt;/em&gt; and &lt;em&gt;error_count_total&lt;/em&gt; time series based on their &lt;em&gt;service&lt;/em&gt; and &lt;em&gt;instance&lt;/em&gt; label values. The &lt;em&gt;sum_series()&lt;/em&gt; function then calculates the sum of the resulting time series &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;How to write a query that returns the value of a label?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; To write a query that returns the value of a label in Prometheus, you can use the *label_values* function. The *label_values* function takes two arguments: the name of the label and the name of the metric. &lt;p&gt;For example, if you have a metric called &lt;em&gt;http_requests_total&lt;/em&gt; with a label called &lt;em&gt;method&lt;/em&gt;, and you want to return all the values of the &lt;em&gt;method&lt;/em&gt; label, you can use the following query:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;label_values(http_requests_total, method)
&lt;/code&gt;&lt;/pre&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;This will return a list of all the values for the &lt;em&gt;method&lt;/em&gt; label in the &lt;em&gt;http_requests_total&lt;/em&gt; metric. You can then use this list in further queries or to filter your data. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;How do you convert cpu_user_seconds to cpu usage in percentage?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; To convert *cpu_user_seconds* to CPU usage in percentage, you need to divide it by the total elapsed time and the number of CPU cores, and then multiply by 100. The formula is as follows: &lt;pre&gt;&lt;code&gt;100 * sum(rate(process_cpu_user_seconds_total{job=&quot;&amp;lt;job-name&amp;gt;&quot;}[&amp;lt;time-period&amp;gt;])) by (instance) / (&amp;lt;time-period&amp;gt; * &amp;lt;num-cpu-cores&amp;gt;)
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here, &lt;em&gt;
    &lt;job-name&gt;&lt;/job-name&gt;&lt;/em&gt; is the name of the job you want to query, &lt;em&gt;
    &lt;time-period&gt;&lt;/time-period&gt;&lt;/em&gt; is the time range you want to query (e.g. &lt;em&gt;5m&lt;/em&gt;, &lt;em&gt;1h&lt;/em&gt;), and &lt;em&gt;
    &lt;num-cpu-cores&gt;&lt;/num-cpu-cores&gt;&lt;/em&gt; is the number of CPU cores on the machine you are querying.&lt;/p&gt; &lt;p&gt;For example, to get the CPU usage in percentage for the last 5 minutes for a job named &lt;em&gt;my-job&lt;/em&gt; running on a machine with 4 CPU cores, you can use the following query:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;100 * sum(rate(process_cpu_user_seconds_total{job=&quot;my-job&quot;}[5m])) by (instance) / (5m * 4)
&lt;/code&gt;&lt;/pre&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;h2&gt;Go&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;What are some characteristics of the Go programming language?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Strong and static typing - the type of the variables can&#39;t be changed over time and they have to be defined at compile time&lt;/li&gt; 
   &lt;li&gt;Simplicity&lt;/li&gt; 
   &lt;li&gt;Fast compile times&lt;/li&gt; 
   &lt;li&gt;Built-in concurrency&lt;/li&gt; 
   &lt;li&gt;Garbage collected&lt;/li&gt; 
   &lt;li&gt;Platform independent&lt;/li&gt; 
   &lt;li&gt;Compile to standalone binary - anything you need to run your app will be compiled into one binary. Very useful for version management in run-time.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Go also has good community. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is the difference between &lt;code&gt;var x int = 2&lt;/code&gt; and &lt;code&gt;x := 2&lt;/code&gt;?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;The result is the same, a variable with the value 2.&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;With &lt;code&gt;var x int = 2&lt;/code&gt; we are setting the variable type to integer while with &lt;code&gt;x := 2&lt;/code&gt; we are letting Go figure out by itself the type. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;True or False? In Go we can redeclare variables and once declared we must use it.&lt;/summary&gt; 
 &lt;p&gt;False. We can&#39;t redeclare variables but yes, we must used declared variables. &lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What libraries of Go have you used?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;This should be answered based on your usage but some examples are:&lt;/p&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;fmt - formatted I/O &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What is the problem with the following block of code? How to fix it? &lt;pre&gt;&lt;code&gt;func main() {
    var x float32 = 13.5
    var y int
    y = x
}
&lt;/code&gt;&lt;/pre&gt; &lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;The following block of code tries to convert the integer 101 to a string but instead we get &quot;e&quot;. Why is that? How to fix it? &lt;pre&gt;&lt;code class=&quot;language-go&quot;&gt;package main

import &quot;fmt&quot;

func main() {
    var x int = 101
    var y string
    y = string(x)
    fmt.Println(y)
}
&lt;/code&gt;&lt;/pre&gt; &lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;It looks what unicode value is set at 101 and uses it for converting the integer to a string. If you want to get &quot;101&quot; you should use the package &quot;strconv&quot; and replace &lt;code&gt;y = string(x)&lt;/code&gt; with &lt;code&gt;y = strconv.Itoa(x)&lt;/code&gt; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is wrong with the following code?: &lt;pre&gt;&lt;code&gt;package main

func main() {
    var x = 2
    var y = 3
    const someConst = x + y
}
&lt;/code&gt;&lt;/pre&gt; &lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Constants in Go can only be declared using constant expressions. But &lt;code&gt;x&lt;/code&gt;, &lt;code&gt;y&lt;/code&gt; and their sum is variable. &lt;br&gt; &lt;code&gt;const initializer x + y is not a constant&lt;/code&gt; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What will be the output of the following block of code?: &lt;pre&gt;&lt;code class=&quot;language-go&quot;&gt;package main

import &quot;fmt&quot;

const (
	x = iota
	y = iota
)
const z = iota

func main() {
	fmt.Printf(&quot;%v\n&quot;, x)
	fmt.Printf(&quot;%v\n&quot;, y)
	fmt.Printf(&quot;%v\n&quot;, z)
}
&lt;/code&gt;&lt;/pre&gt; &lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Go&#39;s iota identifier is used in const declarations to simplify definitions of incrementing numbers. Because it can be used in expressions, it provides a generality beyond that of simple enumerations. &lt;br&gt; &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; in the first iota group, &lt;code&gt;z&lt;/code&gt; in the second. &lt;br&gt; &lt;a href=&quot;https://github.com/golang/go/wiki/Iota&quot;&gt;Iota page in Go Wiki&lt;/a&gt; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What _ is used for in Go?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;It avoids having to declare all the variables for the returns values. It is called the &lt;a href=&quot;https://golang.org/doc/effective_go.html#blank&quot;&gt;blank identifier&lt;/a&gt;. &lt;br&gt; &lt;a href=&quot;https://stackoverflow.com/questions/27764421/what-is-underscore-comma-in-a-go-declaration#answer-27764432&quot;&gt;answer in SO&lt;/a&gt; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What will be the output of the following block of code?: &lt;pre&gt;&lt;code class=&quot;language-go&quot;&gt;package main

import &quot;fmt&quot;

const (
	_ = iota + 3
	x
)

func main() {
	fmt.Printf(&quot;%v\n&quot;, x)
}
&lt;/code&gt;&lt;/pre&gt; &lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Since the first iota is declared with the value &lt;code&gt;3&lt;/code&gt; (&lt;code&gt; + 3&lt;/code&gt;), the next one has the value &lt;code&gt;4&lt;/code&gt; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What will be the output of the following block of code?: &lt;pre&gt;&lt;code class=&quot;language-go&quot;&gt;package main

import (
	&quot;fmt&quot;
	&quot;sync&quot;
	&quot;time&quot;
)

func main() {
	var wg sync.WaitGroup

	wg.Add(1)
	go func() {
		time.Sleep(time.Second * 2)
		fmt.Println(&quot;1&quot;)
		wg.Done()
	}()

	go func() {
		fmt.Println(&quot;2&quot;)
	}()

	wg.Wait()
	fmt.Println(&quot;3&quot;)
}
&lt;/code&gt;&lt;/pre&gt; &lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;Output: 2 1 3&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://tutorialedge.net/golang/go-waitgroup-tutorial/&quot;&gt;Aritcle about sync/waitgroup&lt;/a&gt;&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;a href=&quot;https://golang.org/pkg/sync/&quot;&gt;Golang package sync&lt;/a&gt; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What will be the output of the following block of code?: &lt;pre&gt;&lt;code class=&quot;language-go&quot;&gt;package main

import (
	&quot;fmt&quot;
)

func mod1(a []int) {
	for i := range a {
		a[i] = 5
	}

	fmt.Println(&quot;1:&quot;, a)
}

func mod2(a []int) {
	a = append(a, 125) // !

	for i := range a {
		a[i] = 5
	}

	fmt.Println(&quot;2:&quot;, a)
}

func main() {
	s1 := []int{1, 2, 3, 4}
	mod1(s1)
	fmt.Println(&quot;1:&quot;, s1)

	s2 := []int{1, 2, 3, 4}
	mod2(s2)
	fmt.Println(&quot;2:&quot;, s2)
}
&lt;/code&gt;&lt;/pre&gt; &lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;Output: &lt;code&gt;&lt;br&gt; 1 [5 5 5 5]&lt;br&gt; 1 [5 5 5 5]&lt;br&gt; 2 [5 5 5 5 5]&lt;br&gt; 2 [1 2 3 4]&lt;br&gt; &lt;/code&gt;&lt;/p&gt; &lt;p&gt;In &lt;code&gt;mod1&lt;/code&gt; a is link, and when we&#39;re using &lt;code&gt;a[i]&lt;/code&gt;, we&#39;re changing &lt;code&gt;s1&lt;/code&gt; value to. But in &lt;code&gt;mod2&lt;/code&gt;, &lt;code&gt;append&lt;/code&gt; creates new slice, and we&#39;re changing only &lt;code&gt;a&lt;/code&gt; value, not &lt;code&gt;s2&lt;/code&gt;.&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;a href=&quot;https://golangbot.com/arrays-and-slices/&quot;&gt;Aritcle about arrays&lt;/a&gt;, &lt;a href=&quot;https://blog.golang.org/slices&quot;&gt;Blog post about &lt;code&gt;append&lt;/code&gt;&lt;/a&gt; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What will be the output of the following block of code?: &lt;pre&gt;&lt;code class=&quot;language-go&quot;&gt;package main

import (
	&quot;container/heap&quot;
	&quot;fmt&quot;
)

// An IntHeap is a min-heap of ints.
type IntHeap []int

func (h IntHeap) Len() int           { return len(h) }
func (h IntHeap) Less(i, j int) bool { return h[i] &amp;lt; h[j] }
func (h IntHeap) Swap(i, j int)      { h[i], h[j] = h[j], h[i] }

func (h *IntHeap) Push(x interface{}) {
	// Push and Pop use pointer receivers because they modify the slice&#39;s length,
	// not just its contents.
	*h = append(*h, x.(int))
}

func (h *IntHeap) Pop() interface{} {
	old := *h
	n := len(old)
	x := old[n-1]
	*h = old[0 : n-1]
	return x
}

func main() {
	h := &amp;amp;IntHeap{4, 8, 3, 6}
	heap.Init(h)
	heap.Push(h, 7)

  fmt.Println((*h)[0])
}
&lt;/code&gt;&lt;/pre&gt; &lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;Output: 3&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;a href=&quot;https://golang.org/pkg/container/heap/&quot;&gt;Golang container/heap package&lt;/a&gt; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;h2&gt;Mongo&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;What are the advantages of MongoDB? Or in other words, why choosing MongoDB and not other implementation of NoSQL?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;MongoDB advantages are as following:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Schemaless&lt;/li&gt; 
   &lt;li&gt;Easy to scale-out&lt;/li&gt; 
   &lt;li&gt;No complex joins&lt;/li&gt; 
   &lt;li&gt;Structure of a single object is clear&lt;/li&gt; 
  &lt;/ul&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is the difference between SQL and NoSQL?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;The main difference is that SQL databases are structured (data is stored in the form of tables with rows and columns - like an excel spreadsheet table) while NoSQL is unstructured, and the data storage can vary depending on how the NoSQL DB is set up, such as key-value pair, document-oriented, etc. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;In what scenarios would you prefer to use NoSQL/Mongo over SQL?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Heterogeneous data which changes often&lt;/li&gt; &lt;li&gt;Data consistency and integrity is not top priority&lt;/li&gt; &lt;li&gt;Best if the database needs to scale rapidly &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What is a document? What is a collection?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;A document is a record in MongoDB, which is stored in BSON (Binary JSON) format and is the basic unit of data in MongoDB.&lt;/li&gt; &lt;li&gt;A collection is a group of related documents stored in a single database in MongoDB. &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What is an aggregator?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;An aggregator is a framework in MongoDB that performs operations on a set of data to return a single computed result. &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What is better? Embedded documents or referenced?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;There is no definitive answer to which is better, it depends on the specific use case and requirements. Some explanations : Embedded documents provide atomic updates, while referenced documents allow for better normalization. &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;Have you performed data retrieval optimizations in Mongo? If not, can you think about ways to optimize a slow data retrieval?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Some ways to optimize data retrieval in MongoDB are: indexing, proper schema design, query optimization and database load balancing. &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;h5&gt;Queries&lt;/h5&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain this query: &lt;code&gt;db.books.find({&quot;name&quot;: /abc/})&lt;/code&gt;&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain this query: &lt;code&gt;db.books.find().sort({x:1})&lt;/code&gt;&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is the difference between find() and find_one()?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;&lt;code&gt;find()&lt;/code&gt; returns all documents that match the query conditions.&lt;/li&gt; &lt;li&gt;find_one() returns only one document that matches the query conditions (or null if no match is found). &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;How can you export data from Mongo DB?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;mongoexport&lt;/li&gt; &lt;li&gt;programming languages &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;h2&gt;SQL&lt;/h2&gt; 
&lt;h3&gt;SQL Exercises&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Name&lt;/th&gt; 
   &lt;th&gt;Topic&lt;/th&gt; 
   &lt;th&gt;Objective &amp;amp; Instructions&lt;/th&gt; 
   &lt;th&gt;Solution&lt;/th&gt; 
   &lt;th&gt;Comments&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Functions vs. Comparisons&lt;/td&gt; 
   &lt;td&gt;Query Improvements&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/sql/improve_query.md&quot;&gt;Exercise&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/sql/solutions/improve_query.md&quot;&gt;Solution&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;SQL Self Assessment&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is SQL?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;SQL (Structured Query Language) is a standard language for relational databases (like MySQL, MariaDB, ...).&lt;br&gt; It&#39;s used for reading, updating, removing and creating data in a relational database. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;How is SQL Different from NoSQL&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;The main difference is that SQL databases are structured (data is stored in the form of tables with rows and columns - like an excel spreadsheet table) while NoSQL is unstructured, and the data storage can vary depending on how the NoSQL DB is set up, such as key-value pair, document-oriented, etc. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;When is it best to use SQL? NoSQL?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;SQL - Best used when data integrity is crucial. SQL is typically implemented with many businesses and areas within the finance field due to it&#39;s ACID compliance.&lt;/p&gt; &lt;p&gt;NoSQL - Great if you need to scale things quickly. NoSQL was designed with web applications in mind, so it works great if you need to quickly spread the same information around to multiple servers&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Additionally, since NoSQL does not adhere to the strict table with columns and rows structure that Relational Databases require, you can store different data types together. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;h5&gt;Practical SQL - Basics&lt;/h5&gt; 
&lt;p&gt;For these questions, we will be using the Customers and Orders tables shown below:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Customers&lt;/strong&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Customer_ID&lt;/th&gt; 
   &lt;th&gt;Customer_Name&lt;/th&gt; 
   &lt;th&gt;Items_in_cart&lt;/th&gt; 
   &lt;th&gt;Cash_spent_to_Date&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;100204&lt;/td&gt; 
   &lt;td&gt;John Smith&lt;/td&gt; 
   &lt;td&gt;0&lt;/td&gt; 
   &lt;td&gt;20.00&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;100205&lt;/td&gt; 
   &lt;td&gt;Jane Smith&lt;/td&gt; 
   &lt;td&gt;3&lt;/td&gt; 
   &lt;td&gt;40.00&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;100206&lt;/td&gt; 
   &lt;td&gt;Bobby Frank&lt;/td&gt; 
   &lt;td&gt;1&lt;/td&gt; 
   &lt;td&gt;100.20&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;ORDERS&lt;/strong&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Customer_ID&lt;/th&gt; 
   &lt;th&gt;Order_ID&lt;/th&gt; 
   &lt;th&gt;Item&lt;/th&gt; 
   &lt;th&gt;Price&lt;/th&gt; 
   &lt;th&gt;Date_sold&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;100206&lt;/td&gt; 
   &lt;td&gt;A123&lt;/td&gt; 
   &lt;td&gt;Rubber Ducky&lt;/td&gt; 
   &lt;td&gt;2.20&lt;/td&gt; 
   &lt;td&gt;2019-09-18&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;100206&lt;/td&gt; 
   &lt;td&gt;A123&lt;/td&gt; 
   &lt;td&gt;Bubble Bath&lt;/td&gt; 
   &lt;td&gt;8.00&lt;/td&gt; 
   &lt;td&gt;2019-09-18&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;100206&lt;/td&gt; 
   &lt;td&gt;Q987&lt;/td&gt; 
   &lt;td&gt;80-Pack TP&lt;/td&gt; 
   &lt;td&gt;90.00&lt;/td&gt; 
   &lt;td&gt;2019-09-20&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;100205&lt;/td&gt; 
   &lt;td&gt;Z001&lt;/td&gt; 
   &lt;td&gt;Cat Food - Tuna Fish&lt;/td&gt; 
   &lt;td&gt;10.00&lt;/td&gt; 
   &lt;td&gt;2019-08-05&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;100205&lt;/td&gt; 
   &lt;td&gt;Z001&lt;/td&gt; 
   &lt;td&gt;Cat Food - Chicken&lt;/td&gt; 
   &lt;td&gt;10.00&lt;/td&gt; 
   &lt;td&gt;2019-08-05&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;100205&lt;/td&gt; 
   &lt;td&gt;Z001&lt;/td&gt; 
   &lt;td&gt;Cat Food - Beef&lt;/td&gt; 
   &lt;td&gt;10.00&lt;/td&gt; 
   &lt;td&gt;2019-08-05&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;100205&lt;/td&gt; 
   &lt;td&gt;Z001&lt;/td&gt; 
   &lt;td&gt;Cat Food - Kitty quesadilla&lt;/td&gt; 
   &lt;td&gt;10.00&lt;/td&gt; 
   &lt;td&gt;2019-08-05&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;100204&lt;/td&gt; 
   &lt;td&gt;X202&lt;/td&gt; 
   &lt;td&gt;Coffee&lt;/td&gt; 
   &lt;td&gt;20.00&lt;/td&gt; 
   &lt;td&gt;2019-04-29&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;details&gt; 
 &lt;summary&gt;How would I select all fields from this table?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Select * &lt;br&gt; From Customers; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;How many items are in John&#39;s cart?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Select Items_in_cart &lt;br&gt; From Customers &lt;br&gt; Where Customer_Name = &quot;John Smith&quot;; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is the sum of all the cash spent across all customers?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Select SUM(Cash_spent_to_Date) as SUM_CASH &lt;br&gt; From Customers; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;How many people have items in their cart?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Select count(1) as Number_of_People_w_items &lt;br&gt; From Customers &lt;br&gt; where Items_in_cart &amp;gt; 0; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;How would you join the customer table to the order table?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;You would join them on the unique key. In this case, the unique key is Customer_ID in both the Customers table and Orders table &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;How would you show which customer ordered which items?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;Select c.Customer_Name, o.Item &lt;br&gt; From Customers c &lt;br&gt; Left Join Orders o &lt;br&gt; On c.Customer_ID = o.Customer_ID;&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Using a with statement, how would you show who ordered cat food, and the total amount of money spent?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;with cat_food as ( &lt;br&gt; Select Customer_ID, SUM(Price) as TOTAL_PRICE &lt;br&gt; From Orders &lt;br&gt; Where Item like &quot;%Cat Food%&quot; &lt;br&gt; Group by Customer_ID &lt;br&gt; ) &lt;br&gt; Select Customer_name, TOTAL_PRICE &lt;br&gt; From Customers c &lt;br&gt; Inner JOIN cat_food f &lt;br&gt; ON c.Customer_ID = f.Customer_ID &lt;br&gt; where c.Customer_ID in (Select Customer_ID from cat_food);&lt;/p&gt; &lt;p&gt;Although this was a simple statement, the &quot;with&quot; clause really shines when a complex query needs to be run on a table before joining to another. With statements are nice, because you create a pseudo temp when running your query, instead of creating a whole new table.&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;The Sum of all the purchases of cat food weren&#39;t readily available, so we used a with statement to create the pseudo table to retrieve the sum of the prices spent by each customer, then join the table normally. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Which of the following queries would you use? &lt;pre&gt;&lt;code&gt;SELECT count(*)                             SELECT count(*)
FROM shawarma_purchases                     FROM shawarma_purchases
WHERE                               vs.     WHERE
  YEAR(purchased_at) == &#39;2017&#39;              purchased_at &amp;gt;= &#39;2017-01-01&#39; AND
                                            purchased_at &amp;lt;= &#39;2017-31-12&#39;
&lt;/code&gt;&lt;/pre&gt; &lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;pre&gt;&lt;code&gt;SELECT count(*)
FROM shawarma_purchases
WHERE
  purchased_at &amp;gt;= &#39;2017-01-01&#39; AND
  purchased_at &amp;lt;= &#39;2017-31-12&#39;
&lt;/code&gt;&lt;/pre&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;When you use a function (&lt;code&gt;YEAR(purchased_at)&lt;/code&gt;) it has to scan the whole database as opposed to using indexes and basically the column as it is, in its natural state. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;h2&gt;OpenStack&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;What components/projects of OpenStack are you familiar with?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; I’m most familiar with several core OpenStack components: 
  &lt;ul&gt; 
   &lt;li&gt;Nova for compute resource provisioning, including VM lifecycle management.&lt;/li&gt; 
   &lt;li&gt;Neutron for networking, focusing on creating and managing networks, subnets, and routers.&lt;/li&gt; 
   &lt;li&gt;Cinder for block storage, used to attach and manage storage volumes.&lt;/li&gt; 
   &lt;li&gt;Keystone for identity services, handling authentication and authorization.&lt;/li&gt; 
  &lt;/ul&gt; &lt;p&gt;I’ve implemented these in past projects, configuring them for scalability and security to support multi-tenant environments.&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Can you tell me what each of the following services/projects is responsible for?: 
  &lt;ul&gt; 
   &lt;li&gt;Nova&lt;/li&gt; 
   &lt;li&gt;Neutron&lt;/li&gt; 
   &lt;li&gt;Cinder&lt;/li&gt; 
   &lt;li&gt;Glance&lt;/li&gt; 
   &lt;li&gt;Keystone&lt;/li&gt;
  &lt;/ul&gt;&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt;  &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Nova - Manage virtual instances&lt;/li&gt; &lt;li&gt;Neutron - Manage networking by providing Network as a service (NaaS)&lt;/li&gt; &lt;li&gt;Cinder - Block Storage&lt;/li&gt; &lt;li&gt;Glance - Manage images for virtual machines and containers (search, get and register)&lt;/li&gt; &lt;li&gt;Keystone - Authentication service across the cloud &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;Identify the service/project used for each of the following: 
  &lt;ul&gt; 
   &lt;li&gt;Copy or snapshot instances&lt;/li&gt; 
   &lt;li&gt;GUI for viewing and modifying resources&lt;/li&gt; 
   &lt;li&gt;Block Storage&lt;/li&gt; 
   &lt;li&gt;Manage virtual instances&lt;/li&gt; 
  &lt;/ul&gt; &lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Glance - Images Service. Also used for copying or snapshot instances&lt;/li&gt; &lt;li&gt;Horizon - GUI for viewing and modifying resources&lt;/li&gt; &lt;li&gt;Cinder - Block Storage&lt;/li&gt; &lt;li&gt;Nova - Manage virtual instances &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What is a tenant/project?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Determine true or false: 
  &lt;ul&gt; 
   &lt;li&gt;OpenStack is free to use&lt;/li&gt; 
   &lt;li&gt;The service responsible for networking is Glance&lt;/li&gt; 
   &lt;li&gt;The purpose of tenant/project is to share resources between different projects and users of OpenStack&lt;/li&gt;
  &lt;/ul&gt;&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;Describe in detail how you bring up an instance with a floating IP&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;You get a call from a customer saying: &quot;I can ping my instance but can&#39;t connect (ssh) it&quot;. What might be the problem?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What types of networks OpenStack supports?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;How do you debug OpenStack storage issues? (tools, logs, ...)&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;How do you debug OpenStack compute issues? (tools, logs, ...)&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;h4&gt;OpenStack Deployment &amp;amp; TripleO&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;Have you deployed OpenStack in the past? If yes, can you describe how you did it?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Are you familiar with TripleO? How is it different from Devstack or Packstack?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;You can read about TripleO right &lt;a href=&quot;https://docs.openstack.org/tripleo-docs/latest&quot;&gt;here&lt;/a&gt; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;h4&gt;OpenStack Compute&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;Can you describe Nova in detail?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Used to provision and manage virtual instances&lt;/li&gt; &lt;li&gt;It supports Multi-Tenancy in different levels - logging, end-user control, auditing, etc.&lt;/li&gt; &lt;li&gt;Highly scalable&lt;/li&gt; &lt;li&gt;Authentication can be done using internal system or LDAP&lt;/li&gt; &lt;li&gt;Supports multiple types of block storage&lt;/li&gt; &lt;li&gt;Tries to be hardware and hypervisor agnostice &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What do you know about Nova architecture and components?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;nova-api - the server which serves metadata and compute APIs&lt;/li&gt; &lt;li&gt;the different Nova components communicate by using a queue (Rabbitmq usually) and a database&lt;/li&gt; &lt;li&gt;a request for creating an instance is inspected by nova-scheduler which determines where the instance will be created and running&lt;/li&gt; &lt;li&gt;nova-compute is the component responsible for communicating with the hypervisor for creating the instance and manage its lifecycle &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;h4&gt;OpenStack Networking (Neutron)&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain Neutron in detail&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;One of the core component of OpenStack and a standalone project&lt;/li&gt; &lt;li&gt;Neutron focused on delivering networking as a service&lt;/li&gt; &lt;li&gt;With Neutron, users can set up networks in the cloud and configure and manage a variety of network services&lt;/li&gt; &lt;li&gt;Neutron interacts with: 
    &lt;ul&gt; 
     &lt;li&gt;Keystone - authorize API calls&lt;/li&gt; 
     &lt;li&gt;Nova - nova communicates with neutron to plug NICs into a network&lt;/li&gt; 
     &lt;li&gt;Horizon - supports networking entities in the dashboard and also provides topology view which includes networking details &lt;/li&gt;
    &lt;/ul&gt;&lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;    
&lt;details&gt; 
 &lt;summary&gt;Explain each of the following components: 
  &lt;ul&gt; 
   &lt;li&gt;neutron-dhcp-agent&lt;/li&gt; 
   &lt;li&gt;neutron-l3-agent&lt;/li&gt; 
   &lt;li&gt;neutron-metering-agent&lt;/li&gt; 
   &lt;li&gt;neutron-*-agtent&lt;/li&gt; 
   &lt;li&gt;neutron-server&lt;/li&gt;
  &lt;/ul&gt;&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt;  &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;neutron-l3-agent - L3/NAT forwarding (provides external network access for VMs for example)&lt;/li&gt; &lt;li&gt;neutron-dhcp-agent - DHCP services&lt;/li&gt; &lt;li&gt;neutron-metering-agent - L3 traffic metering&lt;/li&gt; &lt;li&gt;neutron-*-agtent - manages local vSwitch configuration on each compute (based on chosen plugin)&lt;/li&gt; &lt;li&gt;neutron-server - exposes networking API and passes requests to other plugins if required &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;Explain these network types: 
  &lt;ul&gt; 
   &lt;li&gt;Management Network&lt;/li&gt; 
   &lt;li&gt;Guest Network&lt;/li&gt; 
   &lt;li&gt;API Network&lt;/li&gt; 
   &lt;li&gt;External Network&lt;/li&gt;
  &lt;/ul&gt;&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt;  &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Management Network - used for internal communication between OpenStack components. Any IP address in this network is accessible only within the datacetner&lt;/li&gt; &lt;li&gt;Guest Network - used for communication between instances/VMs&lt;/li&gt; &lt;li&gt;API Network - used for services API communication. Any IP address in this network is publicly accessible&lt;/li&gt; &lt;li&gt;External Network - used for public communication. Any IP address in this network is accessible by anyone on the internet &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;In which order should you remove the following entities: 
  &lt;ul&gt; 
   &lt;li&gt;Network&lt;/li&gt; 
   &lt;li&gt;Port&lt;/li&gt; 
   &lt;li&gt;Router&lt;/li&gt; 
   &lt;li&gt;Subnet&lt;/li&gt;
  &lt;/ul&gt;&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt;  
  &lt;ul&gt; 
   &lt;li&gt;Port&lt;/li&gt; 
   &lt;li&gt;Subnet&lt;/li&gt; 
   &lt;li&gt;Router&lt;/li&gt; 
   &lt;li&gt;Network&lt;/li&gt; 
  &lt;/ul&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;There are many reasons for that. One for example: you can&#39;t remove router if there are active ports assigned to it. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is a provider network?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What components and services exist for L2 and L3?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is the ML2 plug-in? Explain its architecture&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is the L2 agent? How does it works and what is it responsible for?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is the L3 agent? How does it works and what is it responsible for?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain what the Metadata agent is responsible for&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What networking entities Neutron supports?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;How do you debug OpenStack networking issues? (tools, logs, ...)&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;h4&gt;OpenStack - Glance&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain Glance in detail&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Glance is the OpenStack image service&lt;/li&gt; &lt;li&gt;It handles requests related to instances disks and images&lt;/li&gt; &lt;li&gt;Glance also used for creating snapshots for quick instances backups&lt;/li&gt; &lt;li&gt;Users can use Glance to create new images or upload existing ones &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;Describe Glance architecture&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;glance-api - responsible for handling image API calls such as retrieval and storage. It consists of two APIs: 1. registry-api - responsible for internal requests 2. user API - can be accessed publicly&lt;/li&gt; &lt;li&gt;glance-registry - responsible for handling image metadata requests (e.g. size, type, etc). This component is private which means it&#39;s not available publicly&lt;/li&gt; &lt;li&gt;metadata definition service - API for custom metadata&lt;/li&gt; &lt;li&gt;database - for storing images metadata&lt;/li&gt; &lt;li&gt;image repository - for storing images. This can be a filesystem, swift object storage, HTTP, etc. &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;h4&gt;OpenStack - Swift&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain Swift in detail&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Swift is Object Store service and is an highly available, distributed and consistent store designed for storing a lot of data&lt;/li&gt; &lt;li&gt;Swift is distributing data across multiple servers while writing it to multiple disks&lt;/li&gt; &lt;li&gt;One can choose to add additional servers to scale the cluster. All while swift maintaining integrity of the information and data replications. &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;Can users store by default an object of 100GB in size?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Not by default. Object Storage API limits the maximum to 5GB per object but it can be adjusted. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain the following in regards to Swift: 
  &lt;ul&gt; 
   &lt;li&gt;Container&lt;/li&gt; 
   &lt;li&gt;Account&lt;/li&gt; 
   &lt;li&gt;Object&lt;/li&gt; 
  &lt;/ul&gt; &lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Container - Defines a namespace for objects.&lt;/li&gt; &lt;li&gt;Account - Defines a namespace for containers&lt;/li&gt; &lt;li&gt;Object - Data content (e.g. image, document, ...) &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;True or False? there can be two objects with the same name in the same container but not in two different containers&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;False. Two objects can have the same name if they are in different containers. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;h4&gt;OpenStack - Cinder&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain Cinder in detail&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Cinder is OpenStack Block Storage service&lt;/li&gt; &lt;li&gt;It basically provides used with storage resources they can consume with other services such as Nova&lt;/li&gt; &lt;li&gt;One of the most used implementations of storage supported by Cinder is LVM&lt;/li&gt; &lt;li&gt;From user perspective this is transparent which means the user doesn&#39;t know where, behind the scenes, the storage is located or what type of storage is used &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;Describe Cinder&#39;s components&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;cinder-api - receives API requests&lt;/li&gt; &lt;li&gt;cinder-volume - manages attached block devices&lt;/li&gt; &lt;li&gt;cinder-scheduler - responsible for storing volumes &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;h4&gt;OpenStack - Keystone&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;Can you describe the following concepts in regards to Keystone? 
  &lt;ul&gt; 
   &lt;li&gt;Role&lt;/li&gt; 
   &lt;li&gt;Tenant/Project&lt;/li&gt; 
   &lt;li&gt;Service&lt;/li&gt; 
   &lt;li&gt;Endpoint&lt;/li&gt; 
   &lt;li&gt;Token&lt;/li&gt; 
  &lt;/ul&gt; &lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Role - A list of rights and privileges determining what a user or a project can perform&lt;/li&gt; &lt;li&gt;Tenant/Project - Logical representation of a group of resources isolated from other groups of resources. It can be an account, organization, ...&lt;/li&gt; &lt;li&gt;Service - An endpoint which the user can use for accessing different resources&lt;/li&gt; &lt;li&gt;Endpoint - a network address which can be used to access a certain OpenStack service&lt;/li&gt; &lt;li&gt;Token - Used for access resources while describing which resources can be accessed by using a scope &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What are the properties of a service? In other words, how a service is identified?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;Using:&lt;/p&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Name&lt;/li&gt; &lt;li&gt;ID number&lt;/li&gt; &lt;li&gt;Type&lt;/li&gt; &lt;li&gt;Description &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;Explain the following: - PublicURL - InternalURL - AdminURL&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;PublicURL - Publicly accessible through public internet&lt;/li&gt; &lt;li&gt;InternalURL - Used for communication between services&lt;/li&gt; &lt;li&gt;AdminURL - Used for administrative management &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What is a service catalog?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;A list of services and their endpoints &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;h4&gt;OpenStack Advanced - Services&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;Describe each of the following services 
  &lt;ul&gt; 
   &lt;li&gt;Swift&lt;/li&gt; 
   &lt;li&gt;Sahara&lt;/li&gt; 
   &lt;li&gt;Ironic&lt;/li&gt; 
   &lt;li&gt;Trove&lt;/li&gt; 
   &lt;li&gt;Aodh&lt;/li&gt; 
   &lt;li&gt;Ceilometer&lt;/li&gt; 
  &lt;/ul&gt; &lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Swift - highly available, distributed, eventually consistent object/blob store&lt;/li&gt; &lt;li&gt;Sahara - Manage Hadoop Clusters&lt;/li&gt; &lt;li&gt;Ironic - Bare Metal Provisioning&lt;/li&gt; &lt;li&gt;Trove - Database as a service that runs on OpenStack&lt;/li&gt; &lt;li&gt;Aodh - Alarms Service&lt;/li&gt; &lt;li&gt;Ceilometer - Track and monitor usage &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;Identify the service/project used for each of the following: 
  &lt;ul&gt; 
   &lt;li&gt;Database as a service which runs on OpenStack&lt;/li&gt; 
   &lt;li&gt;Bare Metal Provisioning&lt;/li&gt; 
   &lt;li&gt;Track and monitor usage&lt;/li&gt; 
   &lt;li&gt;Alarms Service&lt;/li&gt; 
   &lt;li&gt;Manage Hadoop Clusters&lt;/li&gt; 
   &lt;li&gt;highly available, distributed, eventually consistent object/blob store&lt;/li&gt; 
  &lt;/ul&gt; &lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Database as a service which runs on OpenStack - Trove&lt;/li&gt; &lt;li&gt;Bare Metal Provisioning - Ironic&lt;/li&gt; &lt;li&gt;Track and monitor usage - Ceilometer&lt;/li&gt; &lt;li&gt;Alarms Service - Aodh&lt;/li&gt; &lt;li&gt;Manage Hadoop Clusters&lt;/li&gt; &lt;li&gt;Manage Hadoop Clusters - Sahara&lt;/li&gt; &lt;li&gt;highly available, distributed, eventually consistent object/blob store - Swift &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;h4&gt;OpenStack Advanced - Keystone&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;Can you describe Keystone service in detail?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;You can&#39;t have OpenStack deployed without Keystone&lt;/li&gt; &lt;li&gt;It Provides identity, policy and token services 
    &lt;ul&gt; 
     &lt;li&gt;The authentication provided is for both users and services&lt;/li&gt; 
     &lt;li&gt;The authorization supported is token-based and user-based.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;There is a policy defined based on RBAC stored in a JSON file and each line in that file defines the level of access to apply &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;Describe Keystone architecture&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;There is a service API and admin API through which Keystone gets requests&lt;/li&gt; &lt;li&gt;Keystone has four backends: 
    &lt;ul&gt; 
     &lt;li&gt;Token Backend - Temporary Tokens for users and services&lt;/li&gt; 
     &lt;li&gt;Policy Backend - Rules management and authorization&lt;/li&gt; 
     &lt;li&gt;Identity Backend - users and groups (either standalone DB, LDAP, ...)&lt;/li&gt; 
     &lt;li&gt;Catalog Backend - Endpoints&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;It has pluggable environment where you can integrate with: 
    &lt;ul&gt; 
     &lt;li&gt;LDAP&lt;/li&gt; 
     &lt;li&gt;KVS (Key Value Store)&lt;/li&gt; 
     &lt;li&gt;SQL&lt;/li&gt; 
     &lt;li&gt;PAM&lt;/li&gt; 
     &lt;li&gt;Memcached &lt;/li&gt;
    &lt;/ul&gt;&lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;    
&lt;details&gt; 
 &lt;summary&gt;Describe the Keystone authentication process&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Keystone gets a call/request and checks whether it&#39;s from an authorized user, using username, password and authURL&lt;/li&gt; &lt;li&gt;Once confirmed, Keystone provides a token.&lt;/li&gt; &lt;li&gt;A token contains a list of user&#39;s projects so there is no to authenticate every time and a token can submitted instead &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;h4&gt;OpenStack Advanced - Compute (Nova)&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;What each of the following does?: 
  &lt;ul&gt; 
   &lt;li&gt;nova-api&lt;/li&gt; 
   &lt;li&gt;nova-compuate&lt;/li&gt; 
   &lt;li&gt;nova-conductor&lt;/li&gt; 
   &lt;li&gt;nova-cert&lt;/li&gt; 
   &lt;li&gt;nova-consoleauth&lt;/li&gt; 
   &lt;li&gt;nova-scheduler&lt;/li&gt; 
  &lt;/ul&gt; &lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;nova-api - responsible for managing requests/calls&lt;/li&gt; &lt;li&gt;nova-compute - responsible for managing instance lifecycle&lt;/li&gt; &lt;li&gt;nova-conductor - Mediates between nova-compute and the database so nova-compute doesn&#39;t access it directly &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What types of Nova proxies are you familiar with?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Nova-novncproxy - Access through VNC connections&lt;/li&gt; &lt;li&gt;Nova-spicehtml5proxy - Access through SPICE&lt;/li&gt; &lt;li&gt;Nova-xvpvncproxy - Access through a VNC connection &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;h4&gt;OpenStack Advanced - Networking (Neutron)&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain BGP dynamic routing&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is the role of network namespaces in OpenStack?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;h4&gt;OpenStack Advanced - Horizon&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;Can you describe Horizon in detail?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Django-based project focusing on providing an OpenStack dashboard and the ability to create additional customized dashboards&lt;/li&gt; &lt;li&gt;You can use it to access the different OpenStack services resources - instances, images, networks, ... 
    &lt;ul&gt; 
     &lt;li&gt;By accessing the dashboard, users can use it to list, create, remove and modify the different resources&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;It&#39;s also highly customizable and you can modify or add to it based on your needs &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What can you tell about Horizon architecture?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;API is backward compatible&lt;/li&gt; &lt;li&gt;There are three type of dashboards: user, system and settings&lt;/li&gt; &lt;li&gt;It provides core support for all OpenStack core projects such as Neutron, Nova, etc. (out of the box, no need to install extra packages or plugins)&lt;/li&gt; &lt;li&gt;Anyone can extend the dashboards and add new components&lt;/li&gt; &lt;li&gt;Horizon provides templates and core classes from which one can build its own dashboard &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;h2&gt;Puppet&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is Puppet? How does it works?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Puppet is a configuration management tool ensuring that all systems are configured to a desired and predictable state. &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;Explain Puppet architecture&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Puppet has a primary-secondary node architecture. The clients are distributed across the network and communicate with the primary-secondary environment where Puppet modules are present. The client agent sends a certificate with its ID to the server; the server then signs that certificate and sends it back to the client. This authentication allows for secure and verifiable communication between the client and the master. &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;Can you compare Puppet to other configuration management tools? Why did you chose to use Puppet?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Puppet is often compared to other configuration management tools like Chef, Ansible, SaltStack, and cfengine. The choice to use Puppet often depends on an organization&#39;s needs, such as ease of use, scalability, and community support. &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;Explain the following: 
  &lt;ul&gt; 
   &lt;li&gt;Module&lt;/li&gt; 
   &lt;li&gt;Manifest&lt;/li&gt; 
   &lt;li&gt;Node&lt;/li&gt; 
  &lt;/ul&gt; &lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Modules - are a collection of manifests, templates, and files&lt;/li&gt; &lt;li&gt;Manifests - are the actual codes for configuring the clients&lt;/li&gt; &lt;li&gt;Node - allows you to assign specific configurations to specific nodes &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;Explain Facter&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Facter is a standalone tool in Puppet that collects information about a system and its configuration, such as the operating system, IP addresses, memory, and network interfaces. This information can be used in Puppet manifests to make decisions about how resources should be managed, and to customize the behavior of Puppet based on the characteristics of the system. Facter is integrated into Puppet, and its facts can be used within Puppet manifests to make decisions about resource management. &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What is MCollective?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;MCollective is a middleware system that integrates with Puppet to provide orchestration, remote execution, and parallel job execution capabilities. &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;Do you have experience with writing modules? Which module have you created and for what?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain what is Hiera&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Hiera is a hierarchical data store in Puppet that is used to separate data from code, allowing data to be more easily separated, managed, and reused. &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;h2&gt;Elastic&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is the Elastic Stack?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;The Elastic Stack consists of:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Elasticsearch&lt;/li&gt; 
   &lt;li&gt;Kibana&lt;/li&gt; 
   &lt;li&gt;Logstash&lt;/li&gt; 
   &lt;li&gt;Beats&lt;/li&gt; 
   &lt;li&gt;Elastic Hadoop&lt;/li&gt; 
   &lt;li&gt;APM Server&lt;/li&gt; 
  &lt;/ul&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Elasticsearch, Logstash and Kibana are also known as the ELK stack. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain what is Elasticsearch&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;From the official &lt;a href=&quot;https://www.elastic.co/guide/en/elasticsearch/reference/current/documents-indices.html&quot;&gt;docs&lt;/a&gt;:&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&quot;Elasticsearch is a distributed document store. Instead of storing information as rows of columnar data, Elasticsearch stores complex data structures that have been serialized as JSON documents&quot; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is Logstash?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;From the &lt;a href=&quot;https://logit.io/blog/post/the-top-50-elk-stack-and-elasticsearch-interview-questions&quot;&gt;blog&lt;/a&gt;:&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&quot;Logstash is a powerful, flexible pipeline that collects, enriches and transports data. It works as an extract, transform &amp;amp; load (ETL) tool for collecting log messages.&quot; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain what beats are&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Beats are lightweight data shippers. These data shippers installed on the client where the data resides. Examples of beats: Filebeat, Metricbeat, Auditbeat. There are much more.&lt;br&gt; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is Kibana?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;From the official docs:&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&quot;Kibana is an open source analytics and visualization platform designed to work with Elasticsearch. You use Kibana to search, view, and interact with data stored in Elasticsearch indices. You can easily perform advanced data analysis and visualize your data in a variety of charts, tables, and maps.&quot; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Describe what happens from the moment an app logged some information until it&#39;s displayed to the user in a dashboard when the Elastic stack is used&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;The process may vary based on the chosen architecture and the processing you may want to apply to the logs. One possible workflow is:&lt;/p&gt; &lt;/b&gt;
 &lt;ol&gt;
  &lt;b&gt; &lt;li&gt;The data logged by the application is picked by filebeat and sent to logstash&lt;/li&gt; &lt;li&gt;Logstash process the log based on the defined filters. Once done, the output is sent to Elasticsearch&lt;/li&gt; &lt;li&gt;Elasticsearch stores the document it got and the document is indexed for quick future access&lt;/li&gt; &lt;li&gt;The user creates visualizations in Kibana which based on the indexed data&lt;/li&gt; &lt;li&gt;The user creates a dashboard which composed out of the visualization created in the previous step &lt;/li&gt;&lt;/b&gt;
 &lt;/ol&gt;
&lt;/details&gt;  
&lt;h5&gt;Elasticsearch&lt;/h5&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is a data node?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;This is where data is stored and also where different processing takes place (e.g. when you search for a data). &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is a master node?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;Part of a master node responsibilities:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Track the status of all the nodes in the cluster&lt;/li&gt; 
   &lt;li&gt;Verify replicas are working and the data is available from every data node.&lt;/li&gt; 
   &lt;li&gt;No hot nodes (no data node that works much harder than other nodes)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;While there can be multiple master nodes in reality only of them is the elected master node. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is an ingest node?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;A node which responsible for processing the data according to ingest pipeline. In case you don&#39;t need to use logstash then this node can receive data from beats and process it, similarly to how it can be processed in Logstash. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is Coordinating only node?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;From the official docs:&lt;/p&gt; &lt;p&gt;Coordinating only nodes can benefit large clusters by offloading the coordinating node role from data and master-eligible nodes. They join the cluster and receive the full cluster state, like every other node, and they use the cluster state to route requests directly to the appropriate place(s).&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;How data is stored in Elasticsearch?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Data is stored in an index&lt;/li&gt; &lt;li&gt;The index is spread across the cluster using shards &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What is an Index?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;Index in Elasticsearch is in most cases compared to a whole database from the SQL/NoSQL world.&lt;br&gt; You can choose to have one index to hold all the data of your app or have multiple indices where each index holds different type of your app (e.g. index for each service your app is running).&lt;/p&gt; &lt;p&gt;The official docs also offer a great explanation (in general, it&#39;s really good documentation, as every project should have):&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&quot;An index can be thought of as an optimized collection of documents and each document is a collection of fields, which are the key-value pairs that contain your data&quot; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain Shards&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;An index is split into shards and documents are hashed to a particular shard. Each shard may be on a different node in a cluster and each one of the shards is a self contained index.&lt;br&gt; This allows Elasticsearch to scale to an entire cluster of servers. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is an Inverted Index?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;From the official docs:&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&quot;An inverted index lists every unique word that appears in any document and identifies all of the documents each word occurs in.&quot; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is a Document?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Continuing with the comparison to SQL/NoSQL a Document in Elasticsearch is a row in table in the case of SQL or a document in a collection in the case of NoSQL. As in NoSQL a document is a JSON object which holds data on a unit in your app. What is this unit depends on the your app. If your app related to book then each document describes a book. If you are app is about shirts then each document is a shirt. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;You check the health of your elasticsearch cluster and it&#39;s red. What does it mean? What can cause the status to be yellow instead of green?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Red means some data is unavailable in your cluster. Some shards of your indices are unassigned. There are some other states for the cluster. Yellow means that you have unassigned shards in the cluster. You can be in this state if you have single node and your indices have replicas. Green means that all shards in the cluster are assigned to nodes and your cluster is healthy. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;True or False? Elasticsearch indexes all data in every field and each indexed field has the same data structure for unified and quick query ability&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;False. From the official docs:&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&quot;Each indexed field has a dedicated, optimized data structure. For example, text fields are stored in inverted indices, and numeric and geo fields are stored in BKD trees.&quot; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What reserved fields a document has?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;_index&lt;/li&gt; &lt;li&gt;_id&lt;/li&gt; &lt;li&gt;_type &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;Explain Mapping&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What are the advantages of defining your own mapping? (or: when would you use your own mapping?)&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;You can optimize fields for partial matching&lt;/li&gt; &lt;li&gt;You can define custom formats of known fields (e.g. date)&lt;/li&gt; &lt;li&gt;You can perform language-specific analysis &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;Explain Replicas&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;In a network/cloud environment where failures can be expected any time, it is very useful and highly recommended to have a failover mechanism in case a shard/node somehow goes offline or disappears for whatever reason. To this end, Elasticsearch allows you to make one or more copies of your index’s shards into what are called replica shards, or replicas for short. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Can you explain Term Frequency &amp;amp; Document Frequency?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Term Frequency is how often a term appears in a given document and Document Frequency is how often a term appears in all documents. They both are used for determining the relevance of a term by calculating Term Frequency / Document Frequency. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;You check &quot;Current Phase&quot; under &quot;Index lifecycle management&quot; and you see it&#39;s set to &quot;hot&quot;. What does it mean?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&quot;The index is actively being written to&quot;. More about the phases &lt;a href=&quot;https://www.elastic.co/guide/en/elasticsearch/reference/7.6/ilm-policy-definition.html&quot;&gt;here&lt;/a&gt; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What this command does? &lt;code&gt;curl -X PUT &quot;localhost:9200/customer/_doc/1?pretty&quot; -H &#39;Content-Type: application/json&#39; -d&#39;{ &quot;name&quot;: &quot;John Doe&quot; }&#39;&lt;/code&gt;&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;It creates customer index if it doesn&#39;t exists and adds a new document with the field name which is set to &quot;John Dow&quot;. Also, if it&#39;s the first document it will get the ID 1. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What will happen if you run the previous command twice? What about running it 100 times?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ol&gt;
  &lt;b&gt; &lt;li&gt;If name value was different then it would update &quot;name&quot; to the new value&lt;/li&gt; &lt;li&gt;In any case, it bumps version field by one &lt;/li&gt;&lt;/b&gt;
 &lt;/ol&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What is the Bulk API? What would you use it for?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Bulk API is used when you need to index multiple documents. For high number of documents it would be significantly faster to use rather than individual requests since there are less network roundtrips. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;h5&gt;Query DSL&lt;/h5&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain Elasticsearch query syntax (Booleans, Fields, Ranges)&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain what is Relevance Score&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain Query Context and Filter Context&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;From the official docs:&lt;/p&gt; &lt;p&gt;&quot;In the query context, a query clause answers the question “How well does this document match this query clause?” Besides deciding whether or not the document matches, the query clause also calculates a relevance score in the _score meta-field.&quot;&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&quot;In a filter context, a query clause answers the question “Does this document match this query clause?” The answer is a simple Yes or No — no scores are calculated. Filter context is mostly used for filtering structured data&quot; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Describe how would an architecture of production environment with large amounts of data would be different from a small-scale environment&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;There are several possible answers for this question. One of them is as follows:&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;A small-scale architecture of elastic will consist of the elastic stack as it is. This means we will have beats, logstash, elastcsearch and kibana.&lt;br&gt; A production environment with large amounts of data can include some kind of buffering component (e.g. Reddis or RabbitMQ) and also security component such as Nginx. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;h5&gt;Logstash&lt;/h5&gt; 
&lt;details&gt; 
 &lt;summary&gt;What are Logstash plugins? What plugins types are there?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Input Plugins - how to collect data from different sources&lt;/li&gt; &lt;li&gt;Filter Plugins - processing data&lt;/li&gt; &lt;li&gt;Output Plugins - push data to different outputs/services/platforms &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What is grok?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;A logstash plugin which modifies information in one format and immerse it in another. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;How grok works?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What grok patterns are you familiar with?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is `_grokparsefailure?`&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;How do you test or debug grok patterns?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What are Logstash Codecs? What codecs are there?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;h5&gt;Kibana&lt;/h5&gt; 
&lt;details&gt; 
 &lt;summary&gt;What can you find under &quot;Discover&quot; in Kibana?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;The raw data as it is stored in the index. You can search and filter it. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;You see in Kibana, after clicking on Discover, &quot;561 hits&quot;. What does it mean?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Total number of documents matching the search results. If not query used then simply the total number of documents. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What can you find under &quot;Visualize&quot;?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&quot;Visualize&quot; is where you can create visual representations for your data (pie charts, graphs, ...) &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What visualization types are supported/included in Kibana?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What visualization type would you use for statistical outliers&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Describe in detail how do you create a dashboard in Kibana&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;h4&gt;Filebeat&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is Filebeat?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Filebeat is used to monitor the logging directories inside of VMs or mounted as a sidecar if exporting logs from containers, and then forward these logs onward for further processing, usually to logstash. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;If one is using ELK, is it a must to also use filebeat? In what scenarios it&#39;s useful to use filebeat?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Filebeat is a typical component of the ELK stack, since it was developed by Elastic to work with the other products (Logstash and Kibana). It&#39;s possible to send logs directly to logstash, though this often requires coding changes for the application. Particularly for legacy applications with little test coverage, it might be a better option to use filebeat, since you don&#39;t need to make any changes to the application code. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is a harvester?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Read &lt;a href=&quot;https://www.elastic.co/guide/en/beats/filebeat/current/how-filebeat-works.html#harvester&quot;&gt;here&lt;/a&gt; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;True or False? a single harvester harvest multiple files, according to the limits set in filebeat.yml&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;False. One harvester harvests one file. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What are filebeat modules?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;These are pre-configured modules for specific types of logging locations (eg, Traefik, Fargate, HAProxy) to make it easy to configure forwarding logs using filebeat. They have different configurations based on where you&#39;re collecting logs from. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;h4&gt;Elastic Stack&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;How do you secure an Elastic Stack?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;You can generate certificates with the provided elastic utils and change configuration to enable security using certificates model. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;h2&gt;Distributed&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain Distributed Computing (or Distributed System)&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;According to Martin Kleppmann:&lt;/p&gt; &lt;p&gt;&quot;Many processes running on many machines...only message-passing via an unreliable network with variable delays, and the system may suffer from partial failures, unreliable clocks, and process pauses.&quot;&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Another definition: &quot;Systems that are physically separated, but logically connected&quot; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What can cause a system to fail?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Network&lt;/li&gt; &lt;li&gt;CPU&lt;/li&gt; &lt;li&gt;Memory&lt;/li&gt; &lt;li&gt;Disk &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;Do you know what is &quot;CAP theorem&quot;? (aka as Brewer&#39;s theorem)&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;According to the CAP theorem, it&#39;s not possible for a distributed data store to provide more than two of the following at the same time:&lt;/p&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Availability: Every request receives a response (it doesn&#39;t has to be the most recent data)&lt;/li&gt; &lt;li&gt;Consistency: Every request receives a response with the latest/most recent data&lt;/li&gt; &lt;li&gt;Partition tolerance: Even if some the data is lost/dropped, the system keeps running &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What are the problems with the following design? How to improve it?&lt;br&gt; &lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/distributed/distributed_design_standby.png&quot; width=&quot;500x;&quot; height=&quot;350px;&quot;&gt; &lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; 1. The transition can take time. In other words, noticeable downtime. 2. Standby server is a waste of resources - if first application server is running then the standby does nothing &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What are the problems with the following design? How to improve it?&lt;br&gt; &lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/distributed/distributed_design_lb.png&quot; width=&quot;700x;&quot; height=&quot;350px;&quot;&gt; &lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; Issues: If load balancer dies , we lose the ability to communicate with the application. &lt;p&gt;Ways to improve:&lt;/p&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Add another load balancer&lt;/li&gt; &lt;li&gt;Use DNS A record for both load balancers&lt;/li&gt; &lt;li&gt;Use message queue &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What is &quot;Shared-Nothing&quot; architecture?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;It&#39;s an architecture in which data is and retrieved from a single, non-shared, source usually exclusively connected to one node as opposed to architectures where the request can get to one of many nodes and the data will be retrieved from one shared location (storage, memory, ...). &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain the Sidecar Pattern (Or sidecar proxy)&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;h2&gt;Misc&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Name&lt;/th&gt; 
   &lt;th&gt;Topic&lt;/th&gt; 
   &lt;th&gt;Objective &amp;amp; Instructions&lt;/th&gt; 
   &lt;th&gt;Solution&lt;/th&gt; 
   &lt;th&gt;Comments&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Highly Available &quot;Hello World&quot;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/devops/ha_hello_world.md&quot;&gt;Exercise&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/devops/solutions/ha_hello_world.md&quot;&gt;Solution&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;details&gt; 
 &lt;summary&gt;What happens when you type in a URL in an address bar in a browser?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; 
  &lt;ol&gt; 
   &lt;li&gt;The browser searches for the record of the domain name IP address in the DNS in the following order:&lt;/li&gt; 
  &lt;/ol&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Browser cache&lt;/li&gt; 
   &lt;li&gt;Operating system cache&lt;/li&gt; 
   &lt;li&gt;The DNS server configured on the user&#39;s system (can be ISP DNS, public DNS, ...)&lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;ol start=&quot;2&quot;&gt; 
   &lt;li&gt;If it couldn&#39;t find a DNS record locally, a full DNS resolution is started.&lt;/li&gt; 
   &lt;li&gt;It connects to the server using the TCP protocol&lt;/li&gt; 
   &lt;li&gt;The browser sends an HTTP request to the server&lt;/li&gt; 
   &lt;li&gt;The server sends an HTTP response back to the browser&lt;/li&gt; 
   &lt;li&gt;The browser renders the response (e.g. HTML)&lt;/li&gt; 
   &lt;li&gt;The browser then sends subsequent requests as needed to the server to get the embedded links, javascript, images in the HTML and then steps 3 to 5 are repeated.&lt;/li&gt; 
  &lt;/ol&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;TODO: add more details! &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;h4&gt;API&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain what is an API&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;I like this definition from &lt;a href=&quot;https://blog.christianposta.com/microservices/api-gateways-are-going-through-an-identity-crisis&quot;&gt;blog.christianposta.com&lt;/a&gt;:&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&quot;An explicitly and purposefully defined interface designed to be invoked over a network that enables software developers to get programmatic access to data and functionality within an organization in a controlled and comfortable way.&quot; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is an API specification?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;From &lt;a href=&quot;https://swagger.io/resources/articles/difference-between-api-documentation-specification&quot;&gt;swagger.io&lt;/a&gt;:&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&quot;An API specification provides a broad understanding of how an API behaves and how the API links with other APIs. It explains how the API functions and the results to expect when using the API&quot; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;True or False? API Definition is the same as API Specification&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;False. From &lt;a href=&quot;https://swagger.io/resources/articles/difference-between-api-documentation-specification&quot;&gt;swagger.io&lt;/a&gt;:&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&quot;An API definition is similar to an API specification in that it provides an understanding of how an API is organized and how the API functions. But the API definition is aimed at machine consumption instead of human consumption of APIs.&quot; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is an API gateway?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;An API gateway is like the gatekeeper that controls how different parts talk to each other and how information is exchanged between them.&lt;/p&gt; &lt;p&gt;The API gateway provides a single point of entry for all clients, and it can perform several tasks, including routing requests to the appropriate backend service, load balancing, security and authentication, rate limiting, caching, and monitoring.&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;By using an API gateway, organizations can simplify the management of their APIs, ensure consistent security and governance, and improve the performance and scalability of their backend services. They are also commonly used in microservices architectures, where there are many small, independent services that need to be accessed by different clients. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What are the advantages of using/implementing an API gateway?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;Advantages:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Simplifies API management: Provides a single entry point for all requests, which simplifies the management and monitoring of multiple APIs.&lt;/li&gt; 
   &lt;li&gt;Improves security: Able to implement security features like authentication, authorization, and encryption to protect the backend services from unauthorized access.&lt;/li&gt; 
   &lt;li&gt;Enhances scalability: Can handle traffic spikes and distribute requests to backend services in a way that maximizes resource utilization and improves overall system performance.&lt;/li&gt; 
   &lt;li&gt;Enables service composition: Can combine different backend services into a single API, providing more granular control over the services that clients can access.&lt;/li&gt; 
   &lt;li&gt;Facilitates integration with external systems: Can be used to expose internal services to external partners or customers, making it easier to integrate with external systems and enabling new business models.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is a Payload in API?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is Automation? How it&#39;s related or different from Orchestration?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Automation is the act of automating tasks to reduce human intervention or interaction in regards to IT technology and systems.&lt;br&gt; While automation focuses on a task level, Orchestration is the process of automating processes and/or workflows which consists of multiple tasks that usually across multiple systems. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Tell me about interesting bugs you&#39;ve found and also fixed&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is a Debugger and how it works?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What services an application might have?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Authorization&lt;/li&gt; &lt;li&gt;Logging&lt;/li&gt; &lt;li&gt;Authentication&lt;/li&gt; &lt;li&gt;Ordering&lt;/li&gt; &lt;li&gt;Front-end&lt;/li&gt; &lt;li&gt;Back-end ... &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What is Metadata?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Data about data. Basically, it describes the type of information that an underlying data will hold. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;You can use one of the following formats: JSON, YAML, XML. Which one would you use? Why?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;I can&#39;t answer this for you :) &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What&#39;s KPI?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What&#39;s OKR?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What&#39;s DSL (Domain Specific Language)?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Domain Specific Language (DSLs) are used to create a customised language that represents the domain such that domain experts can easily interpret it. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What&#39;s the difference between KPI and OKR?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;h4&gt;YAML&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is YAML?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Data serialization language used by many technologies today like Kubernetes, Ansible, etc. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;True or False? Any valid JSON file is also a valid YAML file&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;True. Because YAML is superset of JSON. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is the format of the following data? &lt;pre&gt;&lt;code&gt;{
    applications: [
        {
            name: &quot;my_app&quot;,
            language: &quot;python&quot;,
            version: 20.17
        }
    ]
}
&lt;/code&gt;&lt;/pre&gt; &lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; JSON &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is the format of the following data? &lt;pre&gt;&lt;code&gt;applications:
  - app: &quot;my_app&quot;
    language: &quot;python&quot;
    version: 20.17
&lt;/code&gt;&lt;/pre&gt; &lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; YAML &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;How to write a multi-line string with YAML? What use cases is it good for?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;pre&gt;&lt;code&gt;someMultiLineString: |
  look mama
  I can write a multi-line string
  I love YAML
&lt;/code&gt;&lt;/pre&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;It&#39;s good for use cases like writing a shell script where each line of the script is a different command. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is the difference between &lt;code&gt;someMultiLineString: |&lt;/code&gt; to &lt;code&gt;someMultiLineString: &amp;gt;&lt;/code&gt;?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;using &lt;code&gt;&amp;gt;&lt;/code&gt; will make the multi-line string to fold into a single line&lt;/p&gt; &lt;pre&gt;&lt;code&gt;someMultiLineString: &amp;gt;
  This is actually
  a single line
  do not let appearances fool you
&lt;/code&gt;&lt;/pre&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What are placeholders in YAML?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;They allow you reference values instead of directly writing them and it is used like this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;username: {{ my.user_name }}
&lt;/code&gt;&lt;/pre&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;How can you define multiple YAML components in one file?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;Using this: &lt;code&gt;---&lt;/code&gt; For Examples:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;document_number: 1
---
document_number: 2
&lt;/code&gt;&lt;/pre&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;h4&gt;Firmware&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain what is a firmware&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Firmware&quot;&gt;Wikipedia&lt;/a&gt;: &quot;In computing, firmware is a specific class of computer software that provides the low-level control for a device&#39;s specific hardware. Firmware, such as the BIOS of a personal computer, may contain basic functions of a device, and may provide hardware abstraction services to higher-level software such as operating systems.&quot; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;h2&gt;Cassandra&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;When running a cassandra cluster, how often do you need to run nodetool repair in order to keep the cluster consistent? 
  &lt;ul&gt; 
   &lt;li&gt;Within the columnFamily GC-grace Once a week&lt;/li&gt; 
   &lt;li&gt;Less than the compacted partition minimum bytes&lt;/li&gt; 
   &lt;li&gt;Depended on the compaction strategy&lt;/li&gt; 
  &lt;/ul&gt; &lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;h2&gt;HTTP&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is HTTP?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;a href=&quot;https://avinetworks.com/glossary/layer-7/&quot;&gt;Avinetworks&lt;/a&gt;: HTTP stands for Hypertext Transfer Protocol. HTTP uses TCP port 80 to enable internet communication. It is part of the Application Layer (L7) in OSI Model. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Describe HTTP request lifecycle&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Resolve host by request to DNS resolver&lt;/li&gt; &lt;li&gt;Client SYN&lt;/li&gt; &lt;li&gt;Server SYN+ACK&lt;/li&gt; &lt;li&gt;Client SYN&lt;/li&gt; &lt;li&gt;HTTP request&lt;/li&gt; &lt;li&gt;HTTP response &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;True or False? HTTP is stateful&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;False. It doesn&#39;t maintain state for incoming request. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;How HTTP request looks like?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;It consists of:&lt;/p&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Request line - request type&lt;/li&gt; &lt;li&gt;Headers - content info like length, encoding, etc.&lt;/li&gt; &lt;li&gt;Body (not always included) &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What HTTP method types are there?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;GET&lt;/li&gt; &lt;li&gt;POST&lt;/li&gt; &lt;li&gt;HEAD&lt;/li&gt; &lt;li&gt;PUT&lt;/li&gt; &lt;li&gt;DELETE&lt;/li&gt; &lt;li&gt;CONNECT&lt;/li&gt; &lt;li&gt;OPTIONS&lt;/li&gt; &lt;li&gt;TRACE &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What HTTP response codes are there?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;1xx - informational&lt;/li&gt; &lt;li&gt;2xx - Success&lt;/li&gt; &lt;li&gt;3xx - Redirect&lt;/li&gt; &lt;li&gt;4xx - Error, client fault&lt;/li&gt; &lt;li&gt;5xx - Error, server fault &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What is HTTPS?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;HTTPS is a secure version of the HTTP protocol used to transfer data between a web browser and a web server. It encrypts the communication using SSL/TLS encryption to ensure that the data is private and secure.&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Learn more: &lt;a href=&quot;https://www.cloudflare.com/learning/ssl/why-is-http-not-secure/&quot;&gt;https://www.cloudflare.com/learning/ssl/why-is-http-not-secure/&lt;/a&gt; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain HTTP Cookies&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;HTTP is stateless. To share state, we can use Cookies.&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;TODO: explain what is actually a Cookie &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is HTTP Pipelining?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;You get &quot;504 Gateway Timeout&quot; error from an HTTP server. What does it mean?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;The server didn&#39;t receive a response from another server it communicates with in a timely manner. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is a proxy?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;A proxy is a server that acts as a middleman between a client device and a destination server. It can help improve privacy, security, and performance by hiding the client&#39;s IP address, filtering content, and caching frequently accessed data.&lt;/p&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Proxies can be used for load balancing, distributing traffic across multiple servers to help prevent server overload and improve website or application performance. They can also be used for data analysis, as they can log requests and traffic, providing useful insights into user behavior and preferences. &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What is a reverse proxy?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;A reverse proxy is a type of proxy server that sits between a client and a server, but it is used to manage traffic going in the opposite direction of a traditional forward proxy. In a forward proxy, the client sends requests to the proxy server, which then forwards them to the destination server. However, in a reverse proxy, the client sends requests to the destination server, but the requests are intercepted by the reverse proxy before they reach the server.&lt;/p&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;They&#39;re commonly used to improve web server performance, provide high availability and fault tolerance, and enhance security by preventing direct access to the back-end server. They are often used in large-scale web applications and high-traffic websites to manage and distribute requests to multiple servers, resulting in improved scalability and reliability. &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;When you publish a project, you usually publish it with a license. What types of licenses are you familiar with and which one do you prefer to use?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain what is &quot;X-Forwarded-For&quot;&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/X-Forwarded-For&quot;&gt;Wikipedia&lt;/a&gt;: &quot;The X-Forwarded-For (XFF) HTTP header field is a common method for identifying the originating IP address of a client connecting to a web server through an HTTP proxy or load balancer.&quot; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;h4&gt;Load Balancers&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is a load balancer?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;A load balancer accepts (or denies) incoming network traffic from a client, and based on some criteria (application related, network, etc.) it distributes those communications out to servers (at least one). &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Why to used a load balancer?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Scalability - using a load balancer, you can possibly add more servers in the backend to handle more requests/traffic from the clients, as opposed to using one server.&lt;/li&gt; &lt;li&gt;Redundancy - if one server in the backend dies, the load balancer will keep forwarding the traffic/requests to the second server so users won&#39;t even notice one of the servers in the backend is down. &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What load balancer techniques/algorithms are you familiar with?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Round Robin&lt;/li&gt; &lt;li&gt;Weighted Round Robin&lt;/li&gt; &lt;li&gt;Least Connection&lt;/li&gt; &lt;li&gt;Weighted Least Connection&lt;/li&gt; &lt;li&gt;Resource Based&lt;/li&gt; &lt;li&gt;Fixed Weighting&lt;/li&gt; &lt;li&gt;Weighted Response Time&lt;/li&gt; &lt;li&gt;Source IP Hash&lt;/li&gt; &lt;li&gt;URL Hash &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What are the drawbacks of round robin algorithm in load balancing?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;A simple round robin algorithm knows nothing about the load and the spec of each server it forwards the requests to. It is possible, that multiple heavy workloads requests will get to the same server while other servers will got only lightweight requests which will result in one server doing most of the work, maybe even crashing at some point because it unable to handle all the heavy workloads requests by its own.&lt;/li&gt; &lt;li&gt;Each request from the client creates a whole new session. This might be a problem for certain scenarios where you would like to perform multiple operations where the server has to know about the result of operation so basically, being sort of aware of the history it has with the client. In round robin, first request might hit server X, while second request might hit server Y and ask to continue processing the data that was processed on server X already. &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What is an Application Load Balancer?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;In which scenarios would you use ALB?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;At what layers a load balancer can operate?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;L4 and L7 &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Can you perform load balancing without using a dedicated load balancer instance?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Yes, you can use DNS for performing load balancing. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is DNS load balancing? What its advantages? When would you use it?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;h4&gt;Load Balancers - Sticky Sessions&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;What are sticky sessions? What are their pros and cons?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;Recommended read:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://access.redhat.com/solutions/900933&quot;&gt;Red Hat Article&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;p&gt;Cons:&lt;/p&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Can cause uneven load on instance (since requests routed to the same instances) Pros:&lt;/li&gt; &lt;li&gt;Ensures in-proc sessions are not lost when a new request is created &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;Name one use case for using sticky sessions&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;You would like to make sure the user doesn&#39;t lose the current session data. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What sticky sessions use for enabling the &quot;stickiness&quot;?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Cookies. There are application based cookies and duration based cookies. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain application-based cookies&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Generated by the application and/or the load balancer&lt;/li&gt; &lt;li&gt;Usually allows to include custom data &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;Explain duration-based cookies&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Generated by the load balancer&lt;/li&gt; &lt;li&gt;Session is not sticky anymore once the duration elapsed &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;h4&gt;Load Balancers - Load Balancing Algorithms&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain each of the following load balancing techniques 
  &lt;ul&gt; 
   &lt;li&gt;Round Robin&lt;/li&gt; 
   &lt;li&gt;Weighted Round Robin&lt;/li&gt; 
   &lt;li&gt;Least Connection&lt;/li&gt; 
   &lt;li&gt;Weighted Least Connection&lt;/li&gt; 
   &lt;li&gt;Resource Based&lt;/li&gt; 
   &lt;li&gt;Fixed Weighting&lt;/li&gt; 
   &lt;li&gt;Weighted Response Time&lt;/li&gt; 
   &lt;li&gt;Source IP Hash&lt;/li&gt; 
   &lt;li&gt;URL Hash&lt;/li&gt; 
  &lt;/ul&gt; &lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain use case for connection draining?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; To ensure that a Classic Load Balancer stops sending requests to instances that are de-registering or unhealthy, while keeping the existing connections open, use connection draining. This enables the load balancer to complete in-flight requests made to instances that are de-registering or unhealthy. &lt;p&gt;The maximum timeout value can be set between 1 and 3,600 seconds on both GCP and AWS.&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;h4&gt;Licenses&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;Are you familiar with &quot;Creative Commons&quot;? What do you know about it?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;The Creative Commons license is a set of copyright licenses that allow creators to share their work with the public while retaining some control over how it can be used. The license was developed as a response to the restrictive standards of traditional copyright laws, which limited access of creative works. Its creators to choose the terms under which their works can be shared, distributed, and used by others. They&#39;re six main types of Creative Commons licenses, each with different levels of restrictions and permissions, the six licenses are:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Attribution (CC BY): Allows others to distribute, remix, and build upon the work, even commercially, as long as they credit the original creator.&lt;/li&gt; 
   &lt;li&gt;Attribution-ShareAlike (CC BY-SA): Allows others to remix and build upon the work, even commercially, as long as they credit the original creator and release any new creations under the same license.&lt;/li&gt; 
   &lt;li&gt;Attribution-NoDerivs (CC BY-ND): Allows others to distribute the work, even commercially, but they cannot remix or change it in any way and must credit the original creator.&lt;/li&gt; 
   &lt;li&gt;Attribution-NonCommercial (CC BY-NC): Allows others to remix and build upon the work, but they cannot use it commercially and must credit the original creator.&lt;/li&gt; 
   &lt;li&gt;Attribution-NonCommercial-ShareAlike (CC BY-NC-SA): Allows others to remix and build upon the work, but they cannot use it commercially, must credit the original creator, and must release any new creations under the same license.&lt;/li&gt; 
   &lt;li&gt;Attribution-NonCommercial-NoDerivs (CC BY-NC-ND): Allows others to download and share the work, but they cannot use it commercially, remix or change it in any way, and must credit the original creator.&lt;/li&gt; 
  &lt;/ul&gt; &lt;p&gt;Simply stated, the Creative Commons licenses are a way for creators to share their work with the public while retaining some control over how it can be used. The licenses promote creativity, innovation, and collaboration, while also respecting the rights of creators while still encouraging the responsible use of creative works.&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;More information: &lt;a href=&quot;https://creativecommons.org/licenses/&quot;&gt;https://creativecommons.org/licenses/&lt;/a&gt; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain the differences between copyleft and permissive licenses&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;In Copyleft, any derivative work must use the same licensing while in permissive licensing there are no such condition. GPL-3 is an example of copyleft license while BSD is an example of permissive license. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;h4&gt;Random&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;How a search engine works?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;How auto completion works?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is faster than RAM?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;CPU cache. &lt;a href=&quot;https://www.enterprisestorageforum.com/hardware/cache-memory/&quot;&gt;Source&lt;/a&gt; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is a memory leak?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;A memory leak is a programming error that occurs when a program fails to release memory that is no longer needed, causing the program to consume increasing amounts of memory over time.&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;The leaks can lead to a variety of problems, including system crashes, performance degradation, and instability. Usually occurring after failed maintenance on older systems and compatibility with new components over time. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is your favorite protocol?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;SSH HTTP DHCP DNS ... &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is Cache API?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is the C10K problem? Is it relevant today?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;a href=&quot;https://idiallo.com/blog/c10k-2016&quot;&gt;https://idiallo.com/blog/c10k-2016&lt;/a&gt; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;h2&gt;Storage&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;What types of storage are there?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;File&lt;/li&gt; &lt;li&gt;Block&lt;/li&gt; &lt;li&gt;Object &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;Explain Object Storage&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Data is divided to self-contained objects&lt;/li&gt; &lt;li&gt;Objects can contain metadata &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What are the pros and cons of object storage?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;Pros:&lt;/p&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Usually with object storage, you pay for what you use as opposed to other storage types where you pay for the storage space you allocate&lt;/li&gt; &lt;li&gt;Scalable storage: Object storage mostly based on a model where what you use, is what you get and you can add storage as need Cons:&lt;/li&gt; &lt;li&gt;Usually performs slower than other types of storage&lt;/li&gt; &lt;li&gt;No granular modification: to change an object, you have re-create it &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What are some use cases for using object storage?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain File Storage&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;File Storage used for storing data in files, in a hierarchical structure&lt;/li&gt; &lt;li&gt;Some of the devices for file storage: hard drive, flash drive, cloud-based file storage&lt;/li&gt; &lt;li&gt;Files usually organized in directories &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What are the pros and cons of File Storage?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;Pros:&lt;/p&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Users have full control of their own files and can run variety of operations on the files: delete, read, write and move.&lt;/li&gt; &lt;li&gt;Security mechanism allows for users to have a better control at things such as file locking &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What are some examples of file storage?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Local filesystem Dropbox Google Drive &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What types of storage devices are there?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain IOPS&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain storage throughput&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is a filesystem?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;A file system is a way for computers and other electronic devices to organize and store data files. It provides a structure that helps to organize data into files and directories, making it easier to find and manage information. A file system is crucial for providing a way to store and manage data in an organized manner.&lt;/p&gt; &lt;p&gt;Commonly used filed systems: Windows:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;NTFS&lt;/li&gt; 
   &lt;li&gt;exFAT&lt;/li&gt; 
  &lt;/ul&gt; &lt;p&gt;Mac OS:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;HFS+ *APFS&lt;/li&gt; 
  &lt;/ul&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain Dark Data&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain MBR&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;p&gt;&lt;a name=&quot;questions-you-ask&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Questions you CAN ask&lt;/h2&gt; 
&lt;p&gt;A list of questions you as a candidate can ask the interviewer during or after the interview. These are only a suggestion, use them carefully. Not every interviewer will be able to answer these (or happy to) which should be perhaps a red flag warning for your regarding working in such place but that&#39;s really up to you.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What do you like about working here?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;How does the company promote personal growth?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is the current level of technical debt you are dealing with?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;Be careful when asking this question - all companies, regardless of size, have some level of tech debt. Phrase the question in the light that all companies have the deal with this, but you want to see the current pain points they are dealing with &lt;br&gt;&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;This is a great way to figure how managers deal with unplanned work, and how good they are at setting expectations with projects. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Why I should NOT join you? (or &#39;what you don&#39;t like about working here?&#39;)&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What was your favorite project you&#39;ve worked on?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;This can give you insights in some of the cool projects a company is working on, and if you would enjoy working on projects like these. This is also a good way to see if the managers are allowing employees to learn and grow with projects outside of the normal work you&#39;d do. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;If you could change one thing about your day to day, what would it be?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;Similar to the tech debt question, this helps you identify any pain points with the company. Additionally, it can be a great way to show how you&#39;d be an asset to the team.&lt;br&gt;&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;For Example, if they mention they have problem X, and you&#39;ve solved that in the past, you can show how you&#39;d be able to mitigate that problem. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Let&#39;s say that we agree and you hire me to this position, after X months, what do you expect that I have achieved?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Not only this will tell you what is expected from you, it will also provide big hint on the type of work you are going to do in the first months of your job. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;h2&gt;Testing&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain white-box testing&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain black-box testing&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What are unit tests?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;Unit test are a software testing technique that involves systimatically breaking down a system and testing each individual part of the assembly. These tests are automated and can be run repeatedly to allow developers to catch edge case scenarios or bugs quickly while developing.&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;The main objective of unit tests are to verify each function is producing proper outputs given a set of inputs. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What types of tests would you run to test a web application?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain test harness?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is A/B testing?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is network simulation and how do you perform it?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What types of performances tests are you familiar with?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain the following types of tests: 
  &lt;ul&gt; 
   &lt;li&gt;Load Testing&lt;/li&gt; 
   &lt;li&gt;Stress Testing&lt;/li&gt; 
   &lt;li&gt;Capacity Testing&lt;/li&gt; 
   &lt;li&gt;Volume Testing&lt;/li&gt; 
   &lt;li&gt;Endurance Testing&lt;/li&gt; 
  &lt;/ul&gt; &lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;h2&gt;Regex&lt;/h2&gt; 
&lt;p&gt;Given a text file, perform the following exercises&lt;/p&gt; 
&lt;h4&gt;Extract&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;Extract all the numbers&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;&quot;\d+&quot; &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;Extract the first word of each line&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt; &lt;p&gt;&quot;^\w+&quot; Bonus: extract the last word of each line&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&quot;\w+(?=\W*$)&quot; (in most cases, depends on line formatting) &lt;/p&gt;&lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt;   
&lt;details&gt; 
 &lt;summary&gt;Extract all the IP addresses&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;&quot;\b(?:\d{1,3}\ .){3}\d{1,3}\b&quot; IPV4:(This format looks for 1 to 3 digit sequence 3 times) &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;Extract dates in the format of yyyy-mm-dd or yyyy-dd-mm&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Extract email addresses&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;&quot;\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\ .[A-Za-z]{2,}\b&quot; &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;h4&gt;Replace&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;Replace tabs with four spaces&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Replace &#39;red&#39; with &#39;green&#39;&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;h2&gt;System Design&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain what a &quot;single point of failure&quot; is. &lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; A &quot;single point of failure&quot;, in a system or organization, if it were to fail would cause the entire system to fail or significantly disrupt it&#39;s operation. In other words, it is a vulnerability where there is no backup in place to compensate for the failure. &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is CDN?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;CDN (Content Delivery Network) responsible for distributing content geographically. Part of it, is what is known as edge locations, aka cache proxies, that allows users to get their content quickly due to cache features and geographical distribution. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain Multi-CDN&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;In single CDN, the whole content is originated from content delivery network.&lt;br&gt; In multi-CDN, content is distributed across multiple different CDNs, each might be on a completely different provider/cloud. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What are the benefits of Multi-CDN over a single CDN?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Resiliency: Relying on one CDN means no redundancy. With multiple CDNs you don&#39;t need to worry about your CDN being down&lt;/li&gt; &lt;li&gt;Flexibility in Costs: Using one CDN enforces you to specific rates of that CDN. With multiple CDNs you can take into consideration using less expensive CDNs to deliver the content.&lt;/li&gt; &lt;li&gt;Performance: With Multi-CDN there is bigger potential in choosing better locations which more close to the client asking the content&lt;/li&gt; &lt;li&gt;Scale: With multiple CDNs, you can scale services to support more extreme conditions &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;Explain &quot;3-Tier Architecture&quot; (including pros and cons)&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; A &quot;3-Tier Architecture&quot; is a pattern used in software development for designing and structuring applications. It divides the application into 3 interconnected layers: Presentation, Business logic and Data storage. PROS: * Scalability * Security * Reusability CONS: * Complexity * Performance overhead * Cost and development time &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain Mono-repo vs. Multi-repo.What are the cons and pros of each approach?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; In a Mono-repo, all the code for an organization is stored in a single,centralized repository. PROS (Mono-repo): * Unified tooling * Code Sharing CONS (Mono-repo): * Increased complexity * Slower cloning &lt;p&gt;In a Multi-repo setup, each component is stored in it&#39;s own separate repository. Each repository has it&#39;s own version control history. PROS (Multi-repo):&lt;/p&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Simpler to manage&lt;/li&gt; &lt;li&gt;Different teams and developers can work on different parts of the project independently, making parallel development easier. CONS (Multi-repo):&lt;/li&gt; &lt;li&gt;Code duplication&lt;/li&gt; &lt;li&gt;Integration challenges &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What are the drawbacks of monolithic architecture?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Not suitable for frequent code changes and the ability to deploy new features&lt;/li&gt; &lt;li&gt;Not designed for today&#39;s infrastructure (like public clouds)&lt;/li&gt; &lt;li&gt;Scaling a team to work monolithic architecture is more challenging&lt;/li&gt; &lt;li&gt;If a single component in this architecture fails, then the entire application fails. &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What are the advantages of microservices architecture over a monolithic architecture?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Each of the services individually fail without escalating into an application-wide outage.&lt;/li&gt; &lt;li&gt;Each service can be developed and maintained by a separate team and this team can choose its own tools and coding language &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What&#39;s a service mesh?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; It is a layer that facilitates communication management and control between microservices in a containerized application. It handles tasks such as load balancing, encryption, and monitoring. &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain &quot;Loose Coupling&quot;&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; In &quot;Loose Coupling&quot;, components of a system communicate with each other with a little understanding of each other&#39;s internal workings. This improves scalability and ease of modification in complex systems. &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is a message queue? When is it used?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; It is a communication mechanism used in distributed systems to enable asynchronous communication between different components. It is generally used when the systems use a microservices approach. &lt;/b&gt;
&lt;/details&gt; 
&lt;h4&gt;Scalability&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain Scalability&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;The ability easily grow in size and capacity based on demand and usage. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain Elasticity&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;The ability to grow but also to reduce based on what is required &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain Disaster Recovery&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Disaster recovery is the process of restoring critical business systems and data after a disruptive event. The goal is to minimize the impact and resume normal business activities quickly. This involves creating a plan, testing it, backing up critical data, and storing it in safe locations. In case of a disaster, the plan is then executed, backups are restored, and systems are hopefully brought back online. The recovery process may take hours or days depending on the damages of infrastructure. This makes business planning important, as a well-designed and tested disaster recovery plan can minimize the impact of a disaster and keep operations going. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain Fault Tolerance and High Availability&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;Fault Tolerance - The ability to self-heal and return to normal capacity. Also the ability to withstand a failure and remain functional.&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;High Availability - Being able to access a resource (in some use cases, using different platforms) &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is the difference between high availability and Disaster Recovery?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;a href=&quot;https://www.wintellect.com/high-availability-vs-disaster-recovery&quot;&gt;wintellect.com&lt;/a&gt;: &quot;High availability, simply put, is eliminating single points of failure and disaster recovery is the process of getting a system back to an operational state when a system is rendered inoperative. In essence, disaster recovery picks up when high availability fails, so HA first.&quot; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain Vertical Scaling&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Vertical Scaling is the process of adding resources to increase power of existing servers. For example, adding more CPUs, adding more RAM, etc. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What are the disadvantages of Vertical Scaling?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;With vertical scaling alone, the component still remains a single point of failure. In addition, it has hardware limit where if you don&#39;t have more resources, you might not be able to scale vertically. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Which type of cloud services usually support vertical scaling?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Databases, cache. It&#39;s common mostly for non-distributed systems. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain Horizontal Scaling&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Horizontal Scaling is the process of adding more resources that will be able handle requests as one unit &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is the disadvantage of Horizontal Scaling? What is often required in order to perform Horizontal Scaling?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;A load balancer. You can add more resources, but if you would like them to be part of the process, you have to serve them the requests/responses. Also, data inconsistency is a concern with horizontal scaling. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain in which use cases will you use vertical scaling and in which use cases you will use horizontal scaling&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain Resiliency and what ways are there to make a system more resilient&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain &quot;Consistent Hashing&quot;&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;How would you update each of the services in the following drawing without having app (foo.com) downtime?&lt;br&gt; &lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/design/cdn-no-downtime.png&quot; width=&quot;300x;&quot; height=&quot;400px;&quot;&gt; &lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is the problem with the following architecture and how would you fix it?&lt;br&gt; &lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/design/producers_consumers_issue.png&quot; width=&quot;400x;&quot; height=&quot;300px;&quot;&gt; &lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;The load on the producers or consumers may be high which will then cause them to hang or crash.&lt;br&gt; Instead of working in &quot;push mode&quot;, the consumers can pull tasks only when they are ready to handle them. It can be fixed by using a streaming platform like Kafka, Kinesis, etc. This platform will make sure to handle the high load/traffic and pass tasks/messages to consumers only when the ready to get them.&lt;/p&gt; &lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/design/producers_consumers_fix.png&quot; width=&quot;300x;&quot; height=&quot;200px;&quot;&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Users report that there is huge spike in process time when adding little bit more data to process as an input. What might be the problem?&lt;br&gt; &lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/design/input-process-output.png&quot; width=&quot;300x;&quot; height=&quot;200px;&quot;&gt; &lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;How would you scale the architecture from the previous question to hundreds of users?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;h4&gt;Cache&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is &quot;cache&quot;? In which cases would you use it?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is &quot;distributed cache&quot;?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is a &quot;cache replacement policy&quot;?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Take a look &lt;a href=&quot;https://en.wikipedia.org/wiki/Cache_replacement_policies&quot;&gt;here&lt;/a&gt; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Which cache replacement policies are you familiar with?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;You can find a list &lt;a href=&quot;https://en.wikipedia.org/wiki/Cache_replacement_policies&quot;&gt;here&lt;/a&gt; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain the following cache policies: 
  &lt;ul&gt; 
   &lt;li&gt;FIFO&lt;/li&gt; 
   &lt;li&gt;LIFO&lt;/li&gt; 
   &lt;li&gt;LRU&lt;/li&gt;
  &lt;/ul&gt;&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt;  &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Read about it &lt;a href=&quot;https://en.wikipedia.org/wiki/Cache_replacement_policies&quot;&gt;here&lt;/a&gt; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Why not writing everything to cache instead of a database/datastore?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; Caching and databases serve different purposes and are optimized for different use cases. &lt;p&gt;Caching is used to speed up read operations by storing frequently accessed data in memory or on a fast storage medium. By keeping data close to the application, caching reduces the latency and overhead of accessing data from a slower, more distant storage system such as a database or disk.&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;On the other hand, databases are optimized for storing and managing persistent data. Databases are designed to handle concurrent read and write operations, enforce consistency and integrity constraints, and provide features such as indexing and querying. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;h4&gt;Migrations&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;How you prepare for a migration? (or plan a migration)&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;You can mention:&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;roll-back &amp;amp; roll-forward cut over dress rehearsals DNS redirection &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain &quot;Branch by Abstraction&quot; technique&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;h4&gt;Design a system&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;Can you design a video streaming website?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Can you design a photo upload website?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;How would you build a URL shortener?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;h4&gt;More System Design Questions&lt;/h4&gt; 
&lt;p&gt;Additional exercises can be found in &lt;a href=&quot;https://github.com/bregman-arie/system-design-notebook&quot;&gt;system-design-notebook repository&lt;/a&gt;.&lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/bregman-arie/system-design-notebook&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/system_design_notebook.png&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Hardware&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is a CPU?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;A central processing unit (CPU) performs basic arithmetic, logic, controlling, and input/output (I/O) operations specified by the instructions in the program. This contrasts with external components such as main memory and I/O circuitry, and specialized processors such as graphics processing units (GPUs). &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is RAM?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;RAM (Random Access Memory) is the hardware in a computing device where the operating system (OS), application programs and data in current use are kept so they can be quickly reached by the device&#39;s processor. RAM is the main memory in a computer. It is much faster to read from and write to than other kinds of storage, such as a hard disk drive (HDD), solid-state drive (SSD) or optical drive. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is a GPU?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; A GPU, or Graphics Processing Unit, is a specialized electronic circuit designed to expedite image and video processing for display on a computer screen. &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is an embedded system?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;An embedded system is a computer system - a combination of a computer processor, computer memory, and input/output peripheral devices—that has a dedicated function within a larger mechanical or electronic system. It is embedded as part of a complete device often including electrical or electronic hardware and mechanical parts. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Can you give an example of an embedded system?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;A common example of an embedded system is a microwave oven&#39;s digital control panel, which is managed by a microcontroller.&lt;/p&gt; &lt;p&gt;When committed to a certain goal, Raspberry Pi can serve as an embedded system.&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What types of storage are there?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;There are several types of storage, including hard disk drives (HDDs), solid-state drives (SSDs), and optical drives (CD/DVD/Blu-ray). Other types of storage include USB flash drives, memory cards, and network-attached storage (NAS). &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What are some considerations DevOps teams should keep in mind when selecting hardware for their job?&lt;/summary&gt;
 &lt;br&gt; 
 &lt;p&gt;Choosing the right DevOps hardware is essential for ensuring streamlined CI/CD pipelines, timely feedback loops, and consistent service availability. Here&#39;s a distilled guide on what DevOps teams should consider:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Understanding Workloads&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;CPU&lt;/strong&gt;: Consider the need for multi-core or high-frequency CPUs based on your tasks.&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;RAM&lt;/strong&gt;: Enough memory is vital for activities like large-scale coding or intensive automation.&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Storage&lt;/strong&gt;: Evaluate storage speed and capacity. SSDs might be preferable for swift operations.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Expandability&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Horizontal Growth&lt;/strong&gt;: Check if you can boost capacity by adding more devices.&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Vertical Growth&lt;/strong&gt;: Determine if upgrades (like RAM, CPU) to individual machines are feasible.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Connectivity Considerations&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Data Transfer&lt;/strong&gt;: Ensure high-speed network connections for activities like code retrieval and data transfers.&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Speed&lt;/strong&gt;: Aim for low-latency networks, particularly important for distributed tasks.&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Backup Routes&lt;/strong&gt;: Think about having backup network routes to avoid downtimes.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Consistent Uptime&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Plan for hardware backups like RAID configurations, backup power sources, or alternate network connections to ensure continuous service.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;System Compatibility&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Make sure your hardware aligns with your software, operating system, and intended platforms.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Power Efficiency&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Hardware that uses energy efficiently can reduce costs in long-term, especially in large setups.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Safety Measures&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Explore hardware-level security features, such as TPM, to enhance protection.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Overseeing &amp;amp; Control&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Tools like ILOM can be beneficial for remote handling.&lt;/li&gt; 
    &lt;li&gt;Make sure the hardware can be seamlessly monitored for health and performance.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Budgeting&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Consider both initial expenses and long-term costs when budgeting.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Support &amp;amp; Community&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Choose hardware from reputable vendors known for reliable support.&lt;/li&gt; 
    &lt;li&gt;Check for available drivers, updates, and community discussions around the hardware.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Planning Ahead&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Opt for hardware that can cater to both present and upcoming requirements.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Operational Environment&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Temperature Control&lt;/strong&gt;: Ensure cooling systems to manage heat from high-performance units.&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Space Management&lt;/strong&gt;: Assess hardware size considering available rack space.&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Reliable Power&lt;/strong&gt;: Factor in consistent and backup power sources.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Cloud Coordination&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;If you&#39;re leaning towards a hybrid cloud setup, focus on how local hardware will mesh with cloud resources.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Life Span of Hardware&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Be aware of the hardware&#39;s expected duration and when you might need replacements or upgrades.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Optimized for Virtualization&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;If utilizing virtual machines or containers, ensure the hardware is compatible and optimized for such workloads.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Adaptability&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Modular hardware allows individual component replacements, offering more flexibility.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Avoiding Single Vendor Dependency&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Try to prevent reliance on a single vendor unless there are clear advantages.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Eco-Friendly Choices&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Prioritize sustainably produced hardware that&#39;s energy-efficient and environmentally responsible.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;In essence, DevOps teams should choose hardware that is compatible with their tasks, versatile, gives good performance, and stays within their budget. Furthermore, long-term considerations such as maintenance, potential upgrades, and compatibility with impending technological shifts must be prioritized.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is the role of hardware in disaster recovery planning and implementation?&lt;/summary&gt;
 &lt;br&gt; 
 &lt;p&gt;Hardware is critical in disaster recovery (DR) solutions. While the broader scope of DR includes things like standard procedures, norms, and human roles, it&#39;s the hardware that keeps business processes running smoothly. Here&#39;s an outline of how hardware works with DR:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Storing Data and Ensuring Its Duplication&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Backup Equipment&lt;/strong&gt;: Devices like tape storage, backup servers, and external HDDs keep essential data stored safely at a different location.&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Disk Arrays&lt;/strong&gt;: Systems such as RAID offer a safety net. If one disk crashes, the others compensate.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Alternate Systems for Recovery&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Backup Servers&lt;/strong&gt;: These step in when the main servers falter, maintaining service flow.&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Traffic Distributors&lt;/strong&gt;: Devices like load balancers share traffic across servers. If a server crashes, they reroute users to operational ones.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Alternate Operation Hubs&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Ready-to-use Centers&lt;/strong&gt;: Locations equipped and primed to take charge immediately when the main center fails.&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Basic Facilities&lt;/strong&gt;: Locations with necessary equipment but lacking recent data, taking longer to activate.&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Semi-prepped Facilities&lt;/strong&gt;: Locations somewhat prepared with select systems and data, taking a moderate duration to activate.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Power Backup Mechanisms&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Instant Power Backup&lt;/strong&gt;: Devices like UPS offer power during brief outages, ensuring no abrupt shutdowns.&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Long-term Power Solutions&lt;/strong&gt;: Generators keep vital systems operational during extended power losses.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Networking Equipment&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Backup Internet Connections&lt;/strong&gt;: Having alternatives ensures connectivity even if one provider faces issues.&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Secure Connection Tools&lt;/strong&gt;: Devices ensuring safe remote access, especially crucial during DR situations.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;On-site Physical Setup&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Organized Housing&lt;/strong&gt;: Structures like racks to neatly store and manage hardware.&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Emergency Temperature Control&lt;/strong&gt;: Backup cooling mechanisms to counter server overheating in HVAC malfunctions.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Alternate Communication Channels&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Orbit-based Phones&lt;/strong&gt;: Handy when regular communication methods falter.&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Direct Communication Devices&lt;/strong&gt;: Devices like radios useful when primary systems are down.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Protection Mechanisms&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Electronic Barriers &amp;amp; Alert Systems&lt;/strong&gt;: Devices like firewalls and intrusion detection keep DR systems safeguarded.&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Physical Entry Control&lt;/strong&gt;: Systems controlling entry and monitoring, ensuring only cleared personnel have access.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Uniformity and Compatibility in Hardware&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;It&#39;s simpler to manage and replace equipment in emergencies if hardware configurations are consistent and compatible.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Equipment for Trials and Upkeep&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;DR drills might use specific equipment to ensure the primary systems remain unaffected. This verifies the equipment&#39;s readiness and capacity to manage real crises.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;In summary, while software and human interventions are important in disaster recovery operations, it is the hardware that provides the underlying support. It is critical for efficient disaster recovery plans to keep this hardware resilient, duplicated, and routinely assessed.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is a RAID?&lt;/summary&gt;
 &lt;br&gt; 
 &lt;b&gt; RAID is an acronym that stands for &quot;Redundant Array of Independent Disks.&quot; It is a technique that combines numerous hard drives into a single device known as an array in order to improve performance, expand storage capacity, and/or offer redundancy to prevent data loss. RAID levels (for example, RAID 0, RAID 1, and RAID 5) provide varied benefits in terms of performance, redundancy, and storage efficiency. &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is a microcontroller?&lt;/summary&gt;
 &lt;br&gt; 
 &lt;b&gt; A microcontroller is a small integrated circuit that controls certain tasks in an embedded system. It typically includes a CPU, memory, and input/output peripherals. &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is a Network Interface Controller or NIC?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; A Network Interface Controller (NIC) is a piece of hardware that connects a computer to a network and allows it to communicate with other devices. &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is a DMA?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Direct memory access (DMA) is a feature of computer systems that allows certain hardware subsystems to access main system memory independently of the central processing unit (CPU).DMA enables devices to share and receive data from the main memory in a computer. It does this while still allowing the CPU to perform other tasks. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is a Real-Time Operating Systems?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;A real-time operating system (RTOS) is an operating system (OS) for real-time computing applications that processes data and events that have critically defined time constraints. An RTOS is distinct from a time-sharing operating system, such as Unix, which manages the sharing of system resources with a scheduler, data buffers, or fixed task prioritization in a multitasking or multiprogramming environment. Processing time requirements need to be fully understood and bound rather than just kept as a minimum. All processing must occur within the defined constraints. Real-time operating systems are event-driven and preemptive, meaning the OS can monitor the relevant priority of competing tasks, and make changes to the task priority. Event-driven systems switch between tasks based on their priorities, while time-sharing systems switch the task based on clock interrupts. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;List of interrupt types&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;There are six classes of interrupts possible:&lt;/p&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;External&lt;/li&gt; &lt;li&gt;Machine check&lt;/li&gt; &lt;li&gt;I/O&lt;/li&gt; &lt;li&gt;Program&lt;/li&gt; &lt;li&gt;Restart&lt;/li&gt; &lt;li&gt;Supervisor call (SVC) &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;h2&gt;Big Data&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain what is exactly Big Data&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;As defined by Doug Laney:&lt;/p&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Volume: Extremely large volumes of data&lt;/li&gt; &lt;li&gt;Velocity: Real time, batch, streams of data&lt;/li&gt; &lt;li&gt;Variety: Various forms of data, structured, semi-structured and unstructured&lt;/li&gt; &lt;li&gt;Veracity or Variability: Inconsistent, sometimes inaccurate, varying data &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What is DataOps? How is it related to DevOps?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;DataOps seeks to reduce the end-to-end cycle time of data analytics, from the origin of ideas to the literal creation of charts, graphs and models that create value. DataOps combines Agile development, DevOps and statistical process controls and applies them to data analytics. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is Data Architecture?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;An answer from &lt;a href=&quot;https://www.talend.com/resources/what-is-data-architecture&quot;&gt;talend.com&lt;/a&gt;:&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&quot;Data architecture is the process of standardizing how organizations collect, store, transform, distribute, and use data. The goal is to deliver relevant data to people who need it, when they need it, and help them make sense of it.&quot; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain the different formats of data&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Structured - data that has defined format and length (e.g. numbers, words)&lt;/li&gt; &lt;li&gt;Semi-structured - Doesn&#39;t conform to a specific format but is self-describing (e.g. XML, SWIFT)&lt;/li&gt; &lt;li&gt;Unstructured - does not follow a specific format (e.g. images, test messages) &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What is a Data Warehouse?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Data_warehouse&quot;&gt;Wikipedia&#39;s explanation on Data Warehouse&lt;/a&gt; &lt;a href=&quot;https://aws.amazon.com/data-warehouse&quot;&gt;Amazon&#39;s explanation on Data Warehouse&lt;/a&gt; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is Data Lake?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Data_lake&quot;&gt;Data Lake - Wikipedia&lt;/a&gt; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Can you explain the difference between a data lake and a data warehouse?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is &quot;Data Versioning&quot;? What models of &quot;Data Versioning&quot; are there?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is ETL?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;h4&gt;Apache Hadoop&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain what is Hadoop&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Apache_Hadoop&quot;&gt;Apache Hadoop - Wikipedia&lt;/a&gt; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain Hadoop YARN&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Responsible for managing the compute resources in clusters and scheduling users&#39; applications &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain Hadoop MapReduce&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;A programming model for large-scale data processing &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain Hadoop Distributed File Systems (HDFS)&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Distributed file system providing high aggregate bandwidth across the cluster.&lt;/li&gt; &lt;li&gt;For a user it looks like a regular file system structure but behind the scenes it&#39;s distributed across multiple machines in a cluster&lt;/li&gt; &lt;li&gt;Typical file size is TB and it can scale and supports millions of files&lt;/li&gt; &lt;li&gt;It&#39;s fault tolerant which means it provides automatic recovery from faults&lt;/li&gt; &lt;li&gt;It&#39;s best suited for running long batch operations rather than live analysis &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What do you know about HDFS architecture?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;&lt;a href=&quot;http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html&quot;&gt;HDFS Architecture&lt;/a&gt;&lt;/p&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Master-slave architecture&lt;/li&gt; &lt;li&gt;Namenode - master, Datanodes - slaves&lt;/li&gt; &lt;li&gt;Files split into blocks&lt;/li&gt; &lt;li&gt;Blocks stored on datanodes&lt;/li&gt; &lt;li&gt;Namenode controls all metadata &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;h2&gt;Ceph&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain what is Ceph&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; Ceph is an Open-Source Distributed Storage System designed to provide excellent performance, reliability, and scalability. It&#39;s often used in cloud computing environments and Data Centers. &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;True or False? Ceph favor consistency and correctness over performances&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; True &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Which services or types of storage Ceph supports?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Object (RGW)&lt;/li&gt; &lt;li&gt;Block (RBD)&lt;/li&gt; &lt;li&gt;File (CephFS) &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What is RADOS?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Reliable Autonomic Distributed Object Storage&lt;/li&gt; &lt;li&gt;Provides low-level data object storage service&lt;/li&gt; &lt;li&gt;Strong Consistency&lt;/li&gt; &lt;li&gt;Simplifies design and implementation of higher layers (block, file, object) &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;Describe RADOS software components&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Monitor 
    &lt;ul&gt; 
     &lt;li&gt;Central authority for authentication, data placement, policy&lt;/li&gt; 
     &lt;li&gt;Coordination point for all other cluster components&lt;/li&gt; 
     &lt;li&gt;Protect critical cluster state with Paxos&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;Manager 
    &lt;ul&gt; 
     &lt;li&gt;Aggregates real-time metrics (throughput, disk usage, etc.)&lt;/li&gt; 
     &lt;li&gt;Host for pluggable management functions&lt;/li&gt; 
     &lt;li&gt;1 active, 1+ standby per cluster&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;OSD (Object Storage Daemon) 
    &lt;ul&gt; 
     &lt;li&gt;Stores data on an HDD or SSD&lt;/li&gt; 
     &lt;li&gt;Services client IO requests &lt;/li&gt;
    &lt;/ul&gt;&lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;    
&lt;details&gt; 
 &lt;summary&gt;What is the workflow of retrieving data from Ceph?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; The work flow is as follows: 
  &lt;ol&gt; 
   &lt;li&gt;The client sends a request to the ceph cluster to retrieve data:&lt;/li&gt; 
  &lt;/ol&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;&lt;strong&gt;Client could be any of the following&lt;/strong&gt;&lt;/p&gt; 
   &lt;blockquote&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Ceph Block Device&lt;/li&gt; 
     &lt;li&gt;Ceph Object Gateway&lt;/li&gt; 
     &lt;li&gt;Any third party ceph client&lt;/li&gt; 
    &lt;/ul&gt; 
   &lt;/blockquote&gt; 
  &lt;/blockquote&gt; 
  &lt;ol start=&quot;2&quot;&gt; 
   &lt;li&gt;The client retrieves the latest cluster map from the Ceph Monitor&lt;/li&gt; 
   &lt;li&gt;The client uses the CRUSH algorithm to map the object to a placement group. The placement group is then assigned to a OSD.&lt;/li&gt; 
   &lt;li&gt;Once the placement group and the OSD Daemon are determined, the client can retrieve the data from the appropriate OSD&lt;/li&gt; 
  &lt;/ol&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is the workflow of writing data to Ceph?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; The work flow is as follows: 
  &lt;ol&gt; 
   &lt;li&gt;The client sends a request to the ceph cluster to retrieve data&lt;/li&gt; 
   &lt;li&gt;The client retrieves the latest cluster map from the Ceph Monitor&lt;/li&gt; 
   &lt;li&gt;The client uses the CRUSH algorithm to map the object to a placement group. The placement group is then assigned to a Ceph OSD Daemon dynamically.&lt;/li&gt; 
   &lt;li&gt;The client sends the data to the primary OSD of the determined placement group. If the data is stored in an erasure-coded pool, the primary OSD is responsible for encoding the object into data chunks and coding chunks, and distributing them to the other OSDs.&lt;/li&gt; 
  &lt;/ol&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What are &quot;Placement Groups&quot;?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Describe in the detail the following: Objects -&amp;gt; Pool -&amp;gt; Placement Groups -&amp;gt; OSDs&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is OMAP?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is a metadata server? How it works?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;h2&gt;Packer&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is Packer? What is it used for?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;In general, Packer automates machine images creation. It allows you to focus on configuration prior to deployment while making the images. This allows you start the instances much faster in most cases. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Packer follows a &quot;configuration-&amp;gt;deployment&quot; model or &quot;deployment-&amp;gt;configuration&quot;?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;A configuration-&amp;gt;deployment which has some advantages like:&lt;/p&gt; &lt;/b&gt;
 &lt;ol&gt;
  &lt;b&gt; &lt;li&gt;Deployment Speed - you configure once prior to deployment instead of configuring every time you deploy. This allows you to start instances/services much quicker.&lt;/li&gt; &lt;li&gt;More immutable infrastructure - with configuration-&amp;gt;deployment it&#39;s not likely to have very different deployments since most of the configuration is done prior to the deployment. Issues like dependencies errors are handled/discovered prior to deployment in this model. &lt;/li&gt;&lt;/b&gt;
 &lt;/ol&gt;
&lt;/details&gt;  
&lt;h2&gt;Release&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain Semantic Versioning&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;&lt;a href=&quot;https://semver.org/&quot;&gt;This&lt;/a&gt; page explains it perfectly:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Given a version number MAJOR.MINOR.PATCH, increment the:

MAJOR version when you make incompatible API changes
MINOR version when you add functionality in a backwards compatible manner
PATCH version when you make backwards compatible bug fixes
Additional labels for pre-release and build metadata are available as extensions to the MAJOR.MINOR.PATCH format.
&lt;/code&gt;&lt;/pre&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;h2&gt;Certificates&lt;/h2&gt; 
&lt;p&gt;If you are looking for a way to prepare for a certain exam this is the section for you. Here you&#39;ll find a list of certificates, each references to a separate file with focused questions that will help you to prepare to the exam. Good luck :)&lt;/p&gt; 
&lt;h4&gt;AWS&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/certificates/aws-cloud-practitioner.md&quot;&gt;Cloud Practitioner&lt;/a&gt; (Latest update: 2020)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/certificates/aws-solutions-architect-associate.md&quot;&gt;Solutions Architect Associate&lt;/a&gt; (Latest update: 2021)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/certificates/aws-cloud-sysops-associate.md&quot;&gt;Cloud SysOps Administration Associate&lt;/a&gt; (Latest update: Oct 2022)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Azure&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/certificates/azure-fundamentals-az-900.md&quot;&gt;AZ-900&lt;/a&gt; (Latest update: 2021)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Kubernetes&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/kubernetes/CKA.md&quot;&gt;Certified Kubernetes Administrator (CKA)&lt;/a&gt; (Latest update: 2022)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Additional DevOps and SRE Projects&lt;/h2&gt; 
&lt;p align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/bregman-arie/sre-checklist&quot;&gt;&lt;img width=&quot;500px&quot; src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/sre_checklist.png&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/bregman-arie/howtheydevops&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/how_they_devops.png&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/bregman-arie/devops-resources&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/devops_resources.png&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/bregman-arie/infraverse&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/infraverse.png&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Credits&lt;/h2&gt; 
&lt;p&gt;Thanks to all of our amazing &lt;a href=&quot;https://github.com/bregman-arie/devops-exercises/graphs/contributors&quot;&gt;contributors&lt;/a&gt; who make it easy for everyone to learn new things :)&lt;/p&gt; 
&lt;p&gt;Logos credits can be found &lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/credits.md&quot;&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://creativecommons.org/licenses/by-nc-nd/3.0/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/License-CC%20BY--NC--ND%203.0-lightgrey.svg?sanitize=true&quot; alt=&quot;License: CC BY-NC-ND 3.0&quot;&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>geekan/MetaGPT</title>
      <link>https://github.com/geekan/MetaGPT</link>
      <description>&lt;p&gt;🌟 The Multi-Agent Framework: First AI Software Company, Towards Natural Language Programming&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MetaGPT: The Multi-Agent Framework&lt;/h1&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;a href=&quot;&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/geekan/MetaGPT/main/docs/resources/MetaGPT-new-log.png&quot; alt=&quot;MetaGPT logo: Enable GPT to work in a software company, collaborating to tackle more complex tasks.&quot; width=&quot;150px&quot;&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt; [ &lt;b&gt;En&lt;/b&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/geekan/MetaGPT/main/docs/README_CN.md&quot;&gt;中&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/geekan/MetaGPT/main/docs/README_FR.md&quot;&gt;Fr&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/geekan/MetaGPT/main/docs/README_JA.md&quot;&gt;日&lt;/a&gt; ] &lt;b&gt;Assign different roles to GPTs to form a collaborative entity for complex tasks.&lt;/b&gt; &lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;a href=&quot;https://opensource.org/licenses/MIT&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/License-MIT-blue.svg?sanitize=true&quot; alt=&quot;License: MIT&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://discord.gg/DYn29wFk9z&quot;&gt;&lt;img src=&quot;https://dcbadge.vercel.app/api/server/DYn29wFk9z?style=flat&quot; alt=&quot;Discord Follow&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://twitter.com/MetaGPT_&quot;&gt;&lt;img src=&quot;https://img.shields.io/twitter/follow/MetaGPT?style=social&quot; alt=&quot;Twitter Follow&quot;&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;h4 align=&quot;center&quot;&gt; &lt;/h4&gt; 
&lt;h2&gt;News&lt;/h2&gt; 
&lt;p&gt;🚀 Mar. 10, 2025: 🎉 &lt;a href=&quot;https://mgx.dev/&quot;&gt;mgx.dev&lt;/a&gt; is the #1 Product of the Week on @ProductHunt! 🏆&lt;/p&gt; 
&lt;p&gt;🚀 Mar. &amp;nbsp; 4, 2025: 🎉 &lt;a href=&quot;https://mgx.dev/&quot;&gt;mgx.dev&lt;/a&gt; is the #1 Product of the Day on @ProductHunt! 🏆&lt;/p&gt; 
&lt;p&gt;🚀 Feb. 19, 2025: Today we are officially launching our natural language programming product: &lt;a href=&quot;https://mgx.dev/&quot;&gt;MGX (MetaGPT X)&lt;/a&gt; - the world&#39;s first AI agent development team. More details on &lt;a href=&quot;https://x.com/MetaGPT_/status/1892199535130329356&quot;&gt;Twitter&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;🚀 Feb. 17, 2025: We introduced two papers: &lt;a href=&quot;https://arxiv.org/pdf/2502.06855&quot;&gt;SPO&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/pdf/2502.12018&quot;&gt;AOT&lt;/a&gt;, check the &lt;a href=&quot;https://raw.githubusercontent.com/geekan/MetaGPT/main/examples&quot;&gt;code&lt;/a&gt;!&lt;/p&gt; 
&lt;p&gt;🚀 Jan. 22, 2025: Our paper &lt;a href=&quot;https://openreview.net/forum?id=z5uVAKwmjf&quot;&gt;AFlow: Automating Agentic Workflow Generation&lt;/a&gt; accepted for &lt;strong&gt;oral presentation (top 1.8%)&lt;/strong&gt; at ICLR 2025, &lt;strong&gt;ranking #2&lt;/strong&gt; in the LLM-based Agent category.&lt;/p&gt; 
&lt;p&gt;👉👉 &lt;a href=&quot;https://raw.githubusercontent.com/geekan/MetaGPT/main/docs/NEWS.md&quot;&gt;Earlier news&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Software Company as Multi-Agent System&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;MetaGPT takes a &lt;strong&gt;one line requirement&lt;/strong&gt; as input and outputs &lt;strong&gt;user stories / competitive analysis / requirements / data structures / APIs / documents, etc.&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Internally, MetaGPT includes &lt;strong&gt;product managers / architects / project managers / engineers.&lt;/strong&gt; It provides the entire process of a &lt;strong&gt;software company along with carefully orchestrated SOPs.&lt;/strong&gt; 
  &lt;ol&gt; 
   &lt;li&gt;&lt;code&gt;Code = SOP(Team)&lt;/code&gt; is the core philosophy. We materialize SOP and apply it to teams composed of LLMs.&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/geekan/MetaGPT/main/docs/resources/software_company_cd.jpeg&quot; alt=&quot;A software company consists of LLM-based roles&quot;&gt;&lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt;Software Company Multi-Agent Schematic (Gradually Implementing)&lt;/p&gt; 
&lt;h2&gt;Get Started&lt;/h2&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Ensure that Python 3.9 or later, but less than 3.12, is installed on your system. You can check this by using: &lt;code&gt;python --version&lt;/code&gt;.&lt;br&gt; You can use conda like this: &lt;code&gt;conda create -n metagpt python=3.9 &amp;amp;&amp;amp; conda activate metagpt&lt;/code&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install --upgrade metagpt
# or `pip install --upgrade git+https://github.com/geekan/MetaGPT.git`
# or `git clone https://github.com/geekan/MetaGPT &amp;amp;&amp;amp; cd MetaGPT &amp;amp;&amp;amp; pip install --upgrade -e .`
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Install &lt;a href=&quot;https://nodejs.org/en/download&quot;&gt;node&lt;/a&gt; and &lt;a href=&quot;https://pnpm.io/installation#using-npm&quot;&gt;pnpm&lt;/a&gt; before actual use.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;For detailed installation guidance, please refer to &lt;a href=&quot;https://docs.deepwisdom.ai/main/en/guide/get_started/installation.html#install-stable-version&quot;&gt;cli_install&lt;/a&gt; or &lt;a href=&quot;https://docs.deepwisdom.ai/main/en/guide/get_started/installation.html#install-with-docker&quot;&gt;docker_install&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Configuration&lt;/h3&gt; 
&lt;p&gt;You can init the config of MetaGPT by running the following command, or manually create &lt;code&gt;~/.metagpt/config2.yaml&lt;/code&gt; file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# Check https://docs.deepwisdom.ai/main/en/guide/get_started/configuration.html for more details
metagpt --init-config  # it will create ~/.metagpt/config2.yaml, just modify it to your needs
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can configure &lt;code&gt;~/.metagpt/config2.yaml&lt;/code&gt; according to the &lt;a href=&quot;https://github.com/geekan/MetaGPT/raw/main/config/config2.example.yaml&quot;&gt;example&lt;/a&gt; and &lt;a href=&quot;https://docs.deepwisdom.ai/main/en/guide/get_started/configuration.html&quot;&gt;doc&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-yaml&quot;&gt;llm:
  api_type: &quot;openai&quot;  # or azure / ollama / groq etc. Check LLMType for more options
  model: &quot;gpt-4-turbo&quot;  # or gpt-3.5-turbo
  base_url: &quot;https://api.openai.com/v1&quot;  # or forward url / other llm url
  api_key: &quot;YOUR_API_KEY&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Usage&lt;/h3&gt; 
&lt;p&gt;After installation, you can use MetaGPT at CLI&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;metagpt &quot;Create a 2048 game&quot;  # this will create a repo in ./workspace
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or use it as library&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from metagpt.software_company import generate_repo
from metagpt.utils.project_repo import ProjectRepo

repo: ProjectRepo = generate_repo(&quot;Create a 2048 game&quot;)  # or ProjectRepo(&quot;&amp;lt;path&amp;gt;&quot;)
print(repo)  # it will print the repo structure with files
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also use &lt;a href=&quot;https://github.com/geekan/MetaGPT/tree/main/examples/di&quot;&gt;Data Interpreter&lt;/a&gt; to write code:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import asyncio
from metagpt.roles.di.data_interpreter import DataInterpreter

async def main():
    di = DataInterpreter()
    await di.run(&quot;Run data analysis on sklearn Iris dataset, include a plot&quot;)

asyncio.run(main())  # or await main() in a jupyter notebook setting
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;QuickStart &amp;amp; Demo Video&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Try it on &lt;a href=&quot;https://huggingface.co/spaces/deepwisdom/MetaGPT-SoftwareCompany&quot;&gt;MetaGPT Huggingface Space&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://youtu.be/uT75J_KG_aY&quot;&gt;Matthew Berman: How To Install MetaGPT - Build A Startup With One Prompt!!&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/geekan/MetaGPT/assets/2707039/5e8c1062-8c35-440f-bb20-2b0320f8d27d&quot;&gt;Official Demo Video&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/user-attachments/assets/888cb169-78c3-4a42-9d62-9d90ed3928c9&quot;&gt;https://github.com/user-attachments/assets/888cb169-78c3-4a42-9d62-9d90ed3928c9&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Tutorial&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;🗒 &lt;a href=&quot;https://docs.deepwisdom.ai/main/en/&quot;&gt;Online Document&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;💻 &lt;a href=&quot;https://docs.deepwisdom.ai/main/en/guide/get_started/quickstart.html&quot;&gt;Usage&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;🔎 &lt;a href=&quot;https://docs.deepwisdom.ai/main/en/guide/get_started/introduction.html&quot;&gt;What can MetaGPT do?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;🛠 How to build your own agents? 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://docs.deepwisdom.ai/main/en/guide/tutorials/agent_101.html&quot;&gt;MetaGPT Usage &amp;amp; Development Guide | Agent 101&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://docs.deepwisdom.ai/main/en/guide/tutorials/multi_agent_101.html&quot;&gt;MetaGPT Usage &amp;amp; Development Guide | MultiAgent 101&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;🧑‍💻 Contribution 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/geekan/MetaGPT/main/docs/ROADMAP.md&quot;&gt;Develop Roadmap&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;🔖 Use Cases 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://docs.deepwisdom.ai/main/en/guide/use_cases/agent/interpreter/intro.html&quot;&gt;Data Interpreter&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://docs.deepwisdom.ai/main/en/guide/use_cases/multi_agent/debate.html&quot;&gt;Debate&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://docs.deepwisdom.ai/main/en/guide/use_cases/agent/researcher.html&quot;&gt;Researcher&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://docs.deepwisdom.ai/main/en/guide/use_cases/agent/receipt_assistant.html&quot;&gt;Receipt Assistant&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;❓ &lt;a href=&quot;https://docs.deepwisdom.ai/main/en/guide/faq.html&quot;&gt;FAQs&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Support&lt;/h2&gt; 
&lt;h3&gt;Discord Join US&lt;/h3&gt; 
&lt;p&gt;📢 Join Our &lt;a href=&quot;https://discord.gg/ZRHeExS6xv&quot;&gt;Discord Channel&lt;/a&gt;! Looking forward to seeing you there! 🎉&lt;/p&gt; 
&lt;h3&gt;Contributor form&lt;/h3&gt; 
&lt;p&gt;📝 &lt;a href=&quot;https://airtable.com/appInfdG0eJ9J4NNL/pagK3Fh1sGclBvVkV/form&quot;&gt;Fill out the form&lt;/a&gt; to become a contributor. We are looking forward to your participation!&lt;/p&gt; 
&lt;h3&gt;Contact Information&lt;/h3&gt; 
&lt;p&gt;If you have any questions or feedback about this project, please feel free to contact us. We highly appreciate your suggestions!&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Email:&lt;/strong&gt; &lt;a href=&quot;mailto:alexanderwu@deepwisdom.ai&quot;&gt;alexanderwu@deepwisdom.ai&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;GitHub Issues:&lt;/strong&gt; For more technical inquiries, you can also create a new issue in our &lt;a href=&quot;https://github.com/geekan/metagpt/issues&quot;&gt;GitHub repository&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We will respond to all questions within 2-3 business days.&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;To stay updated with the latest research and development, follow &lt;a href=&quot;https://twitter.com/MetaGPT_&quot;&gt;@MetaGPT_&lt;/a&gt; on Twitter.&lt;/p&gt; 
&lt;p&gt;To cite &lt;a href=&quot;https://openreview.net/forum?id=VtmBAGCN7o&quot;&gt;MetaGPT&lt;/a&gt; in publications, please use the following BibTeX entries.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bibtex&quot;&gt;@inproceedings{hong2024metagpt,
      title={Meta{GPT}: Meta Programming for A Multi-Agent Collaborative Framework},
      author={Sirui Hong and Mingchen Zhuge and Jonathan Chen and Xiawu Zheng and Yuheng Cheng and Jinlin Wang and Ceyao Zhang and Zili Wang and Steven Ka Shing Yau and Zijuan Lin and Liyang Zhou and Chenyu Ran and Lingfeng Xiao and Chenglin Wu and J{\&quot;u}rgen Schmidhuber},
      booktitle={The Twelfth International Conference on Learning Representations},
      year={2024},
      url={https://openreview.net/forum?id=VtmBAGCN7o}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more work, please refer to &lt;a href=&quot;https://raw.githubusercontent.com/geekan/MetaGPT/main/docs/ACADEMIC_WORK.md&quot;&gt;Academic Work&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
